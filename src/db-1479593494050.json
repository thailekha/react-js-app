{
  "libraries": [
    {
      "id": 1,
      "email": "abc@yahoo.sample.com",
      "name": "Sample library",
      "public": true,
      "paradigms": [
        {
          "pdid": 3,
          "name": "Array-oriented",
          "details": "In computer science, array programming languages (also known as vector or multidimensional languages) generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.\n\nArray programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon to find array programming language one-liners that require more than a couple of pages of Java code.[1]\n\nModern programming languages that support array programming are commonly used in scientific and engineering settings; these include Fortran 90, Mata, MATLAB, Analytica, TK Solver (as lists), Octave, R, Cilk Plus, Julia, and the NumPy extension to Python. In these languages, an operation that operates on entire arrays can be called a vectorized operation,[2] regardless of whether it is executed on a vector processor or not.\nContents  [hide] \n1\tConcepts\n2\tUses\n3\tLanguages\n3.1\tScalar languages\n3.2\tArray languages\n3.2.1\tAda\n3.2.2\tAnalytica\n3.2.3\tBASIC\n3.2.4\tMata\n3.2.5\tMATLAB\n3.2.6\trasql\n3.2.7\tR\n4\tMathematical reasoning and language notation\n5\tThird-party libraries\n6\tSee also\n7\tReferences\n8\tExternal links\nConcepts[edit]\nThe fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\n\nIverson described the rationale behind array programming (actually referring to APL) as follows:[3]\n\nmost programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician. [...]\n\nThe thesis [...] is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. [...] it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\n[...] Users of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted, since a clear statement of an algorithm can usually be used as a basis from which one may easily derive more efficient algorithm.\n\nThe basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\n\nFunction rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.\n\nUses[edit]\nArray programming is very well suited to implicit parallelization; a topic of much research nowadays. Further, Intel and compatible CPUs developed and produced after 1997 contained various instruction set extensions, starting from MMX and continuing through SSSE3 and 3DNow!, which include rudimentary SIMD array capabilities. Array processing is distinct from parallel processing in that one physical processor performs operations on a group of items simultaneously while parallel processing aims to split a larger problem into smaller ones (MIMD) to be solved piecemeal by numerous processors. Processors with two or more cores are increasingly common today.\n\nLanguages[edit]\nThe canonical examples of array programming languages are APL, J, and Fortran. Others include: D, A+, Analytica, Chapel, IDL, Julia, K, Q, Mata, Mathematica, MATLAB, MOLSF, NumPy, GNU Octave, PDL, R, S-Lang, SAC, Nial and ZPL.\n\nScalar languages[edit]\nIn scalar languages such as C and Pascal, operations apply only to single values, so a+b expresses the addition of two numbers. In such languages, adding one array to another requires indexing and looping, the coding of which is tedious and error-prone[citation needed].\n\nfor (i = 0; i < n; i++)\n    for (j = 0; j < n; j++)\n        a[i][j] += b[i][j];\nArray languages[edit]\nIn array languages, operations are generalized to apply to both scalars and arrays. Thus, a+b expresses the sum of two scalars if a and b are scalars, or the sum of two arrays if they are arrays.\n\nAn array language simplifies programming but possibly at a cost known as the abstraction penalty.[4][5][6] Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).\n\nAda[edit]\nThe previous C code would become the following in the Ada language,[7] which supports array-programming syntax.\n\n A := A + B;\nAnalytica[edit]\nAnalytica provides the same economy of expression as Ada.\n\n A := A + B;\nThis operation works whether operands, A or B, are scalar or arrays with one more dimensions. Each dimension is identified by an index variable, which controls the nature of the operation. The result has the union of the dimensions of the operands. If A and B have the same dimensions (indexes), the result has those same dimensions. If A and B are vectors with different dimensions, the resulting A is 2-dimensional, containing both dimensions, with each element the sum of the corresponding values of A and B. Variable A must be a local variable; Analytica, as a declarative language, avoids side effects by disallowing assignment to global variables.\n\nBASIC[edit]\nDartmouth BASIC had MAT statements for matrix and array manipulation in its third edition (1966).\n\n DIM A(4),B(4),C(4)\n MAT A = 1\n MAT B = 2*A\n MAT C = A + B\n MAT PRINT A,B,C\nMata[edit]\nStata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.\n\n. mata:\n\n: A = (1,2,3) \\(4,5,6)\n\n: A\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n: B = (2..4) \\(1..3)\n\n: B\n       1   2   3\n    +-------------+\n  1 |  2   3   4  |\n  2 |  1   2   3  |\n    +-------------+\n\n: C = J(3,2,1)           // A 3 by 2 matrix of ones\n\n: C\n       1   2\n    +---------+\n  1 |  1   1  |\n  2 |  1   1  |\n  3 |  1   1  |\n    +---------+\n\n: D = A + B\n\n: D\n       1   2   3\n    +-------------+\n  1 |  3   5   7  |\n  2 |  5   7   9  |\n    +-------------+\n\n: E = A*C\n\n: E\n        1    2\n    +-----------+\n  1 |   6    6  |\n  2 |  15   15  |\n    +-----------+\n\n: F = A:*B\n\n: F\n        1    2    3\n    +----------------+\n  1 |   2    6   12  |\n  2 |   4   10   18  |\n    +----------------+\n\n: G = E :+ 3\n\n: G\n        1    2\n    +-----------+\n  1 |   9    9  |\n  2 |  18   18  |\n    +-----------+\n\n: H = F[(2\\1), (1, 2)]    // Subscripting to get a submatrix of F and\n\n:                         // switch row 1 and 2\n: H\n        1    2\n    +-----------+\n  1 |   4   10  |\n  2 |   2    6  |\n    +-----------+\n\n: I = invsym(F'*F)        // Generalized inverse (F*F^(-1)F=F) of a\n\n:                         // symmetric positive semi-definite matrix\n: I\n[symmetric]\n                 1             2             3\n    +-------------------------------------------+\n  1 |            0                              |\n  2 |            0          3.25                |\n  3 |            0         -1.75   .9444444444  |\n    +-------------------------------------------+\n\n: end\nMATLAB[edit]\nThe implementation in MATLAB allows the same economy allowed by using the Ada language.\n\nA = A + B;\nA variant of the MATLAB language is the GNU Octave language, which extends the original language with augmented assignments:\n\nA += B;\nBoth MATLAB and GNU Octave natively support linear algebra operations such as matrix multiplication, matrix inversion, and the numerical solution of system of linear equations, even using the Moore–Penrose pseudoinverse.[8][9]\n\nThe Nial example of the inner product of two arrays can be implemented using the native matrix multiplication operator. If a is a row vector of size [1 n] and b is a corresponding column vector of size [n 1].\n\na * b;\nThe inner product between two matrices having the same number of elements can be implemented with the auxiliary operator (:), which reshapes a given matrix into a column vector, and the transpose operator ':\n\nA(:)' * B(:);\nrasql[edit]\nThe rasdaman query language is a database-oriented array-programming language. For example, two arrays could be added with the following query:\n\nSELECT A + B\nFROM   A, B\nR[edit]\nThe R language supports array paradigm by default. The following example illustrates a process of multiplication of two matrices followed by an addition of a scalar (which is, in fact, a one-element vector) and a vector:\n\n> A <- matrix(1:6, nrow=2)                              !!this has nrow=2 ... and A has 2 rows\n> A\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n> B <- t( matrix(6:1, nrow=2) )  # t() is a transpose operator                           !!this has nrow=2 ... and B has 3 rows --- a clear contradiction to the definition of A\n> B\n     [,1] [,2]\n[1,]    6    5\n[2,]    4    3\n[3,]    2    1\n> C <- A %*% B\n> C\n     [,1] [,2]\n[1,]   28   19\n[2,]   40   28\n> D <- C + 1\n> D\n     [,1] [,2]\n[1,]   29   20\n[2,]   41   29\n> D + c(1, 1)  # c() creates a vector\n     [,1] [,2]\n[1,]   30   21\n[2,]   42   30\nMathematical reasoning and language notation[edit]\nThe matrix left-division operator concisely expresses some semantic properties of matrices. As in the scalar equivalent, if the (determinant of the) coefficient (matrix) A is not null then it is possible to solve the (vectorial) equation A * x = b by left-multiplying both sides by the inverse of A: A−1 (in both MATLAB and GNU Octave languages: A^-1). The following mathematical statements hold when A is a full rank square matrix:\n\nA^-1 *(A * x)==A^-1 * (b)\n(A^-1 * A)* x ==A^-1 * b       (matrix-multiplication associativity)\nx = A^-1 * b\nwhere == is the equivalence relational operator. The previous statements are also valid MATLAB expressions if the third one is executed before the others (numerical comparisons may be false because of round-off errors).\n\nIf the system is overdetermined - so that A has more rows than columns - the pseudoinverse A+ (in MATLAB and GNU Octave languages: pinv(A)) can replace the inverse A−1, as follows:\n\npinv(A) *(A * x)==pinv(A) * (b)\n(pinv(A) * A)* x ==pinv(A) * b       (matrix-multiplication associativity)\nx = pinv(A) * b\nHowever, these solutions are neither the most concise ones (e.g. still remains the need to notationally differentiate overdetermined systems) nor the most computationally efficient. The latter point is easy to understand when considering again the scalar equivalent a * x = b, for which the solution x = a^-1 * b would require two operations instead of the more efficient x = b / a. The problem is that generally matrix multiplications are not commutative as the extension of the scalar solution to the matrix case would require:\n\n(a * x)/ a ==b / a\n(x * a)/ a ==b / a       (commutativity does not hold for matrices!)\nx * (a / a)==b / a       (associativity also holds for matrices)\nx = b / a\nThe MATLAB language introduces the left-division operator \\ to maintain the essential part of the analogy with the scalar case, therefore simplifying the mathematical reasoning and preserving the conciseness:\n\nA \\ (A * x)==A \\ b\n(A \\ A)* x ==A \\ b       (associativity also holds for matrices, commutativity is no more required)\nx = A \\ b\nThis is not only an example of terse array programming from the coding point of view but also from the computational efficiency perspective, which in several array programming languages benefits from quite efficient linear algebra libraries such as ATLAS or LAPACK.[10][11]\n\nReturning to the previous quotation of Iverson, the rationale behind it should now be evident:\n\nit is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\nThird-party libraries[edit]\nThe use of specialized and efficient libraries to provide more terse abstractions is also common in other programming languages. In C++ several linear algebra libraries exploit the language ability to overload operators. In some cases a very terse abstraction in those languages is explicitly influenced by the array programming paradigm, as the Armadillo and Blitz++ libraries do.[12][13]",
          "subparadigms": []
        },
        {
          "pdid": 6,
          "name": "Concurrent",
          "details": "Concurrent computing is a form of computing in which several computations are executed during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or \"thread of control\" for each computation (\"process\"). A concurrent system is one where a computation can advance without waiting for all other computations to complete; where more than one computation can advance at the same time.[1]\n\nAs a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.\n\nContents  [hide] \n1\tIntroduction\n1.1\tCoordinating access to shared resources\n1.2\tAdvantages\n2\tModels\n3\tImplementation\n3.1\tInteraction and communication\n4\tHistory\n5\tPrevalence\n6\tLanguages supporting it\n7\tSee also\n8\tNotes\n9\tReferences\n10\tFurther reading\n11\tExternal links\nIntroduction[edit]\nSee also: Parallel computing\nConcurrent computing is related to but distinct from parallel computing, though these concepts are frequently confused,[2][3] and both can be described as \"multiple processes executing during the same period of time\". In parallel computing, execution occurs at the same physical instant, for example on separate processors of a multi-processor machine, with the goal of speeding up computations—parallel computing is impossible on a (one-core) single processor, as only one computation can occur at any instant (during any single clock cycle).[a] By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.[4]:1\n\nFor example, concurrent processes can be executed on one core by interleaving the execution steps of each process via time-sharing slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.\n\nConcurrent computations may be executed in parallel,[2][5] for example by assigning each process to a separate processor or processor core, or distributing a computation across a network, but in general, the languages, tools and techniques for parallel programming may not be suitable for concurrent programming, and vice versa.\n\nThe exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:\n\nT1 may be executed and finished before T2 or vice versa (serial and sequential);\nT1 and T2 may be executed alternately (serial and concurrent);\nT1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent).\nThe word \"sequential\" is used as an antonym for both \"concurrent\" and \"parallel\"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs.[6] A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called a serial schedule. A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control.\n\nCoordinating access to shared resources[edit]\nThe main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.[5] Potential problems include race conditions, deadlocks, and resource starvation. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource balance:\n\n1 bool withdraw(int withdrawal)\n2 {\n3 \n    if (balance >= withdrawal)\n4     {\n5 \n        balance -= withdrawal;\n6         return true;\n7     } \n8     return false;\n9 }\nSuppose balance = 500, and two concurrent threads make the calls withdraw(300) and withdraw(350). If line 3 in both operations executes before line 5 both operations will find that balance >= withdrawal evaluates to true, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources need the use of concurrency control, or non-blocking algorithms.\n\nBecause concurrent systems rely on the use of shared resources (including communication media), concurrent computing in general needs the use of some form of arbiter somewhere in the implementation to mediate access to these resources.\n\nUnfortunately, while many solutions exist to the problem of a conflict over one resource, many of those \"solutions\" have their own concurrency problems such as deadlock when more than one resource is involved.\n\nAdvantages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)\nIncreased program throughput—parallel execution of a concurrent program allows the number of tasks completed in a given time to increase.\nHigh responsiveness for input/output—input/output-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.\nMore appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes.\nModels[edit]\nThere are several models of concurrent computing, which can be used to understand and analyze concurrent systems. These models include:\n\nActor model\nObject-capability model for security\nPetri nets\nProcess calculi such as\nAmbient calculus\nCalculus of communicating systems (CCS)\nCommunicating sequential processes (CSP)\nπ-calculus\nJoin-calculus\nInput/output automaton\nImplementation[edit]\n[icon]\tThis section needs expansion. You can help by adding to it. (February 2014)\nA number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process.\n\nInteraction and communication[edit]\nIn some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. Explicit communication can be divided into two classes:\n\nShared memory communication\nConcurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). This style of concurrent programming usually needs the use of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. A program that properly implements any of these is said to be thread-safe.\nMessage passing communication\nConcurrent components communicate by exchanging messages (exemplified by Scala, Erlang and occam). The exchange of messages may be carried out asynchronously, or may use a synchronous \"rendezvous\" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as \"send and pray\"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.[citation needed] A wide variety of mathematical theories to understand and analyze message-passing systems are available, including the actor model, and various process calculi. Message passing can be efficiently implemented via symmetric multiprocessing, with or without shared memory cache coherence.\nShared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors.\n\nHistory[edit]\nConcurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via time-division multiplexing (1870s).\n\nThe academic study of concurrent algorithms started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.[7]\n\nPrevalence[edit]\nConcurrency is pervasive in computing, occurring from low-level hardware on a single chip to world-wide networks. Examples follow.\n\nAt the programming language level:\n\nChannel\nCoroutine\nFutures and promises\nAt the operating system level:\n\nComputer multitasking, including both cooperative multitasking and preemptive multitasking\nTime-sharing, which replaced sequential batch processing of jobs with concurrent use of a system\nProcess\nThread\nAt the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.\n\nLanguages supporting it[edit]\nConcurrent programming languages are programming languages that use language constructs for concurrency. These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. Such languages are sometimes described as Concurrency Oriented Languages or Concurrency Oriented Programming Languages (COPL).[8]\n\nToday, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.[citation needed]\n\nMany concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include:\n\nAda—general purpose, with native support for message passing and monitor based concurrency\nAlef—concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs\nAlice—extension to Standard ML, adds support for concurrency via futures\nAteji PX—extension to Java with parallel primitives inspired from π-calculus\nAxum—domain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntax\nC++—std::thread\nCω (C omega)—for research, extends C#, uses asynchronous communication\nC#—supports concurrent computing since version 5.0 using lock, yield, async and await keywords\nClojure—modern Lisp for the JVM\nConcurrent Clean—functional programming, similar to Haskell\nConcurrent Collections (CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and control\nConcurrent Haskell—lazy, pure functional language operating concurrent processes on shared memory\nConcurrent ML—concurrent extension of Standard ML\nConcurrent Pascal—by Per Brinch Hansen\nCurry\nD—multi-paradigm system programming language with explicit support for concurrent programming (actor model)\nE—uses promises to preclude deadlocks\nECMAScript—promises available in various libraries, proposed for inclusion in standard in ECMAScript 6\nEiffel—through its SCOOP mechanism based on the concepts of Design by Contract\nElixir—dynamic and functional meta-programming aware language running on the Erlang VM.\nErlang—uses asynchronous message passing with nothing shared\nFAUST—real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler\nFortran—coarrays and do concurrent are part of Fortran 2008 standard\nGo—for system programming, with a concurrent programming model based on CSP\nHume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing\nIo—actor-based concurrency\nJanus—features distinct askers and tellers to logical variables, bag channels; is purely declarative\nJava—Thread class or Runnable interface.\nJavaScript—via web workers, in a browser environment, promises, and callbacks.\nJoCaml—concurrent and distributed channel based, extension of OCaml, implements the Join-calculus of processes\nJoin Java—concurrent, based on Java language\nJoule—dataflow-based, communicates by message passing\nJoyce—concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen\nLabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language\nLimbo—relative of Alef, for system programming in Inferno (operating system)\nMultiLisp—Scheme variant extended to support parallelism\nModula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutines\nModula-3—modern member of Algol family with extensive support for threads, mutexes, condition variables\nNewsqueak—for research, with channels as first-class values; predecessor of Alef\nNode.js—a server-side runtime environment for JavaScript\noccam—influenced heavily by communicating sequential processes (CSP)\noccam-π—a modern variant of occam, which incorporates ideas from Milner's π-calculus\nOrc—heavily concurrent, nondeterministic, based on Kleene algebra\nOz-Mozart—multiparadigm, supports shared-state and message-passing concurrency, and futures\nParaSail—object-oriented, parallel, free of pointers, race conditions\nPict—essentially an executable implementation of Milner's π-calculus\nPerl with AnyEvent and Coro\nPython with Twisted, greenlet and gevent\nReia—uses asynchronous message passing between shared-nothing objects\nRed/System—for system programming, based on Rebol\nRuby with Concurrent Ruby and Celluloid\nRust—for system programming, focus on massive concurrency, using message-passing with move semantics, shared immutable memory, and shared mutable memory that is provably free of race conditions.[9]\nSALSA—actor-based with token-passing, join, and first-class continuations for distributed computing over the Internet\nScala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way\nSequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions\nSR—for research\nStackless Python\nStratifiedJS—combinator-based concurrency, based on JavaScript\nSuperPascal—concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen\nUnicon—for research\nTermite Scheme—adds Erlang-like concurrency to Scheme\nTNSDL—for developing telecommunication exchanges, uses asynchronous message passing\nVHSIC Hardware Description Language (VHDL)—IEEE STD-1076\nXC—concurrency-extended subset of C language developed by XMOS, based on communicating sequential processes, built-in constructs for programmable I/O\nMany other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.",
          "subparadigms": []
        },
        {
          "pdid": 15,
          "name": "Purely functional programming",
          "details": "In computer science, purely functional programming usually designates a programming paradigm—a style of building the structure and elements of computer programs—that treats all computation as the evaluation of mathematical functions. Purely functional programing may also be defined by forbidding changing-state and mutable data.\n\nPurely functional programing consists in restricting programming to its functional paradigm.\n\nContents  [hide] \n1\tDifference between pure and not-pure functional programming\n2\tProperties of purely functional program\n2.1\tStrict versus non-strict evaluation\n2.2\tParallel computing\n2.3\tData structures\n3\tPurely functional language\n4\tReferences\nDifference between pure and not-pure functional programming[edit]\nThe exact difference between pure and impure functional programing is a matter of controversy.[1]\n\nA program is usually said to be functional when it uses some concepts of functional programming, such as first-class functions and higher-order functions.[2] However, a first-class function may use techniques from the imperative paradigm, such as arrays or input/output methods are not purely functional programs, therefore the first-class function needs not be purely functional. In fact, the earliest programming languages cited as being functional, IPL and Lisp,[3][4] were both \"impure\" functional languages by the current definition.\n\nPurely functional data structures are persistent. Persistency is required for functional programming; without it, the same computation could return different results. Functional programming may use persistent non-purely functional data structures, while those data structures may not be used in purely functional programs.\n\nProperties of purely functional program[edit]\nStrict versus non-strict evaluation[edit]\nMain article: Evaluation strategy\nAll evaluation strategy which ends on a purely functional programs returns the same result. In particular, it ensures that the programmer does not have to consider in which order programs are evaluated, since eager evaluation will return the same result than lazy evaluation. However, it is still possible that an eager evaluation may not terminate while the lazy evaluation of the same program halts.\n\nParallel computing[edit]\nPurely functional programing simplifies parallel computing[5] since two purely functional parts of the evaluation never interact.\n\nData structures[edit]\nMain article: Purely functional data structure\nPurely functional data structures are often represented in a different way than their imperative counterparts.[6] For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nPurely functional language[edit]\nMain article: List of programming languages by type § Pure\nA purely functional language is a language which only admits purely functional programming. Purely functional programs can however be written in languages which are not purely functional.",
          "subparadigms": []
        },
        {
          "pdid": 16,
          "name": "Functional",
          "details": "In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions[1] or declarations[2] instead of statements. In functional code, the output value of a function depends only on the arguments that are input to the function, so calling a function f twice with the same value for an argument x will produce the same result f(x) each time. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.\n\nFunctional programming has its roots in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, logic programming, is based on relations.[3]\n\nIn contrast, imperative programming changes state with commands in the source language, the most simple example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.[3]\n\nFunctional programming languages, especially purely functional ones such as Hope, have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme,[4][5][6][7] Clojure,[8][9] Wolfram Language[10] (also known as Mathematica), Racket,[11] Erlang,[12][13][14] OCaml,[15][16] Haskell,[17][18] and F#[19][20] have been used in industrial and commercial applications by a wide variety of organizations. Functional programming is also supported in some domain-specific programming languages like R (statistics),[21] J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML),[22][23] and Opal.[24] Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.[25]\n\nProgramming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts.[26] This is also true of the PHP programming language.[27] C++11, Java 8, and C# 3.0 all added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala[28] – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.\n\nContents  [hide] \n1\tHistory\n2\tConcepts\n2.1\tFirst-class and higher-order functions\n2.2\tPure functions\n2.3\tRecursion\n2.4\tStrict versus non-strict evaluation\n2.5\tType systems\n2.6\tReferential transparency\n2.7\tFunctional programming in non-functional languages\n2.8\tData structures\n3\tComparison to imperative programming\n3.1\tSimulating state\n3.2\tEfficiency issues\n3.3\tCoding styles\n3.3.1\tPython\n3.3.2\tHaskell\n3.3.3\tPerl 6\n3.3.4\tErlang\n3.3.5\tElixir\n3.3.6\tLisp\n3.3.7\tClojure\n3.3.8\tD\n3.3.9\tR\n3.3.10\tSequenceL\n4\tUse in industry\n5\tIn education\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory[edit]\nLambda calculus provides a theoretical framework for describing functions and their evaluation. Although it is a mathematical abstraction rather than a programming language, it forms the basis of almost all functional programming languages today. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.[29]\n\nAn early functional-flavored language was Lisp, developed in the late 1950s for the IBM 700/7000 series scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT).[30] Lisp introduced many features now found in functional languages, though Lisp is technically a multi-paradigm language. Scheme and Dylan were later attempts to simplify and improve Lisp.\n\nInformation Processing Language (IPL) is sometimes cited as the first computer-based functional programming language.[31] It is an assembly-style language for manipulating lists of symbols. It does have a notion of \"generator\", which amounts to a function accepting a function as an argument, and, since it is an assembly-level language, code can be used as data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.\n\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\n\nJohn Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\".[32] He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style which has come to be associated with functional programming.\n\nIn the 1970s, ML was created by Robin Milner at the University of Edinburgh, and David Turner initially developed the language SASL at the University of St Andrews and later the language Miranda at the University of Kent. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL.[33] NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation.[34] Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope.[35] ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML. Meanwhile, the development of Scheme (a partly functional dialect of Lisp), as described in the influential Lambda Papers and the 1985 textbook Structure and Interpretation of Computer Programs, brought awareness of the power of functional programming to the wider programming-languages community.\n\nIn the 1980s, Per Martin-Löf developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs of arbitrarily complex mathematical propositions expressed as dependent types. This led to powerful new approaches to interactive theorem proving and has influenced the development of many subsequent functional programming languages.\n\nThe Haskell language began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\n\nConcepts[edit]\nA number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages are often hybrids of several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.[36]\n\nFirst-class and higher-order functions[edit]\nMain articles: First-class function and Higher-order function\nHigher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator {\\displaystyle d/dx} d/dx, which returns the derivative of a function {\\displaystyle f} f.\n\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term that describes programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\n\nHigher-order functions enable partial application or currying, a technique in which a function is applied to its arguments one at a time, with each application returning a new function that accepts the next argument. This allows one to succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\n\nPure functions[edit]\nPure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\n\nIf the result of a pure expression is not used, it can be removed without affecting other expressions.\nIf a pure function is called with arguments that cause no side-effects, the result is constant with respect to that argument list (sometimes called referential transparency), i.e. if the pure function is again called with the same arguments, the same result will be returned (this can enable caching optimizations such as memoization).\nIf there is no data dependency between two pure expressions, then their order can be reversed, or they can be performed in parallel and they cannot interfere with one another (in other terms, the evaluation of any pure expression is thread-safe).\nIf the entire language does not allow side-effects, then any evaluation strategy can be used; this gives the compiler freedom to reorder or combine the evaluation of expressions in a program (for example, using deforestation).\nWhile most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also allows functions to be designated \"pure\".\n\nRecursion[edit]\nMain article: Recursion (computer science)\nIteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Though some recursion requires maintaining a stack, tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. The Scheme language standard requires implementations to recognize and optimize tail recursion. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\n\nCommon patterns of recursion can be factored out using higher order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples. Such higher order functions play a role analogous to built-in control structures such as loops in imperative languages.\n\nMost general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.[37]\n\nStrict versus non-strict evaluation[edit]\nMain article: Evaluation strategy\nFunctional languages can be categorized by whether they use strict (eager) or non-strict (lazy) evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm will itself fail. For example, the expression:\n\nprint length([2+1, 3*2, 1/0, 5-4])\nwill fail under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function will return the value 4 (i.e., the number of items in the list), since evaluating it will not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\n\nThe usual implementation strategy for lazy evaluation in functional languages is graph reduction.[38] Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.\n\nHughes 1984 argues for lazy evaluation as a mechanism for improving program modularity through separation of concerns, by easing independent implementation of producers and consumers of data streams.[39] Launchbury 1993 describes some difficulties that lazy evaluation introduces, particularly in analyzing a program's storage requirements, and proposes an operational semantics to aid in such analysis.[40] Harper 2009 proposes including both strict and lazy evaluation in the same language, using the language's type system to distinguish them.[41]\n\nType systems[edit]\nEspecially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), although they reject all invalid programs at runtime, when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.\n\nSome research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which allows types to depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with[citation needed]. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.[42]\n\nA limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience.[43] GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as \"case classes\"), and have been proposed as additions to other languages including Java and C#.[44]\n\nReferential transparency[edit]\nMain article: Referential transparency\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.[45]\n\nConsider C assignment statement x = x * 10, this changes the value assigned to the variable x. Let us say that the initial value of x was 1, then two consecutive evaluations of the variable x will yield 10 and 100 respectively. Clearly, replacing x = x * 10 with either 10 or 100 gives a program with different meaning, and so the expression is not referentially transparent. In fact, assignment statements are never referentially transparent.\n\nNow, consider another function such as int plusone(int x) {return x+1;} is transparent, as it will not implicitly change the input x and thus has no such side effects. Functional programs exclusively use this type of function and are therefore referentially transparent.\n\nFunctional programming in non-functional languages[edit]\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages.[46] For example, both D and Fortran 95 explicitly support pure functions.[47]\n\nJavaScript, Lua[48] and Python had first class functions from their inception.[49] Amrit Prem added support to Python for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2,[50] though Python 3 relegated \"reduce\" to the functools standard library module.[51] First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, and C++11.[citation needed]\n\nIn Java, anonymous classes can sometimes be used to simulate closures;[52] however, anonymous classes are not always proper replacements to closures because they have more limited capabilities.[53] Java 8 supports lambda expressions as a replacement for some anonymous classes.[54] However, the presence of checked exceptions in Java can make functional programming inconvenient, because it can be necessary to catch checked exceptions and then rethrow them—a problem that does not occur in other JVM languages that do not have checked exceptions, such as Scala.[citation needed]\n\nIn C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\n\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\n\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages,[55] for example the tuple in Python, which is an immutable array.\n\nData structures[edit]\nMain article: Purely functional data structure\nPurely functional data structures are often represented in a different way than their imperative counterparts.[56] For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nComparison to imperative programming[edit]\nFunctional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\n\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.\n\nSimulating state[edit]\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\n\nThe pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).[57]\n\nAnother way in which functional languages can simulate state is by passing around a data structure that represents the current state as a parameter to function calls. On each function call, a copy of this data structure is created with whatever differences are the result of the function. This is referred to as 'state-passing style'.\n\nImpure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.\n\nAlternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.\n\nEfficiency issues[edit]\nFunctional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal.[58] This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware (which is a highly evolved Turing machine). Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree).[59] However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C.[60] For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\n\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.[61]\n\nLazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993[40] discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan et al. 2008[62] give some practical advice for analyzing and fixing them. However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles)[citation needed].\n\nCoding styles[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2013) (Learn how and when to remove this template message)\nImperative programs have the environment and a sequence of steps manipulating the environment. Functional programs have an expression that is successively substituted until it reaches normal form. An example illustrates this with different solutions to the same programming goal (calculating Fibonacci numbers).\n\nPython[edit]\nPrinting first 10 Fibonacci numbers, iterative\n\ndef fibonacci(n, first=0, second=1):\n    while n != 0:\n        print(first, end=\"\\n\") # side-effect\n        n, first, second = n - 1, second, first + second # assignment\nfibonacci(10)\nPrinting first 10 Fibonacci numbers, functional expression style\n\nfibonacci = (lambda n, first=0, second=1:\n    \"\" if n == 0 else\n    str(first) + \"\\n\" + fibonacci(n - 1, second, first + second))\nprint(fibonacci(10), end=\"\")\nPrinting a list with first 10 Fibonacci numbers, with generators\n\ndef fibonacci(n, first=0, second=1):\n    while n != 0:\n        yield first\n        n, first, second = n - 1, second, first + second # assignment\nprint(list(fibonacci(10)))\nPrinting a list with first 10 Fibonacci numbers, functional expression style\n\nfibonacci = (lambda n, first=0, second=1:\n    [] if n == 0 else\n    [first] + fibonacci(n - 1, second, first + second))\nprint(fibonacci(10))\nHaskell[edit]\nPrinting first 10 fibonacci numbers, functional expression style[1]\n\nfibonacci_aux = \\n first second->\n    if n == 0 then \"\" else\n    show first ++ \"\\n\" ++ fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStr (fibonacci 10)\nPrinting a list with first 10 fibonacci numbers, functional expression style[1]\n\nfibonacci_aux = \\n first second->\n    if n == 0 then [] else\n    [first] ++ fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1]\n\nfibonacci = \\n-> if n == 0 then 0\n                 else if n == 1 then 1\n                      else fibonacci(n - 1) + fibonacci(n - 2)\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style,[1] tail recursive\n\nfibonacci_aux = \\n first second->\n    if n == 0 then first else\n    fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with recursive lists\n\nfibonacci_aux = \\first second-> first : fibonacci_aux second (first + second)\nselect = \\n zs-> if n==0 then head zs\n                 else select (n - 1) (tail zs)\nfibonacci = \\n-> select n (fibonacci_aux 0 1)\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with primitives for recursive lists\n\nfibonacci_aux = \\first second-> first : fibonacci_aux second (first + second)\nfibonacci = \\n-> (fibonacci_aux 0 1) !! n\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with primitives for recursive lists, more concisely\n\nfibonacci_aux = 0:1:zipWith (+) fibonacci_aux (tail fibonacci_aux)\nfibonacci = \\n-> fibonacci_aux !! n\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional declaration style,[2] tail recursive\n\nfibonacci_aux 0 first _ = first\nfibonacci_aux n first second = fibonacci_aux (n - 1) second (first + second)\nfibonacci n = fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional declaration style, using lazy infinite lists and primitives\n\nfibs = 1 : 1 : zipWith (+) fibs (tail fibs) \n-- an infinite list of the fibonacci numbers\n-- fibs is defined in terms of fibs\nfibonacci = (fibs !!)\nmain = putStrLn $ show $ fibonacci 11\nPerl 6[edit]\nAs influenced by Haskell and others, Perl 6 has several functional and declarative approaches to problems. For example, you can declaratively build up a well-typed recursive version (the type constraints are optional) through signature pattern matching:\n\nsubset NonNegativeInt of Int where * >= 0;\n\nproto fib (|) is cached returns NonNegativeInt {*}\nmulti fib (0) { 0 }\nmulti fib (1) { 1 }\nmulti fib (NonNegativeInt $n) { fib($n - 1) + fib($n - 2) }\n\nfor ^10 -> $n { say fib($n) }\nAn alternative to this is to construct a lazy iterative sequence, which appears as an almost direct illustration of the sequence:\n\nmy @fib = 0, 1, *+* ... *; # Each additional entry is the sum of the previous two\n                           # and this sequence extends lazily indefinitely\nsay @fib[^10];             # Display the first 10 entries\nErlang[edit]\nErlang is a functional, concurrent, general-purpose programming language. A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. Use other algorithms for fast performance[63]):\n\n-module(fib).    % This is the file 'fib.erl', the module and the filename must match\n-export([fib/1]). % This exports the function 'fib' of arity 1\n\nfib(1) -> 1; % If 1, then return 1, otherwise (note the semicolon ; meaning 'else')\nfib(2) -> 1; % If 2, then return 1, otherwise\nfib(N) -> fib(N - 2) + fib(N - 1).\nElixir[edit]\nElixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM).\n\nThe Fibonacci function can be written in Elixir as follows:\n\ndefmodule Fibonacci do\n   def fib(0), do: 0\n   def fib(1), do: 1\n   def fib(n), do: fib(n-1) + fib(n-2)\nend\nLisp[edit]\nThe Fibonacci function can be written in Common Lisp as follows:\n\n(defun fib (n &optional (a 0) (b 1))\n  (if (= n 0)\n      a\n      (fib (- n 1) b (+ a b))))\nThe program can then be called as\n\n(fib 10)\nClojure[edit]\nThe Fibonacci function can be written in Clojure as follows:\n\n(defn fib\n  [n]\n  (loop [a 0 b 1 i n]\n    (if (zero? i)\n      a\n      (recur b (+ a b) (dec i)))))\nThe program can then be called as\n\n(fib 7)\nD[edit]\nD has support for functional programming[clarification needed][citation needed]:\n\nimport std.stdio;\nimport std.range;\n\nvoid main()\n{\n    /* 'f' is a range representing the first 10 Fibonacci numbers */\n    auto f = recurrence!((seq, i) => seq[0] + seq[1])(0, 1)\n             .take(10);\n\n    writeln(f);\n}\nR[edit]\nR is an environment for statistical computing and graphics. It is also a functional programming language.\n\nThe Fibonacci function can be written in R as a recursive function as follows:\n\nfib <- function(n) {\n if (n <= 2) 1\n else fib(n - 1) + fib(n - 2)\n}\nOr it can be written as a singly recursive function:\n\nfib <- function(n,a=1,b=1) { \n if (n == 1) a \n else fib(n-1,b,a+b) \n}\nOr it can be written as an iterative function:\n\nfib <- function(n) {\n if (n == 1) 1\n else if (n == 2) 1\n else {\n  fib<-c(1,1)\n  for (i in 3:n) fib<-c(0,fib[1])+fib[2]\n  fib[2]\n }\n}\nThe function can then be called as\n\nfib(10)\nSequenceL[edit]\nSequenceL is a functional, concurrent, general-purpose programming language. The Fibonacci function can be written in SequenceL as follows:\n\nfib(n) := n when n < 2 else\n          fib(n - 1) + fib(n - 2);\nThe function can then be called as\n\nfib(10)\nTo reduce the memory consumed by the call stack when computing a large Fibonacci term, a tail-recursive version can be used. A tail-recursive function is implemented by the SequenceL compiler as a memory-efficient looping structure:\n\nfib(n) := fib_Helper(0, 1, n);\n\nfib_Helper(prev, next, n) :=\n    prev when n < 1 else\n    next when n = 1 else\n    fib_Helper(next, next + prev, n - 1);\nUse in industry[edit]\nFunctional programming has long been popular in academia, but with few industrial applications.[64]:page 11 However, recently several prominent functional programming languages have been used in commercial or industrial systems. For example, the Erlang programming language, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems.[13] It has since become popular for building a range of applications at companies such as T-Mobile, Nortel, Facebook, Électricité de France and WhatsApp.[12][14][65][66][67] The Scheme dialect of Lisp was used as the basis for several applications on early Apple Macintosh computers,[4][5] and has more recently been applied to problems such as training simulation software[6] and telescope control.[7] OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis,[15] driver verification, industrial robot programming, and static analysis of embedded software.[16] Haskell, although initially intended as a research language,[18] has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.[17][18]\n\nOther functional programming languages that have seen use in industry include Scala,[68] F#,[19][20] (both being functional-OO hybrids with support for both purely functional and imperative programming) Wolfram Language,[10] Lisp,[69] Standard ML[70][71] and Clojure[72]",
          "subparadigms": [
            15
          ]
        },
        {
          "pdid": 21,
          "name": "Logic programming",
          "details": "Logic programming is a programming paradigm based on formal logic. A program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n\nH :- B1, …, Bn.\nand are read declaratively as logical implications:\n\nH if B1 and … and Bn.\nH is called the head of the rule and B1, …, Bn is called the body. Facts are rules that have no body, and are written in the simplified form:\n\nH.\nIn the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.\n\nIn ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:\n\nto solve H, solve B1, and ... and solve Bn.\nConsider, for example, the following clause:\n\nfallible(X) :- human(X).\nbased on an example used by Terry Winograd [1] to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:\n\nhuman(socrates).\ncan be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by \"assigning\" socrates to X.\n\nThe declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.\n\nContents  [hide] \n1\tHistory\n2\tConcepts\n2.1\tLogic and control\n2.2\tProblem solving\n2.3\tNegation as failure\n2.4\tKnowledge representation\n3\tVariants and extensions\n3.1\tProlog\n3.2\tAbductive logic programming\n3.3\tMetalogic programming\n3.4\tConstraint logic programming\n3.5\tConcurrent logic programming\n3.6\tConcurrent constraint logic programming\n3.7\tInductive logic programming\n3.8\tHigher-order logic programming\n3.9\tLinear logic programming\n3.10\tObject-oriented logic programming\n3.11\tTransaction logic programming\n4\tSee also\n5\tReferences\n5.1\tGeneral introductions\n5.2\tOther sources\n6\tFurther reading\n7\tExternal links\nHistory[edit]\nThe use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.[2] This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.[3]\n\nLogic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.[citation needed]\n\nAlthough it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm.[4] Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time.[1] To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.[citation needed]\n\nHayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover.[5] Kowalski, on the other hand, developed SLD resolution,[6] a variant of SL-resolution,[7] and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.\n\nThe Association for Logic Programming was founded to promote Logic Programming in 1986.\n\nProlog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages,[8] constraint logic programming languages and datalog.[citation needed]\n\nConcepts[edit]\nLogic and control[edit]\nMain article: Declarative programming\nLogic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan\n\nAlgorithm = Logic + Control\nwhere \"Logic\" represents a logic program and \"Control\" represents different theorem-proving strategies.[9]\n\nProblem solving[edit]\nIn the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an \"and\". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an \"or\".\n\nAny search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.\n\nIn the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.\n\nNegation as failure[edit]\nMain article: Negation as failure\nFor most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:\n\nH :- A1, …, An, not B1, …, not Bn.\nand is read declaratively as a logical implication:\n\nH if A1 and … and An and not B1 and … and not Bn.\nwhere H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as \"negation as failure\", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:\n\ncanfly(X) :- bird(X), not abnormal(X).\nabnormal(X) :-  wounded(X).\nbird(john).\nbird(mary).\nwounded(john).\nGiven the goal of finding something that can fly:\n\n:- canfly(X).\nthere are two candidate solutions, which solve the first subgoal bird(X), namely X = john and X = mary. The second subgoal not abnormal(john) of the first candidate solution fails, because wounded(john) succeeds and therefore abnormal(john) succeeds. However, The second subgoal not abnormal(mary) of the second candidate solution succeeds, because wounded(mary) fails and therefore abnormal(mary) fails. Therefore, X = mary is the only solution of the goal.\n\nMicro-Planner had a construct, called \"thnot\", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as not(Goal) or \\+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \\+ X == 1 fails when the variable X has been bound to the atom 1, but it succeeds in all other cases, including when X is unbound. This makes Prolog's reasoning non-monotonic: X = 1, \\+ X == 1 always fails, while \\+ X == 1, X = 1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).\n\nThe logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say\n\nH :- Body1.\n…\nH :- Bodyk.\nas a definition of the predicate\n\nH iff (Body1 or … or Bodyk)\nwhere \"iff\" means \"if and only if\". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.\n\nFor example, the completion of the program above is:\n\ncanfly(X) iff bird(X), not abnormal(X).\nabnormal(X) iff wounded(X).\nbird(X) iff X = john or X = mary.\nX = X.\nnot john = mary.\nnot mary = john.\nThe notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.\n\nAs an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in \"extended logic programming\", to formalise such phrases as \"the contrary can not be shown\", where \"contrary\" is classical negation and \"can not be shown\" is the epistemic interpretation of negation as failure.\n\nKnowledge representation[edit]\nThe fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.\n\nDespite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor [10] credit the representation of the British Nationality Act as a logic program [11] with being \"hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences\".\n\nVariants and extensions[edit]\nProlog[edit]\nMain article: Prolog\nThe programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.\n\nIt was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation\n\nH :- B1, …, Bn.\nwhich can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.[6]\n\nColmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.\n\nAbductive logic programming[edit]\nAbductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be \"open\" or undefined. A clause in an abductive logic program has the form:\n\nH :- B1, …, Bn, A1, …, An.\nwhere H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:\n\nfalse :- B1, …, Bn.\nwhere the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:\n\ncanfly(X) :- bird(X), normal(X).\nfalse :-  normal(X), wounded(X).\nbird(john).\nbird(mary).\nwounded(john).\nwhere the predicate normal is abducible.\n\nProblem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:\n\n:- canfly(X).\nAbductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.\n\nMetalogic programming[edit]\nBecause mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called \"vanilla\" meta-interpreter:\n\n    solve(true).\n    solve((A,B)):- solve(A),solve(B).\n    solve(A):- clause(A,B),solve(B).\nwhere true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.\n\nMetalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.\n\nConstraint logic programming[edit]\nMain article: Constraint logic programming\nConstraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:\n\nH :- C1, …, Cn {\\displaystyle \\Diamond } \\Diamond  B1, …, Bn.\nwhere H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:\n\nH if C1 and … and Cn and B1 and … and Bn.\nHowever, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.\n\nProcedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.\n\nThe following constraint logic program represents a toy temporal database of john's history as a teacher:\n\nteaches(john, hardware, T) :- 1990 ≤ T, T < 1999.\nteaches(john, software, T) :- 1999 ≤ T, T < 2005.\nteaches(john, logic, T) :- 2005 ≤ T, T ≤ 2012.\nrank(john, instructor, T) :- 1990 ≤ T, T < 2010.\nrank(john, professor, T) :- 2010 ≤ T, T < 2014.\nHere ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:\n\n:- teaches(john, logic, T), rank(john, professor, T).\nThe solution is 2010 ≤ T, T ≤ 2012.\n\nConstraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.\n\nConcurrent logic programming[edit]\nMain article: Concurrent logic programming\nConcurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).[12]\n\nA concurrent logic program is a set of guarded Horn clauses of the form:\n\nH :- G1, …, Gn | B1, …, Bn.\nThe conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:\n\nH if G1 and … and Gn and B1 and … and Bn.\nHowever, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of \"don't care nondeterminism\", rather than \"don't know nondeterminism\".\n\nFor example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:\n\nshuffle([], [], []).\nshuffle(Left, Right, Merge) :-\n    Left = [First | Rest] |\n    Merge = [First | ShortMerge],\n    shuffle(Rest, Right, ShortMerge).\nshuffle(Left, Right, Merge) :-\n    Right = [First | Rest] |\n    Merge = [First | ShortMerge],\n    shuffle(Left, Rest, ShortMerge).\nHere, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:\n\nshuffle([ace, queen, king], [1, 4, 2], Merge).\nThe program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].\n\nArguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced.[13] However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.[neutrality is disputed]\n\nConcurrent constraint logic programming[edit]\nMain article: Concurrent constraint logic programming\nConcurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.\n\nInductive logic programming[edit]\nMain article: Inductive logic programming\nInductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.\n\nHigher-order logic programming[edit]\nSeveral researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.\n\nLinear logic programming[edit]\nBasing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli,[14] ACL,[15] and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.\n\nObject-oriented logic programming[edit]\nF-logic extends logic programming with objects and the frame syntax. A number of systems are based on F-logic, including Flora-2, FLORID, and a highly scalable commercial system Ontobroker.\n\nLogtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. Highly portable, it supports most standard-complaint Prolog systems as backend compilers.\n\nTransaction logic programming[edit]\nTransaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.\n\nSee also[edit]\nBoolean satisfiability problem\nConstraint logic programming\nDatalog\nFril\nFunctional programming\nFuzzy logic\nInductive logic programming\nLogic in computer science (includes Formal methods)\nLogic programming languages\nProgramming paradigm\nR++\nReasoning system\nRule-based machine learning\nSatisfiability",
          "subparadigms": [
            16
          ]
        },
        {
          "pdid": 22,
          "name": "Declarative programming",
          "details": "In computer science, declarative programming is a programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.[1]\n\nMany languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives[2] (the how being left up to the language's implementation). This is in contrast with imperative programming, which implements algorithms in explicit steps.\n\nDeclarative programming often considers programs as theories of a formal logic, and computations as deductions in that logic space. Declarative programming may greatly simplify writing parallel programs.[3]\n\nCommon declarative languages include those of database query languages (e.g., SQL, XQuery), regular expressions, logic programming, functional programming, and configuration management systems.\n\nContents  [hide] \n1\tDefinition\n2\tSubparadigms\n2.1\tConstraint programming\n2.2\tDomain-specific languages\n2.3\tFunctional programming\n2.4\tHybrid languages\n2.5\tLogic programming\n2.6\tModeling\n3\tSee also\n4\tReferences\n5\tExternal links\nDefinition[edit]\nDeclarative programming is often defined as any style of programming that is not imperative. A number of other common definitions exist that attempt to give the term a definition other than simply contrasting it with imperative programming. For example:\n\nA program that describes what computation should be performed and not how to compute it\nAny programming language that lacks side effects (or more specifically, is referentially transparent)\nA language with a clear correspondence to mathematical logic.[4]\nThese definitions overlap substantially.\n\nDeclarative programming contrasts with imperative and procedural programming. Declarative programming is a non-imperative style of programming in which programs describe their desired results without explicitly listing commands or steps that must be performed. Functional and logical programming languages are characterized by a declarative programming style. In logical programming languages, programs consist of logical statements, and the program executes by searching for proofs of the statements.\n\nIn a pure functional language, such as Haskell, all functions are without side effects, and state changes are only represented as functions that transform the state, which is explicitly represented as a first class object in the program. Although pure functional languages are non-imperative, they often provide a facility for describing the effect of a function as a series of steps. Other functional languages, such as Lisp, OCaml and Erlang, support a mixture of procedural and functional programming.\n\nSome logical programming languages, such as Prolog, and database query languages, such as SQL, while declarative in principle, also support a procedural style of programming.\n\nSubparadigms[edit]\nDeclarative programming is an umbrella term that includes a number of better-known programming paradigms.\n\nConstraint programming[edit]\nConstraint programming states relations between variables in the form of constraints that specify the properties of the target solution. The set of constraints is solved by giving a value to each variable so that the solution is consistent with the maximum number of constraints. Constraint programming often complements other paradigms: functional, logical, or even imperative programming.\n\nDomain-specific languages[edit]\nWell-known examples of declarative domain-specific languages (DSLs) include the yacc parser generator input language, the Make build specification language, Puppet's configuration management language, regular expressions, and a subset of SQL (SELECT queries, for example). DSLs have the advantage of being useful while not necessarily needing to be Turing-complete, which makes it easier for a language to be purely declarative.\n\nMany markup languages such as HTML, MXML, XAML, XSLT or other user-interface markup languages are often declarative. HTML, for example, only describes what should appear on a webpage - it specifies neither control flow rendering a page nor its possible interactions with a user.\n\nAs of 2013 some software systems combine traditional user-interface markup languages (such as HTML) with declarative markup that defines what (but not how) the back-end server systems should do to support the declared interface. Such systems, typically using a domain-specific XML namespace, may include abstractions of SQL database syntax or parameterised calls to web services using representational state transfer (REST) and SOAP.\n\nFunctional programming[edit]\nFunctional programming, and in particular purely functional programming, attempts to minimize or eliminate side effects, and is therefore considered declarative. Most[citation needed] functional languages, such as Scheme, Clojure, Erlang, Haskell, OCaml, Standard ML and Unlambda, however, do permit side effects in practice.\n\nWhile functional languages typically do appear to specify \"how\", a compiler for a purely functional programming language is free to extensively rewrite the operational behavior of a function, so long as the same result is returned for the same inputs. This can be used to, for example, make a function compute its result in parallel, or to perform substantial optimizations (such as deforestation) that a compiler may not be able to safely apply to a language with side effects.\n\nHybrid languages[edit]\nSee also: Multi-paradigm programming language\nMakefiles, for example, specify dependencies in a declarative fashion,[5] but include an imperative list of actions to take as well. Similarly, yacc specifies a context free grammar declaratively, but includes code snippets from a host language, which is usually imperative (such as C).\n\nLogic programming[edit]\nLogic programming languages such as Prolog state and query relations. The specifics of how these queries are answered is up to the implementation and its theorem prover, but typically take the form of some sort of unification. Like functional programming, many logic programming languages permit side effects, and as a result are not strictly declarative.\n\nModeling[edit]\nMain article: Mathematical model\nModels, or mathematical representations, of physical systems may be implemented in computer code that is declarative. The code contains a number of equations, not imperative assignments, that describe (\"declare\") the behavioral relationships. When a model is expressed in this formalism, a computer is able to perform algebraic manipulations to best formulate the solution algorithm. The mathematical causality is typically imposed at the boundaries of the physical system, while the behavioral description of the system itself is declarative or acausal. Declarative modeling languages and environments include Analytica, Modelica and Simile.[6]\n\nSee also[edit]\nList of declarative programming languages\nComparison of programming paradigms\nInductive programming",
          "subparadigms": [
            16,
            21
          ]
        },
        {
          "pdid": 33,
          "name": "Procedural",
          "details": "Procedural programming is a programming paradigm, derived from structured programming, based upon the concept of the procedure call. Procedures, also known as routines, subroutines, or functions (not to be confused with mathematical functions, but similar to those used in functional programming), simply contain a series of computational steps to be carried out. Any given procedure might be called at any point during a program's execution, including by other procedures or itself. Procedural programming languages include C, Go, Fortran, Pascal, Ada, and BASIC.[1]\n\nComputer processors provide hardware support for procedural programming through a stack register and instructions for calling procedures and returning from them. Hardware support for other types of programming is possible, but no attempt was commercially successful (for example Lisp machines or Java processors).\n\nContents  [hide] \n1\tProcedures and modularity\n2\tComparison with imperative programming\n3\tComparison with object-oriented programming\n4\tComparison with functional programming\n5\tComparison with logic programming\n6\tSee also\n7\tReferences\n8\tExternal links\nProcedures and modularity[edit]\nMain article: Modular programming\nModularity is generally desirable, especially in large, complicated programs. Inputs are usually specified syntactically in the form of arguments and the outputs delivered as return values.\n\nScoping is another technique that helps keep procedures modular. It prevents the procedure from accessing the variables of other procedures (and vice versa), including previous instances of itself, without explicit authorization.\n\nLess modular procedures, often used in small or quickly written programs, tend to interact with a large number of variables in the execution environment, which other procedures might also modify.\n\nBecause of the ability to specify a simple interface, to be self-contained, and to be reused, procedures are a convenient vehicle for making pieces of code written by different people or different groups, including through programming libraries.\n\nComparison with imperative programming[edit]\nProcedural programming languages are also imperative languages, because they make explicit references to the state of the execution environment. This could be anything from variables (which may correspond to processor registers) to something like the position of the \"turtle\" in the Logo programming language.\n\nOften, the terms \"procedural programming\" and \"imperative programming\" are used synonymously. However, procedural programming relies heavily on blocks and scope, whereas imperative programming as a whole may or may not have such features. As such, procedural languages generally use reserved words that act on blocks, such as if, while, and for, to implement control flow, whereas non-structured imperative languages use goto statements and branch tables for the same purpose.\n\nComparison with object-oriented programming[edit]\nThe focus of procedural programming is to break down a programming task into a collection of variables, data structures, and subroutines, whereas in object-oriented programming it is to break down a programming task into objects that expose behavior (methods) and data (members or attributes) using interfaces. The most important distinction is that while procedural programming uses procedures to operate on data structures, object-oriented programming bundles the two together, so an \"object\", which is an instance of a class, operates on its \"own\" data structure.[2]\n\nNomenclature varies between the two, although they have similar semantics:\n\nProcedural\tObject-oriented\nprocedure\tmethod\nrecord\tobject\nmodule\tclass\nprocedure call\tmessage\nComparison with functional programming[edit]\nThe principles of modularity and code reuse in practical functional languages are fundamentally the same as in procedural languages, since they both stem from structured programming. So for example:\n\nProcedures correspond to functions. Both allow the reuse of the same code in various parts of the programs, and at various points of its execution.\nBy the same token, procedure calls correspond to function application.\nFunctions and their invocations are modularly separated from each other in the same manner, by the use of function arguments, return values and variable scopes.\nThe main difference between the styles is that functional programming languages remove or at least deemphasize the imperative elements of procedural programming. The feature set of functional languages is therefore designed to support writing programs as much as possible in terms of pure functions:\n\nWhereas procedural languages model execution of the program as a sequence of imperative commands that may implicitly alter shared state, functional programming languages model execution as the evaluation of complex expressions that only depend on each other in terms of arguments and return values. For this reason, functional programs can have a freer order of code execution, and the languages may offer little control over the order in which various parts of the program are executed. (For example, the arguments to a procedure invocation in Scheme are executed in an arbitrary order.)\nFunctional programming languages support (and heavily use) first-class functions, anonymous functions and closures, although these concepts are being included in newer procedural languages.\nFunctional programming languages tend to rely on tail call optimization and higher-order functions instead of imperative looping constructs.\nMany functional languages, however, are in fact impurely functional and offer imperative/procedural constructs that allow the programmer to write programs in procedural style, or in a combination of both styles. It is common for input/output code in functional languages to be written in a procedural style.\n\nThere do exist a few esoteric functional languages (like Unlambda) that eschew structured programming precepts for the sake of being difficult to program in (and therefore challenging). These languages are the exception to the common ground between procedural and functional languages.\n\nComparison with logic programming[edit]\nIn logic programming, a program is a set of premises, and computation is performed by attempting to prove candidate theorems. From this point of view, logic programs are declarative, focusing on what the problem is, rather than on how to solve it.\n\nHowever, the backward reasoning technique, implemented by SLD resolution, used to solve problems in logic programming languages such as Prolog, treats programs as goal-reduction procedures. Thus clauses of the form:\n\nH :- B1, …, Bn.\nhave a dual interpretation, both as procedures\n\nto show/solve H, show/solve B1 and … and Bn\nand as logical implications:\n\nB1 and … and Bn implies H.\nExperienced logic programmers use the procedural interpretation to write programs that are effective and efficient, and they use the declarative interpretation to help ensure that programs are correct.\n\nSee also[edit]\nComparison of programming paradigms\nDeclarative programming\nFunctional programming (contrast)\nImperative programming\nLogic programming\nObject-oriented programming\nProgramming paradigms\nProgramming language\nStructured programming\nSQL procedural extensions",
          "subparadigms": []
        },
        {
          "pdid": 34,
          "name": "Imperative",
          "details": "In computer science, imperative programming is a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\n\nThe term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.\n\nContents  [hide] \n1\tImperative and procedural programming\n2\tRationale and foundations of imperative programming\n3\tHistory of imperative and object-oriented languages\n4\tSee also\n5\tNotes\n6\tReferences\nImperative and procedural programming[edit]\nProcedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavily-procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. From the 1960s onwards, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.[1]\n\nProcedural programming could be considered a step towards declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.\n\nRationale and foundations of imperative programming[edit]\nThe hardware implementation of almost all computers is imperative.[note 1] Nearly all computer hardware is designed to execute machine code, which is native to the computer, written in the imperative style. From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.\n\nAssignment statements, in imperative paradigm, perform an operation on information located in memory and store the results in memory for later use. High-level imperative languages, in addition, permit the evaluation of complex expressions, which may consist of a combination of arithmetic operations and function evaluations, and the assignment of the resulting value to memory. Looping statements (as in while loops, do while loops, and for loops) allow a sequence of statements to be executed multiple times. Loops can either execute the statements they contain a predefined number of times, or they can execute them repeatedly until some condition changes. Conditional branching statements allow a sequence of statements to be executed only if some condition is met. Otherwise, the statements are skipped and the execution sequence continues from the statement following them. Unconditional branching statements allow an execution sequence to be transferred to another part of a program. These include the jump (called goto in many languages), switch, and the subprogram, subroutine, or procedure call (which usually returns to the next statement after the call).\n\nEarly in the development of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by hierarchical decomposition into simpler procedural structures.\n\nMany imperative programming languages (such as Fortran, BASIC, and C) are abstractions of assembly language.[2]\n\nHistory of imperative and object-oriented languages[edit]\nThe earliest imperative languages were the machine languages of the original computers. In these languages, instructions were very simple, which made hardware implementation easier, but hindered the creation of complex programs. FORTRAN, developed by John Backus at International Business Machines (IBM) starting in 1954, was the first major programming language to remove the obstacles presented by machine code in the creation of complex programs. FORTRAN was a compiled language that allowed named variables, complex expressions, subprograms, and many other features now common in imperative languages. The next two decades saw the development of many other major high-level imperative programming languages. In the late 1950s and 1960s, ALGOL was developed in order to allow mathematical algorithms to be more easily expressed, and even served as the operating system's target language for some computers. MUMPS (1966) carried the imperative paradigm to a logical extreme, by not having any statements at all, relying purely on commands, even to the extent of making the IF and ELSE commands independent of each other, connected only by an intrinsic variable named $TEST. COBOL (1960) and BASIC (1964) were both attempts to make programming syntax look more like English. In the 1970s, Pascal was developed by Niklaus Wirth, and C was created by Dennis Ritchie while he was working at Bell Laboratories. Wirth went on to design Modula-2 and Oberon. For the needs of the United States Department of Defense, Jean Ichbiah and a team at Honeywell began designing Ada in 1978, after a 4-year project to define the requirements for the language. The specification was first published in 1983, with revisions in 1995 and 2005–06.\n\nThe 1980s saw a rapid growth in interest in object-oriented programming. These languages were imperative in style, but added features to support objects. The last two decades of the 20th century saw the development of many such languages. Smalltalk-80, originally conceived by Alan Kay in 1969, was released in 1980, by the Xerox Palo Alto Research Center (PARC). Drawing from concepts in another object-oriented language—Simula (which is considered the world's first object-oriented programming language, developed in the 1960s)—Bjarne Stroustrup designed C++, an object-oriented language based on C. C++ was first implemented in 1985. In the late 1980s and 1990s, the notable imperative languages drawing on object-oriented concepts were Perl, released by Larry Wall in 1987; Python, released by Guido van Rossum in 1990; Visual Basic and Visual C++ (which included Microsoft Foundation Class Library (MFC) 2.0), released by Microsoft in 1991 and 1993 respectively; PHP, released by Rasmus Lerdorf in 1994; Java, released by Sun Microsystems in 1994, and Ruby, released by Yukihiro \"Matz\" Matsumoto in 1995. Microsoft's .NET Framework (2002) is imperative at its core, as are its main target languages, VB.NET and C# that run on it; however Microsoft's F#, a functional language, also runs on it.\n\nSee also[edit]\nicon\tComputer science portal\nFunctional programming\nComparison of programming paradigms\nDeclarative programming (contrast)\nProgramming paradigms\nObject-oriented programming\nReactive programming\nHistory of programming languages\nList of imperative programming languages\nCategory:Procedural programming languages lists additional imperative programming languages",
          "subparadigms": [
            33
          ]
        },
        {
          "pdid": 43,
          "name": "Automatic",
          "details": "In computer science, the term automatic programming[1] identifies a type of computer programming in which some mechanism generates a computer program to allow human programmers to write the code at a higher abstraction level.\n\nThere has been little agreement on the precise definition of automatic programming, mostly because its meaning has changed over time. David Parnas, tracing the history of \"automatic programming\" in published research, noted that in the 1940s it described automation of the manual process of punching paper tape. Later it referred to translation of high-level programming languages like Fortran and ALGOL. In fact, one of the earliest programs identifiable as a compiler was called Autocode. Parnas concluded that \"automatic programming has always been a euphemism for programming in a higher-level language than was then available to the programmer.\"[2]\n\nProgram synthesis is one type of automatic programming where a procedure is created from scratch, based on mathematical requirements.\n\nContents  [hide] \n1\tOrigin\n2\tGenerative programming\n3\tSource code generation\n3.1\tImplementations\n4\tSee also\n5\tReferences\n6\tExternal links\nOrigin[edit]\nMildred Koss, an early UNIVAC programmer, explains: \"Writing machine code involved several tedious steps—breaking down a process into discrete instructions, assigning specific memory locations to all the commands, and managing the I/O buffers. After following these steps to implement mathematical routines, a sub-routine library, and sorting programs, our task was to look at the larger programming process. We needed to understand how we might reuse tested code and have the machine help in programming. As we programmed, we examined the process and tried to think of ways to abstract these steps to incorporate them into higher-level language. This led to the development of interpreters, assemblers, compilers, and generators—programs designed to operate on or produce other programs, that is, automatic programming.\"[3]\n\nGenerative programming[edit]\nGenerative programming is a style of computer programming that uses automated source code creation through generic frames, classes, prototypes, templates, aspects, and code generators to improve programmer productivity.[4] It is often related to code-reuse topics such as component-based software engineering and product family engineering.\n\nSource code generation[edit]\nSource code generation is the process of generating source code based on an ontological model such as a template and is accomplished with a programming tool such as a template processor or an integrated development environment (IDE). These tools allow the generation of source code through any of various means. A macro processor, such as the C preprocessor, which replaces patterns in source code according to relatively simple rules, is a simple form of source code generator.\n\nImplementations[edit]\n\nThis section contains content that is written like an advertisement. Please help improve it by removing promotional content and inappropriate external links, and by adding encyclopedic content written from a neutral point of view. (October 2010) (Learn how and when to remove this template message)\nSome IDEs for Java and other languages have more advanced forms of source code generation, with which the programmer can interactively select and customize \"snippets\" of source code. Program \"wizards\", which allow the programmer to design graphical user interfaces interactively while the compiler invisibly generates the corresponding source code, are another common form of source code generation. This may be contrasted with, for example, user interface markup languages, which define user interfaces declaratively.\n\nBesides the generation of code from a wizard or template, IDEs can also generate and manipulate code to automate code refactorings that would require multiple (error prone) manual steps, thereby improving developer productivity.[5] Examples of such features in IDEs are the refactoring class browsers for Smalltalk and those found in Java IDEs like Eclipse.\n\nA specialized alternative involves the generation of optimized code for quantities defined mathematically within a Computer algebra system (CAS). Compiler optimization consisting of finding common intermediates of a vector of size {\\displaystyle n} n requires a complexity of {\\displaystyle O(n^{2})} O(n^{2}) or {\\displaystyle O(n^{3})} O(n^{3}) operations whereas the very design of a computer algebra system requires only {\\displaystyle O(n)} O(n) operations.[6][7][8] These facilities can be used as pre-optimizer before processing by the compiler. This option has been used for handling mathematically large expressions in e.g. computational (quantum) chemistry.\n\nExamples:\n\nAcceleo is an open source code generator for Eclipse used to generate any textual language (Java, PHP, Python, etc.) from EMF models defined from any metamodel (UML, SysML, etc.).\nActifsource is a plugin for Eclipse that allows graphical modelling and model-based code generation using custom templates.\nAltova MapForce is a graphical data mapping, conversion, and integration tool capable of generating application code in Java, C#, or C++ for executing recurrent transformations.\nCodeFluent Entities from SoftFluent is a graphical tool integrated into Microsoft Visual Studio that generates .NET source code, in C# or Visual Basic.\nDMS Software Reengineering Toolkit is a system for defining arbitrary domain specific languages and translating them to other languages.\nHPRCARCHITECT (from MNB Technologies, Inc) is an artificial intelligence-based software development tool with a Virtual Whiteboard human interface. Language and technology agnostic, the tool's development was funded by the US Air Force to solve the problem of code generation for systems targeting mixed processor technologies.\nSpring Roo is an open source active code generator for Spring Framework based Java applications. It uses AspectJ mixins to provide separation of concerns during round-trip maintenance.\nRISE is a free information modeling suite for system development using ERD or UML. Database code generation for MySQL, PostgreSQL and Microsoft SQL Server. Persistence code generation for C# (.NET) and PHP including both SOAP and JSON style web services and AJAX proxy code.\nThe Maple computer algebra system offers code generators optimizers with Fortran, MATLAB, C and Java. Wolfram Language (Mathematica), and MuPAD have comparable interfaces.\nScreen Sculptor,[9] SoftCode,[10] UI Programmer,[11] and Genifer[12] are examples of pioneering program generators that arose during the mid-1980s through the early 1990s. They developed and advanced the technology of extendable, template based source code generators on a mass market scale.\nGeneXus is a Cross-Platform, knowledge representation-based, development tool, mainly oriented to enterprise-class applications for Web applications, smart devices and the Microsoft Windows platform. A developer describes an application in a high-level, mostly declarative language, from which native code is generated for multiple environments.\nBidji is an Apache Ant project for code generation and data transformation.\nSee also[edit]\n\nThis \"see also\" section may contain an excessive number of suggestions. Please ensure that only the most relevant links are given, that they are not red links, and that any links are not already in this article. (June 2013) (Learn how and when to remove this template message)\nComparison of code generation tools\nSource-to-source compiler\nModel Driven Engineering (MDE)\nModel Driven Architecture (MDA)\nDomain-Specific Modeling (DSM)\nFeature Oriented Programming\nProgram synthesis\nProgram transformation\nInductive programming\nModeling language\nData transformation\nSemantic translation\nVocabulary-based transformation\nMetaprogramming\nLanguage-oriented programming",
          "subparadigms": []
        },
        {
          "pdid": 45,
          "name": "Reflection",
          "details": "In computer science, reflection is the ability of a computer program to examine, introspect, and modify its own structure and behavior at runtime.[1]\n\nContents  [hide] \n1\tHistorical background\n2\tUses\n3\tImplementation\n4\tExamples\n4.1\teC\n4.2\tECMAScript\n4.3\tGo\n4.4\tJava\n4.5\tObjective-C\n4.6\tDelphi\n4.7\tPerl\n4.8\tPHP\n4.9\tPython\n4.10\tR\n4.11\tRuby\n5\tSee also\n6\tReferences\n7\tFurther reading\n8\tExternal links\nHistorical background[edit]\nThe earliest computers were programmed in their native assembly language, which were inherently reflective as these original architectures could be programmed by defining instructions as data and using self-modifying code. As programming moved to compiled higher-level languages such as Fortran, Algol and Cobol (but also Pascal and C and many other languages) this reflective ability largely disappeared until programming languages with reflection built into their type systems appeared.[citation needed]\n\nBrian Cantwell Smith's 1982 doctoral dissertation[2][3] introduced the notion of computational reflection in programming languages, and the notion of the meta-circular interpreter as a component of 3-Lisp.\n\nUses[edit]\nReflection can be used for observing and modifying program execution at runtime. A reflection-oriented program component can monitor the execution of an enclosure of code and can modify itself according to a desired goal related to that enclosure. This is typically accomplished by dynamically assigning program code at runtime.\n\nIn object-oriented programming languages such as Java, reflection allows inspection of classes, interfaces, fields and methods at runtime without knowing the names of the interfaces, fields, methods at compile time. It also allows instantiation of new objects and invocation of methods.\n\nReflection can be used to adapt a given program to different situations dynamically. Reflection-oriented programming almost always requires additional knowledge, framework, relational mapping, and object relevance in order to take advantage of more generic code execution.\n\nReflection is often used as part of software testing, such as for the runtime creation/instantiation of mock objects.\n\nReflection is also a key strategy for metaprogramming.\n\nIn some object-oriented programming languages, such as C# and Java, reflection can be used to override member accessibility rules. For example, reflection makes it possible to change the value of a field marked \"private\" in a third-party library's class.\n\nImplementation[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2008) (Learn how and when to remove this template message)\nA language supporting reflection provides a number of features available at runtime that would otherwise be difficult to accomplish in a lower-level language. Some of these features are the abilities to:\n\nDiscover and modify source code constructions (such as code blocks, classes, methods, protocols, etc.) as a first-class object at runtime.\nConvert a string matching the symbolic name of a class or function into a reference to or invocation of that class or function.\nEvaluate a string as if it were a source code statement at runtime.\nCreate a new interpreter for the language's bytecode to give a new meaning or purpose for a programming construct.\nThese features can be implemented in different ways. In MOO, reflection forms a natural part of everyday programming idiom. When verbs (methods) are called, various variables such as verb (the name of the verb being called) and this (the object on which the verb is called) are populated to give the context of the call. Security is typically managed by accessing the caller stack programmatically: Since callers() is a list of the methods by which the current verb was eventually called, performing tests on callers()[1] (the command invoked by the original user) allows the verb to protect itself against unauthorised use.\n\nCompiled languages rely on their runtime system to provide information about the source code. A compiled Objective-C executable, for example, records the names of all methods in a block of the executable, providing a table to correspond these with the underlying methods (or selectors for these methods) compiled into the program. In a compiled language that supports runtime creation of functions, such as Common Lisp, the runtime environment must include a compiler or an interpreter.\n\nReflection can be implemented for languages not having built-in reflection facilities by using a program transformation system to define automated source code changes.\n\nExamples[edit]\nThe following code snippets create an instance foo of class Foo, and invoke its method hello. For each programming language, normal and reflection-based call sequences are shown.\n\n\nIt has been suggested that this article be split into a new article titled Comparison of programming languages (reflection). (Discuss.) (May 2016)\neC[edit]\nThe following is an example in eC:\n\n// without reflection\nFoo foo { };\nfoo.hello();\n\n// with reflection\nClass fooClass = eSystem_FindClass(__thisModule, \"Foo\");\nInstance foo = eInstance_New(fooClass);\nMethod m = eClass_FindMethod(fooClass, \"hello\", fooClass.module);\n((void (*)())(void *)m.function)(foo);\nECMAScript[edit]\nThe following is an example in ECMAScript, and therefore also applies to JavaScript and ActionScript:\n\n// Without reflection\nnew Foo().hello()\n\n// With reflection\n\n// assuming that Foo resides in this\nnew this['Foo']()['hello']()\n\n// or without assumption\nnew (eval('Foo'))()['hello']()\n\n// or simply\neval('new Foo().hello()')\n\n// Using ECMAScript 2015's new Reflect class:\nReflect.construct(Foo, [])['hello']()\nGo[edit]\nThe following is an example in Go:\n\nimport \"reflect\"\n\n// without reflection\nf := Foo{}\nf.Hello()\n\n// with reflection\nfT := reflect.TypeOf(Foo{})\nfV := reflect.New(fT)\n\nm := fV.MethodByName(\"Hello\")\nif m.IsValid() {\n    m.Call(nil)\n}\nJava[edit]\nThe following is an example in Java:\n\n// without reflection\nFoo foo = new Foo();\nfoo.hello();\n\n// with reflection\nObject foo = Class.forName(\"complete.classpath.and.Foo\").newInstance();\n// Alternatively: Object foo = Foo.class.newInstance();\nMethod m = foo.getClass().getDeclaredMethod(\"hello\", new Class<?>[0]);\nm.invoke(foo);\nObjective-C[edit]\nThe following is an example in Objective-C—implying either the OpenStep or Foundation Kit framework is used:\n\n// Foo class.\n@interface Foo : NSObject\n- (void)hello;\n@end\n\n// Sending \"hello\" to a Foo instance without reflection.\nFoo *obj = [[Foo alloc] init];\n[obj hello];\n\n// Sending \"hello\" to a Foo instance with reflection.\nid obj = [[NSClassFromString(@\"Foo\") alloc] init];\n[obj performSelector: @selector(hello)];\nDelphi[edit]\nThis Delphi example assumes a TFoo class has been declared in a unit called Unit1:\n\nuses RTTI, Unit1;\n\nprocedure WithoutReflection;\nvar\n  Foo: TFoo;\nbegin\n  Foo := TFoo.Create;\n  try\n    Foo.Hello;\n  finally\n    Foo.Free;\n  end;\nend;\n\nprocedure WithReflection;\nvar\n  RttiContext: TRttiContext;\n  RttiType: TRttiInstanceType;\n  Foo: TObject;\nbegin\n  RttiType := RttiContext.FindType('Unit1.TFoo') as TRttiInstanceType;\n  Foo := RttiType.GetMethod('Create').Invoke(RttiType.MetaclassType, []).AsObject;\n  try\n    RttiType.GetMethod('Hello').Invoke(Foo, []);\n  finally\n    Foo.Free;\n  end;\nend;\nThis is a notable example since Delphi is an unmanaged, fully natively compiled language, unlike most other languages that support reflection. Its language architecture inherits from strongly-typed Pascal, but with significant influence from SmallTalk. Compare with the other examples here, many of which are dynamic or script languages like Perl, Python or PHP, or languages with a runtime like Java or C#.\n\nPerl[edit]\nThe following is an example in Perl:\n\n# without reflection\nmy $foo = Foo->new;\n$foo->hello;\n\n# or\nFoo->new->hello;\n\n# with reflection\nmy $class = \"Foo\"\nmy $constructor = \"new\";\nmy $method = \"hello\";\n\nmy $f = $class->$constructor;\n$f->$method;\n\n# or\n$class->$constructor->$method;\n\n# with eval\neval \"new Foo->hello;\";\nPHP[edit]\nThe following is an example in PHP:\n\n// without reflection\n$foo = new Foo();\n$foo->hello();\n\n// with reflection\n$reflector = new ReflectionClass('Foo');\n$foo = $reflector->newInstance();\n$hello = $reflector->getMethod('hello');\n$hello->invoke($foo);\n\n// using callback\n$foo = new Foo();\ncall_user_func(array($foo, 'hello'));\n\n// using variable variables syntax\n$className = 'Foo';\n$foo = new $className();\n$method = 'hello';\n$foo->$method();\nPython[edit]\nThe following is an example in Python:\n\n# without reflection\nobj = Foo()\nobj.hello()\n\n# with reflection\nclass_name = \"Foo\"\nmethod = \"hello\"\nobj = globals()[class_name]()\ngetattr(obj, method)()\n\n# with eval\neval(\"Foo().hello()\")\nR[edit]\nThe following is an example in R:\n\n# Without reflection, assuming foo() returns an S3-type object that has method \"hello\"\nobj <- foo()\nhello(obj)\n\n# With reflection\nthe.class <- \"foo\"\nthe.method <- \"hello\"\nobj <- do.call(the.class, list())\ndo.call(the.method, alist(obj))\nRuby[edit]\nThe following is an example in Ruby:\n\n# without reflection\nobj = Foo.new\nobj.hello\n\n# with reflection\nclass_name = \"Foo\"\nmethod_name = :hello\nobj = Object.const_get(class_name).new\nobj.send method_name\n\n# with eval\neval \"Foo.new.hello\"\nSee also[edit]\nType introspection\nSelf-modifying code\nSelf-hosting\nProgramming paradigms\nList of reflective programming languages and platforms\nMirror (programming)",
          "subparadigms": []
        },
        {
          "pdid": 60,
          "name": "Prototype-based",
          "details": "Prototype-based programming is a style of object-oriented programming in which behaviour reuse (known as inheritance) is performed via a process of reusing existing objects via delegation that serve as prototypes. This model can also be known as prototypal, prototype-oriented, classless, or instance-based programming. Delegation is the language feature that supports prototype-based programming.\n\nA fruit bowl serves as one example. A \"fruit\" object would represent the properties and functionality of fruit in general. A \"banana\" object would be cloned from the \"fruit\" object, and would also be extended to include general properties specific to bananas. Each individual \"banana\" object would be cloned from the generic \"banana\" object.\n\nThe first prototype-oriented programming language was Self, developed by David Ungar and Randall Smith in the mid-1980s to research topics in object-oriented language design. Since the late 1990s, the classless paradigm has grown increasingly popular.[citation needed] Some current prototype-oriented languages are JavaScript (and other ECMAScript implementations, JScript and Flash's ActionScript 1.0), Lua, Cecil, NewtonScript, Io, Ioke, MOO, REBOL, Lisaac and AHk.\n\nContents  [hide] \n1\tDesign and implementation\n2\tObject construction\n3\tDelegation\n4\tConcatenation\n5\tCriticism\n6\tLanguages supporting prototype-based programming\n7\tSee also\n8\tReferences\n9\tFurther reading\nDesign and implementation[edit]\nPrototypal inheritance in JavaScript is described by Douglas Crockford as: you make prototype objects, and then … make new instances. Objects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects. We don't need classes to make lots of similar objects… Objects inherit from objects. What could be more object oriented than that?.[1]\n\nAdvocates of prototype-based programming argue that it encourages the programmer to focus on the behavior of some set of examples and only later worry about classifying these objects into archetypal objects that are later used in a fashion similar to classes.[2] Many prototype-based systems encourage the alteration of prototypes during run-time, whereas only very few class-based object-oriented systems (such as the dynamic object-oriented system, Common Lisp, Dylan, Objective-C, Perl, Python, Ruby, or Smalltalk) allow classes to be altered during the execution of a program.\n\nAlmost all prototype-based systems are based on interpreted and dynamically typed languages. Systems based on statically typed languages are technically feasible, however. The Omega language discussed in Prototype-Based Programming[3] is an example of such a system, though according to Omega's website even Omega is not exclusively static, but rather its \"compiler may choose to use static binding where this is possible and may improve the efficiency of a program.\"\n\nObject construction[edit]\nIn prototype-based languages there are no explicit classes. Objects inherit directly from other objects through a prototype property. The prototype property is called prototype in Self, proto in Io and __proto__ in JavaScript. There are two methods of constructing new objects: ex nihilo (\"from nothing\") object creation or through cloning an existing object. The former is supported through some form of object literal, declarations where objects can be defined at runtime through special syntax such as {...} and passed directly to a variable. While most systems support a variety of cloning, ex nihilo object creation is not as prominent.[4]\n\nIn class-based languages, a new instance is constructed through a class's constructor function, a special function that reserves a block of memory for the object's members (properties and methods) and returns a reference to that block. An optional set of constructor arguments can be passed to the function and are usually held in properties. The resulting instance will inherit all the methods and properties that were defined in the class, which acts as a kind of template from which similar typed objects can be constructed.\n\nSystems that support ex nihilo object creation allow new objects to be created from scratch without cloning from an existing prototype. Such systems provide a special syntax for specifying the properties and behaviors of new objects without referencing existing objects. In many prototype languages there exists a root object, often called Object, which is set as the default prototype for all other objects created in run-time and which carries commonly needed methods such as a toString() function to return a description of the object as a string. One useful aspect of ex nihilo object creation is to ensure that a new object's slot (properties and methods) names do not have namespace conflicts with the top-level Object object. (In the JavaScript language, one can do this by using a null prototype, i.e. Object.create(null).)\n\nCloning refers to a process whereby a new object is constructed by copying the behavior of an existing object (its prototype). The new object then carries all the qualities of the original. From this point on, the new object can be modified. In some systems the resulting child object maintains an explicit link (via delegation or resemblance) to its prototype, and changes in the prototype cause corresponding changes to be apparent in its clone. Other systems, such as the Forth-like programming language Kevo, do not propagate change from the prototype in this fashion, and instead follow a more concatenative model where changes in cloned objects do not automatically propagate across descendants.[2]\n\n// Example of true prototypal inheritance style \n// in JavaScript.\n\n// \"ex nihilo\" object creation using the literal \n// object notation {}.\nvar foo = {name: \"foo\", one: 1, two: 2};\n\n// Another \"ex nihilo\" object.\nvar bar = {two: \"two\", three: 3};\n\n// Object.setPrototypeOf() is a method introduced in ECMAScript 2015.\n// For the sake of simplicity, let us pretend \n// that the following line works regardless of the \n// engine used:\nObject.setPrototypeOf(bar, foo); // foo is now the prototype of bar.\n\n// If we try to access foo's properties from bar \n// from now on, we'll succeed. \nbar.one // Resolves to 1.\n\n// The child object's properties are also accessible.\nbar.three // Resolves to 3.\n\n// Own properties shadow prototype properties\nbar.two; // Resolves to \"two\"\nbar.name; // unaffected, resolves to \"foo\"\nfoo.name; // Resolves to \"foo\"\nThis example in JS 1.8.5 + ( see http://kangax.github.com/es5-compat-table/ )\n\nvar foo = {one: 1, two: 2};\n\n// bar.[[prototype]] = foo\nvar bar = Object.create( foo );\n\nbar.three = 3;\n\nbar.one; // 1\nbar.two; // 2\nbar.three; // 3\nDelegation[edit]\nIn prototype-based languages that use delegation, the language runtime is capable of dispatching the correct method or finding the right piece of data simply by following a series of delegation pointers (from object to its prototype) until a match is found. All that is required to establish this behavior-sharing between objects is the delegation pointer. Unlike the relationship between class and instance in class-based object-oriented languages, the relationship between the prototype and its offshoots does not require that the child object have a memory or structural similarity to the prototype beyond this link. As such, the child object can continue to be modified and amended over time without rearranging the structure of its associated prototype as in class-based systems. It is also important to note that not only data, but also methods can be added or changed. For this reason, some prototype-based languages refer to both data and methods as \"slots\" or \"members\".[citation needed]\n\nConcatenation[edit]\nUnder pure prototyping, which is also referred to as concatenative prototyping, and is exemplified in the Kevo language, there are no visible pointers or links to the original prototype from which an object is cloned. The prototype (parent) object is copied rather than linked to. As a result, changes to the prototype will not be reflected in cloned objects.[5]\n\nThe main conceptual difference under this arrangement is that changes made to a prototype object are not automatically propagated to clones. This may be seen as an advantage or disadvantage. (However, Kevo does provide additional primitives for publishing changes across sets of objects based on their similarity — so-called family resemblances or clone family mechanism[5] — rather than through taxonomic origin, as is typical in the delegation model.) It is also sometimes claimed that delegation-based prototyping has an additional disadvantage in that changes to a child object may affect the later operation of the parent. However, this problem is not inherent to the delegation-based model and does not exist in delegation-based languages such as JavaScript, which ensure that changes to a child object are always recorded in the child object itself and never in parents (i.e. the child's value shadows the parent's value rather than changing the parent's value).\n\nIn simplistic implementations, concatenative prototyping will have faster member lookup than delegation-based prototyping (because there is no need to follow the chain of parent objects), but will conversely use more memory (because all slots are copied, rather than there being a single slot pointing to the parent object). More sophisticated implementations can avoid these problems, however, although trade-offs between speed and memory are required. For example, systems with concatenative prototyping can use a copy-on-write implementation to allow for behind-the-scenes data sharing — and such an approach is indeed followed by Kevo.[6] Conversely, systems with delegation-based prototyping can use caching to speed up data lookup..\n\nCriticism[edit]\nAdvocates of class-based object models who criticize prototype-based systems often have concerns similar to the concerns that proponents of static type systems for programming languages have of dynamic type systems (see datatype). Usually, such concerns involve: correctness, safety, predictability, efficiency and programmer unfamiliarity.\n\nOn the first three points, classes are often seen as analogous to types (in most statically typed object-oriented languages they serve that role) and are proposed to provide contractual guarantees to their instances, and to users of their instances, that they will behave in some given fashion.\n\nRegarding efficiency, declaring classes simplifies many compiler optimizations that allow developing efficient method and instance-variable lookup. For the Self language, much development time was spent on developing, compiling, and interpreting techniques to improve the performance of prototype-based systems versus class-based systems.\n\nA common criticism made against prototype-based languages is that the community of software developers is unfamiliar with them, despite the popularity and market permeation of JavaScript. This knowledge level of prototype-based systems seems to be increasing with the proliferation of JavaScript frameworks and the complex use of JavaScript as the Web matures.[citation needed] ECMAScript 6 introduced classes as syntactic sugar over JavaScript's existing prototype-based inheritance, providing an alternative way to create objects and deal with inheritance.[7]\n\nLanguages supporting prototype-based programming[edit]\nActor-Based Concurrent Language (ABCL): ABCL/1, ABCL/R, ABCL/R2, ABCL/c+\nAgora\nAutoHotkey\nCecil and Diesel of Craig Chambers\nColdC\nCOLA\nCommon Lisp\nECMAScript\nActionScript 1.0, used by Adobe Flash and Adobe Flex\nE4X\nJavaScript\nJScript\nFalcon\nIo\nIoke\nLisaac\nLogtalk\nLPC\nLua\nMaple\nMOO\nNeko\nNewtonScript\nObject Lisp\nObliq\nOmega\nOpenLaszlo\nPerl, with the Class::Prototyped module\nPython with prototype.py.\nR, with the proto package\nREBOL\nSelf\nSeph\nSmartFrog\nTADS\nTcl with snit extension\nUmajin[8]",
          "subparadigms": []
        },
        {
          "pdid": 62,
          "name": "Class-based",
          "details": "Class-based programming, or more commonly class-orientation, is a style of object-oriented programming (OOP) in which inheritance is achieved by defining classes of objects, as opposed to the objects themselves (compare prototype-based programming).\n\nThe most popular and developed model of OOP is a class-based model, as opposed to an object-based model. In this model, objects are entities that combine state (i.e. data), behavior (i.e. procedures, or methods) and identity (unique existence among all other objects). The structure and behavior of an object are defined by a class, which is a definition, or blueprint, of all objects of a specific type. An object must be explicitly created based on a class and an object thus created is considered to be an instance of that class. An object is similar to a structure, with the addition of method pointers, member access control, and an implicit data member which locates instances of the class (i.e. actual objects of that class) in the class hierarchy (essential for runtime inheritance features).\n\nContents  [hide] \n1\tEncapsulation\n2\tInheritance\n3\tCritique of class-based models\n4\tExample languages\n5\tSee also\n6\tReferences\nEncapsulation[edit]\nEncapsulation prevents users from breaking the invariants of the class, which is useful because it allows the implementation of a class of objects to be changed for aspects not exposed in the interface without impact to user code. The definitions of encapsulation focus on the grouping and packaging of related information (cohesion) rather than security issues. OOP languages do not normally offer formal security restrictions to the internal object state. Using a method of access is a matter of convention for the interface design.\n\nInheritance[edit]\nMain article: Inheritance\nIn class-based programming, inheritance is done by defining new classes as extensions of existing classes: the existing class is the parent class and the new class is the child class. If a child class has only one parent class, this is known as single inheritance, while if a child class can have more than one parent class, this is known as multiple inheritance. This organizes classes into a hierarchy, either a tree (if single inheritance) or lattice (if multiple inheritance).\n\nThe defining feature of inheritance is that both interface and implementation are inherited; if only interface is inherited, this is known as interface inheritance or subtyping. Inheritance can also be done without classes, as in prototype-based programming.\n\nCritique of class-based models[edit]\nClass-based languages, or, to be more precise, typed languages, where subclassing is the only way of subtyping, have been criticized for mixing up implementations and interfaces—the essential principle in object-oriented programming. The critics say one might create a bag class that stores a collection of objects, then extend it to make a new class called a set class where the duplication of objects is eliminated.[1][2] Now, a function that takes an object of the bag class may expect that adding two objects increases the size of a bag by two, yet if one passes an object of a set class, then adding two objects may or may not increase the size of a bag by two. The problem arises precisely because subclassing implies subtyping even in the instances where the principle of subtyping, known as the Liskov substitution principle, does not hold. Barbara Liskov and Jeannette Wing formulated the principle succinctly in a 1994 paper as follows:\n\nSubtype Requirement: Let {\\displaystyle \\phi (x)} \\phi (x) be a property provable about objects x of type T. Then {\\displaystyle \\phi (y)} {\\displaystyle \\phi (y)} should be true for objects y of type S where S is a subtype of T.\n\nTherefore normally one must distinguish subtyping and subclassing. Most current object-oriented languages distinguish subtyping and subclassing, however some approaches to design do not.\n\nAlso, another common example is that a person object created from a child class cannot become an object of parent class because a child class and a parent class inherit a person class but class-based languages mostly do not allow to change the kind of class of the object at runtime. For class-based languages, this restriction is essential in order to preserve unified view of the class to its users. The users should not need to care whether one of the implementations of a method happens to cause changes that break the invariants of the class. Such changes can be made by destroying the object and constructing another in its place. Polymorphism can be used to preserve the relevant interfaces even when such changes are done, because the objects are viewed as black box abstractions and accessed via object identity. However, usually the value of object references referring to the object is changed, which causes effects to client code.\n\nExample languages[edit]\nSee also: Category:Class-based programming languages\nAlthough Simula introduced the class abstraction, the canonical example of a class-based language is Smalltalk. Others include PHP, C++, Java, C#, and Objective-C.\n\nSee also[edit]\nPrototype-based programming (contrast)\nProgramming paradigms\nClass (computer programming)",
          "subparadigms": []
        },
        {
          "pdid": 63,
          "name": "Actor-based",
          "details": "The actor model in computer science is a mathematical model of concurrent computation that treats \"actors\" as the universal primitives of concurrent computation. In response to a message that it receives, an actor can: make local decisions, create more actors, send more messages, and determine how to respond to the next message received. Actors may modify private state, but can only affect each other through messages (avoiding the need for any locks).\n\nThe actor model originated in 1973.[1] It has been used both as a framework for a theoretical understanding of computation and as the theoretical basis for several practical implementations of concurrent systems. The relationship of the model to other work is discussed in Indeterminacy in concurrent computation and Actor model and process calculi.\n\nContents  [hide] \n1\tHistory\n2\tFundamental concepts\n3\tFormal systems\n4\tApplications\n5\tMessage-passing semantics\n5.1\tUnbounded nondeterminism controversy\n5.2\tDirect communication and asynchrony\n5.3\tActor creation plus addresses in messages means variable topology\n5.4\tInherently concurrent\n5.5\tNo requirement on order of message arrival\n5.6\tLocality\n5.7\tComposing Actor Systems\n5.8\tBehaviors\n5.9\tModeling other concurrency systems\n5.10\tComputational Representation Theorem\n5.11\tRelationship to logic programming\n5.12\tMigration\n5.13\tSecurity\n5.14\tSynthesizing addresses of actors\n5.15\tContrast with other models of message-passing concurrency\n6\tInfluence\n6.1\tTheory\n6.2\tPractice\n7\tCurrent issues\n8\tEarly Actor researchers\n9\tProgramming with Actors\n9.1\tEarly Actor programming languages\n9.2\tLater Actor programming languages\n9.3\tActor libraries and frameworks\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\n13.1\tVideos\n13.2\tArticles\n13.3\tProcedural Libraries\nHistory[edit]\nMain article: History of the Actor model\nAccording to Carl Hewitt, unlike previous models of computation, the Actor model was inspired by physics, including general relativity and quantum mechanics. It was also influenced by the programming languages Lisp, Simula and early versions of Smalltalk, as well as capability-based systems and packet switching. Its development was \"motivated by the prospect of highly parallel computing machines consisting of dozens, hundreds, or even thousands of independent microprocessors, each with its own local memory and communications processor, communicating via a high-performance communications network.\"[2] Since that time, the advent of massive concurrency through multi-core and manycore computer architectures has revived interest in the Actor model.\n\nFollowing Hewitt, Bishop, and Steiger's 1973 publication, Irene Greif developed an operational semantics for the Actor model as part of her doctoral research.[3] Two years later, Henry Baker and Hewitt published a set of axiomatic laws for Actor systems.[4][5] Other major milestones include William Clinger's 1981 dissertation introducing a denotational semantics based on power domains[2] and Gul Agha's 1985 dissertation which further developed a transition-based semantic model complementary to Clinger's.[6] This resulted in the full development of actor model theory.\n\nMajor software implementation work was done by Russ Atkinson, Giuseppe Attardi, Henry Baker, Gerry Barber, Peter Bishop, Peter de Jong, Ken Kahn, Henry Lieberman, Carl Manning, Tom Reinhardt, Richard Steiger and Dan Theriault in the Message Passing Semantics Group at Massachusetts Institute of Technology (MIT). Research groups led by Chuck Seitz at California Institute of Technology (Caltech) and Bill Dally at MIT constructed computer architectures that further developed the message passing in the model. See Actor model implementation.\n\nResearch on the Actor model has been carried out at California Institute of Technology, Kyoto University Tokoro Laboratory, MCC, MIT Artificial Intelligence Laboratory, SRI, Stanford University, University of Illinois at Urbana-Champaign,[7] Pierre and Marie Curie University (University of Paris 6), University of Pisa, University of Tokyo Yonezawa Laboratory, Centrum Wiskunde & Informatica (CWI) and elsewhere.\n\nFundamental concepts[edit]\nThe Actor model adopts the philosophy that everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages.\n\nAn actor is a computational entity that, in response to a message it receives, can concurrently:\n\nsend a finite number of messages to other actors;\ncreate a finite number of new actors;\ndesignate the behavior to be used for the next message it receives.\nThere is no assumed sequence to the above actions and they could be carried out in parallel.\n\nDecoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages.[8]\n\nRecipients of messages are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created.\n\nThe Actor model is characterized by inherent concurrency of computation within and among actors, dynamic creation of actors, inclusion of actor addresses in messages, and interaction only through direct asynchronous message passing with no restriction on message arrival order.\n\nFormal systems[edit]\nOver the years, several different formal systems have been developed which permit reasoning about systems in the Actor model. These include:\n\nOperational semantics[3][9]\nLaws for Actor systems[4]\nDenotational semantics[2][10]\nTransition semantics[6]\nThere are also formalisms that are not fully faithful to the Actor model in that they do not formalize the guaranteed delivery of messages including the following (See Attempts to relate Actor semantics to algebra and linear logic):\n\nSeveral different Actor algebras[11][12][13]\nLinear logic[14]\nApplications[edit]\n\nThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)\nThe Actor model can be used as a framework for modeling, understanding, and reasoning about a wide range of concurrent systems. For example:\n\nElectronic mail (e-mail) can be modeled as an Actor system. Accounts are modeled as Actors and email addresses as Actor addresses.\nWeb Services can be modeled with SOAP endpoints modeled as Actor addresses.\nObjects with locks (e.g., as in Java and C#) can be modeled as a Serializer, provided that their implementations are such that messages can continually arrive (perhaps by being stored in an internal queue). A serializer is an important kind of Actor defined by the property that it is continually available to the arrival of new messages; every message sent to a serializer is guaranteed to arrive.\nTesting and Test Control Notation (TTCN), both TTCN-2 and TTCN-3, follows Actor model rather closely. In TTCN, Actor is a test component: either parallel test component (PTC) or main test component (MTC). Test components can send and receive messages to and from remote partners (peer test components or test system interface), the latter being identified by its address. Each test component has a behaviour tree bound to it; test components run in parallel and can be dynamically created by parent test components. Built-in language constructs allow the definition of actions to be taken when an expected message is received from the internal message queue, like sending a message to another peer entity or creating new test components.\nMessage-passing semantics[edit]\nThe Actor model is about the semantics of message passing.\n\nUnbounded nondeterminism controversy[edit]\nArguably, the first concurrent programs were interrupt handlers. During the course of its normal operation a computer needed to be able to receive information from outside (characters from a keyboard, packets from a network, etc). So when the information arrived the execution of the computer was \"interrupted\" and special code called an interrupt handler was called to put the information in a buffer where it could be subsequently retrieved.\n\nIn the early 1960s, interrupts began to be used to simulate the concurrent execution of several programs on a single processor.[15] Having concurrency with shared memory gave rise to the problem of concurrency control. Originally, this problem was conceived as being one of mutual exclusion on a single computer. Edsger Dijkstra developed semaphores and later, between 1971 and 1973,[16] Tony Hoare[17] and Per Brinch Hansen[18] developed monitors to solve the mutual exclusion problem. However, neither of these solutions provided a programming-language construct that encapsulated access to shared resources. This encapsulation was later accomplished by the serializer construct ([Hewitt and Atkinson 1977, 1979] and [Atkinson 1980]).\n\nThe first models of computation (e.g., Turing machines, Post productions, the lambda calculus, etc.) were based on mathematics and made use of a global state to represent a computational step (later generalized in [McCarthy and Hayes 1969] and [Dijkstra 1976] see Event orderings versus global state). Each computational step was from one global state of the computation to the next global state. The global state approach was continued in automata theory for finite state machines and push down stack machines, including their nondeterministic versions. Such nondeterministic automata have the property of bounded nondeterminism; that is, if a machine always halts when started in its initial state, then there is a bound on the number of states in which it halts.\n\nEdsger Dijkstra further developed the nondeterministic global state approach. Dijkstra's model gave rise to a controversy concerning unbounded nondeterminism (also called unbounded indeterminacy), a property of concurrency by which the amount of delay in servicing a request can become unbounded as a result of arbitration of contention for shared resources while still guaranteeing that the request will eventually be serviced. Hewitt argued that the Actor model should provide the guarantee of service. In Dijkstra's model, although there could be an unbounded amount of time between the execution of sequential instructions on a computer, a (parallel) program that started out in a well defined state could terminate in only a bounded number of states [Dijkstra 1976]. Consequently, his model could not provide the guarantee of service. Dijkstra argued that it was impossible to implement unbounded nondeterminism.\n\nHewitt argued otherwise: there is no bound that can be placed on how long it takes a computational circuit called an arbiter to settle (see metastability in electronics).[19] Arbiters are used in computers to deal with the circumstance that computer clocks operate asynchronously with respect to input from outside, e.g., keyboard input, disk access, network input, etc. So it could take an unbounded time for a message sent to a computer to be received and in the meantime the computer could traverse an unbounded number of states.\n\nThe Actor Model features unbounded nondeterminism which was captured in a mathematical model by Will Clinger using domain theory.[2] There is no global state in the Actor model.[dubious – discuss]\n\nDirect communication and asynchrony[edit]\nMessages in the Actor model are not necessarily buffered. This was a sharp break with previous approaches to models of concurrent computation. The lack of buffering caused a great deal of misunderstanding at the time of the development of the Actor model and is still a controversial issue. Some researchers argued that the messages are buffered in the \"ether\" or the \"environment\". Also, messages in the Actor model are simply sent (like packets in IP); there is no requirement for a synchronous handshake with the recipient.\n\nActor creation plus addresses in messages means variable topology[edit]\nA natural development of the Actor model was to allow addresses in messages. Influenced by packet switched networks [1961 and 1964], Hewitt proposed the development of a new model of concurrent computation in which communications would not have any required fields at all: they could be empty. Of course, if the sender of a communication desired a recipient to have access to addresses which the recipient did not already have, the address would have to be sent in the communication.\n\nFor example, an Actor might need to send a message to a recipient Actor from which it later expects to receive a response, but the response will actually be handled by a third Actor component that has been configured to receive and handle the response (for example, a different Actor implementing the Observer pattern). The original Actor could accomplish this by sending a communication that includes the message it wishes to send, along with the address of the third Actor that will handle the response. This third Actor that will handle the response is called the resumption (sometimes also called a continuation or stack frame). When the recipient Actor is ready to send a response, it sends the response message to the resumption Actor address that was included in the original communication.\n\nSo, the ability of Actors to create new Actors with which they can exchange communications, along with the ability to include the addresses of other Actors in messages, gives Actors the ability to create and participate in arbitrarily variable topological relationships with one another, much as the objects in Simula and other object-oriented languages may also be relationally composed into variable topologies of message-exchanging objects.\n\nInherently concurrent[edit]\nAs opposed to the previous approach based on composing sequential processes, the Actor model was developed as an inherently concurrent model. In the Actor model sequentiality was a special case that derived from concurrent computation as explained in Actor model theory.\n\nNo requirement on order of message arrival[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nHewitt argued against adding the requirement that messages must arrive in the order in which they are sent to the Actor. If output message ordering is desired, then it can be modeled by a queue Actor that provides this functionality. Such a queue Actor would queue the messages that arrived so that they could be retrieved in FIFO order. So if an Actor X sent a message M1 to an Actor Y, and later X sent another message M2 to Y, there is no requirement that M1 arrives at Y before M2.\n\nIn this respect the Actor model mirrors packet switching systems which do not guarantee that packets must be received in the order sent. Not providing the order of delivery guarantee allows packet switching to buffer packets, use multiple paths to send packets, resend damaged packets, and to provide other optimizations.\n\nFor example, Actors are allowed to pipeline the processing of messages. What this means is that in the course of processing a message M1, an Actor can designate the behavior to be used to process the next message, and then in fact begin processing another message M2 before it has finished processing M1. Just because an Actor is allowed to pipeline the processing of messages does not mean that it must pipeline the processing. Whether a message is pipelined is an engineering tradeoff. How would an external observer know whether the processing of a message by an Actor has been pipelined? There is no ambiguity in the definition of an Actor created by the possibility of pipelining. Of course, it is possible to perform the pipeline optimization incorrectly in some implementations, in which case unexpected behavior may occur.\n\nLocality[edit]\nAnother important characteristic of the Actor model is locality.\n\nLocality means that in processing a message, an Actor can send messages only to addresses that it receives in the message, addresses that it already had before it received the message, and addresses for Actors that it creates while processing the message. (But see Synthesizing Addresses of Actors.)\n\nAlso locality means that there is no simultaneous change in multiple locations. In this way it differs from some other models of concurrency, e.g., the Petri net model in which tokens are simultaneously removed from multiple locations and placed in other locations.\n\nComposing Actor Systems[edit]\nThe idea of composing Actor systems into larger ones is an important aspect of modularity that was developed in Gul Agha's doctoral dissertation,[6] developed later by Gul Agha, Ian Mason, Scott Smith, and Carolyn Talcott.[9]\n\nBehaviors[edit]\nA key innovation was the introduction of behavior specified as a mathematical function to express what an Actor does when it processes a message, including specifying a new behavior to process the next message that arrives. Behaviors provided a mechanism to mathematically model the sharing in concurrency.\n\nBehaviors also freed the Actor model from implementation details, e.g., the Smalltalk-72 token stream interpreter. However, it is critical to understand that the efficient implementation of systems described by the Actor model require extensive optimization. See Actor model implementation for details.\n\nModeling other concurrency systems[edit]\nOther concurrency systems (e.g., process calculi) can be modeled in the Actor model using a two-phase commit protocol.[20]\n\nComputational Representation Theorem[edit]\nSee also: Denotational semantics of the Actor model\nThere is a Computational Representation Theorem in the Actor model for systems which are closed in the sense that they do not receive communications from outside. The mathematical denotation denoted by a closed system S is constructed from an initial behavior ⊥S and a behavior-approximating function progressionS. These obtain increasingly better approximations and construct a denotation (meaning) for S as follows [Hewitt 2008; Clinger 1981]:\n\n{\\displaystyle \\mathbf {Denote} _{\\mathtt {S}}\\equiv \\lim _{i\\to \\infty }\\mathbf {progression} _{{\\mathtt {S}}^{i}}(\\bot _{\\mathtt {S}})} {\\displaystyle \\mathbf {Denote} _{\\mathtt {S}}\\equiv \\lim _{i\\to \\infty }\\mathbf {progression} _{{\\mathtt {S}}^{i}}(\\bot _{\\mathtt {S}})}\nIn this way, S can be mathematically characterized in terms of all its possible behaviors (including those involving unbounded nondeterminism). Although DenoteS is not an implementation of S, it can be used to prove a generalization of the Church-Turing-Rosser-Kleene thesis [Kleene 1943]:\n\nA consequence of the above theorem is that a finite Actor can nondeterministically respond with an uncountable[clarify] number of different outputs.\n\nRelationship to logic programming[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nOne of the key motivations for the development of the actor model was to understand and deal with the control structure issues that arose in development of the Planner programming language.[citation needed] Once the actor model was initially defined, an important challenge was to understand the power of the model relative to Robert Kowalski's thesis that \"computation can be subsumed by deduction\". Hewitt argued that Kowalski's thesis turned out to be false for the concurrent computation in the actor model (see Indeterminacy in concurrent computation).\n\nNevertheless, attempts were made to extend logic programming to concurrent computation. However, Hewitt and Agha [1991] claimed that the resulting systems were not deductive in the following sense: computational steps of the concurrent logic programming systems do not follow deductively from previous steps (see Indeterminacy in concurrent computation). Recently, logic programming has been integrated into the actor model in a way that maintains logical semantics.[19]\n\nMigration[edit]\nMigration in the Actor model is the ability of Actors to change locations. E.g., in his dissertation, Aki Yonezawa modeled a post office that customer Actors could enter, change locations within while operating, and exit. An Actor that can migrate can be modeled by having a location Actor that changes when the Actor migrates. However the faithfulness of this modeling is controversial and the subject of research.[citation needed]\n\nSecurity[edit]\nThe security of Actors can be protected in the following ways:\n\nhardwiring in which Actors are physically connected\ncomputer hardware as in Burroughs B5000, Lisp machine, etc.\nvirtual machines as in Java virtual machine, Common Language Runtime, etc.\noperating systems as in capability-based systems\nsigning and/or encryption of Actors and their addresses\nSynthesizing addresses of actors[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nA delicate point in the Actor model is the ability to synthesize the address of an Actor. In some cases security can be used to prevent the synthesis of addresses (see Security). However, if an Actor address is simply a bit string then clearly it can be synthesized although it may be difficult or even infeasible to guess the address of an Actor if the bit strings are long enough. SOAP uses a URL for the address of an endpoint where an Actor can be reached. Since a URL is a character string, it can clearly be synthesized although encryption can make it virtually impossible to guess.\n\nSynthesizing the addresses of Actors is usually modeled using mapping. The idea is to use an Actor system to perform the mapping to the actual Actor addresses. For example, on a computer the memory structure of the computer can be modeled as an Actor system that does the mapping. In the case of SOAP addresses, it's modeling the DNS and the rest of the URL mapping.\n\nContrast with other models of message-passing concurrency[edit]\nRobin Milner's initial published work on concurrency[21] was also notable in that it was not based on composing sequential processes. His work differed from the Actor model because it was based on a fixed number of processes of fixed topology communicating numbers and strings using synchronous communication. The original Communicating Sequential Processes model[22] published by Tony Hoare differed from the Actor model because it was based on the parallel composition of a fixed number of sequential processes connected in a fixed topology, and communicating using synchronous message-passing based on process names (see Actor model and process calculi history). Later versions of CSP abandoned communication based on process names in favor of anonymous communication via channels, an approach also used in Milner's work on the Calculus of Communicating Systems and the π-calculus.\n\nThese early models by Milner and Hoare both had the property of bounded nondeterminism. Modern, theoretical CSP ([Hoare 1985] and [Roscoe 2005]) explicitly provides unbounded nondeterminism.\n\nPetri nets and their extensions (e.g., coloured Petri nets) are like actors in that they are based on asynchronous message passing and unbounded nondeterminism, while they are like early CSP in that they define fixed topologies of elementary processing steps (transitions) and message repositories (places).\n\nInfluence[edit]\nThe Actor Model has been influential on both theory development and practical software development.\n\nTheory[edit]\nThe Actor Model has influenced the development of the Pi-calculus and subsequent Process calculi. In his Turing lecture, Robin Milner wrote:[23]\n\nNow, the pure lambda-calculus is built with just two kinds of thing: terms and variables. Can we achieve the same economy for a process calculus? Carl Hewitt, with his Actors model, responded to this challenge long ago; he declared that a value, an operator on values, and a process should all be the same kind of thing: an Actor.\nThis goal impressed me, because it implies the homogeneity and completeness of expression ... But it was long before I could see how to attain the goal in terms of an algebraic calculus...\nSo, in the spirit of Hewitt, our first step is to demand that all things denoted by terms or accessed by names--values, registers, operators, processes, objects--are all of the same kind of thing; they should all be processes.\nPractice[edit]\nThe Actor Model has had extensive influence on commercial practice. For example, Twitter has used actors for scalability.[24] Also, Microsoft has used the Actor Model in the development of its Asynchronous Agents Library.[25] There are numerous other Actor libraries listed in the Actor Libraries and Frameworks section below.\n\nCurrent issues[edit]\nAccording to Hewitt [2006], the Actor model addresses issues in computer and communications architecture, concurrent programming languages, and Web Services including the following:\n\nscalability: the challenge of scaling up concurrency both locally and nonlocally.\ntransparency: bridging the chasm between local and nonlocal concurrency. Transparency is currently a controversial issue. Some researchers[who?] have advocated a strict separation between local concurrency using concurrent programming languages (e.g., Java and C#) from nonlocal concurrency using SOAP for Web services. Strict separation produces a lack of transparency that causes problems when it is desirable/necessary to change between local and nonlocal access to Web Services (see distributed computing).\ninconsistency: Inconsistency is the norm because all very large knowledge systems about human information system interactions are inconsistent. This inconsistency extends to the documentation and specifications of very large systems (e.g., Microsoft Windows software, etc.), which are internally inconsistent.\nMany of the ideas introduced in the Actor model are now also finding application in multi-agent systems for these same reasons [Hewitt 2006b 2007b]. The key difference is that agent systems (in most definitions) impose extra constraints upon the Actors, typically requiring that they make use of commitments and goals.\n\nThe Actor model is also being applied to client cloud computing.[26]\n\nEarly Actor researchers[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nGnome-searchtool.svg\nThis article's factual accuracy is disputed. Please help to ensure that disputed statements are reliably sourced. See the relevant discussion on the talk page. (March 2012) (Learn how and when to remove this template message)\nThere is a growing community of researchers working on the Actor Model as it is becoming commercially more important. Early Actor researchers included:\n\nImportant contributions to the semantics of Actors have been made by: Gul Agha, Beppe Attardi, Henry Baker, Will Clinger, Irene Greif, Carl Hewitt, Carl Manning, Ian Mason, Ugo Montanari, Maria Simi, Scott Smith, Carolyn Talcott, Prasanna Thati, and Akinori Yonezawa.\nImportant contributions to the implementation of Actors have been made by: Bill Athas, Russ Atkinson, Beppe Attardi, Henry Baker, Gerry Barber, Peter Bishop, Nanette Boden, Jean-Pierre Briot, Bill Dally, Peter de Jong, Jessie Dedecker, Travis Desell, Ken Kahn, Carl Hewitt, Henry Lieberman, Carl Manning, Tom Reinhardt, Chuck Seitz, Richard Steiger, Dan Theriault, Mario Tokoro, Carlos Varela, Darrell Woelk.\nProgramming with Actors[edit]\nA number of different programming languages employ the Actor model or some variation of it. These languages include:\n\nEarly Actor programming languages[edit]\nAct 1, 2 and 3[27][28]\nActtalk[29]\nAni[30]\nCantor[31]\nRosette[32]\nLater Actor programming languages[edit]\nABCL\nAmbientTalk[33]\nAxum[34]\nCAL Actor Language\nD\nE\nElixir\nErlang\nFantom\nHumus[35]\nIo\nLFE\nEncore[36]\nPony[37]\nPtolemy Project\nRebeca Modeling Language\nReia\nSALSA[38]\nScala[39][40]\nScratch\nTNSDL\nActor libraries and frameworks[edit]\nActor libraries or frameworks have also been implemented to permit actor-style programming in languages that don't have actors built-in. Among these frameworks are:\n\nName\tStatus\tLatest release\tLicense\tLanguages\nAojet\tActive\t2016-10-17\tMIT\tSwift\nActor\tActive\t2013-05-31\tMIT\tJava\nVert.x\tActive\t2016-09-12\tApache 2.0\tJava, Groovy, Javascript, Ruby, Scala\nActor Framework\tActive\t2013-11-13\tApache 2.0\t.NET\nAkka (toolkit)\tActive\t2016-08-19\tApache 2.0\tJava and Scala\nAkka.NET\tActive\t2016-01-18\tApache 2.0\t.NET\nRemact.Net\tActive\t2016-06-26\tMIT\t.NET, Javascript\nAteji PX\tActive\t ?\t ?\tJava\nF# MailboxProcessor\tActive\tsame as F# (built-in core library)\tApache License\tF#\nKorus\tActive\t2010-02-04\tGPL 3\tJava\nKilim[41]\tActive\t2011-10-13[42]\tMIT\tJava\nActorFoundry (based on Kilim)\tActive\t2008-12-28\t ?\tJava\nActorKit\tActive\t2011-09-13[43]\tBSD\tObjective-C\nCloud Haskell\tActive\t2015-06-17[44]\tBSD\tHaskell\nCloudI\tActive\t2015-12-24[45]\tBSD\tC/C++, Elixir/Erlang/LFE, Java, Javascript, Perl, PHP, Python, Ruby\nNAct\tActive\t2012-02-28\tLGPL 3.0\t.NET\nRetlang\tActive\t2011-05-18[46]\tNew BSD\t.NET\nJActor\tActive\t2013-01-22\tLGPL\tJava\nJetlang\tActive\t2013-05-30[47]\tNew BSD\tJava\nHaskell-Actor\tActive?\t2008\tNew BSD\tHaskell\nGPars\tActive\t2014-05-09[48]\tApache 2.0\tGroovy\nOOSMOS\tActive\t2016-02-17[49]\tGPL 2.0 and commercial (dual licensing)\tC. C++ friendly\nPanini\tActive\t2014-05-22\tMPL 1.1\tProgramming Language by itself\nPARLEY\tActive?\t2007-22-07\tGPL 2.1\tPython\nPeernetic\tActive\t2007-06-29\tLGPL 3.0\tJava\nPostSharp\tActive\t2014-09-24\tCommercial / Freemium\t.NET\nPulsar\tActive\t2016-07-09[50]\tNew BSD\tPython\nPulsar\tActive\t2016-02-18[51]\tLGPL/Eclipse\tClojure\nPykka\tActive\t2015-07-20[52]\tApache 2.0\tPython\nTermite Scheme\tActive?\t2009-05-21\tLGPL\tScheme (Gambit implementation)\nTheron\tActive\t2014-01-18[53]\tMIT[54]\tC++\nThespian\tActive\t2016-02-11\tGoDaddy Public Release[55]\tPython\nQuasar\tActive\t2016-01-18[56]\tLGPL/Eclipse\tJava\nLibactor\tActive?\t2009\tGPL 2.0\tC\nActor-CPP\tActive\t2012-03-10[57]\tGPL 2.0\tC++\nS4\tActive\t2012-07-31[58]\tApache 2.0\tJava\nC++ Actor Framework (CAF)\tActive\t2016-03-14[59]\tBoost Software License 1.0 and BSD 3-Clause\tC++11\nCelluloid\tActive\t2016-01-19[60]\tMIT\tRuby\nLabVIEW Actor Framework\tActive\t2012-03-01[61]\tNational Instruments SLA\tLabVIEW\nLabVIEW Messenger Library\tActive\t2016-06-01\tBSD\tLabVIEW\nOrbit\tActive\t2016-04-22[62]\tNew BSD\tJava\nQP frameworks for real-time embedded systems\tActive\t2015-09-29[63]\tGPL 2.0 and commercial (dual licensing)\tC and C++\nlibprocess\tActive\t2013-06-19\tApache 2.0\tC++\nSObjectizer\tActive\t2016-09-29\tNew BSD\tC++11\nOrleans\tActive\t2016-05-18[64]\tMIT License\tC#/.NET\nSkynet\tActive\t2016-07-11\tMIT License\tC/Lua\nReactors.IO\tActive\t2016-06-14\tBSD License\tJava/Scala\nPlease note that not all frameworks and libraries are listed here.\n\nSee also[edit]\nActor model theory\nActor model early history\nActor model and process calculi\nActor model implementation\nData flow\nMulti-agent system\nGordon Pask\nScientific Community Metaphor\nCommunicating sequential processes\nInput/output automaton",
          "subparadigms": []
        },
        {
          "pdid": 64,
          "name": "Object-oriented",
          "details": "Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated (objects have a notion of \"this\" or \"self\"). In OOP, computer programs are designed by making them out of objects that interact with one another.[1][2] There is significant diversity of OOP languages, but the most popular ones are class-based, meaning that objects are instances of classes, which typically also determine their type.\n\nMany of the most widely used programming languages are multi-paradigm programming languages that support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include Java, C++, C#, Python, PHP, Ruby, Perl, Delphi, Objective-C, Swift, Common Lisp, and Smalltalk.\n\nContents  [hide] \n1\tFeatures\n1.1\tShared with non-OOP predecessor languages\n1.2\tObjects and classes\n1.3\tDynamic dispatch/message passing\n1.4\tEncapsulation\n1.5\tComposition, inheritance, and delegation\n1.6\tPolymorphism\n1.7\tOpen recursion\n2\tHistory\n3\tOOP languages\n3.1\tOOP in dynamic languages\n3.2\tOOP in a network protocol\n4\tDesign patterns\n4.1\tInheritance and behavioral subtyping\n4.2\tGang of Four design patterns\n4.3\tObject-orientation and databases\n4.4\tReal-world modeling and relationships\n4.5\tOOP and control flow\n4.6\tResponsibility- vs. data-driven design\n4.7\tSOLID and GRASP guidelines\n5\tCriticism\n6\tFormal semantics\n7\tSee also\n7.1\tSystems\n7.2\tModeling languages\n8\tReferences\n9\tFurther reading\n10\tExternal links\nFeatures[edit]\nObject-oriented Programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP. The features listed below are, however, common among languages considered strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.[3][4][5][6]\n\nSee also: Comparison of programming languages (object-oriented programming) and List of object-oriented programming terms\nShared with non-OOP predecessor languages[edit]\nObject-oriented programming languages typically share low-level features with high-level procedural programming languages (which were invented first). The fundamental tools that can be used to construct a program include:\n\nVariables that can store information formatted in a small number of built-in data types like integers and alphanumeric characters. This may include data structures like strings, lists, and hash tables that are either built-in or result from combining variables using memory pointers\nProcedures – also known as functions, methods, routines, or subroutines – that take input, generate output, and manipulate data. Modern languages include structured programming constructs like loops and conditionals.\nModular programming support provides the ability to group procedures into files and modules for organizational purposes. Modules are namespaced so code in one module will not be accidentally confused with the same procedure or variable name in another file or module.\n\nObjects and classes[edit]\nLanguages that support object-oriented programming typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. Those that use classes support two main concepts:\n\nClasses – the definitions for the data format and available procedures for a given type or class of object; may also contain data and procedures (known as class methods) themselves, i.e. classes contains the data members and member functions\nObjects – instances of classes\nObjects sometimes correspond to things found in the real world. For example, a graphics program may have objects such as \"circle\", \"square\", \"menu\". An online shopping system might have objects such as \"shopping cart\", \"customer\", and \"product\".[7] Sometimes objects represent more abstract entities, like an object that represents an open file, or an object that provides the service of translating measurements from U.S. customary to metric.\n\nEach object is said to be an instance of a particular class (for example, an object with its name field set to \"Mary\" might be an instance of class Employee). Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties. This leads to the following terms:\n\nClass variables – belong to the class as a whole; there is only one copy of each one\nInstance variables or attributes – data that belongs to individual objects; every object has its own copy of each one\nMember variables – refers to both the class and instance variables that are defined by a particular class\nClass methods – belong to the class as a whole and have access only to class variables and inputs from the procedure call\nInstance methods – belong to individual objects, and have access to instance variables for the specific object they are called on, inputs, and class variables\nObjects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack. They provide a layer of abstraction which can be used to separate internal from external code. External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. Objects are created by calling a special type of method in the class known as a constructor. A program may create many instances of the same class as it runs, which operate independently. This is an easy way for the same procedures to be used on different sets of data.\n\nObject-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. As a result, a significantly different yet analogous terminology is used to define the concepts of object and instance.\n\nIn some languages classes and objects can be composed using other concepts like traits and mixins.\n\nDynamic dispatch/message passing[edit]\nIt is the responsibility of the object, not any external code, to select the procedural code to execute in response to a method call, typically by looking up the method at run time in a table associated with the object. This feature is known as dynamic dispatch, and distinguishes an object from an abstract data type (or module), which has a fixed (static) implementation of the operations for all instances. If there are multiple methods that might be run for a given name, it is known as multiple dispatch.\n\nA method call is also known as message passing. It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.\n\nEncapsulation[edit]\nEncapsulation is an Object Oriented Programming concept that binds together the data and functions that manipulate the data, and that keeps both safe from outside interference and misuse. Data encapsulation led to the important OOP concept of data hiding.\n\nIf a class does not allow calling code to access internal object data and permits access through methods only, this is a strong form of abstraction or information hiding known as encapsulation. Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the private keyword and designating methods intended for use by code outside the class with the public keyword. Methods may also be designed public, private, or intermediate levels such as protected (which allows access from the same class and its subclasses, but not objects of a different class). In other languages (like Python) this is enforced only by convention (for example, private methods may have names that start with an underscore). Encapsulation prevents external code from being concerned with the internal workings of an object. This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as \"public\" method calls work the same way). It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers. Encapsulation is a technique that encourages decoupling.\n\nComposition, inheritance, and delegation[edit]\nObjects can contain other objects in their instance variables; this is known as object composition. For example, an object in the Employee class might contain (point to) an object in the Address class, in addition to its own instance variables like \"first_name\" and \"position\". Object composition is used to represent \"has-a\" relationships: every employee has an address, so every Employee object has a place to store an Address object.\n\nLanguages that support classes almost always support inheritance. This allows classes to be arranged in a hierarchy that represents \"is-a-type-of\" relationships. For example, class Employee might inherit from class Person. All the data and methods available to the parent class also appear in the child class with the same names. For example, class Person might define variables \"first_name\" and \"last_name\" with method \"make_full_name()\". These will also be available in class Employee, which might add the variables \"position\" and \"salary\". This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.[8]\n\nSubclasses can override the methods defined by superclasses. Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated. Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes. For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don't share a common parent.\n\nAbstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other \"concrete\" classes which can be instantiated. In Java, the final keyword can be used to prevent a class from being subclassed.\n\nThe doctrine of composition over inheritance advocates implementing has-a relationships using composition instead of inheritance. For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods. Some languages, like Go do not support inheritance at all.\n\nThe \"open/closed principle\" advocates that classes and functions \"should be open for extension, but closed for modification\".\n\nDelegation is another language feature that can be used as an alternative to inheritance.\n\nPolymorphism[edit]\nSubtyping, a form of polymorphism, is when calling code can be agnostic as to whether an object belongs to a parent class or one of its descendants. For example, a function might call \"make_full_name()\" on an object, which will work whether the object is of class Person or class Employee. This is another type of abstraction which simplifies code external to the class hierarchy and enables strong separation of concerns.\n\nOpen recursion[edit]\nIn languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called this or self. This variable is late-bound; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.\n\nHistory[edit]\nTerminology invoking \"objects\" and \"oriented\" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. In the environment of the artificial intelligence group, as early as 1960, \"object\" could refer to identified items (LISP atoms) with properties (attributes);[9][10] Alan Kay was later to cite a detailed understanding of LISP internals as a strong influence on his thinking in 1966.[11] Another early MIT example was Sketchpad created by Ivan Sutherland in 1960–61; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.[12] Also, an MIT ALGOL version, AED-0, established a direct link between data structures (\"plexes\", in that dialect) and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".[13][14]\n\nThe formal programming concept of objects was introduced in the mid-1960s with Simula 67, a major revision of Simula I, a programming language designed for discrete event simulation, created by Ole-Johan Dahl and Kristen Nygaard of the Norwegian Computing Center in Oslo.[15][not in citation given][citation needed] [16][not in citation given][citation needed] [17] [18] [19]\n\nSimula 67 was influenced by SIMSCRIPT and C.A.R. \"Tony\" Hoare's proposed \"record classes\".[13][20] Simula introduced the notion of classes and instances or objects (as well as subclasses, virtual procedures, coroutines, and discrete event simulation) as part of an explicit programming paradigm. The language also used automatic garbage collection that had been invented earlier for the functional programming language Lisp. Simula was used for physical modeling, such as models to study and improve the movement of ships and their content through cargo ports. The ideas of Simula 67 influenced many later languages, including Smalltalk, derivatives of LISP (CLOS), Object Pascal, and C++.\n\nThe Smalltalk language, which was developed at Xerox PARC (by Alan Kay and others) in the 1970s, introduced the term object-oriented programming to represent the pervasive use of objects and messages as the basis for computation. Smalltalk creators were influenced by the ideas introduced in Simula 67, but Smalltalk was designed to be a fully dynamic system in which classes could be created and modified dynamically rather than statically as in Simula 67.[21] Smalltalk and with it OOP were introduced to a wider audience by the August 1981 issue of Byte Magazine.\n\nIn the 1970s, Kay's Smalltalk work had influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine. Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.\n\nIn 1985, Bertrand Meyer produced the first design of the Eiffel language. Focused on software quality, Eiffel is among the purely object-oriented languages, but differs in the sense that the language itself is not only a programming language, but a notation supporting the entire software lifecycle. Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. Essential to the quality focus of Eiffel is Meyer's reliability mechanism, Design by Contract, which is an integral part of both the method and language.\n\nObject-oriented programming developed as the dominant programming methodology in the early and mid 1990s when programming languages supporting the techniques became widely available. These included Visual FoxPro 3.0,[22][23][24] C++,[25] and Delphi[citation needed]. Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).\n\nAt ETH Zürich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.\n\nObject-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.\n\nMore recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. Two such languages are Python and Ruby. Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft's .NET platform. Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.\n\nOOP languages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2009) (Learn how and when to remove this template message)\nSee also: List of object-oriented programming languages\nSimula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. It was created for making simulation programs, in which what came to be called objects were the most important information representation. Smalltalk (1972 to 1980) is another early example, and the one with which much of the theory of OOP was developed. Concerning the degree of object orientation, the following distinctions can be made:\n\nLanguages called \"pure\" OO languages, because everything in them is treated consistently as an object, from primitives such as characters and punctuation, all the way up to whole classes, prototypes, blocks, modules, etc. They were designed specifically to facilitate, even enforce, OO methods. Examples: Python, Ruby, Scala, Smalltalk, Eiffel, Emerald,[26] JADE, Self.\nLanguages designed mainly for OO programming, but with some procedural elements. Examples: Java, C++, C#, Delphi/Object Pascal, VB.NET.\nLanguages that are historically procedural languages, but have been extended with some OO features. Examples: PHP, Perl, Visual Basic (derived from BASIC), MATLAB, COBOL 2002, Fortran 2003, ABAP, Ada 95, Pascal.\nLanguages with most of the features of objects (classes, methods, inheritance), but in a distinctly original form. Examples: Oberon (Oberon-1 or Oberon-2).\nLanguages with abstract data type support which may be used to resemble OO programming, but without all features of object-orientation. This includes object-based and prototype-based languages. Examples: JavaScript, Lua, Modula-2, CLU.\nChameleon languages that support multiple paradigms, including OO. Tcl stands out among these for TclOO, a hybrid object system that supports both prototype-based programming and class-based OO.\nOOP in dynamic languages[edit]\nIn recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\n\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.\n\nOOP in a network protocol[edit]\nThe messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server. For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value. A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command's parameters. Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:\n\nFields defining the data values that form messages, such as their length, codepoint and data values.\nObjects and collections of objects similar to what would be found in a Smalltalk program for messages and parameters.\nManagers similar to AS/400 objects, such as a directory to files and files consisting of metadata and records. Managers conceptually provide memory and processing resources for their contained objects.\nA client or server consisting of all the managers necessary to implement a full processing environment, supporting such aspects as directory services, security and concurrency control.\nThe initial version of DDM defined distributed file services. It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).\n\nDesign patterns[edit]\nChallenges of object-oriented design are addressed by several methodologies. Most common is known as the design patterns codified by Gamma et al.. More broadly, the term \"design patterns\" can be used to refer to any general, repeatable solution to a commonly occurring problem in software design. Some of these commonly occurring problems have implications and solutions particular to object-oriented development.\n\nInheritance and behavioral subtyping[edit]\nSee also: Object-oriented design\nIt is intuitive to assume that inheritance creates a semantic \"is a\" relationship, and thus to infer that objects instantiated from subclasses can always be safely used instead of those instantiated from the superclass. This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. This issue is known as the Liskov substitution principle.\n\nGang of Four design patterns[edit]\nMain article: Design pattern (computer science)\nDesign Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1995 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them. As of April 2007, the book was in its 36th printing.\n\nThe book describes the following patterns:\n\nCreational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern\nStructural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern\nBehavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern\nObject-orientation and databases[edit]\nMain articles: Object-relational impedance mismatch, Object-relational mapping, and Object database\nBoth object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. Since relational databases don't store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides.[27] One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails' ActiveRecord.\n\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.\n\nReal-world modeling and relationships[edit]\nOOP can be used to associate real-world objects and processes with digital counterparts. However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in Object-Oriented Software Construction[28] that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\". At the same time, some principal limitations of OOP had been noted.[29] For example, the circle-ellipse problem is difficult to handle using OOP's concept of inheritance.\n\nHowever, Niklaus Wirth (who popularized the adage now known as Wirth's law: \"Software is getting slower more rapidly than hardware becomes faster\") said of OOP in his paper, \"Good Ideas through the Looking Glass\", \"This paradigm closely reflects the structure of systems 'in the real world', and it is therefore well suited to model complex systems with complex behaviours\"[30] (contrast KISS principle).\n\nSteve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs).[31] This problem may cause OOP to suffer more convoluted solutions than procedural programming.[32]\n\nOOP and control flow[edit]\nOOP was developed to increase the reusability and maintainability of source code.[33] Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.[34][35][36][37]\n\nResponsibility- vs. data-driven design[edit]\nResponsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. The authors hold that responsibility-driven design is preferable.\n\nSOLID and GRASP guidelines[edit]\nSOLID is a mnemonic invented by Michael Feathers that stands for and advocates five programming practices:\n\nSingle responsibility principle\nOpen/closed principle\nLiskov substitution principle\nInterface segregation principle\nDependency inversion principle\nGRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.\n\nCriticism[edit]\nThe OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity,[38][39] and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).[40][41]\n\nLuca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex.[38] The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:[39]\n\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\n\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.[42]\n\nChristopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP;[43] however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.[44]\n\nIn an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.[45]\n\nAlexander Stepanov compares object orientation unfavourably to generic programming:[40]\n\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.\n\nPaul Graham has suggested that OOP's popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".[46]\n\nSteve Yegge noted that, as opposed to functional programming:[47]\n\nObject Oriented Programming puts the Nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It's not as if OOP has suddenly made verbs less important in the way we actually think. It's a strangely skewed perspective.\n\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.[41]\n\nEric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency.[48] Raymond compares this unfavourably to the approach taken with Unix and the C programming language.[48]\n\nRob Pike, a programmer involved in the creation of UTF-8 and Go, has called the design of object-oriented programming \"the Roman numerals of computing\"[49] and has said that OOP languages frequently shift the focus from data structures and algorithms to types.[50] Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.[51]\n\nFormal semantics[edit]\nSee also: Formal semantics of programming languages\nObjects are the run-time entities in an object-oriented system. They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.\n\nThere have been several attempts at formalizing the concepts used in object-oriented programming. The following concepts and constructs have been used as interpretations of OOP concepts:\n\nco algebraic data types[52]\nabstract data types (which have existential types) allow the definition of modules but these do not support dynamic dispatch\nrecursive types\nencapsulated state\ninheritance\nrecords are basis for understanding objects if function literals can be stored in fields (like in functional programming languages), but the actual calculi need be considerably more complex to incorporate essential features of OOP. Several extensions of System F<: that deal with mutable objects have been studied;[53] these allow both subtype polymorphism and parametric polymorphism (generics)\nAttempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, A Theory of Objects[53] for formal definitions of many OOP concepts and constructs), and often diverge widely. For example, some definitions focus on mental activities, and some on program structuring. One of the simpler definitions is that OOP is the act of using \"map\" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. Inheritance can be performed by cloning the maps (sometimes called \"prototyping\").\n\nSee also[edit]\nicon\tComputer programming portal\nComparison of programming languages (object-oriented programming)\nComparison of programming paradigms\nComponent-based software engineering\nDesign by contract\nObject association\nObject database\nObject modeling language\nObject-oriented analysis and design\nObject-relational impedance mismatch (and The Third Manifesto)\nObject-relational mapping\nSystems[edit]\nCADES\nCommon Object Request Broker Architecture (CORBA)\nDistributed Component Object Model\nDistributed Data Management Architecture\nJeroo\nModeling languages[edit]\nIDEF4\nInterface description language\nLepus3\nUML",
          "subparadigms": [
            63,
            62,
            60
          ]
        },
        {
          "pdid": 67,
          "name": "Structured",
          "details": "Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of subroutines, block structures, for and while loops—in contrast to using simple tests and jumps such as the go to statement which could lead to \"spaghetti code\" causing difficulty to both follow and maintain.\n\nIt emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages,[1] with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966,[2] and the publication of the influential \"Go To Statement Considered Harmful\" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term \"structured programming\".[3]\n\nStructured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.\n\nContents  [hide] \n1\tElements\n1.1\tControl structures\n1.2\tSubroutines\n1.3\tBlocks\n2\tStructured programming languages\n3\tHistory\n3.1\tTheoretical foundation\n3.2\tDebate\n3.3\tOutcome\n4\tCommon deviations\n4.1\tEarly exit\n4.2\tException handling\n4.3\tMultiple entry\n4.4\tState machines\n5\tSee also\n6\tReferences\n7\tExternal links\nElements[edit]\nControl structures[edit]\nFollowing the structured program theorem, all programs are seen as composed of control structures:\n\n\"Sequence\"; ordered statements or subroutines executed in sequence.\n\"Selection\"; one or a number of statements is executed depending on the state of the program. This is usually expressed with keywords such as if..then..else..endif.\n\"Iteration\"; a statement or block is executed until the program reaches a certain state, or operations have been applied to every element of a collection. This is usually expressed with keywords such as while, repeat, for or do..until. Often it is recommended that each loop should only have one entry point (and in the original structural programming, also only one exit point, and a few languages enforce this).\n\"Recursion\"; a statement is executed by repeatedly calling itself until termination conditions are met. While similar in practice to iterative loops, recursive loops may be more computationally efficient, and are implemented differently as a cascading stack.\n\nGraphical representations of the three basic patterns using NS diagrams (blue) and flow charts (green).\nSubroutines[edit]\nSubroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.\n\nBlocks[edit]\nBlocks are used to enable groups of statements to be treated as if they were one statement. Block-structured languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by if..fi as in ALGOL 68, or a code section bracketed by BEGIN..END, as in PL/I, whitespace indentation as in Python - or the curly braces {...} of C and many later languages.\n\nStructured programming languages[edit]\nIt is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada – but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult. Structured programming (sometimes known as modular programming) is a subset of imperative programming that enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.\n\nHistory[edit]\nTheoretical foundation[edit]\nThe structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a \"structured program\" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself.[4] The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.\n\nDebate[edit]\nP. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:\n\nUs converts waved this interesting bit of news under the noses of the unreconstructed assembly-language programmers who kept trotting forth twisty bits of logic and saying, 'I betcha can't structure this.' Neither the proof by Böhm and Jacopini nor our repeated successes at writing structured code brought them around one day sooner than they were ready to convince themselves.[5]\nDonald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees[citation needed]) with abolishing the GOTO statement. In his 1974 paper, \"Structured Programming with Goto Statements\",[6] he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs[when defined as?].[who?]\n\nStructured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for the New York Times research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.[citation needed]\n\nAs late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled \"\"GOTO considered harmful\" considered harmful\".[7] Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.\n\nOutcome[edit]\nBy the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.\n\nCommon deviations[edit]\nWhile goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.\n\nEarly exit[edit]\nThe most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a return statement. At the level of loops, this is a break statement (terminate the loop) or continue statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have \"labeled breaks\", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.\n\nMultiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered \"exceptional\" circumstances that prevent it from continuing, hence needing exception handling.\n\nThe most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not deallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal don't have this problem.\n\nMost modern languages provide language-level support to prevent such leaks;[8] see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a goto. This is most often known as try...finally, and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.\n\nKent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that \"one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t\". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors).[9] Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.[10]\n\nIn his 2004 textbook, David Watt writes that \"single-entry multi-exit control flows are often desirable\". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as escape sequencers, defined as a \"sequencer that terminates execution of a textually enclosing command or procedure\", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce \"spaghetti code\". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.[11]\n\nIn contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like break and continue \"are just the old goto in sheep's clothing\" and strongly advised against their use.[12]\n\nException handling[edit]\nBased on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and propose that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as throw()[13] Bonang proposes that all single-exit conforming C++ should be written along the lines of:\n\nbool myCheck1() throw()\n{\n  bool success = false;\n  try {\n    // do something that may throw exceptions\n    if(myCheck2() == false) {\n      throw SomeInternalException();\n    }\n    // other code similar to the above\n    success = true;\n  }\n  catch(...) { // all exceptions caught and logged\n  }\n  return success;\n}\nPeter Ritchie also notes that, in principle, even a single throw right before the return in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.[14]\n\nDavid Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic overflows or input/output failures like file not found) is a kind of error that \"is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit\". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where \"the application code tends to get cluttered by tests of status flags\" and that \"the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!\" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.[15]\n\nThe textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like while loops because the transfer of control \"is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred.\"[16] Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like for, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if init() throws an exception in for (init(); check(); increm()), then the usual exit point after check() is not reached.[17] Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they \"create hidden control-flow paths that are difficult for programmers to reason about\".[18]:8:27\n\nThe necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like parallel do, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from break to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.[19]\n\nMultiple entry[edit]\nFurther information: Coroutine\nMore rarely, subprograms allow multiple entry. This is most commonly only re-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.\n\nIt is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.\n\nState machines[edit]\nSome programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers (including Knuth[citation needed]) implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.\n\nHowever, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.\n\nSee also[edit]\nDRAKON\nMinimal evaluation\nNassi–Shneiderman diagram\nStructure chart\nSwitch statement",
          "subparadigms": [
            64
          ]
        }
      ],
      "programminglanguages": [
        {
          "name": "A+",
          "details": "A+ is an array programming language descendent from the programming language A, which in turn was created to replace APL in 1988.[1] Arthur Whitney developed the \"A\" portion of A+, while other developers at Morgan Stanley extended it, adding a graphical user interface and other language features. A+ was designed for numerically intensive applications, especially those found in financial applications. A+ runs on many Unix variants, including Linux. A+ is a high-level, interactive, interpreted language.\n\nA+ provides an extended set of functions and operators, a graphical user interface with automatic synchronization of widgets and variables, asynchronous execution of functions associated with variables and events, dynamic loading of user compiled subroutines, and other features. A newer graphical user interface has not yet been ported to all supported platforms\n\n\nThe A+ language implements the following changes to the APL language:\n\nan A+ function may have up to nine formal parameters\nA+ code statements are separated by semicolons, so a single statement may be divided into two or more physical lines\nThe explicit result of a function or operator is the result of the last statement executed\nA+ implements an object called a dependency, which is a global variable (the dependent variable) and an associated definition that is like a function with no arguments. Values can be explicitly set and referenced in exactly the same ways as for a global variable, but they can also be set through the associated definition.\nInteractive A+ development is primarily done in the Xemacs editor, through extensions to the editor. Because A+ code uses the original APL symbols, displaying A+ requires a font with those special characters; a font called \"kapl\" is provided on the web site for that purpose.\n\nArthur Whitney went on to create the K language, a proprietary array language. Like J, K omits the APL character set. It does not have some of the perceived complexities of A+, such as the existence of statements and two different modes of syntax.",
          "type": "compiled",
          "plid": 1
        },
        {
          "name": "Prolog",
          "details": "Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.[1][2][3]\n\nProlog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is declarative: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations.[4]\n\nThe language was first conceived by a group around Alain Colmerauer in Marseille, France, in the early 1970s and the first Prolog system was developed in 1972 by Colmerauer with Philippe Roussel.[5][6]\n\nProlog was one of the first logic programming languages,[7] and remains the most popular among such languages today, with several free and commercial implementations available. The language has been used for theorem proving,[8] expert systems,[9] as well as its original intended field of use, natural language processing.[10][11] Modern Prolog environments support creating graphical user interfaces, as well as administrative and networked applications.\n\nProlog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.\n\nContents  [hide] \n1\tSyntax and semantics\n1.1\tData types\n1.2\tRules and facts\n1.3\tExecution\n1.4\tLoops and recursion\n1.5\tNegation\n2\tProgramming in Prolog\n2.1\tHello world\n2.2\tCompiler optimization\n2.3\tQuicksort\n3\tDesign patterns\n4\tHigher-order programming\n5\tModules\n6\tParsing\n7\tMeta-interpreters and reflection\n8\tTuring completeness\n9\tImplementation\n9.1\tISO Prolog\n9.2\tCompilation\n9.3\tTail recursion\n9.4\tTerm indexing\n9.5\tHashing\n9.6\tTabling\n9.7\tImplementation in hardware\n10\tLimitations\n11\tExtensions\n11.1\tTypes\n11.2\tModes\n11.3\tConstraints\n11.4\tObject-orientation\n11.5\tGraphics\n11.6\tConcurrency\n11.7\tWeb programming\n11.8\tAdobe Flash\n11.9\tOther\n12\tInterfaces to other languages\n13\tHistory\n14\tUse in industry\n15\tSee also\n15.1\tRelated languages\n16\tReferences\n17\tFurther reading\n18\tExternal links\nSyntax and semantics[edit]\nMain article: Prolog syntax and semantics\nIn Prolog, program logic is expressed in terms of relations, and a computation is initiated by running a query over these relations. Relations and queries are constructed using Prolog's single data type, the term.[4] Relations are defined by clauses. Given a query, the Prolog engine attempts to find a resolution refutation of the negated query. If the negated query can be refuted, i.e., an instantiation for all free variables is found that makes the union of clauses and the singleton set consisting of the negated query false, it follows that the original query, with the found instantiation applied, is a logical consequence of the program. This makes Prolog (and other logic programming languages) particularly useful for database, symbolic mathematics, and language parsing applications. Because Prolog allows impure predicates, checking the truth value of certain special predicates may have some deliberate side effect, such as printing a value to the screen. Because of this, the programmer is permitted to use some amount of conventional imperative programming when the logical paradigm is inconvenient. It has a purely logical subset, called \"pure Prolog\", as well as a number of extralogical features.\n\nData types[edit]\nProlog's single data type is the term. Terms are either atoms, numbers, variables or compound terms.\n\nAn atom is a general-purpose name with no inherent meaning. Examples of atoms include x, red, 'Taco', and 'some atom'.\nNumbers can be floats or integers.\nVariables are denoted by a string consisting of letters, numbers and underscore characters, and beginning with an upper-case letter or underscore. Variables closely resemble variables in logic in that they are placeholders for arbitrary terms.\nA compound term is composed of an atom called a \"functor\" and a number of \"arguments\", which are again terms. Compound terms are ordinarily written as a functor followed by a comma-separated list of argument terms, which is contained in parentheses. The number of arguments is called the term's arity. An atom can be regarded as a compound term with arity zero. Examples of compound terms are truck_year('Mazda', 1986) and 'Person_Friends'(zelda,[tom,jim]).\nSpecial cases of compound terms:\n\nA List is an ordered collection of terms. It is denoted by square brackets with the terms separated by commas or in the case of the empty list, []. For example, [1,2,3] or [red,green,blue].\nStrings: A sequence of characters surrounded by quotes is equivalent to a list of (numeric) character codes, generally in the local character encoding, or Unicode if the system supports Unicode. For example, \"to be, or not to be\".\nRules and facts[edit]\nProlog programs describe relations, defined by means of clauses. Pure Prolog is restricted to Horn clauses. There are two types of clauses: facts and rules. A rule is of the form\n\nHead :- Body.\nand is read as \"Head is true if Body is true\". A rule's body consists of calls to predicates, which are called the rule's goals. The built-in predicate ,/2 (meaning a 2-arity operator with name ,) denotes conjunction of goals, and ;/2 denotes disjunction. Conjunctions and disjunctions can only appear in the body, not in the head of a rule.\n\nClauses with empty bodies are called facts. An example of a fact is:\n\ncat(tom).\nwhich is equivalent to the rule:\n\ncat(tom) :- true.\nThe built-in predicate true/0 is always true.\n\nGiven the above fact, one can ask:\n\nis tom a cat?\n\n ?- cat(tom).\n Yes\nwhat things are cats?\n\n ?- cat(X).\n X = tom\nClauses with bodies are called rules. An example of a rule is:\n\nanimal(X) :- cat(X).\nIf we add that rule and ask what things are animals?\n\n ?- animal(X).\n X = tom\nDue to the relational nature of many built-in predicates, they can typically be used in several directions. For example, length/2 can be used to determine the length of a list (length(List, L), given a list List) as well as to generate a list skeleton of a given length (length(X, 5)), and also to generate both list skeletons and their lengths together (length(X, L)). Similarly, append/3 can be used both to append two lists (append(ListA, ListB, X) given lists ListA and ListB) as well as to split a given list into parts (append(X, Y, List), given a list List). For this reason, a comparatively small set of library predicates suffices for many Prolog programs.\n\nAs a general purpose language, Prolog also provides various built-in predicates to perform routine activities like input/output, using graphics and otherwise communicating with the operating system. These predicates are not given a relational meaning and are only useful for the side-effects they exhibit on the system. For example, the predicate write/1 displays a term on the screen.\n\nExecution[edit]\nExecution of a Prolog program is initiated by the user's posting of a single goal, called the query. Logically, the Prolog engine tries to find a resolution refutation of the negated query. The resolution method used by Prolog is called SLD resolution. If the negated query can be refuted, it follows that the query, with the appropriate variable bindings in place, is a logical consequence of the program. In that case, all generated variable bindings are reported to the user, and the query is said to have succeeded. Operationally, Prolog's execution strategy can be thought of as a generalization of function calls in other languages, one difference being that multiple clause heads can match a given call. In that case, the system creates a choice-point, unifies the goal with the clause head of the first alternative, and continues with the goals of that first alternative. If any goal fails in the course of executing the program, all variable bindings that were made since the most recent choice-point was created are undone, and execution continues with the next alternative of that choice-point. This execution strategy is called chronological backtracking. For example:\n\nmother_child(trude, sally).\n \nfather_child(tom, sally).\nfather_child(tom, erica).\nfather_child(mike, tom).\n \nsibling(X, Y)      :- parent_child(Z, X), parent_child(Z, Y).\n \nparent_child(X, Y) :- father_child(X, Y).\nparent_child(X, Y) :- mother_child(X, Y).\nThis results in the following query being evaluated as true:\n\n ?- sibling(sally, erica).\n Yes\nThis is obtained as follows: Initially, the only matching clause-head for the query sibling(sally, erica) is the first one, so proving the query is equivalent to proving the body of that clause with the appropriate variable bindings in place, i.e., the conjunction (parent_child(Z,sally), parent_child(Z,erica)). The next goal to be proved is the leftmost one of this conjunction, i.e., parent_child(Z, sally). Two clause heads match this goal. The system creates a choice-point and tries the first alternative, whose body is father_child(Z, sally). This goal can be proved using the fact father_child(tom, sally), so the binding Z = tom is generated, and the next goal to be proved is the second part of the above conjunction: parent_child(tom, erica). Again, this can be proved by the corresponding fact. Since all goals could be proved, the query succeeds. Since the query contained no variables, no bindings are reported to the user. A query with variables, like:\n\n?- father_child(Father, Child).\nenumerates all valid answers on backtracking.\n\nNotice that with the code as stated above, the query ?- sibling(sally, sally). also succeeds. One would insert additional goals to describe the relevant restrictions, if desired.\n\nLoops and recursion[edit]\nIterative algorithms can be implemented by means of recursive predicates.\n\nNegation[edit]\nThe built-in Prolog predicate \\+/1 provides negation as failure, which allows for non-monotonic reasoning. The goal \\+ illegal(X) in the rule\n\nlegal(X) :- \\+ illegal(X).\nis evaluated as follows: Prolog attempts to prove illegal(X). If a proof for that goal can be found, the original goal (i.e., \\+ illegal(X)) fails. If no proof can be found, the original goal succeeds. Therefore, the \\+/1 prefix operator is called the \"not provable\" operator, since the query ?- \\+ Goal. succeeds if Goal is not provable. This kind of negation is sound if its argument is \"ground\" (i.e. contains no variables). Soundness is lost if the argument contains variables and the proof procedure is complete. In particular, the query ?- legal(X). can now not be used to enumerate all things that are legal.\n\nProgramming in Prolog[edit]\nIn Prolog, loading code is referred to as consulting. Prolog can be used interactively by entering queries at the Prolog prompt ?-. If there is no solution, Prolog writes no. If a solution exists then it is printed. If there are multiple solutions to the query, then these can be requested by entering a semi-colon ;. There are guidelines on good programming practice to improve code efficiency, readability and maintainability.[12]\n\nHere follow some example programs written in Prolog.\n\nHello world[edit]\nAn example of a query:\n\n?- write('Hello world!'), nl.\nHello world!\ntrue.\n\n?-\nCompiler optimization[edit]\nAny computation can be expressed declaratively as a sequence of state transitions. As an example, an optimizing compiler with three optimization passes could be implemented as a relation between an initial program and its optimized form:\n\nprogram_optimized(Prog0, Prog) :-\n    optimization_pass_1(Prog0, Prog1),\n    optimization_pass_2(Prog1, Prog2),\n    optimization_pass_3(Prog2, Prog).\nor equivalently using DCG notation:\n\nprogram_optimized --> optimization_pass_1, optimization_pass_2, optimization_pass_3.\nQuicksort[edit]\nThe quicksort sorting algorithm, relating a list to its sorted version:\n\npartition([], _, [], []).\npartition([X|Xs], Pivot, Smalls, Bigs) :-\n    (   X @< Pivot ->\n        Smalls = [X|Rest],\n        partition(Xs, Pivot, Rest, Bigs)\n    ;   Bigs = [X|Rest],\n        partition(Xs, Pivot, Smalls, Rest)\n    ).\n \nquicksort([])     --> [].\nquicksort([X|Xs]) -->\n    { partition(Xs, X, Smaller, Bigger) },\n    quicksort(Smaller), [X], quicksort(Bigger).\nDesign patterns[edit]\nA design pattern is a general reusable solution to a commonly occurring problem in software design. In Prolog, design patterns go under various names: skeletons and techniques,[13][14] cliches,[15] program schemata,[16] and logic description schemata.[17] An alternative to design patterns is higher order programming.[18]\n\nHigher-order programming[edit]\nMain articles: Higher-order logic and Higher-order programming\nA higher-order predicate is a predicate that takes one or more other predicates as arguments. Although support for higher-order programming takes Prolog outside the domain of first-order logic, which does not allow quantification over predicates,[19] ISO Prolog now has some built-in higher-order predicates such as call/1, call/2, call/3, findall/3, setof/3, and bagof/3.[20] Furthermore, since arbitrary Prolog goals can be constructed and evaluated at run-time, it is easy to write higher-order predicates like maplist/2, which applies an arbitrary predicate to each member of a given list, and sublist/3, which filters elements that satisfy a given predicate, also allowing for currying.[18]\n\nTo convert solutions from temporal representation (answer substitutions on backtracking) to spatial representation (terms), Prolog has various all-solutions predicates that collect all answer substitutions of a given query in a list. This can be used for list comprehension. For example, perfect numbers equal the sum of their proper divisors:\n\n perfect(N) :-\n     between(1, inf, N), U is N // 2,\n     findall(D, (between(1,U,D), N mod D =:= 0), Ds),\n     sumlist(Ds, N).\nThis can be used to enumerate perfect numbers, and also to check whether a number is perfect.\n\nAs another example, the predicate maplist applies a predicate P to all corresponding positions in a pair of lists:\n\nmaplist(_, [], []).\nmaplist(P, [X|Xs], [Y|Ys]) :-\n   call(P, X, Y),\n   maplist(P, Xs, Ys).\nWhen P is a predicate that for all X, P(X,Y) unifies Y with a single unique value, maplist(P, Xs, Ys) is equivalent to applying the map function in functional programming as Ys = map(Function, Xs).\n\nHigher-order programming style in Prolog was pioneered in HiLog and λProlog.\n\nModules[edit]\nFor programming in the large, Prolog provides a module system. The module system is standardised by ISO.[21] However, not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers.[22] Consequently, modules written on one Prolog compiler will not necessarily work on others.\n\nParsing[edit]\nMain articles: Prolog syntax and semantics § Definite clause grammars, and Definite clause grammar\nThere is a special notation called definite clause grammars (DCGs). A rule defined via -->/2 instead of :-/2 is expanded by the preprocessor (expand_term/2, a facility analogous to macros in other languages) according to a few straightforward rewriting rules, resulting in ordinary Prolog clauses. Most notably, the rewriting equips the predicate with two additional arguments, which can be used to implicitly thread state around,[clarification needed] analogous to monads in other languages. DCGs are often used to write parsers or list generators, as they also provide a convenient interface to difference lists.\n\nMeta-interpreters and reflection[edit]\nProlog is a homoiconic language and provides many facilities for reflection. Its implicit execution strategy makes it possible to write a concise meta-circular evaluator (also called meta-interpreter) for pure Prolog code:\n\nsolve(true).\nsolve((Subgoal1,Subgoal2)) :- \n    solve(Subgoal1),\n    solve(Subgoal2).\nsolve(Head) :- \n    clause(Head, Body),\n    solve(Body).\nwhere true represents an empty conjunction, and clause(Head, Body) unifies with clauses in the database of the form Head :- Body.\n\nSince Prolog programs are themselves sequences of Prolog terms (:-/2 is an infix operator) that are easily read and inspected using built-in mechanisms (like read/1), it is possible to write customized interpreters that augment Prolog with domain-specific features. For example, Sterling and Shapiro present a meta-interpreter that performs reasoning with uncertainty, reproduced here with slight modifications:[23]:330\n\nsolve(true, 1) :- !.\nsolve((Subgoal1,Subgoal2), Certainty) :-\n    !,\n    solve(Subgoal1, Certainty1),\n    solve(Subgoal2, Certainty2),\n    Certainty is min(Certainty1, Certainty2).\nsolve(Goal, 1) :-\n    builtin(Goal), !, \n    Goal.\nsolve(Head, Certainty) :-\n    clause_cf(Head, Body, Certainty1),\n    solve(Body, Certainty2),\n    Certainty is Certainty1 * Certainty2.\nThis interpreter uses a table of built-in Prolog predicates of the form[23]:327\n\nbuiltin(A is B).\nbuiltin(read(X)).\n% etc.\nand clauses represented as clause_cf(Head, Body, Certainty). Given those, it can be called as solve(Goal, Certainty) to execute Goal and obtain a measure of certainty about the result.\n\nTuring completeness[edit]\nPure Prolog is based on a subset of first-order predicate logic, Horn clauses, which is Turing-complete. Turing completeness of Prolog can be shown by using it to simulate a Turing machine:\n\nturing(Tape0, Tape) :-\n    perform(q0, [], Ls, Tape0, Rs),\n    reverse(Ls, Ls1),\n    append(Ls1, Rs, Tape).\n \nperform(qf, Ls, Ls, Rs, Rs) :- !.\nperform(Q0, Ls0, Ls, Rs0, Rs) :-\n    symbol(Rs0, Sym, RsRest),\n    once(rule(Q0, Sym, Q1, NewSym, Action)),\n    action(Action, Ls0, Ls1, [NewSym|RsRest], Rs1),\n    perform(Q1, Ls1, Ls, Rs1, Rs).\n \nsymbol([], b, []).\nsymbol([Sym|Rs], Sym, Rs).\n \naction(left, Ls0, Ls, Rs0, Rs) :- left(Ls0, Ls, Rs0, Rs).\naction(stay, Ls, Ls, Rs, Rs).\naction(right, Ls0, [Sym|Ls0], [Sym|Rs], Rs).\n \nleft([], [], Rs0, [b|Rs0]).\nleft([L|Ls], Ls, Rs, [L|Rs]).\nA simple example Turing machine is specified by the facts:\n\nrule(q0, 1, q0, 1, right).\nrule(q0, b, qf, 1, stay).\nThis machine performs incrementation by one of a number in unary encoding: It loops over any number of \"1\" cells and appends an additional \"1\" at the end. Example query and result:\n\n?- turing([1,1,1], Ts).\nTs = [1, 1, 1, 1] ;\nThis illustrates how any computation can be expressed declaratively as a sequence of state transitions, implemented in Prolog as a relation between successive states of interest.\n\nImplementation[edit]\nFurther information: Comparison of Prolog implementations\nISO Prolog[edit]\nThe ISO Prolog standard consists of two parts. ISO/IEC 13211-1,[20][24] published in 1995, aims to standardize the existing practices of the many implementations of the core elements of Prolog. It has clarified aspects of the language that were previously ambiguous and leads to portable programs. There are two corrigenda: Cor.1:2007[25] and Cor.2:2012.[26] ISO/IEC 13211-2,[20] published in 2000, adds support for modules to the standard. The standard is maintained by the ISO/IEC JTC1/SC22/WG17[27] working group. ANSI X3J17 is the US Technical Advisory Group for the standard.[28]\n\nCompilation[edit]\nFor efficiency, Prolog code is typically compiled to abstract machine code, often influenced by the register-based Warren Abstract Machine (WAM) instruction set.[29] Some implementations employ abstract interpretation to derive type and mode information of predicates at compile time, or compile to real machine code for high performance.[30] Devising efficient implementation methods for Prolog code is a field of active research in the logic programming community, and various other execution methods are employed in some implementations. These include clause binarization and stack-based virtual machines.[citation needed]\n\nTail recursion[edit]\nProlog systems typically implement a well-known optimization method called tail call optimization (TCO) for deterministic predicates exhibiting tail recursion or, more generally, tail calls: A clause's stack frame is discarded before performing a call in a tail position. Therefore, deterministic tail-recursive predicates are executed with constant stack space, like loops in other languages.\n\nTerm indexing[edit]\nMain article: Term indexing\nFinding clauses that are unifiable with a term in a query is linear in the number of clauses. Term indexing uses a data structure that enables sub-linear-time lookups.[31] Indexing only affects program performance, it does not affect semantics. Most Prologs only use indexing on the first term, as indexing on all terms is expensive, but techniques based on field-encoded words or superimposed codewords provide fast indexing across the full query and head.[32][33]\n\nHashing[edit]\nSome Prolog systems, such as LPA Prolog and SWI-Prolog, now implement hashing to help handle large datasets more efficiently. This tends to yield very large performance gains when working with large corpora such as WordNet.\n\nTabling[edit]\nSome Prolog systems, (BProlog, XSB Yap, B-Prolog and Ciao), implement a memoization method called tabling, which frees the user from manually storing intermediate results.[34][35]\n\nSubgoals encountered in a query evaluation are maintained in a table, along with answers to these subgoals. If a subgoal is re-encountered, the evaluation reuses information from the table rather than re-performing resolution against program clauses.[36]\n\nTabling is a space-time tradeoff; execution time can be reduced by using more memory to store intermediate results.\n\nImplementation in hardware[edit]\nDuring the Fifth Generation Computer Systems project, there were attempts to implement Prolog in hardware with the aim of achieving faster execution with dedicated architectures.[37][38][39] Furthermore, Prolog has a number of properties that may allow speed-up through parallel execution.[40] A more recent approach has been to compile restricted Prolog programs to a field programmable gate array.[41] However, rapid progress in general-purpose hardware has consistently overtaken more specialised architectures.\n\nLimitations[edit]\nAlthough Prolog is widely used in research and education, Prolog and other logic programming languages have not had a significant impact on the computer industry in general.[42] Most applications are small by industrial standards, with few exceeding 100,000 lines of code.[42][43] Programming in the large is considered to be complicated because not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers.[22] Portability of Prolog code across implementations has also been a problem, but developments since 2007 have meant: \"the portability within the family of Edinburgh/Quintus derived Prolog implementations is good enough to allow for maintaining portable real-world applications.\"[44]\n\nSoftware developed in Prolog has been criticised for having a high performance penalty compared to conventional programming languages. In particular, Prolog's non-deterministic evaluation strategy can be problematic when programming deterministic computations, or when even using \"don't care non-determinism\" (where a single choice is made instead of backtracking over all possibilities). Cuts and other language constructs may have to be used to achieve desirable performance, destroying one of Prolog's main attractions, the ability to run programs \"backwards and forwards\".[45]\n\nProlog is not purely declarative: because of constructs like the cut operator, a procedural reading of a Prolog program is needed to understand it.[46] The order of clauses in a Prolog program is significant, as the execution strategy of the language depends on it.[47] Other logic programming languages, such as Datalog, are truly declarative but restrict the language. As a result, many practical Prolog programs are written to conform to Prolog's depth-first search order, rather than as purely declarative logic programs.[45]\n\nExtensions[edit]\nVarious implementations have been developed from Prolog to extend logic programming capabilities in numerous directions. These include types, modes, constraint logic programming (CLP), object-oriented logic programming (OOLP), concurrency, linear logic (LLP), functional and higher-order logic programming capabilities, plus interoperability with knowledge bases:\n\nTypes[edit]\nProlog is an untyped language. Attempts to introduce types date back to the 1980s,[48][49] and as of 2008 there are still attempts to extend Prolog with types.[50] Type information is useful not only for type safety but also for reasoning about Prolog programs.[51]\n\nModes[edit]\nMode specifier\tInterpretation\n+\tnonvar on entry\n-\tvar on entry\n?\tNot specified\nThe syntax of Prolog does not specify which arguments of a predicate are inputs and which are outputs.[52] However, this information is significant and it is recommended that it be included in the comments.[53] Modes provide valuable information when reasoning about Prolog programs[51] and can also be used to accelerate execution.[54]\n\nConstraints[edit]\nConstraint logic programming extends Prolog to include concepts from constraint satisfaction.[55][56] A constraint logic program allows constraints in the body of clauses, such as: A(X,Y) :- X+Y>0. It is suited to large-scale combinatorial optimisation problems.[57] and is thus useful for applications in industrial settings, such as automated time-tabling and production scheduling. Most Prolog systems ship with at least one constraint solver for finite domains, and often also with solvers for other domains like rational numbers.\n\nObject-orientation[edit]\nFlora-2 is an object-oriented knowledge representation and reasoning system based on F-logic and incorporates HiLog, Transaction logic, and defeasible reasoning.\n\nLogtalk is an object-oriented logic programming language that can use most Prolog implementations as a back-end compiler. As a multi-paradigm language, it includes support for both prototypes and classes.\n\nOblog is a small, portable, object-oriented extension to Prolog by Margaret McDougall of EdCAAD, University of Edinburgh.\n\nObjlog was a frame-based language combining objects and Prolog II from CNRS, Marseille, France.\n\nProlog++ was developed by Logic Programming Associates and first released in 1989 for MS-DOS PCs. Support for other platforms was added, and a second version was released in 1995. A book about Prolog++ by Chris Moss was published by Addison-Wesley in 1994.\n\nGraphics[edit]\nProlog systems that provide a graphics library are SWI-prolog,[58] Visual-prolog, LPA Prolog for Windows and B-Prolog.\n\nConcurrency[edit]\nProlog-MPI is an open-source SWI-Prolog extension for distributed computing over the Message Passing Interface.[59] Also there are various concurrent Prolog programming languages.[60]\n\nWeb programming[edit]\nSome Prolog implementations, notably SWI-Prolog and Ciao, support server-side web programming with support for web protocols, HTML and XML.[61] There are also extensions to support semantic web formats such as RDF and OWL.[62][63] Prolog has also been suggested as a client-side language.[64]\n\nAdobe Flash[edit]\nCedar is a free and basic Prolog interpreter. From version 4 and above Cedar has a FCA (Flash Cedar App) support. This provides a new platform to programming in Prolog through ActionScript.\n\nOther[edit]\nF-logic extends Prolog with frames/objects for knowledge representation.\nTransaction logic extends Prolog with a logical theory of state-changing update operators. It has both a model-theoretic and procedural semantics.\nOW Prolog has been created in order to answer Prolog's lack of graphics and interface.\nInterfaces to other languages[edit]\nFrameworks exist which can bridge between Prolog and other languages:\n\nThe LPA Intelligence Server allows the embedding of LPA Prolog within C, C#, C++, Java, VB, Delphi, .Net, Lua, Python and other languages. It exploits the dedicated string data-type which LPA Prolog provides\nThe Logic Server API allows both the extension and embedding of Prolog in C, C++, Java, VB, Delphi, .NET and any language/environment which can call a .dll or .so. It is implemented for Amzi! Prolog Amzi! Prolog + Logic Server but the API specification can be made available for any implementation.\nJPL is a bi-directional Java Prolog bridge which ships with SWI-Prolog by default, allowing Java and Prolog to call each other (recursively). It is known to have good concurrency support and is under active development.\nInterProlog, a programming library bridge between Java and Prolog, implementing bi-directional predicate/method calling between both languages. Java objects can be mapped into Prolog terms and vice versa. Allows the development of GUIs and other functionality in Java while leaving logic processing in the Prolog layer. Supports XSB, with support for SWI-Prolog and YAP planned for 2013.\nProva provides native syntax integration with Java, agent messaging and reaction rules. Prova positions itself as a rule-based scripting (RBS) system for middleware. The language breaks new ground in combining imperative and declarative programming.\nPROL An embeddable Prolog engine for Java. It includes a small IDE and a few libraries.\nGNU Prolog for Java is an implementation of ISO Prolog as a Java library (gnu.prolog)\nCiao provides interfaces to C, C++, Java, and relational databases.\nC#-Prolog is a Prolog interpreter written in (managed) C#. Can easily be integrated in C# programs. Characteristics: reliable and fairly fast interpreter, command line interface, Windows-interface, builtin DCG, XML-predicates, SQL-predicates, extendible. The complete source code is available, including a parser generator that can be used for adding special purpose extensions.\nJekejeke Prolog API provides tightly coupled concurrent call-in and call-out facilities between Prolog and Java or Android, with the marked possibility to create individual knowledge base objects. It can be used to embed the ISO Prolog interpreter in standalones, applets, servlets, APKs, etc..\nA Warren Abstract Machine for PHP A Prolog compiler and interpreter in PHP 5.3. A library that can be used standalone or within Symfony2.1 framework\nHistory[edit]\nThe name Prolog was chosen by Philippe Roussel as an abbreviation for programmation en logique (French for programming in logic). It was created around 1972 by Alain Colmerauer with Philippe Roussel, based on Robert Kowalski's procedural interpretation of Horn clauses. It was motivated in part by the desire to reconcile the use of logic as a declarative knowledge representation language with the procedural representation of knowledge that was popular in North America in the late 1960s and early 1970s. According to Robert Kowalski, the first Prolog system was developed in 1972 by Colmerauer and Phillipe Roussel.[5] The first implementations of Prolog were interpreters. However, David H. D. Warren created the Warren Abstract Machine, an early and influential Prolog compiler which came to define the \"Edinburgh Prolog\" dialect which served as the basis for the syntax of most modern implementations.\n\nEuropean AI researchers favored Prolog while Americans favored Lisp, reportedly causing many nationalistic debates on the merits of the languages.[65] Much of the modern development of Prolog came from the impetus of the Fifth Generation Computer Systems project (FGCS), which developed a variant of Prolog named Kernel Language for its first operating system.\n\nPure Prolog was originally restricted to the use of a resolution theorem prover with Horn clauses of the form:\n\nH :- B1, ..., Bn.\nThe application of the theorem-prover treats such clauses as procedures:\n\nto show/solve H, show/solve B1 and ... and Bn.\nPure Prolog was soon extended, however, to include negation as failure, in which negative conditions of the form not(Bi) are shown by trying and failing to solve the corresponding positive conditions Bi.\n\nSubsequent extensions of Prolog by the original team introduced constraint logic programming abilities into the implementations.\n\nUse in industry[edit]\nProlog has been used in Watson. Watson uses IBM's DeepQA software and the Apache UIMA (Unstructured Information Management Architecture) framework. The system was written in various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using Apache Hadoop framework to provide distributed computing. Prolog is used for pattern matching over natural language parse trees. The developers have stated: \"We required a language in which we could conveniently express pattern matching rules over the parse trees and other annotations (such as named entity recognition results), and a technology that could execute these rules very efficiently. We found that Prolog was the ideal choice for the language due to its simplicity and expressiveness.\"[11]\n\nSee also[edit]\nComparison of Prolog implementations\nLogico-linguistic modeling. A method for building knowledge-based system that uses Prolog.\nAnswer set programming. A fully declarative approach to logic programming.\nAssociation for Logic Programming\nRelated languages[edit]\nThe Gödel language is a strongly typed implementation of concurrent constraint logic programming. It is built on SICStus Prolog.\nVisual Prolog, formerly known as PDC Prolog and Turbo Prolog, is a strongly typed object-oriented dialect of Prolog, which is very different from standard Prolog. As Turbo Prolog, it was marketed by Borland, but it is now developed and marketed by the Danish firm PDC (Prolog Development Center) that originally produced it.\nDatalog is a subset of Prolog. It is limited to relationships that may be stratified and does not allow compound terms. In contrast to Prolog, Datalog is not Turing-complete.\nMercury is an offshoot of Prolog geared toward software engineering in the large with a static, polymorphic type system, as well as a mode and determinism system.\nCSC GraphTalk is a proprietary implementation of Warren's Abstract Machine, with additional object-oriented properties.\nIn some ways[which?] Prolog is a subset of Planner. The ideas in Planner were later further developed in the Scientific Community Metaphor.\nAgentSpeak is a variant of Prolog for programming agent behavior in multi-agent systems.\nErlang began life with a Prolog-based implementation and maintains much of Prolog's unification-based syntax.",
          "type": "compiled",
          "plid": 11
        },
        {
          "name": "Clojure",
          "details": "(pronunciation: /ˈkloʊʒɜːr/, like \"closure\"[6]) is a dialect of the Lisp programming language created by Rich Hickey.[7] Clojure is a general-purpose programming language with an emphasis on functional programming.[8] It runs on the Java virtual machine, Common Language Runtime,[9] and JavaScript[10] engines. Like other Lisps, Clojure treats code as data and has a macro system.[11] The current development process is community-driven,[12] overseen by Rich Hickey as its benevolent dictator for life (BDFL).[13]\n\nClojure encourages immutability and immutable data structures. While its type system is entirely dynamic, recent efforts have also sought the implementation of gradual typing.[14] Clojure encourages programmers to be explicit about managing state and identity.[15] This focus on programming with immutable values and explicit progression-of-time constructs is intended to facilitate developing more robust programs, especially multithreaded ones.[16][17]\n\nClojure is used in industry by firms such as Walmart,[18] Puppet Labs,[19] and other large software firms.[20] Commercial support for Clojure is provided by Cognitect.[20] Annual Clojure conferences are organised every year across the globe, the most famous of them being Clojure/conj (US east coast),[21] Clojure/West (US west coast),[22] and EuroClojure (Europe).[23]\n\nThe latest stable version of Clojure is 1.8,[24] released on January 19, 2016. The first stable release was version 1.0, released on May 4, 2009.[25] Clojure is free software released under the Eclipse Public License.[26]\n\nContents  [hide] \n1\tHistory and development process\n2\tDesign philosophy\n3\tFeatures\n4\tPlatforms and popularity\n5\tExamples\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory and development process[edit]\n\nRich Hickey in San Francisco\nRich Hickey is the creator of the Clojure language.[7] Before Clojure, he developed dotLisp, a similar project based on the .NET Framework,[27] and three earlier attempts to provide interoperability between Lisp and Java: a Java foreign language interface for Common Lisp (jfli),[28] A Foreign Object Interface for Lisp (FOIL),[29] and a Lisp-friendly interface to Java Servlets (Lisplets).[30]\n\nHickey spent about 2½ years working on Clojure before releasing it publicly, much of that time working exclusively on Clojure with no outside funding. At the end of this time, Hickey sent an email announcing the language to some friends in the Common Lisp community.\n\nThe development process is community-driven[12] and is managed at the Clojure Community website.[31] The website contains planning documents and an issue tracker where bugs may be filed. General development discussion occurs at the Clojure Dev Google Group.[32] While anyone can submit bug reports and ideas, to contribute patches one must sign the Clojure Contributor agreement.[33] JIRA tickets are processed by a team of screeners and finally Rich Hickey approves the changes.[34]\n\nDesign philosophy[edit]\nRich Hickey developed Clojure because he wanted a modern Lisp for functional programming, symbiotic with the established Java platform, and designed for concurrency.[16][17][35]\n\nClojure's approach to state is characterized by the concept of identities,[36] which are represented as a series of immutable states over time. Since states are immutable values, any number of workers can operate on them in parallel, and concurrency becomes a question of managing changes from one state to another. For this purpose, Clojure provides several mutable reference types, each having well-defined semantics for the transition between states.[15][36]\n\nFeatures[edit]\nVersion\tRelease date\tMajor features added\n2007-10-16[37]\tInitial public release\n1.0\t2009-05-04[38]\tFirst stable release[39]\n1.1\t2009-12-31[38]\tFutures[40]\n1.2\t2010-08-19[38]\tProtocols[41]\n1.3\t2011-09-23[38]\tEnhanced primitive support[42]\n1.4\t2012-04-15[38]\tReader literals\n1.5\t2013-03-01[38]\tReducers\n1.6\t2014-03-25[43]\tJava API, improved hashing algorithms\n1.7\t2015-06-30[24]\tTransducers, reader conditionals\n1.8\t2016-01-19[44]\tAdditional string functions, direct linking, socket server\n1.9\tfuture\nLatest versionFuture release\nClojure runs on the Java virtual machine and as a result integrates with Java and fully supports calling Java code from Clojure,[45] and Clojure code can be called from Java also.[46] The community uses Leiningen[47] for project automation, providing support for Maven integration. Leiningen handles project package management and dependencies and is configured using Clojure syntax.[47]\n\nLike most other Lisps, Clojure's syntax is built on S-expressions that are first parsed into data structures by a reader before being compiled.[48] Clojure's reader supports literal syntax for maps, sets and vectors in addition to lists, and these are compiled to the mentioned structures directly.[48] Clojure is a Lisp-1 and is not intended to be code-compatible with other dialects of Lisp, since it uses its own set of data structures incompatible with other Lisps.[48][49]\n\nAs a Lisp dialect, Clojure supports functions as first-class objects, a read–eval–print loop (REPL), and a macro system.[50] Clojure's macro system is very similar to that in Common Lisp with the exception that Clojure's version of the backquote (called \"syntax quote\") qualifies symbols with their namespace. This helps prevent unintended name capture, as binding to namespace-qualified names is forbidden. It is possible to force a capturing macro expansion, but it must be done explicitly. Clojure does not allow user-defined reader macros, but the reader supports a more constrained form of syntactic extension.[51] Clojure supports multimethods[52] and for interface-like abstractions has a protocol[53] based polymorphism and data type system using records,[54] providing high-performance and dynamic polymorphism designed to avoid the expression problem.\n\nClojure has support for lazy sequences and encourages the principle of immutability and persistent data structures. As a functional language, emphasis is placed on recursion and higher-order functions instead of side-effect-based looping. Automatic tail call optimization is not supported as the JVM does not support it natively;[55] it is possible to do so explicitly by using the recur keyword.[56] For parallel and concurrent programming Clojure provides software transactional memory[57] a reactive agent system,[58] and channel-based concurrent programming.[59]\n\nRecently Clojure introduced reader conditionals by allowing the embedding of Clojure and ClojureScript code in the same namespace.[24][60] Transducers have been added as a way for composing transformations. Transducers enable higher-order functions such as map and fold to generalize over any source of input data, as traditionally these functions operate on sequences, transducers allow them to work on channels and let the user define their own models for transduction.[61][62][63]\n\nPlatforms and popularity[edit]\nThe primary platform of Clojure is the JVM,[8][45] but other target implementations exist. The most notable of these are ClojureScript,[10] which compiles to JavaScript, and ClojureCLR,[64] a full port to the Common Language Runtime, interoperable with the .NET ecosystem. A survey of the Clojure community with 1,060 respondents conducted in 2013[65] found that 47% of respondents used both Clojure and ClojureScript when working with Clojure. In 2014 this number had increased to 55%,[66] in 2015, based on 2,445 respondents, to 66%.[67] Popular ClojureScript projects include implementations of the React library such as Reagent and Om.[68]\n\nClojure has also been used for creative computing, including visual art, music, games, and poetry.[69]\n\nVariant implementations of the Clojure language have been developed for platforms other than the above:\n\nlas3r,[70] a subset of Clojure that runs on the ActionScript Virtual Machine (the Adobe Flash Player platform)\nclojure-py,[71] Clojure in pure Python\nrouge,[72] Clojure atop YARV in Ruby\nCljPerl,[73] Clojure atop Perl\nPixie, Clojure-inspired Lisp dialect written in RPython\nExamples[edit]\nHello world:\n\n(println \"Hello world!\")\nDefining a function:\n\n(defn square [x]\n  (* x x))\nGUI \"Hello world\" by calling the Java Swing library:\n\n(javax.swing.JOptionPane/showMessageDialog nil \"Hello World\")\nUsing Unicode (Hello 世 (\"World\") using the CJK code point for that word):\n\n(println (str \"Hello, \" \\u4e16)) ; to the console\n(javax.swing.JOptionPane/showMessageDialog nil (str \"Hello, \" \\u4e16 \"!\")) ; using Java GUI\nA thread-safe generator of unique serial numbers (though, like many other Lisp dialects, Clojure has a built-in gensym function that it uses internally):\n\n(let [i (atom 0)]\n  (defn generate-unique-id\n    \"Returns a distinct numeric ID for each call.\"\n    []\n    (swap! i inc)))\nAn anonymous subclass of java.io.Writer that doesn't write to anything, and a macro using it to silence all prints within it:\n\n(def bit-bucket-writer\n  (proxy [java.io.Writer] []\n    (write [buf] nil)\n    (close []    nil)\n    (flush []    nil)))\n\n(defmacro noprint\n  \"Evaluates the given expressions with all printing to *out* silenced.\"\n  [& forms]\n  `(binding [*out* bit-bucket-writer]\n     ~@forms))\n\n(noprint\n  (println \"Hello, nobody!\"))\n10 threads manipulating one shared data structure, which consists of 100 vectors each one containing 10 (initially sequential) unique numbers. Each thread then repeatedly selects two random positions in two random vectors and swaps them. All changes to the vectors occur in transactions by making use of Clojure's software transactional memory system.\n\n(defn run [nvecs nitems nthreads niters]\n  (let [vec-refs (->> (range (* nvecs nitems)) (partition nitems) (map (comp ref vec)) vec)\n        swap #(let [v1 (rand-int nvecs)\n                    v2 (rand-int nvecs)\n                    i1 (rand-int nitems)\n                    i2 (rand-int nitems)]\n                (dosync\n                 (let [tmp (nth @(vec-refs v1) i1)]\n                   (alter (vec-refs v1) assoc i1 (nth @(vec-refs v2) i2))\n                   (alter (vec-refs v2) assoc i2 tmp))))\n        report #(let [derefed (map deref vec-refs)]\n                  (prn derefed)\n                  (println \"Distinct:\" (->> derefed (apply concat) distinct count)))]\n    (report)\n    (dorun (apply pcalls (repeat nthreads #(dotimes [_ niters] (swap)))))\n    (report)))\n\n(run 100 10 10 100000)\nOutput of prior example:\n\n([0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] ...\n[990 991 992 993 994 995 996 997 998 999])\nDistinct: 1000\n([382 318 466 963 619 22 21 273 45 596] [808 639 804 471 394 904 952 75 289 778] ...\n[484 216 622 139 651 592 379 228 242 355])\nDistinct: 1000",
          "type": "functional",
          "plid": 13
        },
        {
          "name": "Erlang",
          "details": "Erlang (/ˈɜːrlæŋ/ er-lang) is a general-purpose, concurrent, functional programming language. It is also a garbage-collected runtime system. The sequential subset of Erlang supports eager evaluation, single assignment, and dynamic typing. Erlang is known for its designs that are well suited for systems with the following characteristics:\n\nDistributed\nFault-tolerant\nSoft real-time,\nHighly available, non-stop applications\nHot swapping, where code can be changed without stopping a system.[3]\nIt was originally a proprietary language within Ericsson, developed by Joe Armstrong, Robert Virding and Mike Williams in 1986,[4] but was released as open source in 1998.[5][6] Erlang, along with OTP, a collection of middleware and libraries in Erlang, are now supported and maintained by the OTP product unit at Ericsson and have been widely referred to as Erlang/OTP.\n\nContents  [hide] \n1\tHistory\n1.1\tErlang Worldview\n1.2\tUsage\n2\tFunctional programming examples\n3\tData types\n4\tConcurrency and distribution orientation\n5\tImplementation\n6\tHot code loading and modules\n7\tDistribution\n8\tErlang in Industry\n8.1\tSoftware projects written in Erlang\n8.2\tCompanies using Erlang\n9\tVariants\n10\tReferences\n11\tFurther reading\n12\tExternal links\nHistory[edit]\nThe name \"Erlang\", attributed to Bjarne Däcker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang or the ubiquitous use of the unit named for him, and (initially at least) simultaneously as a syllabic abbreviation of \"Ericsson Language\".[4][7]\n\nErlang was designed with the aim of improving the development of telephony applications. The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. By 1988 Erlang had proven that it was suitable for prototyping telephone exchanges, but the Prolog interpreter was far too slow. One group within Ericsson estimated that it would need to be 40 times faster in order to be suitable for production use. In 1992 work began on the BEAM virtual machine which compiles Erlang to C using a mix of natively compiled code and threaded code to strike a balance between performance and disk space.[8] According to Armstrong, the language went from lab product to real applications following the collapse of the next-generation AXE exchange named AXE-N in 1995. As a result, Erlang was chosen for the next ATM exchange AXD.[4]\n\nIn 1998 Ericsson announced the AXD301 switch, containing over a million lines of Erlang and reported to achieve a high availability of nine \"9\"s.[9] Shortly thereafter, Ericsson Radio Systems banned the in-house use of Erlang for new products, citing a preference for non-proprietary languages. The ban caused Armstrong and others to leave Ericsson.[10] The implementation was open-sourced at the end of the year.[4] Ericsson eventually lifted the ban; it re-hired Armstrong in 2004.[10]\n\nIn 2006, native symmetric multiprocessing support was added to the runtime system and virtual machine.[4]\n\nErlang Worldview[edit]\nThe Erlang view of the world, as Joe Armstrong, co-inventor of Erlang summarized in his PhD thesis:[11]\n\nEverything is a process.\nProcesses are strongly isolated.\nProcess creation and destruction is a lightweight operation.\nMessage passing is the only way for processes to interact.\nProcesses have unique names.\nIf you know the name of a process you can send it a message.\nProcesses share no resources.\nError handling is non-local.\nProcesses do what they are supposed to do or fail.\nJoe Armstrong pointed out in an interview with Rackspace in 2013:[12] “If Java is the right one to run anywhere, then Erlang is the right one to run forever.”\n\nUsage[edit]\nErlang has now been adopted by companies worldwide, including Nortel and T-Mobile. Erlang is used in Ericsson’s support nodes, and in GPRS, 3G and LTE mobile networks worldwide.[13]\n\nAs Tim Bray, director of Web Technologies at Sun Microsystems, expressed in his keynote at OSCON in July 2008:\n\nIf somebody came to me and wanted to pay me a lot of money to build a large scale message handling system that really had to be up all the time, could never afford to go down for years at a time, I would unhesitatingly choose Erlang to build it in.\n\nFunctional programming examples[edit]\nAn Erlang function that uses recursion to count to ten:[14]\n\n1 -module(count_to_ten).\n2 -export([count_to_ten/0]).\n3  \n4 count_to_ten() -> do_count(0).\n5  \n6 do_count(10) -> 10;\n7 do_count(code) -> do_count(code + 1).\nA factorial algorithm implemented in Erlang:\n\n-module(fact).    % This is the file 'fact.erl', the module and the filename must match\n-export([fac/1]). % This exports the function 'fac' of arity 1 (1 parameter, no type, no name)\n\nfac(0) -> 1; % If 0, then return 1, otherwise (note the semicolon ; meaning 'else')\nfac(N) when N > 0, is_integer(N) -> N * fac(N-1).\n% Recursively determine, then return the result\n% (note the period . meaning 'endif' or 'function end')\n%% This function will crash if anything other than a nonnegative integer is given.\n%% It illustrates the \"Let it crash\" philosophy of Erlang.\nA Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. This algorithm is rather slow.[15]):\n\n-module(fib).    % This is the file 'fib.erl', the module and the filename must match\n-export([fib/1]). % This exports the function 'fib' of arity 1\n\nfib(1) -> 1; % If 1, then return 1, otherwise (note the semicolon ; meaning 'else')\nfib(2) -> 1; % If 2, then return 1, otherwise\nfib(N) -> fib(N - 2) + fib(N - 1).\nQuicksort in Erlang, using list comprehension:[16]\n\n%% qsort:qsort(List)\n%% Sort a list of items\n-module(qsort).     % This is the file 'qsort.erl'\n-export([qsort/1]). % A function 'qsort' with 1 parameter is exported (no type, no name)\n\nqsort([]) -> []; % If the list [] is empty, return an empty list (nothing to sort)\nqsort([Pivot|Rest]) ->\n    % Compose recursively a list with 'Front' for all elements that should be before 'Pivot'\n    % then 'Pivot' then 'Back' for all elements that should be after 'Pivot'\n    qsort([Front || Front <- Rest, Front < Pivot]) ++ \n    [Pivot] ++\n    qsort([Back || Back <- Rest, Back >= Pivot]).\nThe above example recursively invokes the function qsort until nothing remains to be sorted. The expression [Front || Front <- Rest, Front < Pivot] is a list comprehension, meaning \"Construct a list of elements Front such that Front is a member of Rest, and Front is less than Pivot.\" ++ is the list concatenation operator.\n\nA comparison function can be used for more complicated structures for the sake of readability.\n\nThe following code would sort lists according to length:\n\n% This is file 'listsort.erl' (the compiler is made this way)\n-module(listsort).\n% Export 'by_length' with 1 parameter (don't care about the type and name)\n-export([by_length/1]).\n\nby_length(Lists) -> % Use 'qsort/2' and provides an anonymous function as a parameter\n   qsort(Lists, fun(A,B) -> length(A) < length(B) end).\n\nqsort([], _)-> []; % If list is empty, return an empty list (ignore the second parameter)\nqsort([Pivot|Rest], Smaller) ->\n    % Partition list with 'Smaller' elements in front of 'Pivot' and not-'Smaller' elements\n    % after 'Pivot' and sort the sublists.\n    qsort([X || X <- Rest, Smaller(X,Pivot)], Smaller)\n    ++ [Pivot] ++\n    qsort([Y || Y <- Rest, not(Smaller(Y, Pivot))], Smaller).\nHere again, a Pivot is taken from the first parameter given to qsort() and the rest of Lists is named Rest. Note that the expression\n\n[X || X <- Rest, Smaller(X,Pivot)]\nis no different in form from\n\n[Front || Front <- Rest, Front < Pivot]\n(in the previous example) except for the use of a comparison function in the last part, saying \"Construct a list of elements X such that X is a member of Rest, and Smaller is true\", with Smaller being defined earlier as\n\nfun(A,B) -> length(A) < length(B) end\nNote also that the anonymous function is named Smaller in the parameter list of the second definition of qsort so that it can be referenced by that name within that function. It is not named in the first definition of qsort, which deals with the base case of an empty list and thus has no need of this function, let alone a name for it.\n\nData types[edit]\nErlang has eight primitive data types:\n\nIntegers\nIntegers are written as sequences of decimal digits, for example, 12, 12375 and -23427 are integers. Integer arithmetic is exact and only limited by available memory on the machine. (This is called arbitrary-precision arithmetic.)\nAtoms\nAtoms are used within a program to denote distinguished values. They are written as strings of consecutive alphanumeric characters, the first character being lowercase. Atoms can contain any character if they are enclosed within single quotes and an escape convention exists which allows any character to be used within an atom.\nFloats\nFloating point numbers use the IEEE 754 64-bit representation.\nReferences\nReferences are globally unique symbols whose only property is that they can be compared for equality. They are created by evaluating the Erlang primitive make_ref().\nBinaries\nA binary is a sequence of bytes. Binaries provide a space-efficient way of storing binary data. Erlang primitives exist for composing and decomposing binaries and for efficient input/output of binaries.\nPids\nPid is short for process identifier – a Pid is created by the Erlang primitive spawn(...) Pids are references to Erlang processes.\nPorts\nPorts are used to communicate with the external world. Ports are created with the built-in function open_port. Messages can be sent to and received from ports, but these messages must obey the so-called \"port protocol.\"\nFuns\nFuns are function closures. Funs are created by expressions of the form: fun(...) -> ... end.\nAnd three compound data types:\n\nTuples\nTuples are containers for a fixed number of Erlang data types. The syntax {D1,D2,...,Dn} denotes a tuple whose arguments are D1, D2, ... Dn. The arguments can be primitive data types or compound data types. Any element of a tuple can be accessed in constant time.\nLists\nLists are containers for a variable number of Erlang data types. The syntax [Dh|Dt] denotes a list whose first element is Dh, and whose remaining elements are the list Dt. The syntax [] denotes an empty list. The syntax [D1,D2,..,Dn] is short for [D1|[D2|..|[Dn|[]]]]. The first element of a list can be accessed in constant time. The first element of a list is called the head of the list. The remainder of a list when its head has been removed is called the tail of the list.\nMaps\nMaps contain a variable number of key-value associations. The syntax is#{Key1=>Value1,...,KeyN=>ValueN}.\nTwo forms of syntactic sugar are provided:\n\nStrings\nStrings are written as doubly quoted lists of characters. This is syntactic sugar for a list of the integer ASCII codes for the characters in the string. Thus, for example, the string \"cat\" is shorthand for [99,97,116]. It has partial support for Unicode strings.[17]\nRecords\nRecords provide a convenient way for associating a tag with each of the elements in a tuple. This allows one to refer to an element of a tuple by name and not by position. A pre-compiler takes the record definition and replaces it with the appropriate tuple reference.\nErlang has no method of defining classes, although there are external libraries available.[18]\n\nConcurrency and distribution orientation[edit]\nErlang's main strength is support for concurrency. It has a small but powerful set of primitives to create processes and communicate among them. Erlang is conceptually similar to the occam programming language, though it recasts the ideas of communicating sequential processes (CSP) in a functional framework and uses asynchronous message passing.[19] Processes are the primary means to structure an Erlang application. They are neither operating system processes nor operating system threads, but lightweight processes that are scheduled by Erlang's BEAM VM. Like operating system processes (but unlike operating system threads), they share no state with each other. The estimated minimal overhead for each is 300 words.[20] Thus, many processes can be created without degrading performance. A benchmark with 20 million processes has been successfully performed.[21] Erlang has supported symmetric multiprocessing since release R11B of May 2006.\n\nWhile threads require external library support in most languages, Erlang provides language-level features for creating and managing processes with the aim of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using message passing instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM[22]).\n\nInter-process communication works via a shared-nothing asynchronous message passing system: every process has a \"mailbox\", a queue of messages that have been sent by other processes and not yet consumed. A process uses the receive primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.\n\nThe code example below shows the built-in support for distributed processes:\n\n % Create a process and invoke the function web:start_server(Port, MaxConnections)\n ServerProcess = spawn(web, start_server, [Port, MaxConnections]),\n\n % Create a remote process and invoke the function\n % web:start_server(Port, MaxConnections) on machine RemoteNode\n RemoteProcess = spawn(RemoteNode, web, start_server, [Port, MaxConnections]),\n\n % Send a message to ServerProcess (asynchronously). The message consists of a tuple\n % with the atom \"pause\" and the number \"10\".\n ServerProcess ! {pause, 10},\n\n % Receive messages sent to this process\n receive\n         a_message -> do_something;\n         {data, DataContent} -> handle(DataContent);\n         {hello, Text} -> io:format(\"Got hello message: ~s\", [Text]);\n         {goodbye, Text} -> io:format(\"Got goodbye message: ~s\", [Text])\n end.\nAs the example shows, processes may be created on remote nodes, and communication with them is transparent in the sense that communication with remote processes works exactly as communication with local processes.\n\nConcurrency supports the primary method of error-handling in Erlang. When a process crashes, it neatly exits and sends a message to the controlling process which can then take action, such as for instance starting a new process that takes over the old process's task.[23][24]\n\nImplementation[edit]\nThe Ericsson Erlang implementation loads virtual machine bytecode which is converted to threaded code at load time. It also includes a native code compiler on most platforms, developed by the High Performance Erlang Project (HiPE) at Uppsala University. Since October 2001 the HiPE system is fully integrated in Ericsson's Open Source Erlang/OTP system.[25] It also supports interpreting, directly from source code via abstract syntax tree, via script as of R11B-5 release of Erlang.\n\nHot code loading and modules[edit]\nErlang supports language-level Dynamic Software Updating. To implement this, code is loaded and managed as \"module\" units; the module is a compilation unit. The system can keep two versions of a module in memory at the same time, and processes can concurrently run code from each. The versions are referred to as the \"new\" and the \"old\" version. A process will not move into the new version until it makes an external call to its module.\n\nAn example of the mechanism of hot code loading:\n\n  %% A process whose only job is to keep a counter.\n  %% First version\n  -module(counter).\n  -export([start/0, codeswitch/1]).\n\n  start() -> loop(0).\n\n  loop(Sum) ->\n    receive\n       {increment, Count} ->\n          loop(Sum+Count);\n       {counter, Pid} ->\n          Pid ! {counter, Sum},\n          loop(Sum);\n       code_switch ->\n          ?MODULE:codeswitch(Sum)\n          % Force the use of 'codeswitch/1' from the latest MODULE version\n    end.\n\n  codeswitch(Sum) -> loop(Sum).\nFor the second version, we add the possibility to reset the count to zero.\n\n  %% Second version\n  -module(counter).\n  -export([start/0, codeswitch/1]).\n\n  start() -> loop(0).\n\n  loop(Sum) ->\n    receive\n       {increment, Count} ->\n          loop(Sum+Count);\n       reset ->\n          loop(0);\n       {counter, Pid} ->\n          Pid ! {counter, Sum},\n          loop(Sum);\n       code_switch ->\n          ?MODULE:codeswitch(Sum)\n    end.\n\n  codeswitch(Sum) -> loop(Sum).\nOnly when receiving a message consisting of the atom 'code_switch' will the loop execute an external call to codeswitch/1 (?MODULE is a preprocessor macro for the current module). If there is a new version of the \"counter\" module in memory, then its codeswitch/1 function will be called. The practice of having a specific entry-point into a new version allows the programmer to transform state to what is required in the newer version. In our example we keep the state as an integer.\n\nIn practice, systems are built up using design principles from the Open Telecom Platform which leads to more code upgradable designs. Successful hot code loading is a tricky subject; Code needs to be written with care to make use of Erlang's facilities.\n\nDistribution[edit]\nIn 1998, Ericsson released Erlang as open source to ensure its independence from a single vendor and to increase awareness of the language. Erlang, together with libraries and the real-time distributed database Mnesia, forms the Open Telecom Platform (OTP) collection of libraries. Ericsson and a few other companies offer commercial support for Erlang.\n\nSince the open source release, Erlang has been used by several firms worldwide, including Nortel and T-Mobile.[26] Although Erlang was designed to fill a niche and has remained an obscure language for most of its existence, its popularity is growing due to demand for concurrent services.[27][28] Erlang has found some use in fielding MMORPG servers.[29]\n\nErlang in Industry[edit]\nSoftware projects written in Erlang[edit]\nConfiguration management:\nChef (software), for which the core API server, originally written in Ruby, was completely re-written in version 11 in Erlang[30]\nContent Management System:\nZotonic, a content management system and web framework written in Erlang\nDistributed databases:\nCloudant, a database service based on the company's fork of CouchDB, BigCouch\nCouchDB, a document-based database that uses MapReduce\nCouchbase Server (née Membase), database management system optimized for storing data behind interactive web applications[31]\nMnesia, a distributed database\nRiak, a distributed database\nSimpleDB, a distributed database that is part of Amazon Web Services[32]\nMessage broker:\nRabbitMQ, an implementation of Advanced Message Queuing Protocol (AMQP)\nOnline messaging:\nejabberd, an Extensible Messaging and Presence Protocol (XMPP) instant messaging server\nMongooseIM, a massively scalable XMPP platform\nSolution stacks\nLYME (software bundle), to serve dynamic web pages\nLYCE (software bundle), to serve dynamic web pages\nTools\nWings 3D, a 3D subdivision modeler, used to model and texture polygon meshes\nWeb servers:\nCowboy (Erlang), a small, fast, modular HTTP server written in Erlang.\nYaws web server\nCompanies using Erlang[edit]\nCompanies using Erlang in their production systems include:\n\nAmazon.com uses Erlang to implement SimpleDB, providing database services as a part of the Amazon Web Services offering.[33]\nAOL's digital advertising business is using Erlang for its real time bidding exchange systems.[34]\nBattlestar Galactica Online game server by Bigpoint\nBet365, the online gambling firm uses the language in production to drive its InPlay betting service, pushing live odds of sporting events to millions of customers in near real-time.[35]\nCall of Duty server core[36]\nCisco acquired Tail-f Systems, a leading provider of multi-vendor network service orchestration solutions for traditional and virtualized networks.[37]\nDiscord[38]\nDNSimple, a DNS provider that uses Erlang to run DNS servers in a globally distributed Anycast network, managing billions of requests per day.\nEricsson uses Erlang in its support nodes, used in GPRS, 3G and LTE mobile networks worldwide.[39][40]\nFacebook uses Erlang to power the backend of its chat service, handling more than 200 million active users.[41] It can be observed in some of its HTTP response headers.[citation needed]\nFacebook Chat system was running on ejabberd based servers[42][43][44]\nGitHub, a web-based hosting service for software development projects that use the Git version control system. Erlang is used for RPC proxies to ruby processes.[45]\nGoldman Sachs, high-frequency trading programs\"ØMQ: Mission Accomplished\". July 2016.\nHuffington Post uses Erlang for its commenting system on HuffPost Live.[46]\nIssuu, an online digital publisher[47]\nKlarna, a Swedish e-commerce company, has been using Erlang to handle 9 million customers and 50 million transaction since 2005.[48]\nLeague of Legends chat system by Riot Games, based on ejabberd\nLinden Lab uses Erlang in its games.[49]\nMachine Zone, a developer of Free-to-play games, uses Erlang in Game of War: Fire Age.[50]\nSmarkets, sports betting exchange\nTuenti chat is based on ejabberd[51]\nTwitterfall, a service to view trends and patterns from Twitter[52][53]\nRackspace uses Erlang in some of its internal applications to manage networking devices.[54]\nRakuten uses Erlang for its distributed file system.[55]\nVendetta Online Naos game server[56]\nWorld of Tanks uses Erlang for message delivery and communication between game players.[57]\nWhatsApp uses Erlang to run messaging servers, achieving up to 2 million connected users per server.[58][59][60][61]\nWhisper, an anonymous social network on mobile[62]\nYahoo! uses it in its social bookmarking service, Delicious, which has more than 5 million users and 150 million bookmarked URLs.[63]\nVariants[edit]\nElixir: a functional, concurrent, general-purpose programming language that runs on the Erlang Virtual Machine (BEAM).\nLisp Flavored Erlang: a LISP based programming language that runs on the Erlang Virtual Machine (BEAM).",
          "type": "compiled",
          "plid": 14
        },
        {
          "name": "Go",
          "details": "Go (often referred to as golang) is a free and open source[12] programming language created at Google[13] in 2007 by Robert Griesemer, Rob Pike, and Ken Thompson.[10] It is a compiled, statically typed language in the tradition of Algol and C, with garbage collection, limited structural typing,[3] memory safety features and CSP-style concurrent programming features added.[14]\n\nThe language was announced in November 2009; it is used in some of Google's production systems,[15] as well as by other firms. Two major implementations exist: Google's Go compiler, \"gc\", is developed as open source software and targets various platforms including Linux, OS X, Windows, various BSD and Unix versions, and since 2015 also mobile devices, including smartphones.[16] A second compiler, gccgo, is a GCC frontend.[17][18] The \"gc\" toolchain is self-hosting since version 1.5.[19]\n\nContents  [hide] \n1\tHistory\n2\tLanguage design\n2.1\tCriticism\n2.2\tSyntax\n2.3\tTypes\n2.3.1\tInterface system\n2.4\tPackage system\n2.5\tConcurrency: goroutines and channels\n2.5.1\tSuitability for parallel programming\n2.5.2\tLack of race condition safety\n2.6\tOmissions\n3\tConventions and code style\n4\tLanguage tools\n5\tExamples\n5.1\tHello world\n5.2\tConcurrency example\n6\tProjects using Go\n7\tReception\n8\tNaming dispute\n9\tSee also\n10\tNotes\n11\tReferences\n12\tExternal links\n12.1\tCommunity and conferences\nHistory[edit]\nGo originated as an experiment by Google engineers Robert Griesemer, Rob Pike, and Ken Thompson to design a new programming language that would resolve common criticisms of other languages while maintaining their positive characteristics.[20] The new language was to:\n\nbe statically typed, scalable to large systems (as Java and C++);\nbe productive and readable, without too many mandatory keywords and repetition[21] (\"light on the page\" like dynamic languages);\nnot require tooling, but support it well,\nsupport networking and multiprocessing.\nIn later interviews, all three of the language designers cited their shared dislike of C++'s complexity as a primary motivation for designing a new language.[22][23][24]\n\nGo 1.0 was released in March 2012.[25]\n\nGo 1.7 added \"one tiny language change\"[26] and one port to macOS 10.12 Sierra plus some experimental ports, e.g. for Linux on z Systems (linux/s390x). Some library changes apply, and e.g. Unicode 9.0 is now supported.\n\nLanguage design[edit]\nGo is recognizably in the tradition of C, but makes many changes to improve conciseness, simplicity, and safety. The following is a brief overview of the features which define Go:\n\nA syntax and environment adopting patterns more common in dynamic languages:[27]\nOptional concise variable declaration and initialization through type inference (x := 0 not var x int = 0;).\nFast compilation times.[28]\nRemote package management (go get)[29] and online package documentation.[30]\nDistinctive approaches to particular problems:\nBuilt-in concurrency primitives: light-weight processes (goroutines), channels, and the select statement.\nAn interface system in place of virtual inheritance, and type embedding instead of non-virtual inheritance.\nA toolchain that, by default, produces statically linked native binaries without external dependencies.\nA desire to keep the language specification simple enough to hold in a programmer's head,[31] in part by omitting features common to similar languages.\nCriticism[edit]\nGo critics assert that:\n\nlack of compile-time generics leads to code duplication, metaprogramming cannot be statically checked[32][33] and standard library cannot offer generic algorithms[34]\nlack of language extensibility (through, for instance, operator overloading) makes certain tasks more verbose[32][35]\nthe type system's lack of Hindley-Milner typing inhibiting safety and/or expressiveness[36][37]\nthe pauses and overhead of garbage collection (GC) limit Go's use in systems programming compared to languages with manual memory management.[32][36]\nThe language designers argue that these trade-offs are important to Go's success,[38] and explain some particular decisions at length,[39] though they do express openness to adding some form of generic programming in the future, and to pragmatic improvements in areas like standardizing ways to apply code generation.[40] Regarding GC, Go defenders point to pause-time reduction in later versions[41] (e.g. Go 1.6), while acknowledging their GC algorithm is not hard real-time.\n\nSyntax[edit]\nGo's syntax includes changes from C aimed at keeping code concise and readable. A combined declaration/initialization operator was introduced that allows the programmer to write i := 3 or s := \"some words\", without specifying the types of variables. This contrasts with C's int i = 3; and const char *s = \"some words\";. Semicolons still terminate statements, but are implicit when they would occur at the end of a line. Functions may return multiple values, and returning a result, err pair is the conventional way a function indicates an error to its caller in Go.[a] Go adds literal syntaxes for initializing struct parameters by name, and for initializing maps and slices. As an alternative to C's three-statement for loop, Go's range expressions allow concise iteration over arrays, slices, strings, maps and channels.\n\nTypes[edit]\nGo has a number of built-in types, including numeric ones (byte, int64, float32, etc.), booleans, and character strings (string). Strings are immutable; built-in operators and keywords (rather than functions) provide concatenation, comparison, and UTF-8 encoding and decoding.[42] Record types can be defined with the struct keyword.\n\nFor each type T and each non-negative integer constant n, there is an array type denoted [n]T; arrays of differing lengths are thus of different types. Dynamic arrays are available as \"slices\", denoted []T for some type T. These have a length and a capacity specifying when new memory needs to be allocated to expand the array. Several slices may share their underlying memory.[43][44][45]\n\nPointers are available for all types, and the pointer-to-T type is denoted *T. Address-taking and indirection use the & and * operators as in C, or happen implicitly through the method call or attribute access syntax.[46] There is no pointer arithmetic, except via the special unsafe.Pointer type in the standard library.\n\nFor a pair of types K, V, the type map[K]V is the type of hash tables mapping type-K keys to type-V values. Hash tables are built into the language, with special syntax and built-in functions. chan T is a channel that allows sending values of type T between concurrent Go processes.\n\nAside from its support for interfaces, Go's type system is nominal: the type keyword can be used to define a new named type, which is distinct from other named types that have the same layout (in the case of a struct, the same members in the same order). Some conversions between types (e.g., between the various integer types) are pre-defined and adding a new type may define additional conversions, but conversions between named types must always be invoked explicitly.[47] For example, the type keyword can be used to define a type for IPv4 addresses, which are 32-bit unsigned integers:\n\ntype ipv4addr uint32\nWith this type definition, ipv4addr(x) interprets the uint32 value x as an IP address. Simply assigning x to a variable of type ipv4addr is a type error.\n\nConstant expressions may be either typed or \"untyped\"; they are given a type when assigned to a typed variable, if the value they represent passes a compile-time check.[48]\n\nFunction types are indicated by the func keyword; they take zero or more parameters and return zero or more values, all of which are typed. The parameter and return values determine a function type; thus, func(string, int32) (int, error) is the type of functions that take a string and a 32-bit signed integer, and return a signed integer (of default width) and a value of the built-in interface type error.\n\nAny named type has a method set associated with it. The IP address example above can be extended with a method for converting an address to a human-readable representation, viz.,\n\n// Is this the zero broadcast address 255.255.255.255?\nfunc (addr ipv4addr) ZeroBroadcast() bool {\n    return addr == 0xFFFFFFFF\n}\nDue to nominal typing, this method definition adds a method to ipv4addr, but not on uint32. While methods have special definition and call syntax, there is no distinct method type.[49]\n\nInterface system[edit]\nGo provides two features that replace class inheritance.\n\nThe first is embedding, which can be viewed as an automated form of composition[50] or delegation.[51]:255\n\nThe second are its interfaces, which provides runtime polymorphism.[52]:266 Interfaces provide a limited form of structural typing in the otherwise nominal type system of Go. Any type that implements all methods of an interface conforms to that interface. Go interfaces were designed after protocols from the Smalltalk programming language.[53] Multiple sources use the term duck typing when describing Go interface.[54][55] Although the term duck typing is not precisely defined and therefore not wrong, it usually implies that type conformance is not statically checked. Since conformance to a Go interface is checked statically by the Go compiler (except when performing a type assertion), the Go authors prefer to use the term structural typing.\n\nAn interface specifies a set of types by listing required methods and their types, and is satisfied by any type that has the required methods. Implementing types do not need to specify their implementing of interfaces, so if Shape, Square and Circle are defined as:\n\nimport \"math\"\n\ntype Shape interface {\n    Area() float64\n}\n\ntype Square struct { // Note: no \"implements\" declaration\n    side float64\n}\n\nfunc (sq Square) Area() float64 { return sq.side * sq.side }\n\ntype Circle struct { // No \"implements\" declaration here either\n    radius float64\n}\n\nfunc (c Circle) Area() float64 { return math.Pi * math.Pow(c.radius, 2) }\nthen both Square and Circle are implicitly a Shape and can be assigned to a Shape-typed variable.[52]:263–268 In formal language, Go's interface system provides structural rather than nominal typing. Interfaces can embed other interfaces with the effect of creating a combined interface that is satisfied by exactly the types that implement the embedded interface and any methods that the newly defined interface adds.[52]:270\n\nThe Go standard library uses interfaces to provide genericity in several places, including the input/output system that is based on the concepts of Reader and Writer.[52]:282–283\n\nBesides calling methods via interfaces, Go allows converting interface values to other types with a run-time type check. The language constructs to do so are the type assertion,[56] which checks against a single potential type, and the type switch,[57] which checks against multiple types.\n\nThe empty interface interface{} is an important corner case because it can refer to an item of any concrete type. It is similar to the Object class in Java or C#, but with the difference that the empty interface is satisfied by any type, including built-in types like int (while in Java and C#, an Object variable can only hold instances of reference type).[52]:284 Code using the empty interface cannot simply call methods (or built-in operators) on the referred-to object, but it can store the interface{} value, try to convert it to a more useful type via a type assertion or type switch, or inspect it with Go's reflect package.[58] Because interface{} can refer to any value, it is a limited way to escape the restrictions of static typing, like void* in C but with additional run-time type checks.\n\nInterface values are implemented using pointer to data and a second pointer to run-time type information.[59] Like some other types implemented using pointers in Go, interface values are nil if uninitialized.[60]\n\nPackage system[edit]\nIn Go's package system, each package has a path (e.g., \"compress/bzip2\" or \"golang.org/x/net/html\") and a name (e.g., bzip2 or html). References to other packages' definitions must always be prefixed with the other package's name, and only the capitalized names from other packages are accessible: io.Reader is public but bzip2.reader is not.[61] The go get command can retrieve packages stored in a remote repository such as GitHub.,[62] and developers are encouraged to develop packages inside a base path corresponding to a source repository (such as github.com/user_name/package_name) to reduce the likelihood of name collision with future additions to the standard library or other external libraries.[63]\n\nProposals exist to introduce a proper package management solution for Go similar to Rust's cargo system or Node's npm system.[64]\n\nConcurrency: goroutines and channels[edit]\nThe Go language has built-in facilities, as well as library support, for writing concurrent programs. Concurrency refers not only to CPU parallelism, but also to asynchrony: letting slow operations like a database or network-read run while the program does other work, as is common in event-based servers.[65]\n\nThe primary concurrency construct is the goroutine, a type of light-weight process. A function call prefixed with the go keyword starts a function in a new goroutine. The language specification does not specify how goroutines should be implemented, but current implementations multiplex a Go process's goroutines onto a smaller set of operating system threads, similar to the scheduling performed in Erlang.[66]:10\n\nWhile a standard library package featuring most of the classical concurrency control structures (mutex locks, etc.) is available,[66]:151–152 idiomatic concurrent programs instead prefer channels, which provide send messages between goroutines.[67] Optional buffers store messages in FIFO order[51]:43 and allow sending goroutines to proceed before their messages are received.\n\nChannels are typed, so that a channel of type chan T can only be used to transfer messages of type T. Special syntax is used to operate on them; <-ch is an expression that causes the executing goroutine to block until a value comes in over the channel ch, while ch <- x sends the value x (possibly blocking until another goroutine receives the value). The built-in switch-like select statement can be used to implement non-blocking communication on multiple channels; see below for an example. Go has a memory model describing how goroutines must use channels or other operations to safely share data.\n\nThe existence of channels sets Go apart from actor model-style concurrent languages like Erlang, where messages are addressed directly to actors (corresponding to goroutines); the actor style can be simulated in Go by maintaining a one-to-one correspondence between goroutines and channels, but the language allows multiple goroutines to share a channel, or a single goroutine to send and receive on multiple channels.[66]:147\n\nFrom these tools one can build concurrent constructs like worker pools, pipelines (in which, say, a file is decompressed and parsed as it downloads), background calls with timeout, \"fan-out\" parallel calls to a set of services, and others.[68] Channels have also found uses further from the usual notion of interprocess communication, like serving as a concurrency-safe list of recycled buffers,[69] implementing coroutines (which helped inspire the name goroutine),[70] and implementing iterators.[71]\n\nConcurrency-related structural conventions of Go (channels and alternative channel inputs) are derived from Tony Hoare's communicating sequential processes model. Unlike previous concurrent programming languages such as Occam or Limbo (a language on which Go co-designer Rob Pike worked),[72] Go does not provide any built-in notion of safe or verifiable concurrency.[73] While the communicating-processes model is favored in Go, it is not the only one: all goroutines in a program share a single address space. This means that mutable objects and pointers can be shared between goroutines; see § Lack of race condition safety, below.\n\nSuitability for parallel programming[edit]\nAlthough Go's concurrency features are not aimed primarily at parallel processing,[65] they can be used to program shared memory multi-processor machines. Various studies have been done into the effectiveness of this approach.[74] One of these studies compared the size (in lines of code) and speed of programs written by a seasoned programmer not familiar with the language and corrections to these programs by a Go expert (from Google's development team), doing the same for Chapel, Cilk and Intel TBB. The study found that the non-expert tended to write divide-and-conquer algorithms with one go statement per recursion, while the expert wrote distribute-work-synchronize programs using one goroutine per processor. The expert's programs were usually faster, but also longer.[75]\n\nLack of race condition safety[edit]\nThere are no restrictions on how goroutines access shared data, making race conditions possible. Specifically, unless a program explicitly synchronizes via channels or other means, writes from one goroutine might be partly, entirely, or not at all visible to another, often with no guarantees about ordering of writes.[73] Furthermore, Go's internal data structures like interface values, slice headers, hash tables, and string headers are not immune to race conditions, so type and memory safety can be violated in multithreaded programs that modify shared instances of those types without synchronization.[76][77]\n\nInstead of language support, safe concurrent programming thus relies on conventions; for example, Chisnall recommends an idiom called \"aliases xor mutable\", meaning that passing a mutable value (or pointer) over a channel signals a transfer of ownership over the value to its receiver.[66]:155\n\nOmissions[edit]\nGo deliberately omits certain features common in other languages, including (implementation) inheritance, generic programming, assertions, pointer arithmetic, and implicit type conversions.\n\nOf these language features, the Go authors express an openness to generic programming, explicitly argue against assertions and pointer arithmetic, while defending the choice to omit type inheritance as giving a more useful language, encouraging instead the use of interfaces to achieve dynamic dispatch[b] and composition to reuse code. Composition and delegation are in fact largely automated by struct embedding; according to researchers Schmager et al., this feature \"has many of the drawbacks of inheritance: it affects the public interface of objects, it is not fine-grained (i.e, no method-level control over embedding), methods of embedded objects cannot be hidden, and it is static\", making it \"not obvious\" whether programmers will not overuse it to the extent that programmers in other languages are reputed to overuse inheritance.[50]\n\nRegarding generic programming, some built-in functions are in fact type-generic, but these are treated as special cases; Rob Pike calls this a weakness of the language that may at some point be changed.[43] The Google team that designs the language built at least one compiler for an experimental Go dialect with generics, but did not release it.[78]\n\nAfter initially omitting exceptions, the exception-like panic/recover mechanism was eventually added to the language, which the Go authors advise using for unrecoverable errors such as those that should halt an entire program or server request, or as a shortcut to propagate errors up the stack within a package (but not across package boundaries; there, error returns are the standard API).[79][80][81][82]\n\nConventions and code style[edit]\nThe Go authors put substantial effort into molding the style and design of Go programs:\n\nIndentation, spacing, and other surface-level details of code are automatically standardized by the gofmt tool. golint does additional style checks automatically.\nTools and libraries distributed with Go suggest standard approaches to things like API documentation (godoc[83]), testing (go test), building (go build), package management (go get), and so on.\nGo enforces rules that are recommendations in other languages, for example banning cyclic dependencies, unused variables or imports, and implicit type conversions.\nThe omission of certain features (for example, functional-programming shortcuts like map and C++-style try/finally blocks) tends to encourage a particular explicit, concrete, and imperative programming style.\nOn day one the Go team published a collection of Go idioms, and later also collected code review comments, talks, official blog posts to teach Go style and coding philosophy.\nLanguage tools[edit]\nGo includes the same sort of debugging, testing, and code-vetting tools as many language distributions. The Go distribution includes, among other tools,\n\ngo build, which builds Go binaries using only information in the source files themselves, no separate makefiles\ngo test, for unit testing and microbenchmarks\ngo fmt, for formatting code\ngo get, for retrieving and installing remote packages\ngo vet, a static analyzer looking for potential errors in code\ngo run, a shortcut for building and executing code\ngodoc, for displaying documentation or serving it via HTTP\ngorename, for renaming variables, functions, and so on in a type-safe way\ngo generate, a standard way to invoke code generators\nIt also includes profiling and debugging support, runtime instrumentation (to, for example, track garbage collection pauses), and a race condition tester.\n\nThere is an ecosystem of third-party tools that add to the standard distribution, such as gocode, which enables code autocompletion in many text editors, goimports (by a Go team member), which automatically adds/removes package imports as needed, errcheck, which detects code that might unintentionally ignore errors, and more. Plugins exist to add language support in widely used text editors, and at least one IDE, LiteIDE, is branded as \"a simple, open source, cross-platform Go IDE.\"[84]\n\nExamples[edit]\nHello world[edit]\nHere is a Hello world program in Go:\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello, World\")\n}\nConcurrency example[edit]\nThe following simple program demonstrates Go's concurrency features to implement an asynchronous program. It launches two \"goroutines\" (lightweight threads): one waits for the user to type some text, while the other implements a timeout. The select statement waits for either of these goroutines to send a message to the main routine, and acts on the first message to arrive (example adapted from Chisnall).[66]:152\n\npackage main\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\nfunc readword(ch chan string) {\n    fmt.Println(\"Type a word, then hit Enter.\")\n    var word string\n    fmt.Scanf(\"%s\", &word)\n    ch <- word\n}\n\nfunc timeout(t chan bool) {\n    time.Sleep(5 * time.Second)\n    t <- true\n}\n\nfunc main() {\n    t := make(chan bool)\n    go timeout(t)\n\n    ch := make(chan string)\n    go readword(ch)\n\n    select {\n    case word := <-ch:\n        fmt.Println(\"Received\", word)\n    case <-t:\n        fmt.Println(\"Timeout.\")\n    }\n}\nProjects using Go[edit]\nQuestion book-new.svg\nThis section relies too much on references to primary sources. Please improve this section by adding secondary or tertiary sources. (November 2015) (Learn how and when to remove this template message)\nSome notable open-source applications in Go include:\n\nDocker, a set of tools for deploying Linux containers\nDoozer, a lock service by managed hosting provider Heroku[14]\nEthereum, a shared world computing platform.\nJuju, a service orchestration tool by Canonical, packagers of Ubuntu Linux\nPacker, a tool for creating identical machine images for multiple platforms from a single source configuration\nSnappy, a package manager developed by Canonical for Ubuntu.\nSyncthing, an open-source file synchronization client/server application\nSome notable open-source frameworks using Go:\n\nBeego, high-performance web framework in Go, used for web apps and backend services.[citation needed]\nMartini, package for web applications/services.[citation needed]\nGorilla, a web toolkit for Go.[citation needed]\nOther notable companies and sites using Go (generally together with other languages, not exclusively) include:[85][self-published source?][86]\n\nAeroFS, private cloud filesync appliance provider which migrated some microservices from Java to Go with major memory footprint improvements[87]\nChango, a programmatic advertising company uses Go in its real-time bidding systems.[88]\nCloud Foundry, a platform as a service[citation needed]\nCloudFlare, for their delta-coding proxy Railgun, their distributed DNS service, as well as tools for cryptography, logging, stream processing, and accessing SPDY sites.[89][90]\nCoreOS, a Linux-based operating system that utilizes Docker containers.[91]\nCouchbase, Query and Indexing services within the Couchbase Server[92]\nDropbox, migrated some of their critical components from Python to Go[93]\nGoogle, for many projects, notably including download server dl.google.com[94][95][96]\nMercadoLibre, for several public APIs.[citation needed]\nMongoDB, tools for administering MongoDB instances[97]\nNetflix, for two portions of their server architecture[98]\nNovartis, for an internal inventory system[citation needed]\nPlug.dj, an interactive online social music streaming website.[99]\nReplicated, Docker based PaaS for creating enterprise, installable software.[100]\nSendGrid, a Boulder, Colorado-based transactional email delivery and management service.[101]\nSoundCloud, for \"dozens of systems\"[102]\nSplice, for the entire backend (API and parsers) of their online music collaboration platform.[103]\nThoughtWorks, some tools and applications around continuous delivery and instant messages (CoyIM).[104]\nTwitch.tv, for their IRC-based chat system (migrated from Python).[105]\nUber, for handling high volumes of geofence-based queries.[106]\nZerodha, for realtime peering and streaming of market data[citation needed]\nReception[edit]\nGo's initial release led to much discussion.\n\nThe interface system, and the deliberate omission of inheritance, were praised by Michele Simionato, who likened these language characteristics to those of Standard ML, calling it \"a shame that no popular language has followed [this] particular route in the design space\".[107]\n\nDave Astels at Engine Yard wrote:[108]\n\nGo is extremely easy to dive into. There are a minimal number of fundamental language concepts and the syntax is clean and designed to be clear and unambiguous. Go is still experimental and still a little rough around the edges.\n\nArs Technica interviewed Rob Pike, one of the authors of Go, and asked why a new language was needed. He replied that:[109]\n\nIt wasn't enough to just add features to existing programming languages, because sometimes you can get more in the long run by taking things away. They wanted to start from scratch and rethink everything. ... [But they did not want] to deviate too much from what developers already knew because they wanted to avoid alienating Go's target audience.\n\nGo was named Programming Language of the Year by the TIOBE Programming Community Index in its first year, 2009, for having a larger 12-month increase in popularity (in only 2 months, after its introduction in November) than any other language that year, and reached 13th place by January 2010,[110] surpassing established languages like Pascal. By June of 2015, its ranking had dropped to below 50th in the index, placing it lower than COBOL and Fortran.[111] But as of September 2016, its ranking had surged to 19th, indicating significant growth in popularity and adoption.[112]\n\nRegarding Go, Bruce Eckel has stated:[113]\n\nThe complexity of C++ (even more complexity has been added in the new C++), and the resulting impact on productivity, is no longer justified. All the hoops that the C++ programmer had to jump through in order to use a C-compatible language make no sense anymore -- they're just a waste of time and effort. Go makes much more sense for the class of problems that C++ was originally intended to solve.\n\nA 2011 evaluation of the language and its gc implementation in comparison to C++ (GCC), Java and Scala by a Google engineer found that:\n\nGo offers interesting language features, which also allow for a concise and standardized notation. The compilers for this language are still immature, which reflects in both performance and binary sizes.\n\n— R. Hundt[114]\nThe evaluation got a rebuttal from the Go development team. Ian Lance Taylor, who had improved the Go code for Hundt's paper, had not been aware of the intention to publish his code, and says that his version was \"never intended to be an example of idiomatic or efficient Go\"; Russ Cox then did optimize the Go code, as well as the C++ code, and got the Go code to run slightly faster than C++ and more than an order of magnitude faster than the \"optimized\" code in the paper.[115]\n\nNaming dispute[edit]\nOn 10 November 2009, the day of the general release of the language, Francis McCabe, developer of the Go! programming language (note the exclamation point), requested a name change of Google's language to prevent confusion with his language, which he had spent 10 years developing.[116] McCabe raised concerns that \"the 'big guy' will end up steam-rollering over\" him, and this concern resonated with the more than 120 developers who commented on Google's official issues thread saying they should change the name, with some[117] even saying the issue contradicts Google's motto of: Don't be evil.[118] The issue was closed by a Google developer on 12 October 2010 with the custom status \"Unfortunate\" and with the following comment: \"there are many computing products and services named Go. In the 11 months since our release, there has been minimal confusion of the two languages.\"\n\nSee also[edit]\n\tFree software portal\nComparison of programming languages\nDart, another Google programming language\nUFCS, a way of having 'open methods' in other languages\nNotes[edit]\nJump up ^ Usually, exactly one of the result and error values has a value other than the type's zero value; sometimes both do, as when a read or write can only be partially completed, and sometimes neither, as when a read returns 0 bytes. See Semipredicate problem: Multivalued return.\nJump up ^ Questions \"How do I get dynamic dispatch of methods?\" and \"Why is there no type inheritance?\" in the language FAQ.[10]",
          "type": "compiled",
          "plid": 15
        },
        {
          "name": "Java",
          "details": "Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers \"write once, run anywhere\" (WORA),[15] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[16] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of computer architecture. As of 2016, Java is one of the most popular programming languages in use,[17][18][19][20] particularly for client-server web applications, with a reported 9 million developers.[21] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them.\n\nThe original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licences. As of May 2007, in compliance with the specifications of the Java Community Process, Sun relicensed most of its Java technologies under the GNU General Public License. Others have also developed alternative implementations of these Sun technologies, such as the GNU Compiler for Java (bytecode compiler), GNU Classpath (standard libraries), and IcedTea-Web (browser plugin for applets).\n\nThe latest version is Java 8, which is the only version currently supported for free by Oracle, although earlier versions are supported both by Oracle and other companies on a commercial basis.\n\nContents  [hide] \n1\tHistory\n1.1\tPrinciples\n1.2\tVersions\n2\tPractices\n2.1\tJava platform\n2.1.1\tImplementations\n2.1.2\tPerformance\n2.2\tAutomatic memory management\n3\tSyntax\n4\tExamples\n4.1\t\"Hello, world!\" program\n4.2\tComprehensive example\n5\tSpecial classes\n5.1\tApplet\n5.2\tServlet\n5.3\tJavaServer Pages\n5.4\tSwing application\n5.5\tGenerics\n6\tCriticism\n7\tUse outside of the Java platform\n7.1\tGoogle\n8\tClass libraries\n9\tDocumentation\n10\tEditions\n11\tSee also\n11.1\tComparison of Java with other languages\n12\tNotes\n13\tReferences\n14\tExternal links\nHistory\nSee also: Java (software platform) § History\n\nDuke, the Java mascot\n\nJames Gosling, the creator of Java (2008)\n\nThe TIOBE programming language popularity index graph from 2002 to 2015. Over the course of a decade Java (blue) and C (black) competing for the top position.\nJames Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[22] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[23] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee.[24] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[25]\n\nSun Microsystems released the first public implementation as Java 1.0 in 1995.[26] It promised \"Write Once, Run Anywhere\" (WORA), providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular, while mostly outside of browsers, that wasn't the original plan. In January 2016, Oracle announced that Java runtime environments based on JDK 9 will discontinue the browser plugin.[27] The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[28] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 – 1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.\n\nIn 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[29][30][31] Java remains a de facto standard, controlled through the Java Community Process.[32] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.\n\nOn November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software, (FOSS), under the terms of the GNU General Public License (GPL). On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[33]\n\nSun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an \"evangelist\".[34] Following Oracle Corporation's acquisition of Sun Microsystems in 2009–10, Oracle has described itself as the \"steward of Java technology with a relentless commitment to fostering a community of participation and transparency\".[35] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see Google section below). Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[36] On April 2, 2010, James Gosling resigned from Oracle.[37]\n\nPrinciples\nThere were five primary goals in the creation of the Java language:[16]\n\nIt must be \"simple, object-oriented, and familiar\".\nIt must be \"robust and secure\".\nIt must be \"architecture-neutral and portable\".\nIt must execute with \"high performance\".\nIt must be \"interpreted, threaded, and dynamic\".\nVersions\nMain article: Java version history\nAs of 2015, only Java 8 is supported (\"publicly\"). Major release versions of Java, along with their release dates:\n\nJDK 1.0 (January 21, 1996)\nJDK 1.1 (February 19, 1997)\nJ2SE 1.2 (December 8, 1998)\nJ2SE 1.3 (May 8, 2000)\nJ2SE 1.4 (February 6, 2002)\nJ2SE 5.0 (September 30, 2004)\nJava SE 6 (December 11, 2006)\nJava SE 7 (July 28, 2011)\nJava SE 8 (March 18, 2014)\nPractices\nJava platform\nMain articles: Java (software platform) and Java virtual machine\n\nJava Control Panel, version 7\nOne design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate runtime support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End users commonly use a Java Runtime Environment (JRE) installed on their own machine for standalone Java applications, or in a web browser for Java applets.\n\nStandard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.\n\nThe use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions makes interpreted programs almost always run more slowly than native executables. However, just-in-time (JIT) compilers that compile bytecodes to machine code during runtime were introduced from an early stage. Java itself is platform-independent, and is adapted to the particular platform it is to run on by a Java virtual machine for it, which translates the Java bytecode into the platform's machine language.[38]\n\nImplementations\nSee also: Free Java implementations\nOracle Corporation is the current owner of the official implementation of the Java SE platform, following their acquisition of Sun Microsystems on January 27, 2010. This implementation is based on the original implementation of Java by Sun. The Oracle implementation is available for Microsoft Windows (still works for XP, while only later versions currently \"publicly\" supported), Mac OS X, Linux and Solaris. Because Java lacks any formal standardization recognized by Ecma International, ISO/IEC, ANSI, or other third-party standards organization, the Oracle implementation is the de facto standard.\n\nThe Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger.\n\nOpenJDK is another notable Java SE implementation that is licensed under the GNU GPL. The implementation started when Sun began releasing the Java source code under the GPL. As of Java SE 7, OpenJDK is the official Java reference implementation.\n\nThe goal of Java is to make all implementations of Java compatible. Historically, Sun's trademark license for usage of the Java brand insists that all implementations be \"compatible\". This resulted in a legal dispute with Microsoft after Sun claimed that the Microsoft implementation did not support RMI or JNI and had added platform-specific features of their own. Sun sued in 1997, and in 2001 won a settlement of US$20 million, as well as a court order enforcing the terms of the license from Sun.[39] As a result, Microsoft no longer ships Java with Windows.\n\nPlatform-independent Java is essential to Java EE, and an even more rigorous validation is required to certify an implementation. This environment enables portable server-side applications.\n\nPerformance\nMain article: Java performance\nPrograms written in Java have a reputation for being slower and requiring more memory than those written in C++.[40][41] However, Java programs' execution speed improved significantly with the introduction of just-in-time compilation in 1997/1998 for Java 1.1,[42] the addition of language features supporting better code analysis (such as inner classes, the StringBuilder class, optional assertions, etc.), and optimizations in the Java virtual machine, such as HotSpot becoming the default for Sun's JVM in 2000. With Java 1.5, the performance was improved with the addition of the java.util.concurrent package, including Lock free implementations of the ConcurrentMaps and other multi-core collections, and it was improved further Java 1.6.\n\nSome platforms offer direct hardware support for Java; there are microcontrollers that can run Java in hardware instead of a software Java virtual machine, and ARM based processors can have hardware support for executing Java bytecode through their Jazelle option (while its support is mostly dropped in current implementations of ARM).\n\nAutomatic memory management\nJava uses an automatic garbage collector to manage memory in the object lifecycle. The programmer determines when objects are created, and the Java runtime is responsible for recovering the memory once objects are no longer in use. Once no references to an object remain, the unreachable memory becomes eligible to be freed automatically by the garbage collector. Something similar to a memory leak may still occur if a programmer's code holds a reference to an object that is no longer needed, typically when objects that are no longer needed are stored in containers that are still in use. If methods for a nonexistent object are called, a \"null pointer exception\" is thrown.[43][44]\n\nOne of the ideas behind Java's automatic memory management model is that programmers can be spared the burden of having to perform manual memory management. In some languages, memory for the creation of objects is implicitly allocated on the stack, or explicitly allocated and deallocated from the heap. In the latter case the responsibility of managing memory resides with the programmer. If the program does not deallocate an object, a memory leak occurs. If the program attempts to access or deallocate memory that has already been deallocated, the result is undefined and difficult to predict, and the program is likely to become unstable and/or crash. This can be partially remedied by the use of smart pointers, but these add overhead and complexity. Note that garbage collection does not prevent \"logical\" memory leaks, i.e., those where the memory is still referenced but never used.\n\nGarbage collection may happen at any time. Ideally, it will occur when a program is idle. It is guaranteed to be triggered if there is insufficient free memory on the heap to allocate a new object; this can cause a program to stall momentarily. Explicit memory management is not possible in Java.\n\nJava does not support C/C++ style pointer arithmetic, where object addresses and unsigned integers (usually long integers) can be used interchangeably. This allows the garbage collector to relocate referenced objects and ensures type safety and security.\n\nAs in C++ and some other object-oriented languages, variables of Java's primitive data types are either stored directly in fields (for objects) or on the stack (for methods) rather than on the heap, as is commonly true for non-primitive data types (but see escape analysis). This was a conscious decision by Java's designers for performance reasons.\n\nJava contains multiple types of garbage collectors. By default,[citation needed] HotSpot uses the parallel scavenge garbage collector. However, there are also several other garbage collectors that can be used to manage the heap. For 90% of applications in Java, the Concurrent Mark-Sweep (CMS) garbage collector is sufficient.[45] Oracle aims to replace CMS with the Garbage-First collector (G1).[46]\n\nSyntax\nMain article: Java syntax\nThe syntax of Java is largely influenced by C++. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[16] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, i.e. integers, floating-point numbers, boolean values, and characters, which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as printf() method).\n\nUnlike C++, Java does not support operator overloading[47] or multiple inheritance for classes, though multiple inheritance is supported for interfaces.[48] This simplifies the language and aids in preventing potential errors and anti-pattern design.[citation needed]\n\nJava uses comments similar to those of C++. There are three different styles of comments: a single line style marked with two slashes (//), a multiple line style opened with /* and closed with */, and the Javadoc commenting style opened with /** and closed with */. The Javadoc style of commenting allows the user to run the Javadoc executable to create documentation for the program.\n\nExample:\n\n// This is an example of a single line comment using two slashes\n\n/* This is an example of a multiple line comment using the slash and asterisk.\n This type of comment can be used to hold a lot of information or deactivate\n code, but it is very important to remember to close the comment. */\n\npackage fibsandlies;\nimport java.util.HashMap;\n\n/**\n * This is an example of a Javadoc comment; Javadoc can compile documentation\n * from this text. Javadoc comments must immediately precede the class, method, or field being documented.\n */\npublic class FibCalculator extends Fibonacci implements Calculator {\n    private static Map<Integer, Integer> memoized = new HashMap<Integer, Integer>();\n\n    /*\n     * The main method written as follows is used by the JVM as a starting point for the program.\n     */\n    public static void main(String[] args) {\n        memoized.put(1, 1);\n        memoized.put(2, 1);\n        System.out.println(fibonacci(12)); //Get the 12th Fibonacci number and print to console\n    }\n\n    /**\n     * An example of a method written in Java, wrapped in a class.\n     * Given a non-negative number FIBINDEX, returns\n     * the Nth Fibonacci number, where N equals FIBINDEX.\n     * @param fibIndex The index of the Fibonacci number\n     * @return The Fibonacci number\n     */\n    public static int fibonacci(int fibIndex) {\n        if (memoized.containsKey(fibIndex)) {\n            return memoized.get(fibIndex);\n        } else {\n            int answer = fibonacci(fibIndex - 1) + fibonacci(fibIndex - 2);\n            memoized.put(fibIndex, answer);\n            return answer;\n        }\n    }\n}\nExamples\n\"Hello, world!\" program\nThe traditional \"Hello, world!\" program can be written in Java as:[49]\n\nclass HelloWorldApp {\n    public static void main(String[] args) {\n        System.out.println(\"Hello World!\"); // Prints the string to the console.\n    }\n}\nSource files must be named after the public class they contain, appending the suffix .java, for example, HelloWorldApp.java. It must first be compiled into bytecode, using a Java compiler, producing a file named HelloWorldApp.class. Only then can it be executed, or \"launched\". The Java source file may only contain one public class, but it can contain multiple classes with other than public access and any number of public inner classes. When the source file contains multiple classes, make one class \"public\" and name the source file with that public class name.\n\nA class that is not declared public may be stored in any .java file. The compiler will generate a class file for each class defined in the source file. The name of the class file is the name of the class, with .class appended. For class file generation, anonymous classes are treated as if their name were the concatenation of the name of their enclosing class, a $, and an integer.\n\nThe keyword public denotes that a method can be called from code in other classes, or that a class may be used by classes outside the class hierarchy. The class hierarchy is related to the name of the directory in which the .java file is located. This is called an access level modifier. Other access level modifiers include the keywords private and protected.\n\nThe keyword static in front of a method indicates a static method, which is associated only with the class and not with any specific instance of that class. Only static methods can be invoked without a reference to an object. Static methods cannot access any class members that are not also static. Methods that are not designated static are instance methods, and require a specific instance of a class to operate.\n\nThe keyword void indicates that the main method does not return any value to the caller. If a Java program is to exit with an error code, it must call System.exit() explicitly.\n\nThe method name \"main\" is not a keyword in the Java language. It is simply the name of the method the Java launcher calls to pass control to the program. Java classes that run in managed environments such as applets and Enterprise JavaBeans do not use or need a main() method. A Java program may contain multiple classes that have main methods, which means that the VM needs to be explicitly told which class to launch from.\n\nThe main method must accept an array of String objects. By convention, it is referenced as args although any other legal identifier name can be used. Since Java 5, the main method can also use variable arguments, in the form of public static void main(String... args), allowing the main method to be invoked with an arbitrary number of String arguments. The effect of this alternate declaration is semantically identical (the args parameter is still an array of String objects), but it allows an alternative syntax for creating and passing the array.\n\nThe Java launcher launches Java by loading a given class (specified on the command line or as an attribute in a JAR) and starting its public static void main(String[]) method. Stand-alone programs must declare this method explicitly. The String[] args parameter is an array of String objects containing any arguments passed to the class. The parameters to main are often passed by means of a command line.\n\nPrinting is part of a Java standard library: The System class defines a public static field called out. The out object is an instance of the PrintStream class and provides many methods for printing data to standard out, including println(String) which also appends a new line to the passed string.\n\nThe string \"Hello World!\" is automatically converted to a String object by the compiler.\n\nComprehensive example\n\n[hide]This section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)\nThis section does not cite any sources. (May 2013)\nThis section contains instructions, advice, or how-to content. (May 2013)\n// OddEven.java\nimport javax.swing.JOptionPane;\n\npublic class OddEven {\n\n    private int userInput; // a whole number(\"int\" means integer)\n\n    /**\n     * This is the constructor method. It gets called when an object of the OddEven type\n     * is being created.\n     */\n    public OddEven() {\n        /*\n         * In most Java programs constructors can initialize objects with default values, or create\n         * other objects that this object might use to perform its functions. In some Java programs, the\n         * constructor may simply be an empty function if nothing needs to be initialized prior to the\n         * functioning of the object. In this program's case, an empty constructor would suffice.\n         * A constructor must exist; however, if the user doesn't put one in then the compiler\n         * will create an empty one.\n         */\n    }\n\n    /**\n     * This is the main method. It gets called when this class is run through a Java interpreter.\n     * @param args command line arguments (unused)\n     */\n    public static void main(final String[] args) {\n       /*\n        * This line of code creates a new instance of this class called \"number\" (also known as an\n        * Object) and initializes it by calling the constructor. The next line of code calls\n        * the \"showDialog()\" method, which brings up a prompt to ask you for a number.\n        */\n       OddEven number = new OddEven();\n       number.showDialog();\n    }\n\n    public void showDialog() {\n        /*\n         * \"try\" makes sure nothing goes wrong. If something does,\n         * the interpreter skips to \"catch\" to see what it should do.\n         */\n        try {\n            /*\n             * The code below brings up a JOptionPane, which is a dialog box\n             * The String returned by the \"showInputDialog()\" method is converted into\n             * an integer, making the program treat it as a number instead of a word.\n             * After that, this method calls a second method, calculate() that will\n             * display either \"Even\" or \"Odd.\"\n             */\n            userInput = Integer.parseInt(JOptionPane.showInputDialog(\"Please enter a number.\"));\n            calculate();\n        } catch (final NumberFormatException e) {\n            /*\n             * Getting in the catch block means that there was a problem with the format of\n             * the number. Probably some letters were typed in instead of a number.\n             */\n            System.err.println(\"ERROR: Invalid input. Please type in a numerical value.\");\n        }\n    }\n\n    /**\n     * When this gets called, it sends a message to the interpreter.\n     * The interpreter usually shows it on the command prompt (For Windows users)\n     * or the terminal (For *nix users).(Assuming it's open)\n     */\n    private void calculate() {\n        if ((userInput % 2) == 0) {\n            JOptionPane.showMessageDialog(null, \"Even\");\n        } else {\n            JOptionPane.showMessageDialog(null, \"Odd\");\n        }\n    }\n}\nThe import statement imports the JOptionPane class from the javax.swing package.\nThe OddEven class declares a single private field of type int named userInput. Every instance of the OddEven class has its own copy of the userInput field. The private declaration means that no other class can access (read or write) the userInput field.\nOddEven() is a public constructor. Constructors have the same name as the enclosing class they are declared in, and unlike a method, have no return type. A constructor is used to initialize an object that is a newly created instance of the class.\nThe calculate() method is declared without the static keyword. This means that the method is invoked using a specific instance of the OddEven class. (The reference used to invoke the method is passed as an undeclared parameter of type OddEven named this.) The method tests the expression userInput % 2 == 0 using the if keyword to see if the remainder of dividing the userInput field belonging to the instance of the class by two is zero. If this expression is true, then it prints Even; if this expression is false it prints Odd. (The calculate method can be equivalently accessed as this.calculate and the userInput field can be equivalently accessed as this.userInput, which both explicitly use the undeclared this parameter.)\nOddEven number = new OddEven(); declares a local object reference variable in the main method named number. This variable can hold a reference to an object of type OddEven. The declaration initializes number by first creating an instance of the OddEven class, using the new keyword and the OddEven() constructor, and then assigning this instance to the variable.\nThe statement number.showDialog(); calls the calculate method. The instance of OddEven object referenced by the number local variable is used to invoke the method and passed as the undeclared this parameter to the calculate method.\nuserInput = Integer.parseInt(JOptionPane.showInputDialog(\"Please Enter A Number\")); is a statement that converts the type of String to the primitive data type int by using a utility function in the primitive wrapper class Integer.\nSpecial classes\n\nThis section contains instructions, advice, or how-to content. The purpose of Wikipedia is to present facts, not to train. Please help improve this article either by rewriting the how-to content or by moving it to Wikiversity, Wikibooks or Wikivoyage. (January 2012)\nApplet\nMain article: Java applet\nJava applets are programs that are embedded in other applications, typically in a Web page displayed in a web browser.\n\n// Hello.java\nimport javax.swing.JApplet;\nimport java.awt.Graphics;\n\npublic class Hello extends JApplet {\n    public void paintComponent(final Graphics g) {\n        g.drawString(\"Hello, world!\", 65, 95);\n    }\n}\nThe import statements direct the Java compiler to include the javax.swing.JApplet and java.awt.Graphics classes in the compilation. The import statement allows these classes to be referenced in the source code using the simple class name (i.e. JApplet) instead of the fully qualified class name (FQCN, i.e. javax.swing.JApplet).\n\nThe Hello class extends (subclasses) the JApplet (Java Applet) class; the JApplet class provides the framework for the host application to display and control the lifecycle of the applet. The JApplet class is a JComponent (Java Graphical Component) which provides the applet with the capability to display a graphical user interface (GUI) and respond to user events.\n\nThe Hello class overrides the paintComponent(Graphics) method (additionally indicated with the annotation, supported as of JDK 1.5, Override) inherited from the Container superclass to provide the code to display the applet. The paintComponent() method is passed a Graphics object that contains the graphic context used to display the applet. The paintComponent() method calls the graphic context drawString(String, int, int) method to display the \"Hello, world!\" string at a pixel offset of (65, 95) from the upper-left corner in the applet's display.\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n\"http://www.w3.org/TR/html4/strict.dtd\">\n<!-- Hello.html -->\n<html>\n    <head>\n        <title>Hello World Applet</title>\n    </head>\n    <body>\n        <applet code=\"Hello.class\" width=\"200\" height=\"200\">\n        </applet>\n    </body>\n</html>\nAn applet is placed in an HTML document using the <applet> HTML element. The applet tag has three attributes set: code=\"Hello\" specifies the name of the JApplet class and width=\"200\" height=\"200\" sets the pixel width and height of the applet. Applets may also be embedded in HTML using either the object or embed element,[50] although support for these elements by web browsers is inconsistent.[51] However, the applet tag is deprecated, so the object tag is preferred where supported.\n\nThe host application, typically a Web browser, instantiates the Hello applet and creates an AppletContext for the applet. Once the applet has initialized itself, it is added to the AWT display hierarchy. The paintComponent() method is called by the AWT event dispatching thread whenever the display needs the applet to draw itself.\n\nServlet\nMain article: Java Servlet\nJava Servlet technology provides Web developers with a simple, consistent mechanism for extending the functionality of a Web server and for accessing existing business systems. Servlets are server-side Java EE components that generate responses (typically HTML pages) to requests (typically HTTP requests) from clients. A servlet can almost be thought of as an applet that runs on the server side—without a face.\n\n// Hello.java\nimport java.io.*;\nimport javax.servlet.*;\n\npublic class Hello extends GenericServlet {\n    public void service(final ServletRequest request, final ServletResponse response)\n    throws ServletException, IOException {\n        response.setContentType(\"text/html\");\n        final PrintWriter pw = response.getWriter();\n        try {\n            pw.println(\"Hello, world!\");\n        } finally {\n            pw.close();\n        }\n    }\n}\nThe import statements direct the Java compiler to include all the public classes and interfaces from the java.io and javax.servlet packages in the compilation. Packages make Java well suited for large scale applications.\n\nThe Hello class extends the GenericServlet class; the GenericServlet class provides the interface for the server to forward requests to the servlet and control the servlet's lifecycle.\n\nThe Hello class overrides the service(ServletRequest, ServletResponse) method defined by the Servlet interface to provide the code for the service request handler. The service() method is passed: a ServletRequest object that contains the request from the client and a ServletResponse object used to create the response returned to the client. The service() method declares that it throws the exceptions ServletException and IOException if a problem prevents it from responding to the request.\n\nThe setContentType(String) method in the response object is called to set the MIME content type of the returned data to \"text/html\". The getWriter() method in the response returns a PrintWriter object that is used to write the data that is sent to the client. The println(String) method is called to write the \"Hello, world!\" string to the response and then the close() method is called to close the print writer, which causes the data that has been written to the stream to be returned to the client.\n\nJavaServer Pages\nMain article: JavaServer Pages\nJavaServer Pages (JSP) are server-side Java EE components that generate responses, typically HTML pages, to HTTP requests from clients. JSPs embed Java code in an HTML page by using the special delimiters <% and %>. A JSP is compiled to a Java servlet, a Java application in its own right, the first time it is accessed. After that, the generated servlet creates the response.\n\nSwing application\nMain article: Swing (Java)\nSwing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+ and Motif are supplied by Sun. Apple also provides an Aqua look and feel for Mac OS X. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.\n\nThis example Swing application creates a single window with \"Hello, world!\" inside:\n\n// Hello.java (Java SE 5)\nimport javax.swing.*;\n\npublic class Hello extends JFrame {\n    public Hello() {\n        super(\"hello\");\n        super.setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);\n        super.add(new JLabel(\"Hello, world!\"));\n        super.pack();\n        super.setVisible(true);\n    }\n\n    public static void main(final String[] args) {\n        new Hello();\n    }\n}\nThe first import includes all the public classes and interfaces from the javax.swing package.\n\nThe Hello class extends the JFrame class; the JFrame class implements a window with a title bar and a close control.\n\nThe Hello() constructor initializes the frame by first calling the superclass constructor, passing the parameter \"hello\", which is used as the window's title. It then calls the setDefaultCloseOperation(int) method inherited from JFrame to set the default operation when the close control on the title bar is selected to WindowConstants.EXIT_ON_CLOSE – this causes the JFrame to be disposed of when the frame is closed (as opposed to merely hidden), which allows the Java virtual machine to exit and the program to terminate. Next, a JLabel is created for the string \"Hello, world!\" and the add(Component) method inherited from the Container superclass is called to add the label to the frame. The pack() method inherited from the Window superclass is called to size the window and lay out its contents.\n\nThe main() method is called by the Java virtual machine when the program starts. It instantiates a new Hello frame and causes it to be displayed by calling the setVisible(boolean) method inherited from the Component superclass with the boolean parameter true. Once the frame is displayed, exiting the main method does not cause the program to terminate because the AWT event dispatching thread remains active until all of the Swing top-level windows have been disposed.\n\nGenerics\nMain article: Generics in Java\nIn 2004, generics were added to the Java language, as part of J2SE 5.0. Prior to the introduction of generics, each variable declaration had to be of a specific type. For container classes, for example, this is a problem because there is no easy way to create a container that accepts only specific types of objects. Either the container operates on all subtypes of a class or interface, usually Object, or a different container class has to be created for each contained class. Generics allow compile-time type checking without having to create many container classes, each containing almost identical code. In addition to enabling more efficient code, certain runtime exceptions are converted to compile-time errors, a characteristic known as type safety.\n\nCriticism\nMain article: Criticism of Java\nCriticisms directed at Java include the implementation of generics,[52] speed,[53] the handling of unsigned numbers,[54] the implementation of floating-point arithmetic,[55] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[56]\n\nUse outside of the Java platform\nThe Java programming language requires the presence of a software platform in order for compiled programs to be executed. Oracle supplies the Java platform for use with Java. The Android SDK, is an alternative software platform, used primarily for developing Android applications. It supports Java 6 and some Java 7 features, offering a compatible implementation of a significant part of the standard library (Apache Harmony). The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers.\n\n\nThe Android operating system makes extensive use of Java-related technology.\nGoogle\nSee also: Oracle America, Inc. v. Google, Inc.\nThe Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, was written largely in C, the Android SDK uses the Java language as the basis for Android applications. However, Android does not use the standard Java virtual machine, instead using Java bytecode as an intermediate step which is transformed into Dalvik bytecode. Depending on the Android version, this is then either interpreted by the Dalvik virtual machine, or compiled into native code by the Android Runtime.\n\nAndroid also does not provide the full Java SE standard library, although the Android class library does include an independent implementation of a large subset of it. This led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[57] District Judge William Haskell Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[58] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[59][60][61]\n\nClass libraries\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2014) (Learn how and when to remove this template message)\nMain article: Java Class Library\nThe Java Class Library is the standard library, developed to support application development in Java. It is controlled by Sun Microsystems in cooperation with others through the Java Community Process program. Companies or individuals participating in this process can influence the design and development of the APIs. This process has been a subject of controversy.[when?] The class library contains features such as:\n\nThe core libraries, which include:\nIO/NIO\nNetworking\nReflection\nConcurrency\nGenerics\nScripting/Compiler\nFunctional Programming (Lambda, Streaming)\nCollection libraries that implement data structures such as lists, dictionaries, trees, sets, queues and double-ended queue, or stacks[62]\nXML Processing (Parsing, Transforming, Validating) libraries\nSecurity[63]\nInternationalization and localization libraries[64]\nThe integration libraries, which allow the application writer to communicate with external systems. These libraries include:\nThe Java Database Connectivity (JDBC) API for database access\nJava Naming and Directory Interface (JNDI) for lookup and discovery\nRMI and CORBA for distributed application development\nJMX for managing and monitoring applications\nUser interface libraries, which include:\nThe (heavyweight, or native) Abstract Window Toolkit (AWT), which provides GUI components, the means for laying out those components and the means for handling events from those components\nThe (lightweight) Swing libraries, which are built on AWT but provide (non-native) implementations of the AWT widgetry\nAPIs for audio capture, processing, and playback\nJavaFX\nA platform dependent implementation of the Java virtual machine that is the means by which the bytecodes of the Java libraries and third party applications are executed\nPlugins, which enable applets to be run in web browsers\nJava Web Start, which allows Java applications to be efficiently distributed to end users across the Internet\nLicensing and documentation\nDocumentation\nMain article: Javadoc\nJavadoc is a comprehensive documentation system, created by Sun Microsystems, used by many Java developers[by whom?]. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are /** and */, whereas the normal multi-line comments in Java are set off with the delimiters /* and */.[65]\n\nEditions\nSee also: Free Java implementations § Class library\nJava editions\nWave.svg\nJava Card\nMicro Edition (ME)\nStandard Edition (SE)\nEnterprise Edition (EE)\nJavaFX (Merged to Java SE 8)\nPersonalJava (discontinued)\nv t e\nSun has defined and supports four editions of Java targeting different application environments and segmented many of its APIs so that they belong to one of the platforms. The platforms are:\n\nJava Card for smartcards.[66]\nJava Platform, Micro Edition (Java ME) – targeting environments with limited resources.[67]\nJava Platform, Standard Edition (Java SE) – targeting workstation environments.[68]\nJava Platform, Enterprise Edition (Java EE) – targeting large distributed enterprise or Internet environments.[69]\nThe classes in the Java APIs are organized into separate groups called packages. Each package contains a set of related interfaces, classes and exceptions. Refer to the separate platforms for a description of the packages available.[relevant to this section? – discuss]\n\nSun also provided an edition called PersonalJava that has been superseded by later, standards-based Java ME configuration-profile pairings.\n\nSee also\nicon\tJava portal\nicon\tComputer programming portal\nBook icon\t\nBook: Programming for Students\nDalvik – used in old Android versions, replaced by non-JIT Android Runtime\nJavaOne\nJavapedia\nList of Java virtual machines\nList of Java APIs\nList of JVM languages\nGraal (compiler), a project aiming to implement a high performance Java dynamic compiler and interpreter\nSpring Framework\nComparison of Java with other languages\nComparison of programming languages\nComparison of Java and C++\nComparison of C# and Java",
          "type": "compiled",
          "plid": 16
        },
        {
          "name": "C",
          "details": "C (/ˈsiː/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.\n\nC was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs,[5] and used to re-implement the Unix operating system.[6] It has since become one of the most widely used programming languages of all time,[7][8] with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).\n\nContents  [hide] \n1\tDesign\n2\tOverview\n2.1\tRelations to other languages\n3\tHistory\n3.1\tEarly developments\n3.2\tK&R C\n3.3\tANSI C and ISO C\n3.4\tC99\n3.5\tC11\n3.6\tEmbedded C\n4\tSyntax\n4.1\tCharacter set\n4.2\tReserved words\n4.3\tOperators\n5\t\"Hello, world\" example\n6\tData types\n6.1\tPointers\n6.2\tArrays\n6.3\tArray–pointer interchangeability\n7\tMemory management\n8\tLibraries\n9\tLanguage tools\n10\tUses\n11\tRelated languages\n12\tSee also\n13\tNotes\n14\tReferences\n15\tSources\n16\tFurther reading\n17\tExternal links\nDesign[edit]\nC is an imperative procedural language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. Therefore, C was useful for many applications that had formerly been coded in assembly language, for example in system programming.\n\nDespite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.\n\nOverview[edit]\nLike most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations. In C, all executable code is contained within subroutines, which are called \"functions\" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.\n\nThe C language also exhibits the following characteristics:\n\nThere is a small, fixed number of keywords, including a full set of flow of control primitives: for, if/else, while, switch, and do/while. User-defined names are not distinguished from keywords by any kind of sigil.\nThere are a large number of arithmetical and logical operators, such as +, +=, ++, &, ~, etc.\nMore than one assignment may be performed in a single statement.\nFunction return values can be ignored when not needed.\nTyping is static, but weakly enforced: all data has a type, but implicit conversions may be performed.\nDeclaration syntax mimics usage context. C has no \"define\" keyword; instead, a statement beginning with the name of a type is taken as a declaration. There is no \"function\" keyword; instead, a function is indicated by the parentheses of an argument list.\nUser-defined (typedef) and compound types are possible.\nHeterogeneous aggregate data types (struct) allow related data elements to be accessed and assigned as a unit.\nArray indexing is a secondary notation, defined in terms of pointer arithmetic. Unlike structs, arrays are not first-class objects; they cannot be assigned or compared using single built-in operators. There is no \"array\" keyword, in use or definition; instead, square brackets indicate arrays syntactically, for example month[11].\nEnumerated types are possible with the enum keyword. They are not tagged, and are freely interconvertible with integers.\nStrings are not a separate data type, but are conventionally implemented as null-terminated arrays of characters.\nLow-level access to computer memory is possible by converting machine addresses to typed pointers.\nProcedures (subroutines not returning values) are a special case of function, with an untyped return type void.\nFunctions may not be defined within the lexical scope of other functions.\nFunction and data pointers permit ad hoc run-time polymorphism.\nA preprocessor performs macro definition, source code file inclusion, and conditional compilation.\nThere is a basic form of modularity: files can be compiled separately and linked together, with control over which functions and data objects are visible to other files via static and extern attributes.\nComplex functionality such as I/O, string manipulation, and mathematical functions are consistently delegated to library routines.\nWhile C does not include some features found in some other languages, such as object orientation or garbage collection, such features can be implemented or emulated in C, often by way of external libraries (e.g., the Boehm garbage collector or the GLib Object System).\n\nRelations to other languages[edit]\nMany later languages have borrowed directly or indirectly from C, including C++, D, Go, Rust, Java, JavaScript, Limbo, LPC, C#, Objective-C, Perl, PHP, Python, Verilog (hardware description language),[4] and Unix's C shell. These languages have drawn many of their control structures and other basic features from C. Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.\n\nHistory[edit]\nEarly developments[edit]\n\nKen Thompson (left) with Dennis Ritchie (right, the inventor of the C programming language)\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Ritchie and Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL.[9] However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C. The name of C was chosen simply as the next after B.[10]\n\nThe development of C started in 1972 on the PDP-11 Unix system[11] and first appeared in Version 2 Unix.[12] The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.[1][11]\n\nAlso in 1972, a large part of Unix was rewritten in C.[13] By 1973, with the addition of struct types, the C language had become powerful enough that most of the Unix's kernel was now in C.\n\nUnix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system which was written in PL/I), and Master Control Program (MCP) for the Burroughs B5000 written in ALGOL in 1961. In around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[11]\n\nK&R C[edit]\n\nThe cover of the book, The C Programming Language, first edition by Brian Kernighan and Dennis Ritchie\nIn 1978, Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[1] This book, known to C programmers as \"K&R\", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as K&R C. The second edition of the book[14] covers the later ANSI C standard, described below.\n\nK&R introduced several language features:\n\nStandard I/O library\nlong int data type\nunsigned int data type\nCompound assignment operators of the form =op (such as =-) were changed to the form op= (that is, -=) to remove the semantic ambiguity created by constructs such as i=-10, which had been interpreted as i =- 10 (decrement i by 10) instead of the possibly intended i = -10 (let i be -10).\nEven after the publication of the 1989 ANSI standard, for many years K&R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.\n\nIn early versions of C, only functions that return types other than int must be declared if used before the function definition; functions used without prior declaration were presumed to return type int.\n\nFor example:\n\nlong some_function();\n/* int */ other_function();\n\n/* int */ calling_function()\n{\n    long test1;\n    register /* int */ test2;\n\n    test1 = some_function();\n    if (test1 > 0)\n          test2 = 0;\n    else\n          test2 = other_function();\n    return test2;\n}\nThe int type specifiers which are commented out could be omitted in K&R C, but are required in later standards.\n\nSince K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.\n\nIn the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC[15]) and some other vendors. These included:\n\nvoid functions (i.e., functions with no return value)\nfunctions returning struct or union types (rather than pointers)\nassignment for struct data types\nenumerated types\nThe large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.\n\nANSI C and ISO C[edit]\nMain article: ANSI C\n\nThe cover of the book, The C Programming Language, second edition by Brian Kernighan and Dennis Ritchie covering ANSI C\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\n\nIn 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\n\nIn 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\n\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\n\nOne of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.\n\nC89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\n\nIn cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.\n\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.[citation needed]\n\nC99[edit]\nMain article: C99\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.[16]\n\nC99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with //, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\n\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.[17]\n\nC11[edit]\nMain article: C11 (C standard revision)\nIn 2007, work began on another revision of the C standard, informally called \"C1X\" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\n\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro __STDC_VERSION__ is defined as 201112L to indicate that C11 support is available.\n\nEmbedded C[edit]\nMain article: Embedded C\nHistorically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\n\nIn 2008, the C Standards Committee published a technical report extending the C language[18] to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\n\nSyntax[edit]\nMain article: C syntax\nC has a formal grammar specified by the C standard.[19] Unlike languages such as FORTRAN 77, C source code is free-form which allows arbitrary use of whitespace to format code, rather than column-based or text-line-based restrictions; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters /* and */, or (since C99) following // until the end of the line. Comments delimited by /* and */ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.[20]\n\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\n\nAs an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if(-else) conditional execution and by do-while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression.\n\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\n\nKernighan and Ritchie say in the Introduction of The C Programming Language: \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\"[21] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\n\nCharacter set[edit]\nThe basic C source character set includes the following characters:\n\nLowercase and uppercase letters of ISO Basic Latin Alphabet: a–z A–Z\nDecimal digits: 0–9\nGraphic characters: ! \" # % & ' ( ) * + , - . / : ; < = > ? [ \\ ] ^ _ { | } ~\nWhitespace characters: space, horizontal tab, vertical tab, form feed, newline\nNewline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.\n\nAdditional multi-byte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using \\uXXXX or \\UXXXXXXXX encoding (where the X denotes a hexadecimal character), although this feature is not yet widely implemented.\n\nThe basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.\n\nReserved words[edit]\nC89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:\n\nauto\nbreak\ncase\nchar\nconst\ncontinue\ndefault\ndo\ndouble\nelse\nenum\nextern\nfloat\nfor\ngoto\nif\nint\nlong\nregister\nreturn\nshort\nsigned\nsizeof\nstatic\nstruct\nswitch\ntypedef\nunion\nunsigned\nvoid\nvolatile\nwhile\nC99 reserved five more words:\n\n_Bool\n_Complex\n_Imaginary\ninline\nrestrict\nC11 reserved seven more words:[22]\n\n_Alignas\n_Alignof\n_Atomic\n_Generic\n_Noreturn\n_Static_assert\n_Thread_local\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called entry, but this was seldom implemented, and has now been removed as a reserved word.[23]\n\nOperators[edit]\nMain article: Operators in C and C++\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\n\narithmetic: +, -, *, /, %\nassignment: =\naugmented assignment: +=, -=, *=, /=, %=, &=, |=, ^=, <<=, >>=\nbitwise logic: ~, &, |, ^\nbitwise shifts: <<, >>\nboolean logic: !, &&, ||\nconditional evaluation: ? :\nequality testing: ==, !=\ncalling functions: ( )\nincrement and decrement: ++, --\nmember selection: ., ->\nobject size: sizeof\norder relations: <, <=, >, >=\nreference and dereference: &, *, [ ]\nsequencing: ,\nsubexpression grouping: ( )\ntype conversion: (typename)\nC uses the operator = (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator == to test for equality. The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression if(a==b+1) might mistakenly be written as if(a=b+1), which will be evaluated as true if a is not zero after the assignment.[24]\n\nThe C operator precedence is not always intuitive. For example, the operator == binds more tightly than (is executed prior to) the operators & (bitwise AND) and | (bitwise OR) in expressions such as x & 1 == 0, which must be written as (x & 1) == 0 if that is the coder's intent.[25]\n\n\"Hello, world\" example[edit]\nThe \"hello, world\" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints \"hello, world\" to the standard output, which is usually a terminal or screen display.\n\nThe original version was:[26]\n\nmain()\n{\n    printf(\"hello, world\\n\");\n}\nA standard-conforming \"hello, world\" program is:[a]\n\n#include <stdio.h>\n\nint main(void)\n{\n    printf(\"hello, world\\n\");\n    return 0;\n}\nThe first line of the program contains a preprocessing directive, indicated by #include. This causes the compiler to replace that line with the entire text of the stdio.h standard header, which contains declarations for standard input and output functions such as printf. The angle brackets surrounding stdio.h indicate that stdio.h is located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.\n\nThe next line indicates that a function named main is being defined. The main function serves a special purpose in C programs; the run-time environment calls the main function to begin program execution. The type specifier int indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the main function, is an integer. The keyword void as a parameter list indicates that this function takes no arguments.[b]\n\nThe opening curly brace indicates the beginning of the definition of the main function.\n\nThe next line calls (diverts execution to) a function named printf, which in this case is supplied from a system library. In this call, the printf function is passed (provided with) a single argument, the address of the first character in the string literal \"hello, world\\n\". The string literal is an unnamed array with elements of type char, set up automatically by the compiler with a final 0-valued character to mark the end of the array (printf needs to know this). The \\n is an escape sequence that C translates to a newline character, which on output signifies the end of the current line. The return value of the printf function is of type int, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the printf function succeeded.) The semicolon ; terminates the statement.\n\nThe closing curly brace indicates the end of the code for the main function. According to the C99 specification and newer, the main function, unlike any other function, will implicitly return a status of 0 upon reaching the } that terminates the function. This is interpreted by the run-time system as an exit code indicating successful execution.[27]\n\nData types[edit]\nMain article: C variable types and declarations\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal.[28] There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (enum). Integer type char is often used for single-byte characters. C99 added a boolean datatype. There are also derived types including arrays, pointers, records (struct), and untagged unions (union).\n\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\n\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)[29]\n\nC's usual arithmetic conversions allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\n\nPointers[edit]\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be dereferenced to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated struct objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.[27]\n\nA null pointer value explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a null pointer constant can be written as 0, with or without explicit casting to a pointer type, or as the NULL macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.\n\nVoid pointers (void *) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.[27]\n\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\n\nArrays[edit]\nSee also: C string\nArray types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's malloc function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.\n\nSince arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option.[30] Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.\n\nC does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multi-dimensional array\" can be thought of as increasing in row-major order.\n\nMulti-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional \"row vector\" of pointers to the columns.)\n\nC99 introduced \"variable-length arrays\" which address some, but not all, of the issues with ordinary C arrays.\n\nArray–pointer interchangeability[edit]\nThe subscript notation x[i] (where x designates a pointer) is syntactic sugar for *(x+i).[31] Taking advantage of the compiler's knowledge of the pointer type, the address that x + i points to is not the base address (pointed to by x) incremented by i bytes, but rather is defined to be the base address incremented by i multiplied by the size of an element that x points to. Thus, x[i] designates the i+1th element of the array.\n\nFurthermore, in most expression contexts (a notable exception is as operand of sizeof), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\n\nThe size of an element can be determined by applying the operator sizeof to any dereferenced element of x, as in n = sizeof *x or n = sizeof x[0], and the number of elements in a declared array A can be determined as sizeof A / sizeof A[0]. The latter only applies to array names: variables declared with subscripts (int A[20]). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (malloc); code such as sizeof arr / sizeof arr[0] (where arr designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested.[32][33] Since array name arguments to sizeof are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same sizeof issues as array pointers.\n\nThus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array \"points to\" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the memcpy function, or by accessing the individual elements.\n\nMemory management[edit]\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:[27]\n\nStatic memory allocation: space for the object is provided in the binary at compile-time; these objects have an extent (or lifetime) as long as the binary which contains them is loaded into memory.\nAutomatic memory allocation: temporary objects can be stored on the stack, and this space is automatically freed and reusable after the block in which they are declared is exited.\nDynamic memory allocation: blocks of memory of arbitrary size can be requested at run-time using library functions such as malloc from a region of memory called the heap; these blocks persist until subsequently freed for reuse by calling the library function realloc or free\nThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\n\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[27] Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\n\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\n\nAnother issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before free() is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)\n\nLibraries[edit]\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for \"link the math library\").[27]\n\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.\n\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\n\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.[27]\n\nLanguage tools[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2014) (Learn how and when to remove this template message)\nA number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler. The tool lint was the first such, leading to many others.\n\nAutomated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.[34]\n\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.\n\nTools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.\n\nUses[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)\n\nThe TIOBE index graph from 2002 to 2015, showing a comparison of the popularity of various programming languages[35]\nC is widely used for \"system programming\", including implementing operating systems and embedded system applications, because C code, when written for portability, can be used for most purposes, yet when needed, system-specific code can be used to access specific hardware addresses and to perform type punning to match externally imposed interface requirements, with a low run-time demand on system resources. C can also be used for website programming using CGI as a \"gateway\" for information between the Web application, the server, and the browser.[36] C is often chosen over interpreted languages because of its speed, stability, and near-universal availability.[37]\n\nOne consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The primary implementations of Python, Perl 5 and PHP, for example, are all written in C.\n\nBecause the layer of abstraction is thin and the overhead is low, C enables programmers to create efficient implementations of algorithms and data structures, useful for computationally intense programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.\n\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.\n\nC has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.\n\nRelated languages[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2016) (Learn how and when to remove this template message)\nC has directly or indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical: all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.\n\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\n\nWhen object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\n\nThe C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[38] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\n\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\n\nIn addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.\n\nSee also[edit]\nicon\tComputer programming portal\n\tInformation technology portal\nComparison of Pascal and C\nComparison of programming languages\nInternational Obfuscated C Code Contest\nList of C-based programming languages\nList of C compilers",
          "type": "compiled",
          "plid": 17
        },
        {
          "name": "C++",
          "details": "C++ (pronounced cee plus plus, /ˈsiː plʌs plʌs/) is a general-purpose programming language. It has imperative, object-oriented and generic programming features, while also providing facilities for low-level memory manipulation.\n\nIt was designed with a bias toward system programming and embedded, resource-constrained and large systems, with performance, efficiency and flexibility of use as its design highlights.[5] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[5] including desktop applications, servers (e.g. e-commerce, web search or SQL servers), and performance-critical applications (e.g. telephone switches or space probes).[6] C++ is a compiled language, with implementations of it available on many platforms and provided by various organizations, including the Free Software Foundation (FSF's GCC), LLVM, Microsoft, Intel and IBM.\n\nC++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2014 as ISO/IEC 14882:2014 (informally known as C++14).[7] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, ISO/IEC 14882:2003, standard. The current C++14 standard supersedes these and C++11, with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Bjarne Stroustrup at Bell Labs since 1979, as an extension of the C language as he wanted an efficient and flexible language similar to C, which also provided high-level features for program organization.\n\nMany other programming languages have been influenced by C++, including C#, D, Java, and newer versions of C (after 1998).\n\nContents  [hide] \n1\tHistory\n1.1\tEtymology\n1.2\tPhilosophy\n1.3\tStandardization\n2\tLanguage\n2.1\tObject storage\n2.1.1\tStatic storage duration objects\n2.1.2\tThread storage duration objects\n2.1.3\tAutomatic storage duration objects\n2.1.4\tDynamic storage duration objects\n2.2\tTemplates\n2.3\tObjects\n2.3.1\tEncapsulation\n2.3.2\tInheritance\n2.4\tOperators and operator overloading\n2.5\tPolymorphism\n2.5.1\tStatic polymorphism\n2.5.2\tDynamic polymorphism\n2.5.2.1\tInheritance\n2.5.2.2\tVirtual member functions\n2.6\tLambda expressions\n2.7\tException handling\n3\tStandard library\n4\tCompatibility\n4.1\tWith C\n5\tCriticism\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory[edit]\n\nBjarne Stroustrup, the creator of C++\nIn 1979, Bjarne Stroustrup, a Danish computer scientist, began work on the predecessor to C++, \"C with Classes\".[8] The motivation for creating a new language originated from Stroustrup's experience in programming for his Ph.D. thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his Ph.D. experience, Stroustrup set out to enhance the C language with Simula-like features.[9] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced C++, including ALGOL 68, Ada, CLU and ML.\n\nInitially, Stroustrup's \"C with Classes\" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining and default arguments.[10]\n\nIn 1983, C with Classes was renamed to C++ (\"++\" being the increment operator in C), adding new features that included virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL style single-line comments with two forward slashes (//). Furthermore, it included the development of a standalone compiler for C++, Cfront.\n\nIn 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[11] The first commercial implementation of C++ was released in October of the same year.[8]\n\nIn 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[12] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a boolean type.\n\nAfter the 2.0 update, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions are planned for 2017 and 2020.[13]\n\nEtymology[edit]\nAccording to Stroustrup: \"the name signifies the evolutionary nature of the changes from C\".[14] This name is credited to Rick Mascitti (mid-1983)[10] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's \"++\" operator (which increments the value of a variable) and a common naming convention of using \"+\" to indicate an enhanced computer program.\n\nDuring C++'s development period, the language had been referred to as \"new C\" and \"C with Classes\"[10][15] before acquiring its final name.\n\nPhilosophy[edit]\nThroughout C++'s life, its development and evolution has been informally governed by a set of rules that its evolution should follow:[9]\n\nIt must be driven by actual problems and its features should be useful immediately in real world programs.\nEvery feature should be implementable (with a reasonably obvious way to do so).\nProgrammers should be free to pick their own programming style, and that style should be fully supported by C++.\nAllowing a useful feature is more important than preventing every possible misuse of C++.\nIt should provide facilities for organising programs into well-defined separate parts, and provide facilities for combining separately developed parts.\nNo implicit violations of the type system (but allow explicit violations; that is, those explicitly requested by the programmer).\nUser-created types need to have the same support and performance as built-in types.\nUnused features should not negatively impact created executables (e.g. in lower performance).\nThere should be no language beneath C++ (except assembly language).\nC++ should work alongside other existing programming languages, rather than fostering its own separate and incompatible programming environment.\nIf the programmer's intent is unknown, allow the programmer to specify it by providing manual control.\nStandardization[edit]\nYear\tC++ Standard\tInformal name\n1998\tISO/IEC 14882:1998[16]\tC++98\n2003\tISO/IEC 14882:2003[17]\tC++03\n2011\tISO/IEC 14882:2011[7]\tC++11\n2014\tISO/IEC 14882:2014[18]\tC++14\n2017\tto be determined\tC++17\n2020\tto be determined\tC++20[13]\nC++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published four revisions of the C++ standard and is currently working on the next revision, C++17.\n\nIn 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.\n\nThe next major revision of the standard was informally referred to as \"C++0x\", but it was not released until 2011.[19] C++11 (14882:2011) included many additions to both the core language and the standard library.[7]\n\nIn 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[20] The Draft International Standard ballot procedures completed in mid-August 2014.[21]\n\nAfter C++14, a major revision, informally known as C++17 or C++1z, is planned for 2017,[20] which is almost feature-complete.[22]\n\nAs part of the standardization process, ISO also publishes technical reports and specifications:\n\nISO/IEC TR 18015:2006[23] on the use of C++ in embedded systems and on performance implications of C++ language and library features,\nISO/IEC TR 19768:2007[24] (also known as the C++ Technical Report 1) on library extensions mostly integrated into C++11,\nISO/IEC TR 29124:2010[25] on special mathematical functions,\nISO/IEC TR 24733:2011[26] on decimal floating point arithmetic,\nISO/IEC TS 18822:2015[27] on the standard filesystem library,\nISO/IEC TS 19570:2015[28] on parallel versions of the standard library algorithms,\nISO/IEC TS 19841:2015[29] on software transactional memory,\nISO/IEC TS 19568:2015[30] on a new set of library extensions, some of which are already integrated into C++17,\nISO/IEC TS 19217:2015[31] on the C++ Concepts\nMore technical specifications are in development and pending approval, including concurrency library extensions, a networking standard library, ranges, and modules.[32]\n\nLanguage[edit]\nThe C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as \"a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions\";[5] and \"offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages\".[33]\n\nC++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[34][35]\n\n#include <iostream>\n\nint main()\n{\n\tstd::cout << \"Hello, world!\\n\";\n}\nWithin functions that define a non-void return type, failure to return a value before control reaches the end of the function results in undefined behaviour (compilers typically provide the means to issue a diagnostic in such a case).[36] The sole exception to this rule is the main function, which implicitly returns a value of zero.[37]\n\nObject storage[edit]\nAs in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[38]\n\nStatic storage duration objects[edit]\nStatic storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that \"shall last for the duration of the program\".[39]\n\nStatic storage duration objects are initialized in two phases. First, \"static initialization\" is performed, and only after all static initialization is performed, \"dynamic initialization\" is performed. In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable. Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.\n\nThread storage duration objects[edit]\nVariables of this type are very similar to static storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[40]\n\nAutomatic storage duration objects[edit]\nThe most common variable types in C++ are local variables inside a function or block, and temporary variables.[41] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left.\n\nLocal variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.\n\nMember variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an \"automatic object\" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.\n\nTemporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).\n\nDynamic storage duration objects[edit]\nMain article: new and delete (C++)\nThese objects have a dynamic lifespan and are created with a call to new and destroyed explicitly with a call to delete.[42]\n\nTemplates[edit]\nSee also: Template metaprogramming and Generic programming\nC++ templates enable generic programming. C++ supports function, class, alias and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase \"Substitution failure is not an error\" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code was written by hand.[43] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.\n\nTemplates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.\n\nIn addition, templates are a compile time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.\n\nIn summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.\n\nObjects[edit]\nMain article: C++ classes\nC++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.\n\nEncapsulation[edit]\nEncapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class (\"friends\"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.\n\nThe OO principle is that all of the functions (and only the functions) that access the internal representation of a type should be encapsulated within the type definition. C++ supports this (via member functions and friend functions), but does not enforce it: the programmer can declare parts or all of the representation of a type to be public, and is allowed to make public entities that are not part of the representation of the type. Therefore, C++ supports not just OO programming, but other weaker decomposition paradigms, like modular programming.\n\nIt is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[44][45]\n\nInheritance[edit]\nInheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by \"inheritance\". The other two forms are much less frequently used. If the access specifier is omitted, a \"class\" inherits privately, while a \"struct\" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.\n\nMultiple inheritance is a C++ feature not found in most other languages, allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a \"Flying Cat\" class can inherit from both \"Cat\" and \"Flying Mammal\". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or \"ABC\". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.\n\nOperators and operator overloading[edit]\nOperators that cannot be overloaded\nOperator\tSymbol\nScope resolution operator\t::\nConditional operator\t?:\ndot operator\t.\nMember selection operator\t.*\n\"sizeof\" operator\tsizeof\n\"typeid\" operator\ttypeid\nMain article: Operators in C and C++\nC++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.\n\nOverloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded \"&&\" and \"||\" operators lose their short-circuit evaluation property.\n\nPolymorphism[edit]\nSee also: Polymorphism in object-oriented programming\nPolymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.\n\nC++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.\n\nStatic polymorphism[edit]\nSee also: Parametric polymorphism and ad hoc polymorphism\nFunction overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and would result in a compile-time error message.\n\nWhen declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.\n\nTemplates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the Curiously Recurring Template Pattern, it's possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[43]\n\nDynamic polymorphism[edit]\nInheritance[edit]\nSee also: Subtyping\nVariable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently depending on their (actual, derived) types.\n\nC++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.\n\nVirtual member functions[edit]\nOrdinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[46] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.\n\nIn addition to standard member functions, operator overloads and destructors can be virtual. As a rule of thumb, if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.\n\nA member function can also be made \"pure virtual\" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract data type. Objects cannot be created from abstract data types; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.\n\nLambda expressions[edit]\nC++ provides support for anonymous functions, which are also known as lambda expressions and have the following form:\n\n[capture](parameters) -> return_type { function_body }\nThe [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object. An example lambda function may be defined as follows:\n\n[](int x, int y) -> int { return x + y; }\nException handling[edit]\nException handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[47] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[48] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[49] At the same time, an exception is presented as an object carrying the data about the detected problem.[50]\n\nThe exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[51]\n\n#include <iostream>\n#include <vector>\n#include <stdexcept>\n\nint main() {\n    try {\n        std::vector<int> vec{3,4,3,1};\n        int i{vec.at(4)}; // Throws an exception, std::out_of_range (indexing for vec is from 0-3 not 1-4)\n    }\n\n    // An exception handler, catches std::out_of_range, which is thrown by vec.at(4)\n    catch (std::out_of_range& e) {\n        std::cerr << \"Accessing a non-existent element: \" << e.what() << '\\n';\n    }\n\n    // To catch any other standard library exceptions (they derive from std::exception)\n    catch (std::exception& e) {\n        std::cerr << \"Exception thrown: \" << e.what() << '\\n';\n    }\n\n    // Catch any unrecognised exceptions (i.e. those which don't derive from std::exception)\n    catch (...) {\n        std::cerr << \"Some fatal error\\n\";\n    }\n}\nIt is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the minimum time required for an exception to be handled.[52]\n\nStandard library[edit]\nMain article: C++ Standard Library\nThe C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes vectors, lists, maps, algorithms (find, for_each, binary_search, random_shuffle, etc.), sets, queues, stacks, arrays, tuples, input/output facilities (iostream, for reading from and writing to the console and files), smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that doesn't use C++ exceptions into C++ exceptions, a random number generator and a slightly modified version of the C standard library (to make it comply with the C++ type system).\n\nA large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.\n\nFurthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. C++ provides 105 standard headers, of which 27 are deprecated.\n\nThe standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as \"STL\", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[53]\n\nMost C++ compilers, and all major ones, provide a standards conforming implementation of the C++ standard library.\n\nCompatibility[edit]\nTo give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There were, however, attempts to standardize compilers for particular machines or operating systems (for example C++ ABI),[54] though they seem to be largely abandoned now.\n\nWith C[edit]\nFor more details on this topic, see Compatibility of C and C++.\nC++ is often considered to be a superset of C, but this is not strictly true.[55] Most C code can easily be made to compile correctly in C++, but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types, but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.\n\nSome incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//), and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support, were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code-compatible), designated initializers, compound literals, and the restrict keyword.[56] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[57][58][59] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.\n\nTo intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern \"C\" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).\n\nCriticism[edit]\nMain article: Criticism of C++\nDespite its widespread adoption, many programmers have criticized the C++ language, including Linus Torvalds,[60] Richard Stallman,[61] and Ken Thompson.[62] Issues include a lack of reflection or garbage collection, slow compilation times, perceived feature creep,[63] and verbose error messages, particularly from template metaprogramming.[64]\n\nTo avoid the problems that exist in C++, and to increase productivity,[65] some people suggest alternative languages newer than C++, such as D, Go, Rust and Vala.[66]",
          "type": "compiled",
          "plid": 18
        },
        {
          "name": "Elixir",
          "details": "lixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM). Elixir builds on top of Erlang and shares the same abstractions for building distributed, fault-tolerant applications. Elixir also provides a productive tooling and an extensible design. The latter is supported by compile-time metaprogramming with macros and polymorphism via protocols.[4]\n\nElixir is successfully used in the industry by companies such as Pinterest[5] and Moz.[6] Elixir is also used for web development, by companies such as Bleacher Report and Inverse,[7] and for building embedded-systems.[8][9] The community organizes yearly events in both United States[10][11][12] and Europe[13] as well as minor local events and conferences.[14][15]\n\nContents  [hide] \n1\tHistory\n2\tFeatures\n3\tExamples\n4\tReferences\n5\tExternal links\nHistory[edit]\nJosé Valim is the creator of the Elixir programming language, an R&D project of Plataformatec. His goals were to enable higher extensibility and productivity in the Erlang VM while keeping compatibility with Erlang's ecosystem.[16]\n\nFeatures[edit]\nA language that compiles to bytecode for the Erlang Virtual Machine (BEAM)[17]\nEverything is an expression[17]\nErlang functions can be called from Elixir without run time impact, due to compilation to Erlang bytecode, and vice versa\nMeta programming allowing direct manipulation of AST[17]\nPolymorphism via a mechanism called protocols. Like in Clojure, protocols provide a dynamic dispatch mechanism. However, this is not to be confused with multiple dispatch as Elixir protocols dispatch on a single type.\nSupport for documentation via Python-like docstrings in the Markdown formatting language[17]\nShared nothing concurrent programming via message passing (Actor model)[18]\nEmphasis on recursion and higher-order functions instead of side-effect-based looping\nLightweight concurrency utilizing Erlang's mechanisms.[17]\nLazy and async collections with streams\nPattern matching[17]\nUnicode support and UTF-8 strings\nExamples[edit]\nThe following examples can be run in an iex shell or saved in a file and run from the command line by typing elixir <filename>.\n\nClassic Hello world example:\n\niex> IO.puts \"Hello World!\"\nHello World!\nComprehensions\n\niex> for n <- [1,2,3,4,5], rem(n,2) == 1, do: n*n\n[1, 9, 25]\nPattern Matching\n\niex> [1, a] = [1, 2]\niex> a\n2\n\niex> {:ok, [hello: a]} = {:ok, [hello: \"world\"]}\niex> a\n\"world\"\nModules\n\ndefmodule Fun do\n  def fib(0), do: 0\n  def fib(1), do: 1\n  def fib(n) do \n    fib(n-2) + fib(n-1)  \n  end\nend\nSequentially spawning a thousand processes\n\nfor num <- 1..1000, do: spawn fn -> IO.puts \"#{num * 2}\" end\nAsynchronously performing a task\n\ntask = Task.async fn -> perform_complex_action() end\nother_time_consuming_action()\nTask.await task",
          "type": "dynamic",
          "plid": 20
        },
        {
          "name": "groovy",
          "details": "Apache Groovy is an object-oriented programming language for the Java platform. It is a dynamic language with features similar to those of Python, Ruby, Perl, and Smalltalk. It can be used as a scripting language for the Java Platform, is dynamically compiled to Java Virtual Machine (JVM) bytecode, and interoperates with other Java code and libraries. Groovy uses a Java-like curly-bracket syntax. Most Java code is also syntactically valid Groovy, although semantics may be different.\n\nGroovy 1.0 was released on January 2, 2007, and Groovy 2.0 in July, 2012. Since version 2, Groovy can also be compiled statically, offering type inference and performance very close to that of Java.[1][2] Groovy 2.4 was the last major release under Pivotal Software's sponsorship which ended in March 2015.[3] Groovy has since changed its governance structure to a Project Management Committee (PMC) in the Apache Software Foundation.[4]\n\nContents  [hide] \n1\tHistory\n2\tFeatures\n2.1\tGroovyBeans / Properties\n2.2\tPrototype extension\n2.3\tDot and parentheses\n2.4\tFunctional programming\n2.4.1\tClosures\n2.4.2\tCurry\n2.5\tXML and JSON processing\n2.6\tString interpolation\n2.7\tAST (Abstract Syntax Tree) Transformation\n2.8\tTraits\n3\tAdoption\n4\tIDE support\n5\tSee also\n6\tReferences\n6.1\tCitations\n6.2\tSources\n7\tExternal links\nHistory[edit]\nJames Strachan first talked about the development of Groovy on his blog in August 2003.[5] Several versions were released between 2004 and 2006. After the Java Community Process (JCP) standardization process began, the version numbering changed and a version called \"1.0\" was released on January 2, 2007. After various betas and release candidates numbered 1.1, on December 7, 2007, Groovy 1.1 Final was released and immediately rebranded as Groovy 1.5 as a reflection of the many changes made.\n\nIn 2007, Groovy won the first prize at JAX 2007 innovation award.[6] In 2008, Grails, a Groovy web framework, won the second prize at JAX 2008 innovation award.[7]\n\nIn November 2008, SpringSource acquired the Groovy and Grails company (G2One).[8] In August 2009 VMWare acquired SpringSource.[9]\n\nIn July 2009, Strachan wrote on his blog, \"I can honestly say if someone had shown me the Programming in Scala book by Martin Odersky, Lex Spoon & Bill Venners back in 2003 I'd probably have never created Groovy.\"[10] Strachan had left the project silently a year before the Groovy 1.0 release in 2007.[citation needed]\n\nIn March 2004, Groovy had been submitted to the JCP as JSR 241[11] and accepted by ballot. After 8 years of inactivity, the Spec Lead changed its status to dormant in April 2012.\n\nOn July 2, 2012, Groovy 2.0 was released, which, among other new features, added static compilation and a static type checker to Groovy.\n\nWhen the Pivotal joint venture was spun-off by EMC and VMware in April 2013, Groovy and Grails formed part of its product portfolio. Pivotal ceased sponsoring Groovy and Grails from April 2015.[3] That same month, Groovy changed its governance structure from a Codehaus repository to a Project Management Committee (PMC) in the Apache Software Foundation via its incubator.[4] Groovy graduated from Apache's incubator and become a top-level project in November 2015.[12]\n\nFeatures[edit]\nMost valid Java files are also valid Groovy files. Although the two languages are similar, Groovy code can be more compact, because it does not require all the elements that Java requires.[13] This makes it possible for Java programmers to learn Groovy gradually by starting with familiar Java syntax before acquiring more Groovy idioms.[14]\n\nGroovy features not available in Java include both static and dynamic typing (with the def keyword), operator overloading, native syntax for lists and associative arrays (maps), native support for regular expressions, polymorphic iteration, expressions embedded inside strings, additional helper methods, and the safe navigation operator \"?.\" to check automatically for nulls (for example, \"variable?.method()\", or \"variable?.field\").[15]\n\nSince version 2 Groovy also supports modularity (being able to ship only the needed jars according to the project needs, thus reducing the size of groovy's lib), type checking, static compilation, Project Coin syntax enhancements, multicatch blocks and ongoing performance enhancements using JDK7's invoke dynamic instruction.[16]\n\nGroovy provides native support for various markup languages such as XML and HTML, accomplished via an inline DOM syntax. This feature enables the definition and manipulation of many types of heterogeneous data assets with a uniform and concise syntax and programming methodology.[citation needed]\n\nUnlike Java, a Groovy source code file can be executed as an (uncompiled) script if it contains code outside any class definition, if it is a class with a main method, or if it is a Runnable or GroovyTestCase. A Groovy script is fully parsed, compiled, and generated before execution (similar to Perl and Ruby). This occurs under the hood, and the compiled version is not saved as an artifact of the process.[17]\n\nGroovyBeans / Properties[edit]\nGroovyBeans are Groovy's version of JavaBeans. Groovy implicitly generates getters and setters. In the following code, setColor(String color) and getColor() are implicitly generated; and the last two lines, which appear to access color directly, are actually calling the implicitly generated methods.[18]\n\nclass AGroovyBean {\n  String color\n}\n\ndef myGroovyBean = new AGroovyBean()\n\nmyGroovyBean.setColor('baby blue')\nassert myGroovyBean.getColor() == 'baby blue'\n\nmyGroovyBean.color = 'pewter'\nassert myGroovyBean.color == 'pewter'\nGroovy offers simple, consistent syntax for handling lists and maps, reminiscent of Java's array syntax.[19]\n\ndef movieList = ['Dersu Uzala', 'Ran', 'Seven Samurai']  //looks like an array, but is a list\nassert movieList[2] == 'Seven Samurai'\nmovieList[3] = 'Casablanca'  // adds an element to the list\nassert movieList.size() == 4\n\ndef monthMap = [ 'January' : 31, 'February' : 28, 'March' : 31 ]  //declares a map\nassert monthMap['March'] == 31  // accesses an entry\nmonthMap['April'] = 30  // adds an entry to the map\nassert monthMap.size() == 4\nPrototype extension[edit]\nGroovy offers support for prototype extension through ExpandoMetaClass, Extension Modules (only in Groovy 2), Objective-C-like Categories and DelegatingMetaClass.[20]\n\nExpandoMetaClass offers a domain-specific language (DSL) to express the changes in the class easily, similar to Ruby's open class concept:\n\nNumber.metaClass {\n  sqrt = { Math.sqrt(delegate) }\n}\n\nassert 9.sqrt() == 3\nassert 4.sqrt() == 2\nGroovy's changes in code through prototyping are not visible in Java, since each attribute/method invocation in Groovy goes through the metaclass registry. The changed code can only be accessed from Java by going to the metaclass registry.\n\nGroovy also allows overriding methods as getProperty(), propertyMissing() among others, enabling the developer to intercept calls to an object and specify an action for them, in a simplified aspect-oriented way. The following code enables the class java.lang.String to respond to the hex property:\n\nenum Color {\n  BLACK('#000000'), WHITE('#FFFFFF'), RED('#FF0000'), BLUE('#0000FF')\n  String hex\n  Color(String hex) { \n    this.hex = hex \n  }\n}\n\nString.metaClass.getProperty = { String property ->\n  def stringColor = delegate\n  if (property == 'hex') {\n    Color.values().find { it.name().equalsIgnoreCase stringColor }?.hex\n  }\n}\n\nassert \"WHITE\".hex == \"#FFFFFF\"\nassert \"BLUE\".hex == \"#0000FF\"\nassert \"BLACK\".hex == \"#000000\"\nassert \"GREEN\".hex == null\nThe Grails framework uses metaprogramming extensively to enable GORM dynamic finders, like User.findByName('Josh') and others.[21]\n\nDot and parentheses[edit]\nGroovy's syntax permits omitting parentheses and dots in some situations. The following groovy code\n\ntake(coffee).with(sugar, milk).and(liquor)\ncan be written as\n\ntake coffee with sugar, milk and liquor\nenabling the development of domain-specific languages (DSLs) which look like plain English.\n\nFunctional programming[edit]\nAlthough Groovy is mostly an object-oriented language, it also offers functional features.\n\nClosures[edit]\nAccording to Groovy's documentation: \"Closures in Groovy work similar to a 'method pointer', enabling code to be written and run in a later point in time\".[22] Groovy's closures support free variables, i.e. variables which have not been explicitly passed as a parameter to it, but exist in its declaration context, partial application (which it terms 'currying'[23]), delegation, implicit, typed and untyped parameters.\n\nWhen working on Collections of a determined type, the closure passed to an operation on the collection can be inferred:\n\nlist = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n/* \n * Non-zero numbers are coerced to true, so when it % 2 == 0 (even), it is false.\n * The type of the implicit \"it\" parameter can be inferred as an Integer by the IDE.\n * It could also be written as:\n * list.findAll { Integer i -> i % 2 }\n * list.findAll { i -> i % 2 }\n */\ndef odds = list.findAll { it % 2 }\n\nassert odds == [1, 3, 5, 7, 9]\nA group of expressions can be written in a closure block without reference to an implementation and the responding object can be assigned at a later point using delegation:\n\n// This block of code contains expressions without reference to an implementation\ndef operations = {\n  declare 5\n  sum 4\n  divide 3\n  print\n}\n/* \n * This class will handle the operations that can be used in the closure above. We could declare \n * another class having the same methods, but using, for example, webservice operations in the\n * calculations.\n */\nclass Expression {\n  BigDecimal value\n\n  /* \n   * Though an Integer is passed as a parameter, it is coerced into a BigDecimal, as we defined. \n   * If the class had a 'declare(Integer value)' method, it would be used instead.\n   */\n  def declare(BigDecimal value) {\n    this.value = value\n  }\n  \n  def sum(BigDecimal valueToAdd) {\n    this.value += valueToAdd\n  }\n  \n  def divide(BigDecimal divisor) {\n    this.value /= divisor\n  }\n  \n  def propertyMissing(String property) {\n    if (property == \"print\") println value\n  }\n}\n// Here we define who is going to respond the expressions in the block\noperations.delegate = new Expression()\noperations()\nCurry[edit]\nUsually called partial application,[23] this Groovy feature allows closures' parameters to be set to a default parameter in any of their arguments, creating a new closure with the bound value. If you supply one argument to the curry() method you will fix the first argument. If you supply N arguments you will fix arguments 1..N.\n\ndef joinTwoWordsWithSymbol = { symbol, first, second -> first + symbol + second }\nassert joinTwoWordsWithSymbol('#', 'Hello', 'World') == 'Hello#World'\n\ndef concatWords = joinTwoWordsWithSymbol.curry(' ')\nassert concatWords('Hello', 'World') == 'Hello World'\n\ndef prependHello = concatWords.curry('Hello')\n// def prependHello = joinTwoWordsWithSymbol.curry(' ', 'Hello')\nassert prependHello('World') == 'Hello World'\nCurry can also be used in the reversed direction (fixing arguments N to N-1) using rcurry.\n\ndef power = { BigDecimal value, BigDecimal power ->\n  value ** power\n}\n\ndef square = power.rcurry(2)\ndef cube = power.rcurry(3)\n\nassert power(2, 2) == 4\nassert square(4) == 16\nassert cube(3) == 27\nGroovy also supports lazy evaluation,[24][25] reduce/fold,[26] infinite structures and immutability,[27] among others.[28]\n\nXML and JSON processing[edit]\nOn XML and JSON processing Groovy employs the Builder pattern, making the production of the data structure less verbose. For example, the following XML:\n\n<languages>\n  <language year=\"1995\">\n    <name>java</name>\n    <paradigm>Object oriented</paradigm>\n    <typing>Static</typing>\n  </language>\n  <language year=\"1995\">\n    <name>ruby</name>\n    <paradigm>Object oriented, Functional</paradigm>\n    <typing>Dynamic, duck typing</typing>\n  </language>  \n  <language year=\"2003\">\n    <name>groovy</name>\n    <paradigm>Object oriented, Functional</paradigm>\n    <typing>Dynamic, Static, Duck typing</typing>\n  </language>\n</languages>\nCan be generated through the following Groovy code:\n\ndef writer = new StringWriter()\ndef builder = new groovy.xml.MarkupBuilder(writer)\nbuilder.languages {\n  language(year: 1995) {\n    name \"java\"\n    paradigm \"Object oriented\"\n    typing \"Static\"\n  }\n  language (year: 1995) {\n    name \"ruby\"\n    paradigm \"Object oriented, Functional\"\n    typing \"Dynamic, Duck typing\"\n  }\n  language (year: 2003) {\n    name \"groovy\"\n    paradigm \"Object oriented, Functional\"\n    typing \"Dynamic, Static, Duck typing\"\n  }\n}\nAnd also can be processed in a streaming way through StreamingMarkupBuilder. To change the implementation to JSON, we can just swap the MarkupBuilder to JsonBuilder.[29]\n\nTo parse it and search for a functional language we can use Groovy's findAll method:\n\ndef languages = new XmlSlurper().parseText writer.toString()\n\n// Here we employ groovy's regex syntax for a matcher (=~) which will be coerced to a boolean value: \n// either true if the value contains our string, or false otherwise.\ndef functional = languages.language.findAll { it.paradigm =~ \"functional\" }\nassert functional.collect { it.name } == [\"ruby\", \"groovy\"]\nString interpolation[edit]\nIn Groovy we can interpolate the string with variables and expressions by using GStrings:[30]\n\nBigDecimal account = 10.0\ndef text = \"Your account shows currently a balance of $account\"\nassert text == \"Your account shows currently a balance of 10.0\"\nGStrings containing variables and expressions must be declared using double quotes.\n\nA complex expression must be enclosed in curly brackets. This prevents parts of it from being interpreted as belonging to the surrounding string instead of to the expression:\n\nBigDecimal minus = 4.0\ntext = \"Your account shows currently a balance of ${account - minus}\"\nassert text == \"Your account shows currently a balance of 6.0\"\n\n// Without the brackets to isolate the expression, we would have the following:\ntext = \"Your account shows currently a balance of $account - minus\"\nassert text == \"Your account shows currently a balance of 10.0 - minus\"\nExpression evaluation can be deferred by employing arrow syntax:\n\nBigDecimal tax = 0.15\ntext = \"Your account shows currently a balance of ${->account - account * tax}\"\ntax = 0.10\n\n// The tax value was changed AFTER declaration of the GString. The expression \n// variables are bound only when the expression must actually be evaluated:\n\nassert text == \"Your account shows currently a balance of 9.000\"\nAST (Abstract Syntax Tree) Transformation[edit]\nAccording to Groovy's own documentation, \"When the Groovy compiler compiles Groovy scripts and classes, at some point in the process, the source code will end up being represented in memory in the form of a Concrete Syntax Tree, then transformed into an Abstract Syntax Tree. The purpose of AST Transformations is to let developers hook into the compilation process to be able to modify the AST before it is turned into bytecode that will be run by the JVM. AST Transformations provides Groovy with improved compile-time metaprogramming capabilities allowing powerful flexibility at the language level, without a runtime performance penalty.\".[31]\n\nExamples of ASTs in Groovy are:\n\nSingleton transformation;\nCategory and Mixin transformation;\nImmutable AST Macro;\nNewify transformation;\nAmong others.\n\nTraits[edit]\nAccording to Groovy's documentation, \"Traits are a structural construct of the language which allow: composition of behaviors, runtime implementation of interfaces, behavior overriding, and compatibility with static type checking/compilation.\"\n\nTraits can be seen as interfaces carrying both default implementations and state. A trait is defined using the trait keyword:\n\ntrait FlyingAbility { /* declaration of a trait */\n    String fly() { \"I'm flying!\" } /* declaration of a method inside a trait */\n}\nThen it can be used like a normal interface using the implements keyword:\n\nclass Bird implements FlyingAbility {} /* Adds the trait FlyingAbility to the Bird class capabilities */\ndef b = new Bird() /* instantiate a new Bird */\nassert b.fly() == \"I'm flying!\" /* the Bird class automatically gets the behavior of the FlyingAbility trait */\nTraits allow a wide range of capabilities, from simple composition to testing.\n\nAdoption[edit]\neXo Platform, an Open Source Enterprise Social Collaboration Platform uses Groovy.\nWired.com uses Groovy and Grails for the Product Reviews standalone section of the website.[32]\nThough Groovy can be integrated into any JVM environment, the JBoss Seam framework provides Groovy, besides Java, as a development language, out of the box.[33]\nThe European Patent Office (EPO) developed a Data Flow Language in Groovy \"to leverage similarities in the processes for communicating with each individual country’s patent office, and transform them into a single, universal process.\"[citation needed]\nLinkedin uses Groovy and Grails for some of their subsystems.[34]\nXWiki SAS uses Groovy as scripting language in their collaborative open-source product.[35]\nSoapUI provides Groovy as a language for webservice tests development.[36]\nSky.com uses Groovy and Grails to serve massive online media content.[37]\nDataMelt integrates Groovy into a numeric and statistic data-analysis framework\nvCalc.com uses Groovy for all of the user defined mathematics in its math crowd-sourcing engine.[38]\nEucalyptus, a cloud management system, uses a significant amount of Groovy.\nApache OFBiz, the open source enterprise resource planning (ERP) system, uses Groovy.[39][40]\nSmartThings, an open platform for smart homes and the consumer Internet of Things, uses a security-oriented subset of Groovy[41]\nSurvata, a market research startup, uses Groovy and Grails.[citation needed]\nJenkins, a platform for continuous integration. With version 2, Jenkins includes a Pipeline plugin that allows for build instructions to be written in Groovy.[42]\nIDE support[edit]\nMany integrated development environments and text editors support Groovy:\n\nEclipse, through Groovy-Eclipse\nEmacs, using the groovy-emacs-mode project's groovy-mode.\nIntelliJ IDEA, Community Edition, Grails/Griffon in the Ultimate Edition only\nNetBeans, since version 6.5\nTextMate\nDataMelt Java IDE\nSublime Text 2, a cross platform text editor\nJDeveloper, for use with Oracle ADF\njEdit, an advanced text editor for the Java Platform\nKate, an advanced text editor for KDE supports Groovy and over 200 other file formats\nNotepad++, an advanced text editor for Microsoft Windows\nAndroid Studio, IDE used for making Android apps\nAtom IDE\nSee also[edit]\n\tFree software portal\nGriffon – a desktop framework\nGradle – a build automation tool\nProject Zero\nComparison of programming languages",
          "type": "compiled",
          "plid": 21
        },
        {
          "name": "JavaScript",
          "details": "In computing, JavaScript (/ˈdʒævəˌskrɪpt/[5]) is a high-level, dynamic, untyped, and interpreted programming language.[6] It has been standardized in the ECMAScript language specification.[7] Alongside HTML and CSS, JavaScript is one of the three core technologies of World Wide Web content production; the majority of websites employ it, and all modern Web browsers support it without the need for plug-ins.[6] JavaScript is prototype-based with first-class functions, making it a multi-paradigm language, supporting object-oriented,[8] imperative, and functional programming styles.[6] It has an API for working with text, arrays, dates and regular expressions, but does not include any I/O, such as networking, storage, or graphics facilities, relying for these upon the host environment in which it is embedded.[7]\n\nAlthough there are strong outward similarities between JavaScript and Java, including language name, syntax, and respective standard libraries, the two are distinct languages and differ greatly in their design. JavaScript was influenced by programming languages such as Self and Scheme.[9]\n\nJavaScript is also used in environments that are not Web-based, such as PDF documents, site-specific browsers, and desktop widgets. Newer and faster JavaScript virtual machines (VMs) and platforms built upon them have also increased the popularity of JavaScript for server-side Web applications. On the client side, developers have traditionally implemented JavaScript as an interpreted language, but more recent browsers perform just-in-time compilation. Programmers also use JavaScript in video-game development, in crafting desktop and mobile applications, and in server-side network programming with run-time environments such as Node.js.\n\nContents  [hide] \n1\tHistory\n1.1\tBeginnings at Netscape\n1.2\tServer-side JavaScript\n1.3\tAdoption by Microsoft\n1.4\tStandardization\n1.5\tLater developments\n2\tTrademark\n3\tFeatures\n3.1\tImperative and structured\n3.2\tDynamic\n3.3\tPrototype-based (Object-oriented)\n3.4\tFunctional\n3.5\tDelegative\n3.6\tMiscellaneous\n3.7\tVendor-specific extensions\n4\tSyntax\n4.1\tSimple examples\n4.2\tMore advanced example\n5\tUse in Web pages\n5.1\tExample script\n5.2\tCompatibility considerations\n6\tSecurity\n6.1\tCross-site vulnerabilities\n6.1.1\tMisplaced trust in the client\n6.1.2\tBrowser and plugin coding errors\n6.1.3\tSandbox implementation errors\n7\tUses outside Web pages\n7.1\tEmbedded scripting language\n7.2\tScripting engine\n7.3\tApplication platform\n8\tDevelopment tools\n9\tBenchmark tools for developers\n10\tVersion history\n11\tRelated languages and features\n11.1\tUse as an intermediate language\n11.2\tJavaScript and Java\n12\tReferences\n13\tFurther reading\n14\tExternal links\nHistory[edit]\nBeginnings at Netscape[edit]\nIn 1993, the National Center for Supercomputing Applications (NCSA), a unit of the University of Illinois at Urbana-Champaign, released NCSA Mosaic, the first popular graphical Web browser, which played an important part in expanding the growth of the nascent World Wide Web. In 1994, a company called Mosaic Communications was founded in Mountain View, California and employed many of the original NCSA Mosaic authors to create Mosaic Netscape. However, it intentionally shared no code with NCSA Mosaic. The internal codename for the company's browser was Mozilla, which stood for \"Mosaic killer\", as the company's goal was to displace NCSA Mosaic as the world's number one web browser. The first version of the Web browser, Mosaic Netscape 0.9, was released in late 1994. Within four months it had already taken three-quarters of the browser market and became the main browser for Internet in the 1990s. To avoid trademark ownership problems with the NCSA, the browser was subsequently renamed Netscape Navigator in the same year, and the company took the name Netscape Communications.\n\nNetscape Communications realized that the Web needed to become more dynamic. Marc Andreessen, the founder of the company believed that HTML needed a \"glue language\" that was easy to use by Web designers and part-time programmers to assemble components such as images and plugins, where the code could be written directly in the Web page markup. In 1995, the company recruited Brendan Eich with the goal of embedding the Scheme programming language into its Netscape Navigator. Before he could get started, Netscape Communications collaborated with Sun Microsystems to include in Netscape Navigator Sun's more static programming language Java, in order to compete with Microsoft for user adoption of Web technologies and platforms.[10] Netscape Communications then decided that the scripting language they wanted to create would complement Java and should have a similar syntax, which excluded adopting other languages such as Perl, Python, TCL, or Scheme. To defend the idea of JavaScript against competing proposals, the company needed a prototype. Eich wrote one in 10 days, in May 1995.\n\nAlthough it was developed under the name Mocha, the language was officially called LiveScript when it first shipped in beta releases of Netscape Navigator 2.0 in September 1995, but it was renamed JavaScript[11] when it was deployed in the Netscape Navigator 2.0 beta 3 in December.[12] The final choice of name caused confusion, giving the impression that the language was a spin-off of the Java programming language, and the choice has been characterized as a marketing ploy by Netscape to give JavaScript the cachet of what was then the hot new Web programming language.\n\nThere is a common misconception that JavaScript was influenced by an earlier Web page scripting language developed by Nombas named C-- (not to be confused with the later C-- created in 1997).[13][14] Brendan Eich, however, had never heard of C-- before he created LiveScript.[15] Nombas did pitch their embedded Web page scripting to Netscape, though Web page scripting was not a new concept, as shown by the ViolaWWW Web browser.[16] Nombas later switched to offering JavaScript instead of C-- in their ScriptEase product and was part of the TC39 group that standardized ECMAScript.[17]\n\nServer-side JavaScript[edit]\nNetscape introduced an implementation of the language for server-side scripting with Netscape Enterprise Server in December 1995, soon after releasing JavaScript for browsers.[18] Since the mid-2000s, there has been a resurgence of server-side JavaScript implementations, such as Node.js.[19] and MarkLogic.[20]\n\nAdoption by Microsoft[edit]\nMicrosoft script technologies including VBScript and JScript were released in 1996. JScript, a reverse-engineered implementation of Netscape's JavaScript, was part of Internet Explorer 3 as well as being available server-side in Internet Information Server. Internet Explorer 3 also included Microsoft's first support for CSS and various extensions to HTML, but in each case the implementation was noticeably different to that found in Netscape Navigator at the time.[21][22] These differences made it difficult for designers and programmers to make a single website work well in both browsers, leading to the use of \"best viewed in Netscape\" and \"best viewed in Internet Explorer\" logos that characterized these early years of the browser wars.[23] JavaScript began to acquire a reputation for being one of the roadblocks to a cross-platform and standards-driven Web. Some developers took on the difficult task of trying to make their sites work in both major browsers, but many could not afford the time.[21] With the release of Internet Explorer 4, Microsoft introduced the concept of Dynamic HTML, but the differences in language implementations and the different and proprietary Document Object Models remained and were obstacles to widespread take-up of JavaScript on the Web.[21]\n\nStandardization[edit]\nIn November 1996, Netscape submitted JavaScript to Ecma International to carve out a standard specification, which other browser vendors could then implement based on the work done at Netscape. This led to the official release of the language specification ECMAScript published in the first edition of the ECMA-262 standard in June 1997, with JavaScript being the most well known of the implementations. ActionScript and JScript are other well-known implementations of ECMAScript, with extensions.\n\nThe standards process continued in cycles, with the release of ECMAScript 2 in June 1998, which brings some modifications to conform to the ISO/IEC 16262 international standard. The release of ECMAScript 3 followed in December 1999, which is the baseline for modern day JavaScript. The original ECMAScript 4 work led by Waldemar Horwat (then at Netscape, now at Google) started in 2000 and at first, Microsoft seemed to participate and even implemented some of the proposals in their JScript .NET language.\n\nOver time it was clear though that Microsoft had no intention of cooperating or implementing proper JavaScript in Internet Explorer, even though they had no competing proposal and they had a partial (and diverged at this point) implementation on the .NET server side. So by 2003, the original ECMAScript 4 work was mothballed.\n\nThe next major event was in 2005, with two major happenings in JavaScript's history. First, Brendan Eich and Mozilla rejoined Ecma International as a not-for-profit member and work started on ECMAScript for XML (E4X), the ECMA-357 standard, which came from ex-Microsoft employees at BEA Systems (originally acquired as Crossgain). This led to working jointly with Macromedia (later acquired by Adobe Systems), who were implementing E4X in ActionScript 3 (ActionScript 3 was a fork of original ECMAScript 4).\n\nSo, along with Macromedia, work restarted on ECMAScript 4 with the goal of standardizing what was in ActionScript 3. To this end, Adobe Systems released the ActionScript Virtual Machine 2, code named Tamarin, as an open source project. But Tamarin and ActionScript 3 were too different from web JavaScript to converge, as was realized by the parties in 2007 and 2008.\n\nAlas, there was still turmoil between the various players; Douglas Crockford—then at Yahoo!—joined forces with Microsoft in 2007 to oppose ECMAScript 4, which led to the ECMAScript 3.1 effort. The development of ECMAScript 4 was never completed, but that work influenced subsequent versions.[24]\n\nWhile all of this was happening, the open source and developer communities set to work to revolutionize what could be done with JavaScript. This community effort was sparked in 2005 when Jesse James Garrett released a white paper in which he coined the term Ajax, and described a set of technologies, of which JavaScript was the backbone, used to create web applications where data can be loaded in the background, avoiding the need for full page reloads and leading to more dynamic applications. This resulted in a renaissance period of JavaScript usage spearheaded by open source libraries and the communities that formed around them, with libraries such as Prototype, jQuery, Dojo Toolkit, MooTools and others being released.\n\nIn July 2008, the disparate parties on either side came together in Oslo. This led to the eventual agreement in early 2009 to rename ECMAScript 3.1 to ECMAScript 5 and drive the language forward using an agenda that is known as Harmony. ECMAScript 5 was finally released in December 2009.\n\nIn June 2011, ECMAScript 5.1 was released to fully align with the third edition of the ISO/IEC 16262 international standard. ECMAScript 2015 was released in June 2015. The current version is ECMAScript 2016, released in June 2016.[25]\n\nLater developments[edit]\nJavaScript has become one of the most popular programming languages on the Web. Initially, however, many professional programmers denigrated the language because, among other reasons, its target audience consisted of Web authors and other such \"amateurs\".[26] The advent of Ajax returned JavaScript to the spotlight and brought more professional programming attention. The result was a proliferation of comprehensive frameworks and libraries, improved JavaScript programming practices, and increased usage of JavaScript outside Web browsers, as seen by the proliferation of server-side JavaScript platforms.\n\nIn January 2009, the CommonJS project was founded with the goal of specifying a common standard library mainly for JavaScript development outside the browser.[27]\n\nWith the rise of single-page applications and JavaScript-heavy sites, it is increasingly being used as a compile target for source-to-source compilers from both dynamic languages and static languages.\n\nTrademark[edit]\n\"JavaScript\" is a trademark of Oracle Corporation.[28] It is used under license for technology invented and implemented by Netscape Communications and current entities such as the Mozilla Foundation.[29]\n\nFeatures[edit]\nThe following features are common to all conforming ECMAScript implementations, unless explicitly specified otherwise.\n\nImperative and structured[edit]\nJavaScript supports much of the structured programming syntax from C (e.g., if statements, while loops, switch statements, do while loops, etc.). One partial exception is scoping: JavaScript originally had only function scoping with var. ECMAScript 2015 added a let keyword for block scoping, meaning JavaScript now has both function and block scoping. Like C, JavaScript makes a distinction between expressions and statements. One syntactic difference from C is automatic semicolon insertion, which allows the semicolons that would normally terminate statements to be omitted.[30]\n\nDynamic[edit]\nTyping\nAs with most scripting languages, JavaScript is dynamically typed; a type is associated with each value, rather than just with each expression. For example, a variable that is at one time bound to a number may later be re-bound to a string.[31] JavaScript supports various ways to test the type of an object, including duck typing.[32]\nRun-time evaluation\nJavaScript includes an eval function that can execute statements provided as strings at run-time.\nPrototype-based (Object-oriented)[edit]\nJavaScript is almost entirely object-based. In JavaScript, an object is an associative array, augmented with a prototype (see below); each string key provides the name for an object property, and there are two syntactical ways to specify such a name: dot notation (obj.x = 10) and bracket notation (obj['x'] = 10). A property may be added, rebound, or deleted at run-time. Most properties of an object (and any property that belongs to an object's prototype inheritance chain) can be enumerated using a for...in loop.\n\nJavaScript has a small number of built-in objects, including Function and Date.\n\nPrototypes\nJavaScript uses prototypes where many other object-oriented languages use classes for inheritance.[33] It is possible to simulate many class-based features with prototypes in JavaScript.[34]\nFunctions as object constructors\nFunctions double as object constructors, along with their typical role. Prefixing a function call with new will create an instance of a prototype, inheriting properties and methods from the constructor (including properties from the Object prototype).[35] ECMAScript 5 offers the Object.create method, allowing explicit creation of an instance without automatically inheriting from the Object prototype (older environments can assign the prototype to null).[36] The constructor's prototype property determines the object used for the new object's internal prototype. New methods can be added by modifying the prototype of the function used as a constructor. JavaScript's built-in constructors, such as Array or Object, also have prototypes that can be modified. While it is possible to modify the Object prototype, it is generally considered bad practice because most objects in JavaScript will inherit methods and properties from the Object prototype, and they may not expect the prototype to be modified.[37]\nFunctions as methods\nUnlike many object-oriented languages, there is no distinction between a function definition and a method definition. Rather, the distinction occurs during function calling; when a function is called as a method of an object, the function's local this keyword is bound to that object for that invocation.\nFunctional[edit]\nA function is first-class; a function is considered to be an object. As such, a function may have properties and methods, such as .call() and .bind().[38] A nested function is a function defined within another function. It is created each time the outer function is invoked. In addition, each nested function forms a lexical closure: The lexical scope of the outer function (including any constant, local variable, or argument value) becomes part of the internal state of each inner function object, even after execution of the outer function concludes.[39] JavaScript also supports anonymous functions.\n\nDelegative[edit]\nJavaScript supports implicit and explicit delegation.\n\nFunctions as Roles (Traits and Mixins)\nJavaScript natively supports various function-based implementations of Role[40] patterns like Traits[41][42] and Mixins.[43] Such a function defines additional behavior by at least one method bound to the this keyword within its function body. A Role then has to be delegated explicitly via call or apply to objects that need to feature additional behavior that is not shared via the prototype chain.\nObject Composition and Inheritance\nWhereas explicit function-based delegation does cover composition in JavaScript, implicit delegation already happens every time the prototype chain is walked in order to, e.g., find a method that might be related to but is not directly owned by an object. Once the method is found it gets called within this object's context. Thus inheritance in JavaScript is covered by a delegation automatism that is bound to the prototype property of constructor functions.\nMiscellaneous[edit]\nRun-time environment\nJavaScript typically relies on a run-time environment (e.g., a Web browser) to provide objects and methods by which scripts can interact with the environment (e.g., a webpage DOM). It also relies on the run-time environment to provide the ability to include/import scripts (e.g., HTML <script> elements). This is not a language feature per se, but it is common in most JavaScript implementations.\nJavaScript processes messages from a queue one at a time. Upon loading a new message, JavaScript calls a function associated with that message, which creates a call stack frame (the function's arguments and local variables). The call stack shrinks and grows based on the function's needs. Upon function completion, when the stack is empty, JavaScript proceeds to the next message in the queue. This is called the event loop, described as \"run to completion\" because each message is fully processed before the next message is considered. However, the language's concurrency model describes the event loop as non-blocking: program input/output is performed using events and callback functions. This means, for instance, that JavaScript can process a mouse click while waiting for a database query to return information.[44]\nVariadic functions\nAn indefinite number of parameters can be passed to a function. The function can access them through formal parameters and also through the local arguments object. Variadic functions can also be created by using the bind method.\nArray and object literals\nLike many scripting languages, arrays and objects (associative arrays in other languages) can each be created with a succinct shortcut syntax. In fact, these literals form the basis of the JSON data format.\nRegular expressions\nJavaScript also supports regular expressions in a manner similar to Perl, which provide a concise and powerful syntax for text manipulation that is more sophisticated than the built-in string functions.[45]\nVendor-specific extensions[edit]\nJavaScript is officially managed by Mozilla Foundation, and new language features are added periodically. However, only some JavaScript engines support these new features:\n\nproperty getter and setter functions (supported by WebKit, Gecko, Opera,[46] ActionScript, and Rhino)[47]\nconditional catch clauses\niterator protocol (adopted from Python)\nshallow generators-coroutines (adopted from Python)\narray comprehensions and generator expressions (adopted from Python)\nproper block scope via the let keyword\narray and object destructuring (limited form of pattern matching)\nconcise function expressions (function(args) expr)\nECMAScript for XML (E4X), an extension that adds native XML support to ECMAScript (unsupported in Firefox since version 21[48])\nSyntax[edit]\nMain article: JavaScript syntax\nSimple examples[edit]\nVariables in JavaScript can be defined using the var keyword:[49]\n\nvar x; // defines the variable x and assigns to it the special value \"undefined\" (not to be confused with an undefined value)\nvar y = 2; // defines the variable y and assigns to it the value 2\nNote the comments in the example above, both of which were preceded with two forward slashes.\n\nThere is no built-in I/O functionality in JavaScript; the run-time environment provides that. The ECMAScript specification in edition 5.1 mentions:[50]\n\n… indeed, there are no provisions in this specification for input of external data or output of computed results.\n\nHowever, most runtime environments have a console object [51] that can be used to print output. Here is a minimalist Hello World program:\n\nconsole.log(\"Hello World!\");\nA simple recursive function:\n\nfunction factorial(n) {\n    if (n == 0) {\n        return 1;\n    }\n\n    return n * factorial(n - 1);\n}\n\nfactorial(3); // returns 6\nAn anonymous function (or lambda):\n\nfunction counter() {\n    var count = 0;\n    return function () {\n        return ++count;\n    };\n};\n\nvar closure = counter();\nclosure(); // returns 1\nclosure(); // returns 2\nclosure(); // returns 3\nThis example shows that in JavaScript, function closures captures their non-local variables by reference.\n\nVariadic function demonstration (arguments is a special variable):[52]\n\nfunction sum() {\n    var x = 0;\n\n    for (var i = 0; i < arguments.length; ++i) {\n        x += arguments[i];\n    }\n\n    return x;\n}\n\nsum(1, 2); // returns 3\nsum(1, 2, 3); // returns 6\nImmediately-invoked function expressions are often used to create modules, as before ECMAScript 2015 there was not built-in construct in the language. Modules allow gathering properties and methods in a namespace and making some of them private:\n\nvar counter = (function () {\n    var i = 0; // private property\n\n    return {   // public methods\n        get: function () {\n            alert(i);\n        },\n        set: function (value) {\n            i = value;\n        },\n        increment: function () {\n            alert(++i);\n        }\n    };\n})(); // module\n\ncounter.get();       // shows 0\ncounter.set(6);\ncounter.increment(); // shows 7\ncounter.increment(); // shows 8\nMore advanced example[edit]\nThis sample code displays various JavaScript features.\n\n/* Finds the lowest common multiple (LCM) of two numbers */\nfunction LCMCalculator(x, y) { // constructor function\n    var checkInt = function (x) { // inner function\n        if (x % 1 !== 0) {\n            throw new TypeError(x + \" is not an integer\"); // throw an exception\n        }\n        return x;\n    };\n    this.a = checkInt(x)\n    //   semicolons   ^^^^  are optional, a newline is enough\n    this.b = checkInt(y);\n}\n// The prototype of object instances created by a constructor is\n// that constructor's \"prototype\" property.\nLCMCalculator.prototype = { // object literal\n    constructor: LCMCalculator, // when reassigning a prototype, set the constructor property appropriately\n    gcd: function () { // method that calculates the greatest common divisor\n        // Euclidean algorithm:\n        var a = Math.abs(this.a), b = Math.abs(this.b), t;\n        if (a < b) {\n            // swap variables\n            t = b;\n            b = a;\n            a = t;\n        }\n        while (b !== 0) {\n            t = b;\n            b = a % b;\n            a = t;\n        }\n        // Only need to calculate GCD once, so \"redefine\" this method.\n        // (Actually not redefinition—it's defined on the instance itself,\n        // so that this.gcd refers to this \"redefinition\" instead of LCMCalculator.prototype.gcd.\n        // Note that this leads to a wrong result if the LCMCalculator object members \"a\" and/or \"b\" are altered afterwards.)\n        // Also, 'gcd' === \"gcd\", this['gcd'] === this.gcd\n        this['gcd'] = function () {\n            return a;\n        };\n        return a;\n    },\n    // Object property names can be specified by strings delimited by double (\") or single (') quotes.\n    lcm : function () {\n        // Variable names don't collide with object properties, e.g., |lcm| is not |this.lcm|.\n        // not using |this.a*this.b| to avoid FP precision issues\n        var lcm = this.a/this.gcd()*this.b;\n        // Only need to calculate lcm once, so \"redefine\" this method.\n        this.lcm = function () {\n            return lcm;\n        };\n        return lcm;\n    },\n    toString: function () {\n        return \"LCMCalculator: a = \" + this.a + \", b = \" + this.b;\n    }\n};\n\n// Define generic output function; this implementation only works for Web browsers\nfunction output(x) {\n    document.body.appendChild(document.createTextNode(x));\n    document.body.appendChild(document.createElement('br'));\n}\n\n// Note: Array's map() and forEach() are defined in JavaScript 1.6.\n// They are used here to demonstrate JavaScript's inherent functional nature.\n[[25, 55], [21, 56], [22, 58], [28, 56]].map(function (pair) { // array literal + mapping function\n    return new LCMCalculator(pair[0], pair[1]);\n}).sort(function (a, b) { // sort with this comparative function\n    return a.lcm() - b.lcm();\n}).forEach(function (obj) {\n    output(obj + \", gcd = \" + obj.gcd() + \", lcm = \" + obj.lcm());\n});\nThe following output should be displayed in the browser window.\n\nLCMCalculator: a = 28, b = 56, gcd = 28, lcm = 56\nLCMCalculator: a = 21, b = 56, gcd = 7, lcm = 168\nLCMCalculator: a = 25, b = 55, gcd = 5, lcm = 275\nLCMCalculator: a = 22, b = 58, gcd = 2, lcm = 638\nUse in Web pages[edit]\nSee also: Dynamic HTML and Ajax (programming)\nThe most common use of JavaScript is to add client-side behavior to HTML pages, also known as Dynamic HTML (DHTML). Scripts are embedded in or included from HTML pages and interact with the Document Object Model (DOM) of the page. Some simple examples of this usage are:\n\nLoading new page content or submitting data to the server via Ajax without reloading the page (for example, a social network might allow the user to post status updates without leaving the page).\nAnimation of page elements, fading them in and out, resizing them, moving them, etc.\nInteractive content, for example games, and playing audio and video.\nValidating input values of a Web form to make sure that they are acceptable before being submitted to the server.\nTransmitting information about the user's reading habits and browsing activities to various websites. Web pages frequently do this for Web analytics, ad tracking, personalization or other purposes.[53]\nBecause JavaScript code can run locally in a user's browser (rather than on a remote server), the browser can respond to user actions quickly, making an application more responsive. Furthermore, JavaScript code can detect user actions that HTML alone cannot, such as individual keystrokes. Applications such as Gmail take advantage of this: much of the user-interface logic is written in JavaScript, and JavaScript dispatches requests for information (such as the content of an e-mail message) to the server. The wider trend of Ajax programming similarly exploits this strength.\n\nA JavaScript engine (also known as JavaScript interpreter or JavaScript implementation) is an interpreter that interprets JavaScript source code and executes the script accordingly. The first JavaScript engine was created by Brendan Eich at Netscape, for the Netscape Navigator Web browser. The engine, code-named SpiderMonkey, is implemented in C. It has since been updated (in JavaScript 1.5) to conform to ECMAScript 3. The Rhino engine, created primarily by Norris Boyd (formerly at Netscape, now at Google) is a JavaScript implementation in Java. Rhino, like SpiderMonkey, is ECMAScript 3 compliant.\n\nA Web browser is by far the most common host environment for JavaScript. Web browsers typically create \"host objects\" to represent the DOM in JavaScript. The Web server is another common host environment. A JavaScript Web server would typically expose host objects representing HTTP request and response objects, which a JavaScript program could then interrogate and manipulate to dynamically generate Web pages.\n\nBecause JavaScript is the only language that the most popular browsers share support for, it has become a target language for many frameworks in other languages, even though JavaScript was never intended to be such a language.[54] Despite the performance limitations inherent to its dynamic nature, the increasing speed of JavaScript engines has made the language a surprisingly feasible compilation target.\n\nExample script[edit]\nBelow is a minimal example of a standards-conforming Web page containing JavaScript (using HTML 5 syntax) and the DOM:\n\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Example</title>\n  </head>\n  <body>\n    <button id=\"hellobutton\">Hello</button>\n    <script>\n        document.getElementById('hellobutton').onclick = function() {\n            alert('Hello world!');                     // Show a dialog\n            var myTextNode = document.createTextNode('Some new words.');\n            document.body.appendChild(myTextNode);     // Append \"Some new words\" to the page\n        }\n    </script>\n  </body>\n</html>\nCompatibility considerations[edit]\nMain article: Web interoperability\nBecause JavaScript runs in widely varying environments, an important part of testing and debugging is to test and verify that the JavaScript works across multiple browsers.\n\nThe DOM interfaces for manipulating Web pages are not part of the ECMAScript standard, or of JavaScript itself. Officially, the DOM interfaces are defined by a separate standardization effort by the W3C; in practice, browser implementations differ from the standards and from each other, and not all browsers execute JavaScript.\n\nTo deal with these differences, JavaScript authors can attempt to write standards-compliant code that will also be executed correctly by most browsers; failing that, they can write code that checks for the presence of certain browser features and behaves differently if they are not available.[55] In some cases, two browsers may both implement a feature but with different behavior, and authors may find it practical to detect what browser is running and change their script's behavior to match.[56][57] Programmers may also use libraries or toolkits that take browser differences into account.\n\nFurthermore, scripts may not work for some users. For example, a user may:\n\nuse an old or rare browser with incomplete or unusual DOM support;\nuse a PDA or mobile phone browser that cannot execute JavaScript;\nhave JavaScript execution disabled as a security precaution;\nuse a speech browser due to, for example, a visual disability.\nTo support these users, Web authors can try to create pages that degrade gracefully on user agents (browsers) that do not support the page's JavaScript. In particular, the page should remain usable albeit without the extra features that the JavaScript would have added. An alternative approach that many find preferable is to first author content using basic technologies that work in all browsers, then enhance the content for users that have JavaScript enabled. This is known as progressive enhancement.\n\nSecurity[edit]\nSee also: Browser security\nJavaScript and the DOM provide the potential for malicious authors to deliver scripts to run on a client computer via the Web. Browser authors contain this risk using two restrictions. First, scripts run in a sandbox in which they can only perform Web-related actions, not general-purpose programming tasks like creating files. Second, scripts are constrained by the same origin policy: scripts from one Web site do not have access to information such as usernames, passwords, or cookies sent to another site. Most JavaScript-related security bugs are breaches of either the same origin policy or the sandbox.\n\nThere are subsets of general JavaScript—ADsafe, Secure ECMAScript (SES)—that provide greater level of security, especially on code created by third parties (such as advertisements).[58][59] Caja is another project for safe embedding and isolation of third-party JavaScript and HTML.\n\nContent Security Policy is the main intended method of ensuring that only trusted code is executed on a Web page.\n\nSee also: Content Security Policy\nCross-site vulnerabilities[edit]\nMain articles: Cross-site scripting and Cross-site request forgery\nA common JavaScript-related security problem is cross-site scripting (XSS), a violation of the same-origin policy. XSS vulnerabilities occur when an attacker is able to cause a target Web site, such as an online banking website, to include a malicious script in the webpage presented to a victim. The script in this example can then access the banking application with the privileges of the victim, potentially disclosing secret information or transferring money without the victim's authorization. A solution to XSS vulnerabilities is to use HTML escaping whenever displaying untrusted data.\n\nSome browsers include partial protection against reflected XSS attacks, in which the attacker provides a URL including malicious script. However, even users of those browsers are vulnerable to other XSS attacks, such as those where the malicious code is stored in a database. Only correct design of Web applications on the server side can fully prevent XSS.\n\nXSS vulnerabilities can also occur because of implementation mistakes by browser authors.[60]\n\nAnother cross-site vulnerability is cross-site request forgery (CSRF). In CSRF, code on an attacker's site tricks the victim's browser into taking actions the user didn't intend at a target site (like transferring money at a bank). It works because, if the target site relies only on cookies to authenticate requests, then requests initiated by code on the attacker's site will carry the same legitimate login credentials as requests initiated by the user. In general, the solution to CSRF is to require an authentication value in a hidden form field, and not only in the cookies, to authenticate any request that might have lasting effects. Checking the HTTP Referrer header can also help.\n\n\"JavaScript hijacking\" is a type of CSRF attack in which a <script> tag on an attacker's site exploits a page on the victim's site that returns private information such as JSON or JavaScript. Possible solutions include:\n\nrequiring an authentication token in the POST and GET parameters for any response that returns private information.\nMisplaced trust in the client[edit]\nDevelopers of client-server applications must recognize that untrusted clients may be under the control of attackers. The application author cannot assume that his JavaScript code will run as intended (or at all) because any secret embedded in the code could be extracted by a determined adversary. Some implications are:\n\nWeb site authors cannot perfectly conceal how their JavaScript operates because the raw source code must be sent to the client. The code can be obfuscated, but obfuscation can be reverse-engineered.\nJavaScript form validation only provides convenience for users, not security. If a site verifies that the user agreed to its terms of service, or filters invalid characters out of fields that should only contain numbers, it must do so on the server, not only the client.\nScripts can be selectively disabled, so JavaScript can't be relied on to prevent operations such as right-clicking on an image to save it.[61]\nIt is extremely bad practice to embed sensitive information such as passwords in JavaScript because it can be extracted by an attacker.\nBrowser and plugin coding errors[edit]\nJavaScript provides an interface to a wide range of browser capabilities, some of which may have flaws such as buffer overflows. These flaws can allow attackers to write scripts that would run any code they wish on the user's system. This code is not by any means limited to another JavaScript application. For example, a buffer overrun exploit can allow an attacker to gain access to the operating system's API with superuser privileges.\n\nThese flaws have affected major browsers including Firefox,[62] Internet Explorer,[63] and Safari.[64]\n\nPlugins, such as video players, Adobe Flash, and the wide range of ActiveX controls enabled by default in Microsoft Internet Explorer, may also have flaws exploitable via JavaScript (such flaws have been exploited in the past).[65][66]\n\nIn Windows Vista, Microsoft has attempted to contain the risks of bugs such as buffer overflows by running the Internet Explorer process with limited privileges.[67] Google Chrome similarly confines its page renderers to their own \"sandbox\".\n\nSandbox implementation errors[edit]\nWeb browsers are capable of running JavaScript outside the sandbox, with the privileges necessary to, for example, create or delete files. Of course, such privileges aren't meant to be granted to code from the Web.\n\nIncorrectly granting privileges to JavaScript from the Web has played a role in vulnerabilities in both Internet Explorer[68] and Firefox.[69] In Windows XP Service Pack 2, Microsoft demoted JScript's privileges in Internet Explorer.[70]\n\nMicrosoft Windows allows JavaScript source files on a computer's hard drive to be launched as general-purpose, non-sandboxed programs (see: Windows Script Host). This makes JavaScript (like VBScript) a theoretically viable vector for a Trojan horse, although JavaScript Trojan horses are uncommon in practice.[71]\n\nUses outside Web pages[edit]\nIn addition to Web browsers and servers, JavaScript interpreters are embedded in a number of tools. Each of these applications provides its own object model that provides access to the host environment. The core JavaScript language remains mostly the same in each application.\n\nEmbedded scripting language[edit]\nGoogle's Chrome extensions, Opera's extensions, Apple's Safari 5 extensions, Apple's Dashboard Widgets, Microsoft's Gadgets, Yahoo! Widgets, Google Desktop Gadgets, and Serence Klipfolio are implemented using JavaScript.\nThe MongoDB database accepts queries written in JavaScript. MongoDB and NodeJS are the core components of MEAN: a solution stack for creating Web applications using just JavaScript.\nThe Clusterpoint database accept queries written in JS/SQL, which is a combination of SQL and JavaScript. Clusterpoint has built-in computing engine that allows execution of JavaScript code right inside the distributed database.\nAdobe's Acrobat and Adobe Reader support JavaScript in PDF files.[72]\nTools in the Adobe Creative Suite, including Photoshop, Illustrator, Dreamweaver, and InDesign, allow scripting through JavaScript.\nOpenOffice.org, an office application suite, as well as its popular fork LibreOffice, allows JavaScript to be used as a scripting language.\nThe visual programming language Max, released by Cycling '74, offers a JavaScript model of its environment for use by developers. It allows users to reduce visual clutter by using a object for a task rather than many.\nApple's Logic Pro X digital audio workstation (DAW) software can create custom MIDI effects plugins using JavaScript.[citation needed]\nECMAScript was included in the VRML97 standard for scripting nodes of VRML scene description files.[citation needed]\nThe Unity game engine supports a modified version of JavaScript for scripting via Mono.[73]\nDX Studio (3D engine) uses the SpiderMonkey implementation of JavaScript for game and simulation logic.[74]\nMaxwell Render (rendering software) provides an ECMA standard based scripting engine for tasks automation.[75]\nGoogle Apps Script in Google Spreadsheets and Google Sites allows users to create custom formulas, automate repetitive tasks and also interact with other Google products such as Gmail.[76]\nMany IRC clients, like ChatZilla or XChat, use JavaScript for their scripting abilities.[77][78]\nRPG Maker MV uses Javascript as its scripting language.[79]\nScripting engine[edit]\nMicrosoft's Active Scripting technology supports JScript as a scripting language.[80]\nJava introduced the javax.script package in version 6 that includes a JavaScript implementation based on Mozilla Rhino. Thus, Java applications can host scripts that access the application's variables and objects, much like Web browsers host scripts that access a webpage's Document Object Model (DOM).[81][82]\nThe Qt C++ toolkit includes a QtScript module to interpret JavaScript, analogous to Java's javax.script package.[83]\nOS X Yosemite introduced JavaScript for Automation (JXA), which is built upon JavaScriptCore and the Open Scripting Architecture. It features an Objective-C bridge that enables entire Cocoa applications to be programmed in JavaScript.\nLate Night Software's JavaScript OSA (also known as JavaScript for OSA, or JSOSA) is a freeware alternative to AppleScript for OS X. It is based on the Mozilla JavaScript 1.5 implementation, with the addition of a MacOS object for interaction with the operating system and third-party applications.[84]\nApplication platform[edit]\nActionScript, the programming language used in Adobe Flash, is another implementation of the ECMAScript standard.\nAdobe Integrated Runtime is a JavaScript runtime that allows developers to create desktop applications.\nAtom, the open-source text editor developed by Github, was implemented using Javascript, and has a special API on Javascript for packages that are developed for it.\nCA, Inc.'s AutoShell cross-application scripting environment is built on the SpiderMonkey JavaScript engine. It contains preprocessor-like extensions for command definition, as well as custom classes for various system-related tasks like file I/O, operation system command invocation and redirection, and COM scripting.\nGNOME Shell, the shell for the GNOME 3 desktop environment,[85] made JavaScript its default programming language in 2013.[86]\nThe Mozilla platform, which underlies Firefox, Thunderbird, and some other Web browsers, uses JavaScript to implement the graphical user interface (GUI) of its various products.\nQt Quick's markup language (available since Qt 4.7) uses JavaScript for its application logic. Its declarative syntax is also similar to JavaScript.\nTypeScript is a programming language based on JavaScript that adds support for optional type annotations and some other language extensions such as classes, interfaces and modules. A TS-script compiles into plain JavaScript and can be executed in any JS host supporting ECMAScript 3 or higher. The compiler is itself written in TypeScript.\nUbuntu Touch provides a JavaScript API for its unified usability interface.\nwebOS uses the WebKit implementation of JavaScript in its SDK to allow developers to create stand-alone applications solely in JavaScript.\nWinJS provides a special Windows Library for JavaScript functionality in Windows 8 that enables the development of Modern style (formerly Metro style) applications in HTML5 and JavaScript.\nDevelopment tools[edit]\nWithin JavaScript, access to a debugger becomes invaluable when developing large, non-trivial programs. Because there can be implementation differences between the various browsers (particularly within the DOM), it is useful to have access to a debugger for each of the browsers that a Web application targets.[87]\n\nScript debuggers are integrated within Internet Explorer, Firefox, Safari, Google Chrome, Opera and Node.js.[88][89][90]\n\nIn addition to the native Internet Explorer Developer Tools, three debuggers are available for Internet Explorer: Microsoft Visual Studio is the richest of the three, closely followed by Microsoft Script Editor (a component of Microsoft Office),[91] and finally the free Microsoft Script Debugger that is far more basic than the other two. The free Microsoft Visual Web Developer Express provides a limited version of the JavaScript debugging functionality in Microsoft Visual Studio. Internet Explorer has included developer tools since version 8.\n\nIn comparison to Internet Explorer, Firefox has a more comprehensive set of developer tools, which include a debugger as well. Old versions of Firefox without these tools used a Firefox addon called Firebug, or the older Venkman debugger. Also, WebKit's Web Inspector includes a JavaScript debugger,[92] which is used in Safari. A modified version called Blink DevTools is used in Google Chrome. Node.js has Node Inspector, an interactive debugger that integrates with the Blink DevTools, available in Google Chrome. Opera includes a set of tools called Dragonfly.[93]\n\nIn addition to the native computer software, there are online JavaScript IDEs, debugging aids that are themselves written in JavaScript and built to run on the Web. An example is the program JSLint, developed by Douglas Crockford who has written extensively on the language. JSLint scans JavaScript code for conformance to a set of standards and guidelines. Many libraries for JavaScript, such as three.js, provide links to demonstration code that can be edited by users. They are also used as a pedagogical tool by institutions such as Khan Academy[94] to allow students to experience writing code in an environment where they can see the output of their programs, without needing any setup beyond a Web browser.\n\nBenchmark tools for developers[edit]\nSince JavaScript is getting more important for web development (frontend overtakes many aspects which were done in backend before), there is also more consideration done about performance. Especially mobile devices could have problems with rendering and processing unoptimized complex logic.\n\nA library for doing benchmarks is benchmark.js. A benchmarking library that supports high-resolution timers and returns statistically significant results.\n\nAnother tool is jsben.ch. An online JavaScript benchmarking tool, where code snippets can be tested against each other.\n\nVersion history[edit]\nSee also: ECMAScript § Versions, and ECMAScript § Version correspondence\nJavaScript was initially developed in 1996 for use in the Netscape Navigator Web browser. In the same year Microsoft released an implementation for Internet Explorer. This implementation was called JScript due to trademark issues. In 1997 the first standardized version of the language was released under the name ECMAScript in the first edition of the ECMA-252 standard. The explicit versioning and opt-in of language features was Mozilla-specific and has been removed. Firefox 4 was the last version which referred to a JavaScript version (1.8.5). With new editions of the ECMA-262 standard, JavaScript language features are now often mentioned with their initial definition in the ECMA-262 editions.\n\nThe following table is based on information from multiple sources.[95][96][97]\n\nVersion\tRelease date\tEquivalent to\tNetscape\nNavigator\tMozilla\nFirefox\tInternet\nExplorer\tOpera\tSafari\tGoogle\nChrome\n1.0\tMarch 1996\t\t2.0\t\t3.0\t\t\t\n1.1\tAugust 1996\t\t3.0\t\t\t\t\t\n1.2\tJune 1997\t\t4.0-4.05\t\t\t3[98]\t\t\n1.3\tOctober 1998\tECMA-262 1st + 2nd edition\t4.06-4.7x\t\t4.0\t5[99]\t\t\n1.4\t\t\tNetscape\nServer\t\t\t6\t\t\n1.5\tNovember 2000\tECMA-262 3rd edition\t6.0\t1.0\t5.5 (JScript 5.5),\n6 (JScript 5.6),\n7 (JScript 5.7),\n8 (JScript 5.8)\t7.0\t3.0-5\t1.0-10.0.666\n1.6\tNovember 2005\t1.5 + array extras + array and string generics + E4X\t\t1.5\t\t\t\t\n1.7\tOctober 2006\t1.6 + Pythonic generators + iterators + let\t\t2.0\t\t\t\t28.0.1500.95\n1.8\tJune 2008\t1.7 + generator expressions + expression closures\t\t3.0\t\t11.50\t\t\n1.8.1\t\t1.8 + native JSON support + minor updates\t\t3.5\t\t\t\t\n1.8.2\tJune 22, 2009\t1.8.1 + minor updates\t\t3.6\t\t\t\t\n1.8.5\tJuly 27, 2010\t1.8.2 + new features for ECMA-262 5th edition compliance\t\t4.0\t\t\t\t\nRelated languages and features[edit]\nJSON, or JavaScript Object Notation, is a general-purpose data interchange format that is defined as a subset of JavaScript's object literal syntax. Like much of JavaScript (regexps and anonymous functions as 1st class elements, closures, flexible classes, 'use strict'), JSON, except for replacing Perl's key-value operator '=>' by an RFC 822[100] inspired ':', is syntactically pure Perl.\n\njQuery is a popular JavaScript library designed to simplify DOM-oriented client-side HTML scripting along with offering cross-browser compatibility because various browsers respond differently to certain vanilla JavaScript code.\n\nUnderscore.js is a utility JavaScript library for data manipulation that is used in both client-side and server-side network applications.\n\nAngularJS is a web application framework to use for developing single-page applications and also cross-platform mobile apps.\n\nReact (JavaScript library) is an open-source JavaScript library providing a views that is rendered using components specified as custom HTML tags.\n\nMozilla browsers currently support LiveConnect, a feature that allows JavaScript and Java to intercommunicate on the Web. However, Mozilla-specific support for LiveConnect is scheduled to be phased out in the future in favor of passing on the LiveConnect handling via NPAPI to the Java 1.6+ plug-in (not yet supported on the Mac as of March 2010).[101] Most browser inspection tools, such as Firebug in Firefox, include JavaScript interpreters that can act on the visible page's DOM.\n\nasm.js is a subset of JavaScript that can be run in any JavaScript engine or run faster in an ahead-of-time (AOT) compiling engine.[102]\n\nJSFuck is an esoteric programming language. Programs are written using only six different characters, but are still valid JavaScript code.\n\np5.js[103] is an object oriented JavaScript library designed for artists and designers. It is based on the ideas of the Processing project but is for the web.\n\njsben.ch is an online JavaScript benchmarking tool, where different code snippets can be tested against each other.\n\nUse as an intermediate language[edit]\nAs JavaScript is the most widely supported client-side language that can run within a Web browser, it has become an intermediate language for other languages to target. This has included both newly created languages and ports of existing languages. Some of these include:\n\nOberonScript, a full implementation of the Oberon programming language that compiles to high-level JavaScript.[104]\nObjective-J, a superset of JavaScript that compiles to standard JavaScript. It adds traditional inheritance and Smalltalk/Objective-C style dynamic dispatch and optional pseudo-static typing to JavaScript.\nProcessing.js, a JavaScript port of the Processing programming language designed to write visualizations, images, and interactive content. It allows Web browsers to display animations, visual applications, games and other graphical rich content without the need for a Java applet or Flash plugin.\nCoffeeScript, an alternate syntax for JavaScript intended to be more concise and readable. It adds features like array comprehensions (also available in JavaScript since version 1.7)[105] and pattern matching. Like Objective-J, it compiles to JavaScript. Ruby and Python have been cited as influential on CoffeeScript syntax.\nGoogle Web Toolkit translates a subset of Java to JavaScript.\nScala, an object-oriented and functional programming language, has a Scala-to-JavaScript compiler.[106]\nPyjamas, a port of Google Web Toolkit to Python translates a subset of Python to JavaScript.\nDart, an open-source programming language developed by Google, can be compiled to JavaScript.\nWhalesong,[107] a Racket-to-JavaScript compiler.\nEmscripten, a LLVM-backend for porting native libraries to JavaScript.\nFantom a programming language that runs on JVM, .NET and JavaScript.\nTypeScript, a free and open-source programming language developed by Microsoft. It is a superset of JavaScript, and essentially adds optional static typing and class-based object-oriented programming to the language.\nHaxe, an open-source high-level multiplatform programming language and compiler that can produce applications and source code for many different platforms including JavaScript.\nClojureScript,[108] a compiler for Clojure that targets JavaScript. It is designed to emit JavaScript code that is compatible with the advanced compilation mode of the Google Closure optimizing compiler.\nKotlin, a statically-typed language that also compiles to Java byte code.\nSqueakJS, a virtual machine and DOM environment for the open-source Squeak implementation of the Smalltalk programming language.\nAs JavaScript has unusual limitations – such as no separate integer type, using floating point – languages that compile to JavaScript commonly have slightly different behavior than in other environments.\n\nJavaScript and Java[edit]\nA common misconception is that JavaScript is similar or closely related to Java. It is true that both have a C-like syntax (the C language being their most immediate common ancestor language). They also are both typically sandboxed (when used inside a browser), and JavaScript was designed with Java's syntax and standard library in mind. In particular, all Java keywords were reserved in original JavaScript, JavaScript's standard library follows Java's naming conventions, and JavaScript's Math and Date objects are based on classes from Java 1.0,[109] but the similarities end there.\n\nJava and JavaScript both first appeared on 23 May 1995, but Java was developed by James Gosling of Sun Microsystems, and JavaScript by Brendan Eich of NetScape Communications.\n\nThe differences between the two languages are more prominent than their similarities. Java has static typing, while JavaScript's typing is dynamic. Java is loaded from compiled bytecode, while JavaScript is loaded as human-readable source code. Java's objects are class-based, while JavaScript's are prototype-based. Finally, Java did not support functional programming until Java 8, while JavaScript has done so from the beginning, being influenced by Scheme.",
          "type": "intepreted",
          "plid": 22
        },
        {
          "name": "OCaml",
          "details": "OCaml (/oʊˈkæməl/ oh-kam-əl), originally known as Objective Caml, is the main implementation of the Caml programming language, created by Xavier Leroy, Jérôme Vouillon, Damien Doligez, Didier Rémy, Ascánder Suárez and others in 1996. A member of the ML language family, OCaml extends the core Caml language with object-oriented constructs.\n\nOCaml's toolset includes an interactive top-level interpreter, a bytecode compiler, a reversible debugger, a package manager (OPAM), and an optimizing native code compiler. It has a large standard library that makes it useful for many of the same applications as Python or Perl, as well as robust modular and object-oriented programming constructs that make it applicable for large-scale software engineering. OCaml is the successor to Caml Light. The acronym \"CAML\" originally stood for Categorical Abstract Machine Language, although OCaml abandons this abstract machine.[1]\n\nOCaml is a free open-source project managed and principally maintained by INRIA. In recent years,[when?] many new languages have drawn elements from OCaml, most notably F# and Scala.\n\nContents  [hide] \n1\tPhilosophy\n2\tFeatures\n3\tDevelopment environment\n4\tCode examples\n4.1\tHello World\n4.2\tSumming a list of integers\n4.3\tQuicksort\n4.4\tBirthday paradox\n4.5\tChurch numerals\n4.6\tArbitrary-precision factorial function (libraries)\n4.7\tTriangle (graphics)\n4.8\tFibonacci Sequence\n4.9\tHigher-order functions\n5\tDerived languages\n5.1\tMetaOCaml\n5.2\tOther derived languages\n6\tSoftware written in OCaml\n7\tCommercial users of OCaml\n8\tSee also\n9\tReferences\n10\tExternal links\nPhilosophy[edit]\nML-derived languages are best known for their static type systems and type-inferring compilers. OCaml unifies functional, imperative, and object-oriented programming under an ML-like type system. This means the program author is not required to be overly familiar with the pure functional language paradigm in order to use OCaml.\n\nOCaml's static type system can help eliminate problems at runtime. However, it also forces the programmer to conform to the constraints of the type system, which can require careful thought and close attention. A type-inferring compiler greatly reduces the need for manual type annotations (for example, the data type of variables and the signature of functions usually do not need to be explicitly declared, as they do in Java). Nonetheless, effective use of OCaml's type system can require some sophistication on the part of the programmer.\n\nOCaml is perhaps most distinguished from other languages with origins in academia by its emphasis on performance. Firstly, its static type system renders runtime type mismatches impossible, and thus obviates runtime type and safety checks that burden the performance of dynamically typed languages, while still guaranteeing runtime safety (except when array bounds checking is turned off, or when certain type-unsafe features like serialization are used; these are rare enough that avoiding them is quite possible in practice).\n\nAside from type-checking overhead, functional programming languages are, in general, challenging to compile to efficient machine language code, due to issues such as the funarg problem. In addition to standard loop, register, and instruction optimizations, OCaml's optimizing compiler employs static program analysis techniques to optimize value boxing and closure allocation, helping to maximize the performance of the resulting code even if it makes extensive use of functional programming constructs.\n\nXavier Leroy has stated that \"OCaml delivers at least 50% of the performance of a decent C compiler\",[2] but a direct comparison is impossible. Some functions in the OCaml standard library are implemented with faster algorithms than equivalent functions in the standard libraries of other languages. For example, the implementation of set union in the OCaml standard library in theory is asymptotically faster than the equivalent function in the standard libraries of imperative languages (e.g. C++, Java) because the OCaml implementation exploits the immutability of sets in order to reuse parts of input sets in the output (persistence).\n\nFeatures[edit]\nOCaml features: a static type system, type inference, parametric polymorphism, tail recursion, pattern matching, first class lexical closures, functors (parametric modules), exception handling, and incremental generational automatic garbage collection.\n\nOCaml is particularly notable for extending ML-style type inference to an object system in a general-purpose language. This permits structural subtyping, where object types are compatible if their method signatures are compatible, regardless of their declared inheritance; an unusual feature in statically typed languages.\n\nA foreign function interface for linking to C primitives is provided, including language support for efficient numerical arrays in formats compatible with both C and FORTRAN. OCaml also supports the creation of libraries of OCaml functions that can be linked to a \"main\" program in C, so that one could distribute an OCaml library to C programmers who have no knowledge nor installation of OCaml.\n\nThe OCaml distribution contains:\n\nAn extensible parser and macro language named Camlp4, which permits the syntax of OCaml to be extended or even replaced\nLexer and parser tools called ocamllex and ocamlyacc\nDebugger that supports stepping backwards to investigate errors\nDocumentation generator\nProfiler — for measuring performance\nNumerous general-purpose libraries\nThe native code compiler is available for many platforms, including Unix, Microsoft Windows, and Apple Mac OS X. Portability is achieved through native code generation support for major architectures: IA-32, AMD64, Power, SPARC, ARM, and ARM64.[3]\n\nOCaml bytecode and native code programs can be written in a multithreaded style, with preemptive context switching. However, because the garbage collector of the INRIA OCaml system (which is the only currently available full implementation of the language) is not designed for concurrency, symmetric multiprocessing is not supported.[4] OCaml threads in the same process execute by time sharing only. There are however several libraries for distributed computing such as Functory and ocamlnet/Plasma.\n\nDevelopment environment[edit]\nSince 2011, a lot of new tools and libraries have been contributed to the OCaml development environment:\n\nOPAM, the OCaml Package Manager, developed by OCamlPro, is now an easy way to install OCaml and many of its tools and libraries\nOptimizing compilers for OCaml:\njs_of_ocaml, developed by the Ocsigen team, is an optimizing compiler from OCaml to JavaScript, to create webapps in OCaml.\nocamlcc is a compiler from OCaml to C, to complement the native code compiler for unsupported platforms.\nOCamlJava, developed by Inria, is a compiler from OCaml to the JVM.\nOCaPic, developed by Lip6, is a compiler from OCaml to PIC microcontroller.\nWeb sites:\nOCaml.org is a website managed by the OCaml community.\nTry-OCaml, developed by OCamlPro, is a website containing a complete OCaml REPL in a webpage.\nDevelopment Tools\nTypeRex is a set of open-source tools and libraries for OCaml, developed and maintained by OCamlPro.\nMerlin is an auto-completion tool for editing OCaml code in Emacs and Vim.\nCode examples[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2013) (Learn how and when to remove this template message)\nSnippets of OCaml code are most easily studied by entering them into the \"top-level\". This is an interactive OCaml session that prints the inferred types of resulting or defined expressions. The OCaml top-level is started by simply executing the OCaml program:\n\n  $ ocaml\n       Objective Caml version 3.09.0\n  #\nCode can then be entered at the \"#\" prompt. For example, to calculate 1+2*3:\n\n  # 1 + 2 * 3;;\n  - : int = 7\nOCaml infers the type of the expression to be \"int\" (a machine-precision integer) and gives the result \"7\".\n\nHello World[edit]\nThe following program \"hello.ml\":\n\nprint_endline \"Hello World!\"\ncan be compiled into a bytecode executable:\n\n$ ocamlc hello.ml -o hello\nor compiled into an optimized native-code executable:\n\n$ ocamlopt hello.ml -o hello\nand executed:\n\n$ ./hello\nHello World!\n$\nSumming a list of integers[edit]\nLists are one of the fundamental datatypes in OCaml. The following code example defines a recursive function sum that accepts one argument xs. (Notice the keyword rec). The function recursively iterates over a given list and provides a sum of integer elements. The match statement has similarities to C's switch element, though it is much more general.\n\nlet rec sum xs =\n  match xs with\n    | []       -> 0                  (* yield 0 if xs has the form [] *)\n    | x :: xs' -> x + sum xs';;      (* recursive call if xs has the form x::xs' for suitable x and xs' *)\n # sum [1;2;3;4;5];;\n - : int = 15\nAnother way is to use standard fold function that works with lists.\n\nlet sum xs =\n    List.fold_left (fun acc each_xs -> acc + each_xs) 0 xs;;\n # sum [1;2;3;4;5];;\n - : int = 15\nQuicksort[edit]\nOCaml lends itself to the concise expression of recursive algorithms. The following code example implements an algorithm similar to quicksort that sorts a list in increasing order.\n\n let rec qsort = function\n   | [] -> []\n   | pivot :: rest ->\n       let is_less x = x < pivot in\n       let left, right = List.partition is_less rest in\n       qsort left @ [pivot] @ qsort right\nBirthday paradox[edit]\nThe following program calculates the smallest number of people in a room for whom the probability of completely unique birthdays is less than 50% (the so-called birthday paradox, where for 1 person the probability is 365/365 (or 100%), for 2 it is 364/365, for 3 it is 364/365 × 363/365, etc.) (answer = 23).\n\n let year_size = 365.\n\n let rec birthday_paradox prob people =\n     let prob' = (year_size -. float people) /. year_size *. prob  in\n     if prob' < 0.5 then\n         Printf.printf \"answer = %d\\n\" (people+1)\n     else\n         birthday_paradox prob' (people+1) ;;\n\n birthday_paradox 1.0 1\nChurch numerals[edit]\nThe following code defines a Church encoding of natural numbers, with successor (succ) and addition (add). A Church numeral n is a higher-order function that accepts a function f and a value x and applies f to x exactly n times. To convert a Church numeral from a functional value to a string, we pass it a function that prepends the string \"S\" to its input and the constant string \"0\".\n\nlet zero f x = x\nlet succ n f x = f (n f x)\nlet one = succ zero\nlet two = succ (succ zero)\nlet add n1 n2 f x = n1 f (n2 f x)\nlet to_string n = n (fun k -> \"S\" ^ k) \"0\"\nlet _ = to_string (add (succ two) two)\nArbitrary-precision factorial function (libraries)[edit]\nA variety of libraries are directly accessible from OCaml. For example, OCaml has a built-in library for arbitrary-precision arithmetic. As the factorial function grows very rapidly, it quickly overflows machine-precision numbers (typically 32- or 64-bits). Thus, factorial is a suitable candidate for arbitrary-precision arithmetic.\n\nIn OCaml, the Num module provides arbitrary-precision arithmetic and can be loaded into a running top-level using:\n\n# #load \"nums.cma\";;\n# open Num;;\nThe factorial function may then be written using the arbitrary-precision numeric operators =/, */ and -/ :\n\n# let rec fact n =\n    if n =/ Int 0 then Int 1 else n */ fact(n -/ Int 1);;\nval fact : Num.num -> Num.num = <fun>\nThis function can compute much larger factorials, such as 120!:\n\n# string_of_num (fact (Int 120));;\n- : string =\n\"6689502913449127057588118054090372586752746333138029810295671352301633\n55724496298936687416527198498130815763789321409055253440858940812185989\n8481114389650005964960521256960000000000000000000000000000\"\nThe cumbersome syntax for Num operations can be alleviated thanks to the camlp4 syntax extension called Delimited overloading:\n\n# #require \"pa_do.num\";;\n# let rec fact n = Num.(if n = 0 then 1 else n * fact(n-1));;\nval fact : Num.num -> Num.num = <fun>\n# fact Num.(120);;\n- : Num.num =\n  <num 668950291344912705758811805409037258675274633313802981029567\n  135230163355724496298936687416527198498130815763789321409055253440\n  8589408121859898481114389650005964960521256960000000000000000000000000000>\nTriangle (graphics)[edit]\nThe following program \"simple.ml\" renders a rotating triangle in 2D using OpenGL:\n\nlet () =\n  ignore( Glut.init Sys.argv );\n  Glut.initDisplayMode ~double_buffer:true ();\n  ignore (Glut.createWindow ~title:\"OpenGL Demo\");\n  let angle t = 10. *. t *. t in\n  let render () =\n    GlClear.clear [ `color ];\n    GlMat.load_identity ();\n    GlMat.rotate ~angle: (angle (Sys.time ())) ~z:1. ();\n    GlDraw.begins `triangles;\n    List.iter GlDraw.vertex2 [-1., -1.; 0., 1.; 1., -1.];\n    GlDraw.ends ();\n    Glut.swapBuffers () in\n  GlMat.mode `modelview;\n  Glut.displayFunc ~cb:render;\n  Glut.idleFunc ~cb:(Some Glut.postRedisplay);\n  Glut.mainLoop ()\nThe LablGL bindings to OpenGL are required. The program may then be compiled to bytecode with:\n\n  $ ocamlc -I +lablGL lablglut.cma lablgl.cma simple.ml -o simple\nor to nativecode with:\n\n  $ ocamlopt -I +lablGL lablglut.cmxa lablgl.cmxa simple.ml -o simple\nand run:\n\n  $ ./simple\nFar more sophisticated, high-performance 2D and 3D graphical programs can be developed in OCaml. Thanks to the use of OpenGL and OCaml, the resulting programs can be cross-platform, compiling without any changes on many major platforms.\n\nFibonacci Sequence[edit]\nThe following code calculates the Fibonacci sequence of a number n inputted. It uses tail recursion and pattern matching.\n\nlet rec fib_aux n a b =\n  match n with\n  | 0 -> a\n  | _ -> fib_aux (n - 1) b (a+b) \nlet fib n = fib_aux n 0 1\nHigher-order functions[edit]\nFunctions may take functions as input and return functions as result. For example, applying twice to a function f yields a function that applies f two times to its argument.\n\nlet twice (f : 'a -> 'a) = fun (x : 'a) -> f (f x) ;;\nlet inc (x : int) : int = x + 1 ;;\nlet add2 = twice(inc);;\nlet inc_str (x : string) : string = x ^ \" \" ^ x ;;\nlet add_str = twice(inc_str);;\n  # add2 98;;\n  - : int = 100\n  # add_str \"Test\";;\n  - : string = \"Test Test Test Test\"\nThe function twice uses a type variable 'a to indicate that it can be applied to any function f mapping from a type 'a to itself, rather than only to int->int functions. In particular, twice can even be applied to itself.\n\n  # let fourtimes f = (twice twice) f;;\n  val fourtimes : ('a -> 'a) -> 'a -> 'a = <fun>\n  # let add4 = fourtimes inc;;\n  val add4 : int -> int = <fun>\n  # add4 98;;\n  - : int = 102\nDerived languages[edit]\nMetaOCaml[edit]\nMetaOCaml[5] is a multi-stage programming extension of OCaml enabling incremental compiling of new machine code during runtime. Under certain circumstances, significant speedups are possible using multi-stage programming, because more detailed information about the data to process is available at runtime than at the regular compile time, so the incremental compiler can optimize away many cases of condition checking etc.\n\nAs an example: if at compile time it is known that a certain power function x -> x^n is needed very frequently, but the value of n is known only at runtime, you can use a two-stage power function in MetaOCaml:\n\n let rec power n x =\n   if n = 0\n   then .<1>.\n   else\n     if even n\n     then sqr (power (n/2) x)\n     else .<.~x *. .~(power (n-1) x)>.\nAs soon as you know n at runtime, you can create a specialized and very fast power function:\n\n .<fun x -> .~(power 5 .<x>.)>.\nThe result is:\n\n fun x_1 -> (x_1 *\n     let y_3 = \n         let y_2 = (x_1 * 1)\n         in (y_2 * y_2)\n     in (y_3 * y_3))\nThe new function is automatically compiled.\n\nOther derived languages[edit]\nAtomCaml provides a synchronization primitive for atomic (transactional) execution of code.\nEmily is a subset of OCaml that uses a design rule verifier to enforce object-capability [security] principles.\nF# is a Microsoft .NET language based on OCaml.\nFresh OCaml facilitates the manipulation of names and binders.\nGCaml adds extensional polymorphism to OCaml, thus allowing overloading and type-safe marshalling.\nJoCaml integrates constructions for developing concurrent and distributed programs.\nOCamlDuce extends OCaml with features such as XML expressions and regular-expression types.\nOCamlP3l is a parallel programming system based on OCaml and the P3L language\nSoftware written in OCaml[edit]\nMirageOS, a unikernel programming framework written in pure OCaml\nHack, a new programming language created by Facebook, which is an extension of PHP with static typing. Hack's compiler is written in OCaml.\nFlow, a static analyzer for JavaScript created at Facebook that infers and verifies static types for JavaScript programs.\nInfer, a static analyzer for Java, C and Objective-C created at Facebook, which is used to detect bugs in iOS and Android apps.\n0Install, a multi-platform package manager\nXCP, The Xen Cloud Platform, an open source toolstack for the Xen Virtual Machine Hypervisor\nFFTW, a software library for computing discrete Fourier transforms. Several C routines have been generated by an OCaml program named genfft.\nUnison, a file synchronization program to synchronize files between two directories\nMldonkey, a peer to peer client based on the EDonkey network\nGeneWeb, free open source multi-platform genealogy software\nThe Haxe compiler, a free open source compiler for the Haxe programming language\nFrama-C, a framework for C programs analysis\nCoq, a formal proof management system\nOcsigen, web development framework\nOpa, an open source programming language for web development\nWebAssembly, an experimental, low-level scripting language for in-browser client-side scripting. Its WIP interpreter and parser in the specification repo on Github is written mostly in OCaml.\nCommercial users of OCaml[edit]\nThere are several dozen companies that use OCaml to some degree.[6] Notable examples include:\n\nJane Street Capital, a proprietary trading firm, which adopted OCaml as its preferred language in its early days[7]\nCitrix Systems, which uses OCaml in XenServer, a component of one of its products\nFacebook, which developed Hack, Flow, Infer and Pfff\nAhrefs Site Explorer, which uses OCaml for its back-end framework (database management, web-crawler and web-page parser)\nSee also[edit]\nCaml and Caml Light, languages from which OCaml evolved\nStandard ML, another popular dialect of ML\nExtensible ML, another object-oriented dialect of ML\nO'Haskell, an object-oriented extension to the functional language Haskell\nReason, a new interface to OCaml initiated by Facebook",
          "type": "compiled",
          "plid": 23
        },
        {
          "name": "Objective-J",
          "details": "Objective-J is a programming language developed as part of the Cappuccino web development framework. Its syntax is nearly identical to the Objective-C syntax and it shares with JavaScript the same relationship that Objective-C has with the C programming language: that of being a strict, but small, superset; adding traditional inheritance and Smalltalk/Objective-C style dynamic dispatch. Pure JavaScript, being a prototype-based language, already has a notion of object orientation and inheritance, but Objective-J adds the use of class-based programming to JavaScript.\n\nPrograms written in Objective-J need to be preprocessed before being run by a web browser's JavaScript virtual machine. This step can occur in the web browser at runtime or by a compiler which translates Objective-J programs into pure JavaScript code. The Objective-J compiler is written in JavaScript; consequently, deploying Objective-J programs does not require a web browser plug-in. Objective-J can be compiled and run on Node.js.\n\nContents  [hide] \n1\tApplications\n1.1\tApplications designed using the Cappuccino Framework[1]\n2\tSyntax\n3\tMemory management\n4\tSee also\n5\tReferences\n6\tExternal links\nApplications[edit]\nThe first widely known use of Objective-J was in the Cappuccino-based web application 280 Slides, which was developed by 280 North itself. Even though Objective-J can be used (and has been designed) independently from the Cappuccino framework, Objective-J has primarily been invented to support web development in Cappuccino.\n\nApplications designed using the Cappuccino Framework[1][edit]\nMockingbird\nPicEngine\nGithubIssues\nTimeTable\nEnstore (until October 2013, they rewrote it using Ember [2])\nAlmost At\nAkshell - Online JavaScript Web-App IDE\nArchipel Project - Virtual machine orchestrator\nSpot Specific - Mobile App SDK and IDE\nSyntax[edit]\nObjective-J is a superset of JavaScript, which means that any valid JavaScript code is also valid Objective-J code.\n\nThe following example shows the definition and implementation in Objective-J of a class named Address; this class extends the root object CPObject, which plays a role similar to the Objective-C's NSObject. This example differs from traditional Objective-C in that the root object reflects the underlying Cappuccino framework as opposed to Cocoa, Objective-J does not use pointers and, as such, type definitions do not contain asterisk characters. Instance variables are always defined in the @implementation.\n\n@implementation Address : CPObject\n{\n  CPString name;\n  CPString city;\n}\n\n- (id)initWithName:(CPString)aName city:(CPString)aCity\n{\n  self = [super init];\n\n  name = aName;\n  city = aCity;\n\n  return self;\n}\n\n- (void)setName:(CPString)aName\n{\n  name = aName;\n}\n\n- (CPString)name\n{\n  return name;\n}\n\n+ (id)newAddressWithName:(CPString)aName city:(CPString)aCity\n{\n  return [[self alloc] initWithName:aName city:aCity];\n}\n\n@end\nAs with Objective-C, class method definitions and instance method definitions start with '+' (plus) and '-' (dash), respectively.\n\nMemory management[edit]\nObjective-C uses ARC (Automatic Reference Counting) for deallocating unused objects. In Objective-J, objects are automatically deallocated by JavaScript's Garbage Collector.",
          "type": "compiler",
          "plid": 25
        },
        {
          "plid": 26,
          "name": "Perl",
          "details": "Perl is a family of high-level, general-purpose, interpreted, dynamic programming languages. The languages in this family include Perl 5 and Perl 6.[6]\n\nThough Perl is not officially an acronym,[7] there are various backronyms in use, the best-known being \"Practical Extraction and Reporting Language\".[8] Perl was originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier.[9] Since then, it has undergone many changes and revisions. Perl 6, which began as a redesign of Perl 5 in 2000, eventually evolved into a separate language. Both languages continue to be developed independently by different development teams and liberally borrow ideas from one another.\n\nThe Perl languages borrow features from other programming languages including C, shell script (sh), AWK, and sed.[10] They provide powerful text processing facilities without the arbitrary data-length limits of many contemporary Unix commandline tools,[11] facilitating easy manipulation of text files. Perl 5 gained widespread popularity in the late 1990s as a CGI scripting language, in part due to its unsurpassed[12][13][14] regular expression and string parsing abilities.[15]\n\nIn addition to CGI, Perl 5 is used for graphics programming, system administration, network programming, finance, bioinformatics, and other applications. It has been nicknamed \"the Swiss Army chainsaw of scripting languages\" because of its flexibility and power,[16] and possibly also because of its \"ugliness\".[17] In 1998, it was also referred to as the \"duct tape that holds the Internet together\", in reference to both its ubiquitous use as a glue language and its perceived inelegance.[18]\n\nContents  [hide] \n1\tHistory\n1.1\tEarly versions\n1.2\tEarly Perl 5\n1.3\t2000–present\n1.4\tPONIE\n1.5\tName\n1.6\tCamel symbol\n1.7\tOnion symbol\n2\tOverview\n2.1\tFeatures\n2.2\tDesign\n2.3\tApplications\n2.4\tImplementation\n2.5\tAvailability\n2.5.1\tWindows\n3\tDatabase interfaces\n4\tComparative performance\n4.1\tOptimizing\n5\tPerl 6\n6\tFuture of Perl 5\n7\tPerl community\n7.1\tState of the Onion\n7.2\tPerl pastimes\n7.3\tPerl on IRC\n7.4\tCPAN Acme\n8\tExample code\n9\tCriticism\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory[edit]\nEarly versions[edit]\nLarry Wall began work on Perl in 1987, while working as a programmer at Unisys,[11] and released version 1.0 to the comp.sources.misc newsgroup on December 18, 1987.[19] The language expanded rapidly over the next few years.\n\nPerl 2, released in 1988, featured a better regular expression engine. Perl 3, released in 1989, added support for binary data streams.\n\nOriginally, the only documentation for Perl was a single (increasingly lengthy) man page. In 1991, Programming Perl, known to many Perl programmers as the \"Camel Book\" because of its cover, was published and became the de facto reference for the language. At the same time, the Perl version number was bumped to 4, not to mark a major change in the language but to identify the version that was well documented by the book.\n\nEarly Perl 5[edit]\nMain article: Perl 5 version history\nPerl 4 went through a series of maintenance releases, culminating in Perl 4.036 in 1993. At that point, Wall abandoned Perl 4 to begin work on Perl 5. Initial design of Perl 5 continued into 1994. The perl5-porters mailing list was established in May 1994 to coordinate work on porting Perl 5 to different platforms. It remains the primary forum for development, maintenance, and porting of Perl 5.[20]\n\nPerl 5.000 was released on October 17, 1994.[21] It was a nearly complete rewrite of the interpreter, and it added many new features to the language, including objects, references, lexical (my) variables, and modules. Importantly, modules provided a mechanism for extending the language without modifying the interpreter. This allowed the core interpreter to stabilize, even as it enabled ordinary Perl programmers to add new language features. Perl 5 has been in active development since then.\n\nPerl 5.001 was released on March 13, 1995. Perl 5.002 was released on February 29, 1996 with the new prototypes feature. This allowed module authors to make subroutines that behaved like Perl builtins. Perl 5.003 was released June 25, 1996, as a security release.\n\nOne of the most important events in Perl 5 history took place outside of the language proper and was a consequence of its module support. On October 26, 1995, the Comprehensive Perl Archive Network (CPAN) was established as a repository for Perl modules and Perl itself; as of June 2015, it carries over 150,775 modules in 31,896 distributions, written by more than 12,219 authors, and is mirrored worldwide at more than 253 locations.[22]\n\nPerl 5.004 was released on May 15, 1997, and included among other things the UNIVERSAL package, giving Perl a base object to which all classes were automatically derived and the ability to require versions of modules. Another significant development was the inclusion of the CGI.pm module,[23] which contributed to Perl's popularity as a CGI scripting language.[24]\n\nPerl is also now supported running under Microsoft Windows and several other operating systems.[23]\n\nPerl 5.005 was released on July 22, 1998. This release included several enhancements to the regex engine, new hooks into the backend through the B::* modules, the qr// regex quote operator, a large selection of other new core modules, and added support for several more operating systems, including BeOS.[25]\n\n2000–present[edit]\nMajor version\tLatest update\n5.5\t2004-02-23[26]\n5.6\t2003-11-15[26]\n5.8\t2008-12-14[26]\n5.10\t2009-08-23[26]\n5.12\t2012-11-10[26]\n5.14\t2013-03-10[26]\n5.16\t2013-03-11[26]\n5.18\t2014-10-02[26]\n5.20\t2015-09-12[26]\n5.22\t2016-04-29[26]\n5.24\t2016-05-09[26]\nOld version\nOlder version, still supported\nCurrent stable version\nFuture release\nPerl 5.6 was released on March 22, 2000. Major changes included 64-bit support, Unicode string representation, large file support (i.e. files over 2 GiB) and the \"our\" keyword.[27][28] When developing Perl 5.6, the decision was made to switch the versioning scheme to one more similar to other open source projects; after 5.005_63, the next version became 5.5.640, with plans for development versions to have odd numbers and stable versions to have even numbers.\n\nIn 2000, Wall put forth a call for suggestions for a new version of Perl from the community. The process resulted in 361 RFC (request for comments) documents that were to be used in guiding development of Perl 6. In 2001,[29] work began on the apocalypses for Perl 6, a series of documents meant to summarize the change requests and present the design of the next generation of Perl. They were presented as a digest of the RFCs, rather than a formal document. At this point, Perl 6 existed only as a description of a language.\n\nPerl 5.8 was first released on July 18, 2002, and had nearly yearly updates since then. Perl 5.8 improved Unicode support, added a new I/O implementation, added a new thread implementation, improved numeric accuracy, and added several new modules.[30] As of 2013 this version still remains the most popular version of Perl and is used by Red Hat 5, Suse 10, Solaris 10, HP-UX 11.31 and AIX 5.\n\nIn 2004, work began on the Synopses – documents that originally summarized the Apocalypses, but which became the specification for the Perl 6 language. In February 2005, Audrey Tang began work on Pugs, a Perl 6 interpreter written in Haskell.[31] This was the first concerted effort towards making Perl 6 a reality. This effort stalled in 2006.[32]\n\nOn December 18, 2007, the 20th anniversary of Perl 1.0, Perl 5.10.0 was released. Perl 5.10.0 included notable new features, which brought it closer to Perl 6. These included a switch statement (called \"given\"/\"when\"), regular expressions updates, and the smart match operator, \"~~\".[33][34] Around this same time, development began in earnest on another implementation of Perl 6 known as Rakudo Perl, developed in tandem with the Parrot virtual machine. As of November 2009, Rakudo Perl has had regular monthly releases and now is the most complete implementation of Perl 6.\n\nA major change in the development process of Perl 5 occurred with Perl 5.11; the development community has switched to a monthly release cycle of development releases, with a yearly schedule of stable releases. By that plan, bugfix point releases will follow the stable releases every three months.\n\nOn April 12, 2010, Perl 5.12.0 was released. Notable core enhancements include new package NAME VERSION syntax, the Yada Yada operator (intended to mark placeholder code that is not yet implemented), implicit strictures, full Y2038 compliance, regex conversion overloading, DTrace support, and Unicode 5.2.[35] On January 21, 2011, Perl 5.12.3 was released; it contains updated modules and some documentation changes.[36] Version 5.12.4 was released on June 20, 2011. The latest version of that branch, 5.12.5, was released on November 10, 2012.\n\nOn May 14, 2011, Perl 5.14 was released. JSON support is built-in as of 5.14.2. The latest version of that branch, 5.14.4, was released on March 10, 2013.\n\nOn May 20, 2012, Perl 5.16 was released. Notable new features include the ability to specify a given version of Perl that one wishes to emulate, allowing users to upgrade their version of Perl, but still run old scripts that would normally be incompatible.[37] Perl 5.16 also updates the core to support Unicode 6.1.[37]\n\nOn May 18, 2013, Perl 5.18 was released. Notable new features include the new dtrace hooks, lexical subs, more CORE:: subs, overhaul of the hash for security reasons, support for Unicode 6.2.[38]\n\nOn May 27, 2014, Perl 5.20 was released. Notable new features include subroutine signatures, hash slices/new slice syntax, postfix dereferencing (experimental), Unicode 6.3, rand() using consistent random number generator.[39]\n\nSome observers credit the release of Perl 5.10 with the start of the Modern Perl movement.[40] In particular, this phrase describes a style of development that embraces the use of the CPAN, takes advantage of recent developments in the language, and is rigorous about creating high quality code.[41] While the book \"Modern Perl\"[42] may be the most visible standard-bearer of this idea, other groups such as the Enlightened Perl Organization[43] have taken up the cause.\n\nIn late 2012 and 2013 several projects for alternative implementations for Perl 5 started: Perl5 in Perl6 by the Rakudo Perl team,[44] moe by Stevan Little and friends,[45] p2[46] by the Perl11 team under Reini Urban, gperl by goccy,[47] and rperl a kickstarter project led by Will Braswell and affiliated with the Perll11 project.[48]\n\nPONIE[edit]\nPONIE is an acronym for Perl On New Internal Engine. The PONIE Project existed from 2003 until 2006 and was to be a bridge between Perl 5 and Perl 6. It was an effort to rewrite the Perl 5 interpreter to run on Parrot, the Perl 6 virtual machine. The goal was to ensure the future of the millions of lines of Perl 5 code at thousands of companies around the world.[49]\n\nThe PONIE project ended in 2006 and is no longer being actively developed. Some of the improvements made to the Perl 5 interpreter as part of PONIE were folded into that project.[50]\n\nName[edit]\nPerl was originally named \"Pearl\". Wall wanted to give the language a short name with positive connotations; he claims that he considered (and rejected) every three- and four-letter word in the dictionary. He also considered naming it after his wife Gloria. Wall discovered the existing PEARL programming language before Perl's official release and changed the spelling of the name.[51]\n\nWhen referring to the language, the name is normally capitalized (Perl) as a proper noun. When referring to the interpreter program itself, the name is often uncapitalized (perl) because most Unix-like file systems are case-sensitive. Before the release of the first edition of Programming Perl, it was common to refer to the language as perl; Randal L. Schwartz, however, capitalized the language's name in the book to make it stand out better when typeset. This case distinction was subsequently documented as canonical.[52]\n\nThe name is occasionally expanded as Practical Extraction and Report Language, but this is a backronym.[53] Other expansions have been suggested as equally canonical, including Wall's own humorous Pathologically Eclectic Rubbish Lister.[54] Indeed, Wall claims that the name was intended to inspire many different expansions.[55]\n\nCamel symbol[edit]\n\nThe Camel symbol used by O'Reilly Media\nProgramming Perl, published by O'Reilly Media, features a picture of a dromedary camel on the cover and is commonly called the \"Camel Book\".[56] This image of a camel has become an unofficial symbol of Perl as well as a general hacker emblem, appearing on T-shirts and other clothing items.\n\nO'Reilly owns the image as a trademark but licenses it for non-commercial use, requiring only an acknowledgement and a link to www.perl.com. Licensing for commercial use is decided on a case by case basis.[57] O'Reilly also provides \"Programming Republic of Perl\" logos for non-commercial sites and \"Powered by Perl\" buttons for any site that uses Perl.[57]\n\nOnion symbol[edit]\n\nThe onion logo used by The Perl Foundation\nThe Perl Foundation owns an alternative symbol, an onion, which it licenses to its subsidiaries, Perl Mongers, PerlMonks, Perl.org, and others.[58] The symbol is a visual pun on pearl onion.[59]\n\nOverview[edit]\nMain article: Perl language structure\nAccording to Wall, Perl has two slogans. The first is \"There's more than one way to do it\", commonly known as TMTOWTDI. The second slogan is \"Easy things should be easy and hard things should be possible\".[11]\n\nFeatures[edit]\nThe overall structure of Perl derives broadly from C. Perl is procedural in nature, with variables, expressions, assignment statements, brace-delimited blocks, control structures, and subroutines.\n\nPerl also takes features from shell programming. All variables are marked with leading sigils, which allow variables to be interpolated directly into strings. However, unlike the shell, Perl uses sigils on all accesses to variables, and unlike most other programming languages that use sigils, the sigil doesn't denote the type of the variable but the type of the expression. So for example, to access a list of values in a hash, the sigil for an array (\"@\") is used, not the sigil for a hash (\"%\"). Perl also has many built-in functions that provide tools often used in shell programming (although many of these tools are implemented by programs external to the shell) such as sorting, and calling operating system facilities.\n\nPerl takes lists from Lisp, hashes (\"associative arrays\") from AWK, and regular expressions from sed. These simplify and facilitate many parsing, text-handling, and data-management tasks. Also shared with Lisp are the implicit return of the last value in a block, and the fact that all statements have a value, and thus are also expressions and can be used in larger expressions themselves.\n\nPerl 5 added features that support complex data structures, first-class functions (that is, closures as values), and an object-oriented programming model. These include references, packages, class-based method dispatch, and lexically scoped variables, along with compiler directives (for example, the strict pragma). A major additional feature introduced with Perl 5 was the ability to package code as reusable modules. Wall later stated that \"The whole intent of Perl 5's module system was to encourage the growth of Perl culture rather than the Perl core.\"[60]\n\nAll versions of Perl do automatic data-typing and automatic memory management. The interpreter knows the type and storage requirements of every data object in the program; it allocates and frees storage for them as necessary using reference counting (so it cannot deallocate circular data structures without manual intervention). Legal type conversions — for example, conversions from number to string — are done automatically at run time; illegal type conversions are fatal errors.\n\nDesign[edit]\nThe design of Perl can be understood as a response to three broad trends in the computer industry: falling hardware costs, rising labor costs, and improvements in compiler technology. Many earlier computer languages, such as Fortran and C, aimed to make efficient use of expensive computer hardware. In contrast, Perl was designed so that computer programmers could write programs more quickly and easily.\n\nPerl has many features that ease the task of the programmer at the expense of greater CPU and memory requirements. These include automatic memory management; dynamic typing; strings, lists, and hashes; regular expressions; introspection; and an eval() function. Perl follows the theory of \"no built-in limits\",[56] an idea similar to the Zero One Infinity rule.\n\nWall was trained as a linguist, and the design of Perl is very much informed by linguistic principles. Examples include Huffman coding (common constructions should be short), good end-weighting (the important information should come first), and a large collection of language primitives. Perl favors language constructs that are concise and natural for humans to write, even where they complicate the Perl interpreter.\n\nPerl's syntax reflects the idea that \"things that are different should look different.\"[61] For example, scalars, arrays, and hashes have different leading sigils. Array indices and hash keys use different kinds of braces. Strings and regular expressions have different standard delimiters. This approach can be contrasted with a language such as Lisp, where the same basic syntax, composed of simple and universal Symbolic expressions, is used for all purposes.\n\nPerl does not enforce any particular programming paradigm (procedural, object-oriented, functional, or others) or even require the programmer to choose among them.\n\nThere is a broad practical bent to both the Perl language and the community and culture that surround it. The preface to Programming Perl begins: \"Perl is a language for getting your job done.\"[11] One consequence of this is that Perl is not a tidy language. It includes many features, tolerates exceptions to its rules, and employs heuristics to resolve syntactical ambiguities. Because of the forgiving nature of the compiler, bugs can sometimes be hard to find. Perl's function documentation remarks on the variant behavior of built-in functions in list and scalar contexts by saying, \"In general, they do what you want, unless you want consistency.\"[62]\n\nNo written specification or standard for the Perl language exists for Perl versions through Perl 5, and there are no plans to create one for the current version of Perl. There has been only one implementation of the interpreter, and the language has evolved along with it. That interpreter, together with its functional tests, stands as a de facto specification of the language. Perl 6, however, started with a specification,[63] and several projects[64] aim to implement some or all of the specification.\n\nApplications[edit]\nPerl has many and varied applications, compounded by the availability of many standard and third-party modules.\n\nPerl has chiefly been used to write CGI scripts: large projects written in Perl include cPanel, Slash, Bugzilla, RT, TWiki, and Movable Type; high-traffic websites that use Perl extensively include Priceline.com, Craigslist,[65] IMDb,[66] LiveJournal, DuckDuckGo,[67][68] Slashdot and Ticketmaster. It is also an optional component of the popular LAMP technology stack for Web development, in lieu of PHP or Python.\n\nPerl is often used as a glue language, tying together systems and interfaces that were not specifically designed to interoperate, and for \"data munging\",[69] that is, converting or processing large amounts of data for tasks such as creating reports. In fact, these strengths are intimately linked. The combination makes Perl a popular all-purpose language for system administrators, particularly because short programs, often called \"one-liner programs\", can be entered and run on a single command line.\n\nPerl code can be made portable across Windows and Unix; such code is often used by suppliers of software (both COTS and bespoke) to simplify packaging and maintenance of software build- and deployment-scripts.\n\nGraphical user interfaces (GUIs) may be developed using Perl. For example, Perl/Tk and WxPerl are commonly used to enable user interaction with Perl scripts. Such interaction may be synchronous or asynchronous, using callbacks to update the GUI.\n\nImplementation[edit]\nPerl is implemented as a core interpreter, written in C, together with a large collection of modules, written in Perl and C. As of 2010. The interpreter is 150,000 lines of C code and compiles to a 1 MB executable on typical machine architectures. Alternatively, the interpreter can be compiled to a link library and embedded in other programs. There are nearly 500 modules in the distribution, comprising 200,000 lines of Perl and an additional 350,000 lines of C code (much of the C code in the modules consists of character encoding tables).\n\nThe interpreter has an object-oriented architecture. All of the elements of the Perl language—scalars, arrays, hashes, coderefs, file handles—are represented in the interpreter by C structs. Operations on these structs are defined by a large collection of macros, typedefs, and functions; these constitute the Perl C API. The Perl API can be bewildering to the uninitiated, but its entry points follow a consistent naming scheme, which provides guidance to those who use it.\n\nThe life of a Perl interpreter divides broadly into a compile phase and a run phase.[70] In Perl, the phases are the major stages in the interpreter's life-cycle. Each interpreter goes through each phase only once, and the phases follow in a fixed sequence.\n\nMost of what happens in Perl's compile phase is compilation, and most of what happens in Perl's run phase is execution, but there are significant exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are compile time and run time. Perl is in compile time at most points during the compile phase, but compile time may also be entered during the run phase. The compile time for code in a string argument passed to the eval built-in occurs during the run phase. Perl is often in run time during the compile phase and spends most of the run phase in run time. Code in BEGIN blocks executes at run time but in the compile phase.\n\nAt compile time, the interpreter parses Perl code into a syntax tree. At run time, it executes the program by walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.\n\nPerl has a Turing-complete grammar because parsing can be affected by run-time code executed during the compile phase.[71] Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.\n\nIt is often said that \"Only perl can parse Perl\",[72] meaning that only the Perl interpreter (perl) can parse the Perl language (Perl), but even this is not, in general, true. Because the Perl interpreter can simulate a Turing machine during its compile phase, it would need to decide the halting problem in order to complete parsing in every case. It is a long-standing result that the halting problem is undecidable, and therefore not even perl can always parse Perl. Perl makes the unusual choice of giving the user access to its full programming power in its own compile phase. The cost in terms of theoretical purity is high, but practical inconvenience seems to be rare.\n\nOther programs that undertake to parse Perl, such as source-code analyzers and auto-indenters, have to contend not only with ambiguous syntactic constructs but also with the undecidability of Perl parsing in the general case. Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do). It was Kennedy who first conjectured that \"parsing Perl suffers from the 'halting problem'\",[73] which was later proved.[74]\n\nPerl is distributed with over 250,000 functional tests for core Perl language and over 250,000 functional tests for core modules. These run as part of the normal build process and extensively exercise the interpreter and its core modules. Perl developers rely on the functional tests to ensure that changes to the interpreter do not introduce software bugs; additionally, Perl users who see that the interpreter passes its functional tests on their system can have a high degree of confidence that it is working properly.\n\nAvailability[edit]\nPerl is dual licensed under both the Artistic License 1.0[3][4] and the GNU General Public License.[5] Distributions are available for most operating systems. It is particularly prevalent on Unix and Unix-like systems, but it has been ported to most modern (and many obsolete) platforms. With only six reported exceptions, Perl can be compiled from source code on all POSIX-compliant, or otherwise-Unix-compatible platforms.[75]\n\nBecause of unusual changes required for the classic Mac OS environment, a special port called MacPerl was shipped independently.[76]\n\nThe Comprehensive Perl Archive Network carries a complete list of supported platforms with links to the distributions available on each.[77] CPAN is also the source for publicly available Perl modules that are not part of the core Perl distribution.\n\nWindows[edit]\nUsers of Microsoft Windows typically install one of the native binary distributions of Perl for Win32, most commonly Strawberry Perl or ActivePerl. Compiling Perl from source code under Windows is possible, but most installations lack the requisite C compiler and build tools. This also makes it difficult to install modules from the CPAN, particularly those that are partially written in C.\n\nActivePerl is a closed source distribution from ActiveState that has regular releases that track the core Perl releases.[78] The distribution also includes the Perl package manager (PPM),[79] a popular tool for installing, removing, upgrading, and managing the use of common Perl modules. Included also is PerlScript, a Windows Script Host (WSH) engine implementing the Perl language. Visual Perl is an ActiveState tool that adds Perl to the Visual Studio .NET development suite.\n\nStrawberry Perl is an open source distribution for Windows. It has had regular, quarterly releases since January 2008, including new modules as feedback and requests come in. Strawberry Perl aims to be able to install modules like standard Perl distributions on other platforms, including compiling XS modules.\n\nThe Cygwin emulation layer is another way of running Perl under Windows. Cygwin provides a Unix-like environment on Windows, and both Perl and CPAN are available as standard pre-compiled packages in the Cygwin setup program. Since Cygwin also includes gcc, compiling Perl from source is also possible.\n\nA perl executable is included in several Windows Resource kits in the directory with other scripting tools.\n\nImplementations of Perl come with the MKS Toolkit and UWIN.\n\nDatabase interfaces[edit]\nPerl's text-handling capabilities can be used for generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a Web page using HTML, JavaScript, and SQL in a here document). The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large amounts of data for post-processing by a Perl subprogram.[80] In early versions of Perl, database interfaces were created by relinking the interpreter with a client-side database library. This was sufficiently difficult that it was done for only a few of the most-important and most widely used databases, and it restricted the resulting perl executable to using just one database interface at a time.\n\nIn Perl 5, database interfaces are implemented by Perl DBI modules. The DBI (Database Interface) module presents a single, database-independent interface to Perl applications, while the DBD (Database Driver) modules handle the details of accessing some 50 different databases; there are DBD drivers for most ANSI SQL databases.\n\nDBI provides caching for database handles and queries, which can greatly improve performance in long-lived execution environments such as mod perl,[81] helping high-volume systems avert load spikes as in the Slashdot effect.\n\nIn modern Perl applications, especially those written using web frameworks such as Catalyst, the DBI module is often used indirectly via object-relational mappers such as DBIx::Class, Class::DBI or Rose::DB::Object that generate SQL queries and handle data transparently to the application author.\n\nComparative performance[edit]\nThe Computer Language Benchmarks Game, a project hosted by Alioth, compares the performance of implementations of typical programming problems in several programming languages.[82] The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.[83]\n\nLarge Perl programs start more slowly than similar programs in compiled languages because perl has to compile the source every time it runs. In a talk at the YAPC::Europe 2005 conference and subsequent article \"A Timely Start\", Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path.[84] Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling.[85] Therefore, Perl programs pay this overhead penalty on every execution. The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.\n\nA number of tools have been introduced to improve this situation. The first such tool was Apache's mod perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. ActivePerl, via Microsoft ISAPI, provides similar performance improvements.\n\nOnce Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.\n\nOptimizing[edit]\nBecause Perl is an interpreted language, it can give problems when efficiency is critical; in such situations, the most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex but flexible XS mechanism.[86]\n\nPerl 6[edit]\nMain article: Perl 6\n\nCamelia, the logo for the Perl 6 project.[87]\nAt the 2000 Perl Conference, Jon Orwant made a case for a major new language-initiative.[88] This led to a decision to begin work on a redesign of the language, to be called Perl 6. Proposals for new language features were solicited from the Perl community at large, which submitted more than 300 RFCs.\n\nWall spent the next few years digesting the RFCs and synthesizing them into a coherent framework for Perl 6. He has presented his design for Perl 6 in a series of documents called \"apocalypses\" - numbered to correspond to chapters in Programming Perl. As of January 2011, the developing specification of Perl 6 is encapsulated in design documents called Synopses - numbered to correspond to Apocalypses.[89]\n\nPerl 6 is not intended to be backward compatible, although there will be a compatibility mode. Perl 6 and Perl 5 are distinct languages with a common ancestry.[90]\n\nThesis work by Bradley M. Kuhn, overseen by Wall, considered the possible use of the Java virtual machine as a runtime for Perl.[91] Kuhn's thesis showed this approach to be problematic. In 2001, it was decided that Perl 6 would run on a cross-language virtual machine called Parrot. This will mean that other languages targeting the Parrot will gain native access to CPAN, allowing some level of cross-language development.\n\nIn 2005, Audrey Tang created the pugs project, an implementation of Perl 6 in Haskell. This acted as, and continues to act as, a test platform for the Perl 6 language (separate from the development of the actual implementation) - allowing the language designers to explore. The pugs project spawned an active Perl/Haskell cross-language community centered around the freenode #perl6 IRC channel.\n\nAs of 2012, a number of features in the Perl 6 language show similarities to Haskell.\n\nAs of 2012, Perl 6 development centers primarily around two compilers:[92]\n\nRakudo Perl 6, an implementation running on the Parrot virtual machine and the Java virtual machine.[93] Developers are also working on MoarVM, a C language-based virtual machine designed specifically for Rakudo.[94]\nNiecza, which targets the Common Language Runtime.\nFuture of Perl 5[edit]\nDevelopment of Perl 5 is also continuing. Perl 5.12.0 was released in April 2010 with some new features influenced by the design of Perl 6,[35][95] followed by Perl 5.14.1 (released on June 17, 2011), Perl 5.16.1 (released on August 9, 2012.[96]), and Perl 5.18.0 (released on May 18, 2013). Perl 5 development versions are released on a monthly basis, with major releases coming out once per year.[97]\n\nFuture plans for Perl 5 include making the core language easier to extend from modules, and providing a small, extensible meta-object protocol in core.[98]\n\nThe relative proportion of Internet searches for 'Perl programming', as compared with similar searches for other programming languages, steadily declined from about 10% in 2005 to about 2% in 2011, and has remained around the 2% level since.[99]\n\nPerl community[edit]\nPerl's culture and community has developed alongside the language itself. Usenet was the first public venue in which Perl was introduced, but over the course of its evolution, Perl's community was shaped by the growth of broadening Internet-based services including the introduction of the World Wide Web. The community that surrounds Perl was, in fact, the topic of Wall's first \"State of the Onion\" talk.[100]\n\nState of the Onion[edit]\nState of the Onion is the name for Wall’s yearly keynote-style summaries on the progress of Perl and its community. They are characterized by his hallmark humor, employing references to Perl’s culture, the wider hacker culture, Wall’s linguistic background, sometimes his family life, and occasionally even his Christian background.[101]\n\nEach talk is first given at various Perl conferences and is eventually also published online.\n\nPerl pastimes[edit]\nJAPHs\nIn email, Usenet, and message board postings, \"Just another Perl hacker\" (JAPH) programs are a common trend, originated by Randal L. Schwartz, one of the earliest professional Perl trainers.[102] In the parlance of Perl culture, Perl programmers are known as Perl hackers, and from this derives the practice of writing short programs to print out the phrase \"Just another Perl hacker\". In the spirit of the original concept, these programs are moderately obfuscated and short enough to fit into the signature of an email or Usenet message. The \"canonical\" JAPH as developed by Schwartz includes the comma at the end, although this is often omitted.[103]\nPerl golf\nPerl \"golf\" is the pastime of reducing the number of characters (key \"strokes\") used in a Perl program to the bare minimum, much in the same way that golf players seek to take as few shots as possible in a round. The phrase's first use[104] emphasized the difference between pedestrian code meant to teach a newcomer and terse hacks likely to amuse experienced Perl programmers, an example of the latter being JAPHs that were already used in signatures in Usenet postings and elsewhere. Similar stunts had been an unnamed pastime in the language APL in previous decades. The use of Perl to write a program that performed RSA encryption prompted a widespread and practical interest in this pastime.[105] In subsequent years, the term \"code golf\" has been applied to the pastime in other languages.[106] A Perl Golf Apocalypse was held at Perl Conference 4.0 in Monterey, California in July 2000.\nObfuscation\nAs with C, obfuscated code competitions were a well known pastime in the late 1990s. The Obfuscated Perl Contest was a competition held by The Perl Journal from 1996 to 2000 that made an arch virtue of Perl's syntactic flexibility. Awards were given for categories such as \"most powerful\"—programs that made efficient use of space—and \"best four-line signature\" for programs that fit into four lines of 76 characters in the style of a Usenet signature block.[107]\nPoetry\nPerl poetry is the practice of writing poems that can be compiled as legal Perl code, for example the piece known as Black Perl. Perl poetry is made possible by the large number of English words that are used in the Perl language. New poems are regularly submitted to the community at PerlMonks.[108]\nPerl on IRC[edit]\nThere are a number of IRC channels that offer support for the language and some modules.\n\nIRC Network\tChannels\nirc.freenode.net\t#perl #perl6 #cbstream #perlcafe #poe\nirc.perl.org\t#moose #poe #catalyst #dbix-class #perl-help #distzilla #epo #corehackers #sdl #win32 #toolchain #padre #dancer\nirc.slashnet.org\t#perlmonks\nirc.oftc.net\t#perl\nirc.efnet.net\t#perlhelp\nirc.rizon.net\t#perl\nirc.debian.org\t#debian-perl (packaging Perl modules for Debian)\nCPAN Acme[edit]\nThere are also many examples of code written purely for entertainment on the CPAN. Lingua::Romana::Perligata, for example, allows writing programs in Latin.[109] Upon execution of such a program, the module translates its source code into regular Perl and runs it.\n\nThe Perl community has set aside the \"Acme\" namespace for modules that are fun in nature (but its scope has widened to include exploratory or experimental code or any other module that is not meant to ever be used in production). Some of the Acme modules are deliberately implemented in amusing ways. This includes Acme::Bleach, one of the first modules in the Acme:: namespace,[110] which allows the program's source code to be \"whitened\" (i.e., all characters replaced with whitespace) and yet still work.\n\nExample code[edit]\nIn older versions of Perl, one would write the Hello World program as:\n\nprint \"Hello World!\\n\";\nIn later versions, which support the say statement, one can also write it as:\n\nuse 5.010;\nsay \"Hello World!\";\nGood Perl practices require more complex programs to add the use strict; and use warnings; pragmas, leading into something like:\n\nuse strict;\nuse warnings;\n\nprint \"Hello World!\\n\";\nHere is a more complex Perl program, that counts down the seconds up to a given threshold:\n\n#!/usr/bin/perl\nuse strict;\nuse warnings;\nuse IO::Handle;\n\nmy ( $remaining, $total );\n\n$remaining = $total = shift(@ARGV);\n\nSTDOUT->autoflush(1);\n\nwhile ( $remaining ) {\n    printf ( \"Remaining %s/%s \\r\", $remaining--, $total );\n    sleep 1;\n}\n\nprint \"\\n\";\nThe perl interpreter can also be used for one-off scripts on the command line. The following example (as invoked from an sh-compatible shell, such as Bash) translates the string \"Bob\" in all files ending with .txt in the current directory to \"Robert\":\n\n$ perl -i.bak -lp -e 's/Bob/Robert/g' *.txt\nCriticism[edit]\nPerl has been referred to as \"line noise\" by some programmers who claim its syntax makes it a write-only language. The earliest such mention was in the first edition of the book Learning Perl, a Perl 5 tutorial book written by Randal L. Schwartz,[111] in the first chapter of which he states: \"Yes, sometimes Perl looks like line noise to the uninitiated, but to the seasoned Perl programmer, it looks like checksummed line noise with a mission in life.\"[112] He also stated that the accusation that Perl is a write-only language could be avoided by coding with \"proper care\".[112] The Perl overview document perlintro states that the names of built-in \"magic\" scalar variables \"look like punctuation or line noise\".[113] The perlstyle document states that line noise in regular expressions could be mitigated using the /x modifier to add whitespace.[114]\n\nAccording to the Perl 6 FAQ, Perl 6 was designed to mitigate \"the usual suspects\" that elicit the \"line noise\" claim from Perl 5 critics, including the removal of \"the majority of the punctuation variables\" and the sanitization of the regex syntax.[115] The Perl 6 FAQ also states that what is sometimes referred to as Perl's line noise is \"the actual syntax of the language\" just as gerunds and prepositions are a part of the English language.[115] In a December 2012 blog posting, despite claiming that \"Rakudo Perl 6 has failed and will continue to fail unless it gets some adult supervision\", chromatic stated that the design of Perl 6 has a \"well-defined grammar\" as well as an \"improved type system, a unified object system with an intelligent metamodel, metaoperators, and a clearer system of context that provides for such niceties as pervasive laziness\".[116] He also stated that \"Perl 6 has a coherence and a consistency that Perl 5 lacks.\"[116]",
          "type": "interpreted"
        },
        {
          "name": "PHP",
          "details": "PHP is a server-side scripting language designed primarily for web development but also used as a general-purpose programming language. Originally created by Rasmus Lerdorf in 1994,[5] the PHP reference implementation is now produced by The PHP Development Team.[6] PHP originally stood for Personal Home Page,[5] but it now stands for the recursive acronym PHP: Hypertext Preprocessor.[7]\n\nPHP code may be embedded into HTML code, or it can be used in combination with various web template systems, web content management systems and web frameworks. PHP code is usually processed by a PHP interpreter implemented as a module in the web server or as a Common Gateway Interface (CGI) executable. The web server combines the results of the interpreted and executed PHP code, which may be any type of data, including images, with the generated web page. PHP code may also be executed with a command-line interface (CLI) and can be used to implement standalone graphical applications.[8]\n\nThe standard PHP interpreter, powered by the Zend Engine, is free software released under the PHP License. PHP has been widely ported and can be deployed on most web servers on almost every operating system and platform, free of charge.[9]\n\nThe PHP language evolved without a written formal specification or standard until 2014, leaving the canonical PHP interpreter as a de facto standard. Since 2014 work has gone on to create a formal PHP specification.[10]\n\nDuring the 2010s there have been increased efforts towards standardisation and code sharing in PHP applications by projects such as PHP-FIG in the form of PSR-initiatives as well as Composer dependency manager and the Packagist repository.\n\nContents  [hide] \n1\tHistory\n1.1\tEarly history\n1.2\tPHP 3 and 4\n1.3\tPHP 5\n1.4\tPHP 6 and Unicode\n1.5\tPHP 7\n1.6\tRelease history\n2\tMascot\n3\tSyntax\n3.1\tData types\n3.2\tFunctions\n3.3\tObject-oriented programming\n4\tImplementations\n5\tLicensing\n6\tDevelopment and community\n7\tInstallation and configuration\n8\tUse\n9\tSecurity\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory[edit]\nEarly history[edit]\n\n\n\nRasmus Lerdorf, who wrote the original Common Gateway Interface (CGI) component, together with Andi Gutmans and Zeev Suraski, who rewrote the parser that formed PHP 3.\nPHP development began in 1995 when Rasmus Lerdorf wrote several Common Gateway Interface (CGI) programs in C,[11][12][13] which he used to maintain his personal homepage. He extended them to work with web forms and to communicate with databases, and called this implementation \"Personal Home Page/Forms Interpreter\" or PHP/FI.\n\nPHP/FI could help to build simple, dynamic web applications. To accelerate bug reporting and to improve the code, Lerdorf initially announced the release of PHP/FI as \"Personal Home Page Tools (PHP Tools) version 1.0\" on the Usenet discussion group comp.infosystems.www.authoring.cgi on June 8, 1995.[14][15] This release already had the basic functionality that PHP has as of 2013. This included Perl-like variables, form handling, and the ability to embed HTML. The syntax resembled that of Perl but was simpler, more limited and less consistent.[6]\n\nLerdorf did not intend the early PHP to become a new programming language, but it grew organically, with Lerdorf noting in retrospect: \"I don’t know how to stop it, there was never any intent to write a programming language […] I have absolutely no idea how to write a programming language, I just kept adding the next logical step on the way.\"[16] A development team began to form and, after months of work and beta testing, officially released PHP/FI 2 in November 1997.\n\nThe fact that PHP lacked an original overall design but instead developed organically has led to inconsistent naming of functions and inconsistent ordering of their parameters.[17] In some cases, the function names were chosen to match the lower-level libraries which PHP was \"wrapping\",[18] while in some very early versions of PHP the length of the function names was used internally as a hash function, so names were chosen to improve the distribution of hash values.[19]\n\nPHP 3 and 4[edit]\n\nThis is an example of custom php code on a computer screen.\nZeev Suraski and Andi Gutmans rewrote the parser in 1997 and formed the base of PHP 3, changing the language's name to the recursive acronym PHP: Hypertext Preprocessor.[6][20] Afterwards, public testing of PHP 3 began, and the official launch came in June 1998. Suraski and Gutmans then started a new rewrite of PHP's core, producing the Zend Engine in 1999.[21] They also founded Zend Technologies in Ramat Gan, Israel.[6]\n\nOn May 22, 2000, PHP 4, powered by the Zend Engine 1.0, was released.[6] As of August 2008 this branch reached version 4.4.9. PHP 4 is no longer under development nor will any security updates be released.[22][23]\n\nPHP 5[edit]\nOn July 13, 2004, PHP 5 was released, powered by the new Zend Engine II.[6] PHP 5 included new features such as improved support for object-oriented programming, the PHP Data Objects (PDO) extension (which defines a lightweight and consistent interface for accessing databases), and numerous performance enhancements.[24] In 2008 PHP 5 became the only stable version under development. Late static binding had been missing from PHP and was added in version 5.3.[25][26]\n\nMany high-profile open-source projects ceased to support PHP 4 in new code as of February 5, 2008, because of the GoPHP5 initiative,[27] provided by a consortium of PHP developers promoting the transition from PHP 4 to PHP 5.[28][29]\n\nOver time, PHP interpreters became available on most existing 32-bit and 64-bit operating systems, either by building them from the PHP source code, or by using pre-built binaries.[30] For the PHP versions 5.3 and 5.4, the only available Microsoft Windows binary distributions were 32-bit x86 builds,[31][32] requiring Windows 32-bit compatibility mode while using Internet Information Services (IIS) on a 64-bit Windows platform. PHP version 5.5 made the 64-bit x86-64 builds available for Microsoft Windows.[33]\n\nPHP 6 and Unicode[edit]\nPHP received mixed reviews due to lacking native Unicode support at the core language level.[34][35] In 2005, a project headed by Andrei Zmievski was initiated to bring native Unicode support throughout PHP, by embedding the International Components for Unicode (ICU) library, and representing text strings as UTF-16 internally.[36] Since this would cause major changes both to the internals of the language and to user code, it was planned to release this as version 6.0 of the language, along with other major features then in development.[37]\n\nHowever, a shortage of developers who understood the necessary changes, and performance problems arising from conversion to and from UTF-16, which is rarely used in a web context, led to delays in the project.[38] As a result, a PHP 5.3 release was created in 2009, with many non-Unicode features back-ported from PHP 6, notably namespaces. In March 2010, the project in its current form was officially abandoned, and a PHP 5.4 release was prepared containing most remaining non-Unicode features from PHP 6, such as traits and closure re-binding.[39] Initial hopes were that a new plan would be formed for Unicode integration, but as of 2014 none have been adopted.\n\nPHP 7[edit]\nDuring 2014 and 2015, a new major PHP version was developed, which was numbered PHP 7. The numbering of this version involved some debate.[40] While the PHP 6 Unicode experiment had never been released, several articles and book titles referenced the PHP 6 name, which might have caused confusion if a new release were to reuse the name.[41] After a vote, the name PHP 7 was chosen.[42]\n\nThe foundation of PHP 7 is a PHP branch that was originally dubbed PHP next generation (phpng). It was authored by Dmitry Stogov, Xinchen Hui and Nikita Popov,[43] and aimed to optimize PHP performance by refactoring the Zend Engine while retaining near-complete language compatibility.[44] As of 14 July 2014, WordPress-based benchmarks, which served as the main benchmark suite for the phpng project, showed an almost 100% increase in performance. Changes from phpng are also expected to make it easier to improve performance in the future, as more compact data structures and other changes are seen as better suited for a successful migration to a just-in-time (JIT) compiler.[45] Because of the significant changes, the reworked Zend Engine is called Zend Engine 3, succeeding Zend Engine 2 used in PHP 5.[46]\n\nBecause of major internal changes in phpng, it must receive a new major version number of PHP, rather than a minor PHP 5 release, according to PHP's release process.[47] Major versions of PHP are allowed to break backward-compatibility of code and therefore PHP 7 presented an opportunity for other improvements beyond phpng that require backward-compatibility breaks. In particular, it involved the following changes:\n\nMany fatal- or recoverable-level legacy PHP error mechanisms were replaced with modern object-oriented exceptions[48]\nThe syntax for variable dereferencing was reworked to be internally more consistent and complete, allowing the use of the operators ->, [], (), {}, and :: with arbitrary meaningful left-hand-side expressions[49]\nSupport for legacy PHP 4-style constructor methods was deprecated[50]\nThe behavior of the foreach statement was changed to be more predictable[51]\nConstructors for the few classes built-in to PHP which returned null upon failure were changed to throw an exception instead, for consistency[52]\nSeveral unmaintained or deprecated server application programming interfaces (SAPIs) and extensions were removed from the PHP core, most notably the legacy mysql extension[53]\nThe behavior of the list() operator was changed to remove support for strings[54]\nSupport for legacy ASP-style PHP code delimiters (<% and %>, <script language=php> and </script>) was removed[55]\nAn oversight allowing a switch statement to have multiple default clauses was fixed[56]\nSupport for hexadecimal number support in some implicit conversions from strings to number types was removed[57]\nThe left-shift and right-shift operators were changed to behave more consistently across platforms[58]\nConversions between integers and floating point numbers were tightened and implemented more consistently across platforms[58][59]\nPHP 7 also included new language features. Most notably, it introduces return type declarations for functions,[60] which complement the existing parameter type declarations, and support for the scalar types (integer, float, string, and boolean) in parameter and return type declarations.[61]\n\nRelease history[edit]\nKey\nColor\tMeaning\tDevelopment\nRed\tOld release\tNo development\nYellow\tStable release\tSecurity fixes\nGreen\tStable release\tBug and security fixes\nBlue\tFuture release\tNew features\nVersion\tRelease date\tSupported until[62]\tNotes\n1.0\t8 June 1995\t\tOfficially called \"Personal Home Page Tools (PHP Tools)\". This is the first use of the name \"PHP\".[6]\n2.0\t1 November 1997\t\tOfficially called \"PHP/FI 2.0\". This is the first release that could actually be characterised as PHP, being a standalone language with many features that have endured to the present day.\n3.0\t6 June 1998\t20 October 2000[62]\tDevelopment moves from one person to multiple developers. Zeev Suraski and Andi Gutmans rewrite the base for this version.[6]\n4.0\t22 May 2000\t23 June 2001[62]\tAdded more advanced two-stage parse/execute tag-parsing system called the Zend engine.[63]\n4.1\t10 December 2001\t12 March 2002[62]\tIntroduced \"superglobals\" ($_GET, $_POST, $_SESSION, etc.)[63]\n4.2\t22 April 2002\t6 September 2002[62]\tDisabled register_globals by default. Data received over the network is not inserted directly into the global namespace anymore, closing possible security holes in applications.[63]\n4.3\t27 December 2002\t31 March 2005[62]\tIntroduced the command-line interface (CLI), to supplement the CGI.[63][64]\n4.4\t11 July 2005\t7 August 2008[62]\tFixed a memory corruption bug, which required breaking binary compatibility with extensions compiled against PHP version 4.3.x.[65]\n5.0\t13 July 2004\t5 September 2005[62]\tZend Engine II with a new object model.[66]\n5.1\t24 November 2005\t24 August 2006[62]\tPerformance improvements with introduction of compiler variables in re-engineered PHP Engine.[66] Added PHP Data Objects (PDO) as a consistent interface for accessing databases.[67]\n5.2\t2 November 2006\t6 January 2011[62]\tEnabled the filter extension by default. Native JSON support.[66]\n5.3\t30 June 2009\t14 August 2014[62]\tNamespace support; late static bindings, jump label (limited goto), closures, PHP archives (phar), garbage collection for circular references, improved Windows support, sqlite3, mysqlnd as a replacement for libmysql as underlying library for the extensions that work with MySQL, fileinfo as a replacement for mime_magic for better MIME support, the Internationalization extension, and deprecation of ereg extension.\n5.4\t1 March 2012\t3 September 2015[62]\tTrait support, short array syntax support. Removed items: register_globals, safe_mode, allow_call_time_pass_reference, session_register(), session_unregister() and session_is_registered(). Built-in web server.[68] Several improvements to existing features, performance and reduced memory requirements.\n5.5\t20 June 2013\t21 July 2016[62]\tSupport for generators, finally blocks for exceptions handling, OpCache (based on Zend Optimizer+) bundled in official distribution.[69]\n5.6\t28 August 2014\t31 December 2018[70]\tConstant scalar expressions, variadic functions, argument unpacking, new exponentiation operator, extensions of the use statement for functions and constants, new phpdbg debugger as a SAPI module, and other smaller improvements.[71]\n6.x\tNot released\tN/A\tAbandoned version of PHP that planned to include native Unicode support.[72][73]\n7.0\t3 December 2015[2]\t3 December 2018[70]\tZend Engine 3 (performance improvements[45] and 64-bit integer support on Windows[74]), uniform variable syntax,[49] AST-based compilation process,[75] added Closure::call(),[76] bitwise shift consistency across platforms,[77] ?? (null coalesce) operator,[78] Unicode codepoint escape syntax,[79] return type declarations,[60] scalar type (integer, float, string and boolean) declarations,[61] <=> \"spaceship\" three-way comparison operator,[80] generator delegation,[81] anonymous classes,[82] simpler and more consistently available CSPRNG API,[83] replacement of many remaining internal PHP \"errors\" with the more modern exceptions,[48] and shorthand syntax for importing multiple items from a namespace.[84]\n7.1\tNovember 2016[85]\t3 years after release[47]\tvoid return type,[86] class constant visibility modifiers,[87] nullable types[88]\nBeginning on June 28, 2011, the PHP Group implemented a timeline for the release of new versions of PHP.[47] Under this system, at least one release should occur every month. Once per year, a minor release should occur which may include new features. Every minor release should at least be supported for two years with security and bug fixes, followed by at least one year of only security fixes, for a total of a three-year release process for every minor release. No new features, unless small and self-contained, are to be introduced into a minor release during the three-year release process..\n\nMascot[edit]\n\nThe elePHPant, PHP mascot.\nThe mascot of the PHP project is the elePHPant, a blue elephant with the PHP logo on its side, designed by Vincent Pontier[89] in 1998.[90] The elePHPant is sometimes differently colored when in plush toy form.\n\nSyntax[edit]\nMain article: PHP syntax and semantics\nThe following \"Hello, World!\" program is written in PHP code embedded in an HTML document:\n\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>PHP Test</title>\n    </head>\n    <body>\n        <?php echo '<p>Hello World</p>'; ?>\n    </body>\n</html>\nHowever, as no requirement exists for PHP code to be embedded in HTML, the simplest version of Hello, World! may be written like this, with the closing tag omitted as preferred in files containing pure PHP code[91]\n\n <?='Hello world';\nThe PHP interpreter only executes PHP code within its delimiters. Anything outside its delimiters is not processed by PHP, although non-PHP text is still subject to control structures described in PHP code. The most common delimiters are <?php to open and ?> to close PHP sections. The shortened form <? also exists. This short delimiter makes script files less portable, since support for them can be disabled in the local PHP configuration and it is therefore discouraged.[92][93] However, there is no recommendation against the use of the echo short tag <?=.[94] Prior to PHP 5.4.0, this short syntax for echo() only works with the short_open_tag configuration setting enabled, while for PHP 5.4.0 and later it is always available.[92][95][96] The purpose of all these delimiters is to separate PHP code from non-PHP content, such as JavaScript code or HTML markup.[97]\n\nThe first form of delimiters, <?php and ?>, in XHTML and other XML documents, creates correctly formed XML processing instructions.[98] This means that the resulting mixture of PHP code and other markup in the server-side file is itself well-formed XML.\n\nVariables are prefixed with a dollar symbol, and a type does not need to be specified in advance. PHP 5 introduced type hinting that allows functions to force their parameters to be objects of a specific class, arrays, interfaces or callback functions. However, before PHP 7.0, type hints could not be used with scalar types such as integer or string.[61]\n\nUnlike function and class names, variable names are case sensitive. Both double-quoted (\"\") and heredoc strings provide the ability to interpolate a variable's value into the string.[99] PHP treats newlines as whitespace in the manner of a free-form language, and statements are terminated by a semicolon.[100] PHP has three types of comment syntax: /* */ marks block and inline comments; // as well as # are used for one-line comments.[101] The echo statement is one of several facilities PHP provides to output text, e.g., to a web browser.\n\nIn terms of keywords and language syntax, PHP is similar to the C style syntax. if conditions, for and while loops, and function returns are similar in syntax to languages such as C, C++, C#, Java and Perl.\n\nThe following is an example of PHP for loop:\n\n<?php \nfor ($x = 0; $x <= 100; $x++) {\n    echo \"The number is: $x <br>\";\n} \n?>\nData types[edit]\nPHP stores integers in a platform-dependent range, either a 64-bit or 32-bit signed integer equivalent to the C-language long type. Unsigned integers are converted to signed values in certain situations; this behavior is different from that of other programming languages.[102] Integer variables can be assigned using decimal (positive and negative), octal, hexadecimal, and binary notations.\n\nFloating point numbers are also stored in a platform-specific range. They can be specified using floating point notation, or two forms of scientific notation.[103] PHP has a native Boolean type that is similar to the native Boolean types in Java and C++. Using the Boolean type conversion rules, non-zero values are interpreted as true and zero as false, as in Perl and C++.[103]\n\nThe null data type represents a variable that has no value; NULL is the only allowed value for this data type.[103]\n\nVariables of the \"resource\" type represent references to resources from external sources. These are typically created by functions from a particular extension, and can only be processed by functions from the same extension; examples include file, image, and database resources.[103]\n\nArrays can contain elements of any type that PHP can handle, including resources, objects, and other arrays. Order is preserved in lists of values and in hashes with both keys and values, and the two can be intermingled.[103] PHP also supports strings, which can be used with single quotes, double quotes, nowdoc or heredoc syntax.[104]\n\nThe Standard PHP Library (SPL) attempts to solve standard problems and implements efficient data access interfaces and classes.[105]\n\nFunctions[edit]\nPHP defines a large array of functions in the core language and many are also available in various extensions; these functions are well documented in the online PHP documentation.[106] However, the built-in library has a wide variety of naming conventions and associated inconsistencies, as described under history above.\n\nCustom functions may be defined by the developer, e.g.:\n\nfunction myAge($birthYear) {                                  // defines a function, this one is named \"myAge\"\n    $yearsOld = date('Y') - $birthYear;                       // calculates the age\n    return $yearsOld . ' year' . ($yearsOld != 1 ? 's' : ''); // returns the age in a descriptive form\n}\n\necho 'I am currently ' . myAge(1981) . ' old.';               // outputs the text concatenated\n                                                              // with the return value of myAge()\n// As the result of this syntax, myAge() is called.\nIn 2016, the output of the above sample program is 'I am currently 35 years old.'\n\nIn lieu of function pointers, functions in PHP can be referenced by a string containing their name. In this manner, normal PHP functions can be used, for example, as callbacks or within function tables.[107] User-defined functions may be created at any time without being prototyped.[106][107] Functions may be defined inside code blocks, permitting a run-time decision as to whether or not a function should be defined. There is a function_exists function that determines whether a function with a given name has already been defined. Function calls must use parentheses, with the exception of zero-argument class constructor functions called with the PHP operator new, in which case parentheses are optional.\n\nUntil PHP 5.3, support for anonymous functions and closures did not exist in PHP. While create_function() exists since PHP 4.0.1, it is merely a thin wrapper around eval() that allows normal PHP functions to be created during program execution.[108] PHP 5.3 added syntax to define an anonymous function or \"closure\"[109] which can capture variables from the surrounding scope:\n\nfunction getAdder($x) {\n    return function($y) use ($x) {\n        return $x + $y;\n    };\n}\n\n$adder = getAdder(8);\necho $adder(2); // prints \"10\"\nIn the example above, getAdder() function creates a closure using passed argument $x (the keyword use imports a variable from the lexical context), which takes an additional argument $y, and returns the created closure to the caller. Such a function is a first-class object, meaning that it can be stored in a variable, passed as a parameter to other functions, etc.[110]\n\nUnusually for a dynamically typed language, PHP supports type declarations on function parameters, which are enforced at runtime. This has been supported for classes and interfaces since PHP 5.0, for arrays since PHP 5.1, for \"callables\" since PHP 5.4, and scalar (integer, float, string and boolean) types since PHP 7.0.[61] PHP 7.0 also has type declarations for function return types, expressed by placing the type name after the list of parameters, preceded by a colon.[60] For example, the getAdder function from the earlier example could be annotated with types like so in PHP 7:\n\nfunction getAdder(int $x): \\Closure {\n    return function(int $y) use ($x) : int {\n        return $x + $y;\n    };\n}\n\n$adder = getAdder(8);\necho $adder(2);        // prints \"10\"\necho $adder(null);     // throws an exception because an incorrect type was passed\n$adder = getAdder([]); // would also throw an exception\nBy default, scalar type declarations follow weak typing principles. So, for example, if a parameter's type is int, PHP would allow not only integers, but also convertible numeric strings, floats or booleans to be passed to that function, and would convert them.[61] However, PHP 7 has a \"strict typing\" mode which, when used, disallows such conversions for function calls and returns within a file.[61]\n\nObject-oriented programming[edit]\nBasic object-oriented programming functionality was added in PHP 3 and improved in PHP 4.[6] This allowed for PHP to gain further abstraction, making creative tasks easier for programmers using the language. Object handling was completely rewritten for PHP 5, expanding the feature set and enhancing performance.[111] In previous versions of PHP, objects were handled like value types.[111] The drawback of this method was that code had to make heavy use of PHP's \"reference\" variables if it wanted to modify an object it was passed rather than creating a copy of it. In the new approach, objects are referenced by handle, and not by value.\n\nPHP 5 introduced private and protected member variables and methods, along with abstract classes, final classes, abstract methods, and final methods. It also introduced a standard way of declaring constructors and destructors, similar to that of other object-oriented languages such as C++, and a standard exception handling model. Furthermore, PHP 5 added interfaces and allowed for multiple interfaces to be implemented. There are special interfaces that allow objects to interact with the runtime system. Objects implementing ArrayAccess can be used with array syntax and objects implementing Iterator or IteratorAggregate can be used with the foreach language construct. There is no virtual table feature in the engine, so static variables are bound with a name instead of a reference at compile time.[112]\n\nIf the developer creates a copy of an object using the reserved word clone, the Zend engine will check whether a __clone() method has been defined. If not, it will call a default __clone() which will copy the object's properties. If a __clone() method is defined, then it will be responsible for setting the necessary properties in the created object. For convenience, the engine will supply a function that imports the properties of the source object, so the programmer can start with a by-value replica of the source object and only override properties that need to be changed.[113]\n\nThe following is a basic example of object-oriented programming in PHP:\n\nclass Person\n{\n    public $firstName;\n    public $lastName;\n\n    public function __construct($firstName, $lastName = '') { // optional second argument\n        $this->firstName = $firstName;\n        $this->lastName  = $lastName;\n    }\n\n    public function greet() {\n        return 'Hello, my name is ' . $this->firstName .\n               (($this->lastName != '') ? (' ' . $this->lastName) : '') . '.';\n    }\n\n    public static function staticGreet($firstName, $lastName) {\n        return 'Hello, my name is ' . $firstName . ' ' . $lastName . '.';\n    }\n}\n\n$he    = new Person('John', 'Smith');\n$she   = new Person('Sally', 'Davis');\n$other = new Person('iAmine');\n\necho $he->greet(); // prints \"Hello, my name is John Smith.\"\necho '<br />';\n\necho $she->greet(); // prints \"Hello, my name is Sally Davis.\"\necho '<br />';\n\necho $other->greet(); // prints \"Hello, my name is iAmine.\"\necho '<br />';\n\necho Person::staticGreet('Jane', 'Doe'); // prints \"Hello, my name is Jane Doe.\"\nThe visibility of PHP properties and methods is defined using the keywords public, private, and protected. The default is public, if only var is used; var is a synonym for public. Items declared public can be accessed everywhere. protected limits access to inherited classes (and to the class that defines the item). private limits visibility only to the class that defines the item.[114] Objects of the same type have access to each other's private and protected members even though they are not the same instance. PHP's member visibility features have sometimes been described as \"highly useful.\"[115] However, they have also sometimes been described as \"at best irrelevant and at worst positively harmful.\"[116]\n\nImplementations[edit]\nThe original, only complete and most widely used PHP implementation is powered by the Zend Engine and known simply as PHP. To disambiguate it from other implementations, it is sometimes unofficially referred to as \"Zend PHP\". The Zend Engine compiles PHP source code on-the-fly into an internal format that it can execute, thus it works as an interpreter.[117][118] It is also the \"reference implementation\" of PHP, as PHP has no formal specification, and so the semantics of Zend PHP define the semantics of PHP itself. Due to the complex and nuanced semantics of PHP, defined by how Zend works, it is difficult for competing implementations to offer complete compatibility.\n\nPHP's single-request-per-script-execution model, and the fact the Zend Engine is an interpreter, leads to inefficiency; as a result, various products have been developed to help improve PHP performance. In order to speed up execution time and not have to compile the PHP source code every time the web page is accessed, PHP scripts can also be deployed in the PHP engine's internal format by using an opcode cache, which works by caching the compiled form of a PHP script (opcodes) in shared memory to avoid the overhead of parsing and compiling the code every time the script runs. An opcode cache, Zend Opcache, is built into PHP since version 5.5.[119] Another example of a widely used opcode cache is the Alternative PHP Cache (APC), which is available as a PECL extension.[120]\n\nWhile Zend PHP is still the most popular implementation, several other implementations have been developed. Some of these are compilers or support JIT compilation, and hence offer performance benefits over Zend PHP at the expense of lacking full PHP compatibility. Alternative implementations include the following:\n\nHipHop Virtual Machine (HHVM) – developed at Facebook and available as open source, it converts PHP code into a high-level bytecode (commonly known as an intermediate language), which is then translated into x86-64 machine code dynamically at runtime by a just-in-time (JIT) compiler, resulting in up to 6× performance improvements.[121]\nParrot – a virtual machine designed to run dynamic languages efficiently; Pipp transforms the PHP source code into the Parrot intermediate representation, which is then translated into the Parrot's bytecode and executed by the virtual machine.\nPhalanger – compiles PHP into Common Intermediate Language (CIL) bytecode\nHipHop – developed at Facebook and available as open source, it transforms the PHP scripts into C++ code and then compiles the resulting code, reducing the server load up to 50%. In early 2013, Facebook deprecated it in favor of HHVM due to multiple reasons, including deployment difficulties and lack of support for the whole PHP language, including the create_function() and eval() constructs.[122]\nLicensing[edit]\nPHP is free software released under the PHP License, which stipulates that:[123]\n\nProducts derived from this software may not be called \"PHP\", nor may \"PHP\" appear in their name, without prior written permission from group@php.net. You may indicate that your software works in conjunction with PHP by saying \"Foo for PHP\" instead of calling it \"PHP Foo\" or \"phpfoo\".\n\nThis restriction on use of \"PHP\" makes the PHP License incompatible with the General Public License (GPL), while the Zend License is incompatible due to an advertising clause similar to that of the original BSD license.[124]\n\nDevelopment and community[edit]\nPHP includes various free and open-source libraries in its source distribution, or uses them in resulting PHP binary builds. PHP is fundamentally an Internet-aware system with built-in modules for accessing File Transfer Protocol (FTP) servers and many database servers, including PostgreSQL, MySQL, Microsoft SQL Server and SQLite (which is an embedded database), LDAP servers, and others. Numerous functions familiar to C programmers, such as those in the stdio family, are available in standard PHP builds.[125]\n\nPHP allows developers to write extensions in C to add functionality to the PHP language. PHP extensions can be compiled statically into PHP or loaded dynamically at runtime. Numerous extensions have been written to add support for the Windows API, process management on Unix-like operating systems, multibyte strings (Unicode), cURL, and several popular compression formats. Other PHP features made available through extensions include integration with IRC, dynamic generation of images and Adobe Flash content, PHP Data Objects (PDO) as an abstraction layer used for accessing databases,[126][127][128][129][130][131][132] and even speech synthesis. Some of the language's core functions, such as those dealing with strings and arrays, are also implemented as extensions.[133] The PHP Extension Community Library (PECL) project is a repository for extensions to the PHP language.[134]\n\nSome other projects, such as Zephir, provide the ability for PHP extensions to be created in a high-level language and compiled into native PHP extensions. Such an approach, instead of writing PHP extensions directly in C, simplifies the development of extensions and reduces the time required for programming and testing.[135]\n\nThe PHP Group consists of ten people (as of 2015): Thies C. Arntzen, Stig Bakken, Shane Caraveo, Andi Gutmans, Rasmus Lerdorf, Sam Ruby, Sascha Schumann, Zeev Suraski, Jim Winstead, Andrei Zmievski.[136]\n\nZend Technologies provides a PHP Certification based on PHP 5.5[137] exam for programmers to become certified PHP developers.\n\nInstallation and configuration[edit]\nThere are two primary ways for adding support for PHP to a web server – as a native web server module, or as a CGI executable. PHP has a direct module interface called Server Application Programming Interface (SAPI), which is supported by many web servers including Apache HTTP Server, Microsoft IIS, Netscape (now defunct) and iPlanet. Some other web servers, such as OmniHTTPd, support the Internet Server Application Programming Interface (ISAPI), which is a Microsoft's web server module interface. If PHP has no module support for a web server, it can always be used as a Common Gateway Interface (CGI) or FastCGI processor; in that case, the web server is configured to use PHP's CGI executable to process all requests to PHP files.[138]\n\nPHP-FPM (FastCGI Process Manager) is an alternative FastCGI implementation for PHP, bundled with the official PHP distribution since version 5.3.3.[139] When compared to the older FastCGI implementation, it contains some additional features, mostly useful for heavily loaded web servers.[140]\n\nWhen using PHP for command-line scripting, a PHP command-line interface (CLI) executable is needed. PHP supports a CLI SAPI as of PHP 4.3.0.[141] The main focus of this SAPI is developing shell applications using PHP. There are quite a few differences between the CLI SAPI and other SAPIs, although they do share many of the same behaviors.[142]\n\nPHP has a direct module interface called SAPI for different web servers;[143] in case of PHP 5 and Apache 2.0 on Windows, it is provided in form of a DLL file called php5apache2.dll,[144] which is a module that, among other functions, provides an interface between PHP and the web server, implemented in a form that the server understands. This form is what is known as a SAPI.\n\nThere are different kinds of SAPIs for various web server extensions. For example, in addition to those listed above, other SAPIs for the PHP language include the Common Gateway Interface (CGI) and command-line interface (CLI).[143][145]\n\nPHP can also be used for writing desktop graphical user interface (GUI) applications, by using the PHP-GTK extension. PHP-GTK is not included in the official PHP distribution,[138] and as an extension it can be used only with PHP versions 5.1.0 and newer. The most common way of installing PHP-GTK is compiling it from the source code.[146]\n\nWhen PHP is installed and used in cloud environments, software development kits (SDKs) are provided for using cloud-specific features. For example:\n\nAmazon Web Services provides the AWS SDK for PHP[147]\nWindows Azure can be used with the Windows Azure SDK for PHP.[148]\nNumerous configuration options are supported, affecting both core PHP features and extensions.[149][150] Configuration file php.ini is searched for in different locations, depending on the way PHP is used.[151] The configuration file is split into various sections,[152] while some of the configuration options can be also set within the web server configuration.[153]\n\nUse[edit]\n\nA broad overview of the LAMP software bundle, displayed here together with Squid.\nPHP is a general-purpose scripting language that is especially suited to server-side web development, in which case PHP generally runs on a web server. Any PHP code in a requested file is executed by the PHP runtime, usually to create dynamic web page content or dynamic images used on websites or elsewhere.[154] It can also be used for command-line scripting and client-side graphical user interface (GUI) applications. PHP can be deployed on most web servers, many operating systems and platforms, and can be used with many relational database management systems (RDBMS). Most web hosting providers support PHP for use by their clients. It is available free of charge, and the PHP Group provides the complete source code for users to build, customize and extend for their own use.[9]\n\n\nDynamic web page: example of server-side scripting (PHP and MySQL).\nPHP acts primarily as a filter,[155] taking input from a file or stream containing text and/or PHP instructions and outputting another stream of data. Most commonly the output will be HTML, although it could be JSON, XML or binary data such as image or audio formats. Since PHP 4, the PHP parser compiles input to produce bytecode for processing by the Zend Engine, giving improved performance over its interpreter predecessor.[156]\n\nOriginally designed to create dynamic web pages, PHP now focuses mainly on server-side scripting,[157] and it is similar to other server-side scripting languages that provide dynamic content from a web server to a client, such as Microsoft's ASP.NET, Sun Microsystems' JavaServer Pages,[158] and mod_perl. PHP has also attracted the development of many software frameworks that provide building blocks and a design structure to promote rapid application development (RAD). Some of these include PRADO, CakePHP, Symfony, CodeIgniter, Laravel, Yii Framework, Phalcon and Zend Framework, offering features similar to other web frameworks.\n\nThe LAMP architecture has become popular in the web industry as a way of deploying web applications.[159] PHP is commonly used as the P in this bundle alongside Linux, Apache and MySQL, although the P may also refer to Python, Perl, or some mix of the three. Similar packages, WAMP and MAMP, are also available for Windows and OS X, with the first letter standing for the respective operating system. Although both PHP and Apache are provided as part of the Mac OS X base install, users of these packages seek a simpler installation mechanism that can be more easily kept up to date.\n\nAs of April 2007, over 20 million Internet domains had web services hosted on servers with PHP installed and mod_php was recorded as the most popular Apache HTTP Server module.[160] As of October 2010, PHP was used as the server-side programming language on 75% of all websites whose server-side programming language was known[161] (as of February 2014, the percentage had reached 82%[162]), and PHP was the most-used open source software within enterprises.[163] Web content management systems written in PHP include MediaWiki,[164] Joomla,[165] eZ Publish, eZ Platform, SilverStripe,[166] WordPress,[167] Drupal,[168] Moodle,[169] the user-facing portion of Facebook,[170] and Digg.[171]\n\nFor specific and more advanced usage scenarios, PHP offers a well defined and documented way for writing custom extensions in C or C++.[172][173][174][175][176][177][178] Besides extending the language itself in form of additional libraries, extensions are providing a way for improving execution speed where it is critical and there is room for improvements by using a true compiled language.[179][180] PHP also offers well defined ways for embedding itself into other software projects. That way PHP can be easily used as an internal scripting language for another project, also providing tight interfacing with the project's specific internal data structures.[181]\n\nPHP received mixed reviews due to lacking support for multithreading at the core language level,[182] though using threads is made possible by the \"pthreads\" PECL extension.[183][184]\n\nAs of January 2013, PHP was used in more than 240 million websites (39% of those sampled) and was installed on 2.1 million web servers.[185]\n\nSecurity[edit]\nIn 2013, 9% of all vulnerabilities listed by the National Vulnerability Database were linked to PHP;[186] historically, about 30% of all vulnerabilities listed since 1996 in this database are linked to PHP. Technical security flaws of the language itself or of its core libraries are not frequent; (these numbered 22 in 2009, which was about 1% of the total, although PHP applies to about 20% of programs listed.)[187] Recognizing that programmers make mistakes, some languages include taint checking to automatically detect the lack of input validation which induces many issues. Such a feature is being developed for PHP,[188] but its inclusion into a release has been rejected several times in the past.[189][190]\n\nThere are advanced protection patches, such as Suhosin and Hardening-Patch, that are especially designed for web hosting environments,[191] primarily due to these environments being seen as places where carelessly written code may run. However, many security features, such as function whitelists, have proven more powerful in application-specific environments. Due to PHP's extensive capabilities and code size, criticism of PHP within security communities can be deflected somewhat by the use of Suhosin. This may cease to be the case if PHP7 moves to using a JIT engine, thereby preventing exploitation mitigation technologies such as W^X from being effective.\n\nThere are certain language features and configuration parameters (primarily the default values for such runtime settings) that make PHP applications prone to security issues. Among these, magic_quotes_gpc and register_globals[192] configuration directives are the best known; the latter made any URL parameters become PHP variables, opening a path for serious security vulnerabilities by allowing an attacker to set the value of any uninitialized global variable and interfere with the execution of a PHP script. Support for \"magic quotes\" and \"register globals\" has been deprecated as of PHP 5.3.0, and removed as of PHP 5.4.0.[193]\n\nAnother example for the runtime settings vulnerability comes from failing to disable PHP execution (via engine configuration directive)[194] for the directory where uploaded images are stored; leaving the default settings can result in execution of malicious PHP code embedded within the uploaded images.[195][196][197] Also, leaving enabled the dynamic loading of PHP extensions (via enable_dl configuration directive)[198] in a shared web hosting environment can lead to security issues.[199][200]\n\nAlso, implied type conversions that result in incompatible values being treated as identical against the programmer's intent can lead to security issues. For example, the result of the comparison 0e1234 == 0 comparison is true because the first compared value is treated as scientific notation having the value (0×101234), i.e. zero. This feature resulted in authentication vulnerabilities in Simple Machines Forum,[201] Typo3[202] and phpBB[203] when MD5 password hashes were compared. Instead, either the function strcmp or the identity operator (===) should be used; 0e1234 === 0 results in false.[204]\n\nIn a 2013 analysis of over 170,000 website defacements, published by Zone-H, the most frequently (53%) used technique was exploitation of file inclusion vulnerability, mostly related to insecure usage of the PHP functions include, require, and allow_url_fopen.[205][206]\n\nSee also[edit]\nicon\tComputer programming portal\n\tFree software portal\nPEAR (PHP Extension and Application Repository)\nPHP Extension Community Library (PECL)\nPHP accelerator\nList of PHP accelerators\nList of AMP packages\nList of PHP editors\nPHP-GTK\nTemplate processor\nXAMPP (Free and open source cross-platform web server solution stack package)\nZend Server\nHack (programming language)\nComparison of programming languages\nComparison of web frameworks",
          "type": "interpreted",
          "plid": 27
        },
        {
          "name": "Python",
          "details": "Python is a widely used high-level, general-purpose, interpreted, dynamic programming language.[24][25] Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than possible in languages such as C++ or Java.[26][27] The language provides constructs intended to enable writing clear programs on both a small and large scale.[28]\n\nPython supports multiple programming paradigms, including object-oriented, imperative and functional programming or procedural styles. It features a dynamic type system and automatic memory management and has a large and comprehensive standard library.[29]\n\nPython interpreters are available for many operating systems, allowing Python code to run on a wide variety of systems. Using third-party tools, such as Py2exe or Pyinstaller,[30] Python code can be packaged into stand-alone executable programs for some of the most popular operating systems, so Python-based software can be distributed to, and used on, those environments with no need to install a Python interpreter.\n\nCPython, the reference implementation of Python, is free and open-source software and has a community-based development model, as do nearly all of its variant implementations. CPython is managed by the non-profit Python Software Foundation.\n\nContents  [hide] \n1\tHistory\n2\tFeatures and philosophy\n3\tSyntax and semantics\n3.1\tIndentation\n3.2\tStatements and control flow\n3.3\tExpressions\n3.4\tMethods\n3.5\tTyping\n3.6\tMathematics\n4\tLibraries\n5\tDevelopment environments\n6\tImplementations\n7\tDevelopment\n8\tNaming\n9\tUses\n10\tLanguages influenced by Python\n11\tSee also\n12\tReferences\n13\tFurther reading\n14\tExternal links\nHistory[edit]\n\nGuido van Rossum, the creator of Python\nMain article: History of Python\nPython was conceived in the late 1980s,[31] and its implementation began in December 1989[32] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC language (itself inspired by SETL)[33] capable of exception handling and interfacing with the operating system Amoeba.[8] Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, benevolent dictator for life (BDFL).\n\nAbout the origin of Python, Van Rossum wrote in 1996:[34]\n\nOver six years ago, in December 1989, I was looking for a \"hobby\" programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).\n\nPython 2.0 was released on 16 October 2000 and had many major new features, including a cycle-detecting garbage collector and support for Unicode. With this release the development process was changed and became more transparent and community-backed.[35]\n\nPython 3.0 (which early in its development was commonly referred to as Python 3000 or py3k), a major, backwards-incompatible release, was released on 3 December 2008[36] after a long period of testing. Many of its major features have been backported to the backwards-compatible Python 2.6.x[37] and 2.7.x version series.\n\nFeatures and philosophy[edit]\nPython is a multi-paradigm programming language: object-oriented programming and structured programming are fully supported, and many language features support functional programming and aspect-oriented programming (including by metaprogramming[38] and metaobjects (magic methods)).[39] Many other paradigms are supported via extensions, including design by contract[40][41] and logic programming.[42]\n\nPython uses dynamic typing and a mix of reference counting and a cycle-detecting garbage collector for memory management. An important feature of Python is dynamic name resolution (late binding), which binds method and variable names during program execution.\n\nThe design of Python offers some support for functional programming in the Lisp tradition. The language has map(), reduce() and filter() functions; list comprehensions, dictionaries, and sets; and generator expressions.[43] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[44]\n\nThe core philosophy of the language is summarized by the document The Zen of Python (PEP 20), which includes aphorisms such as:[45]\n\nBeautiful is better than ugly\nExplicit is better than implicit\nSimple is better than complex\nComplex is better than complicated\nReadability counts\nRather than requiring all desired functionality to be built into the language's core, Python was designed to be highly extensible. Python can also be embedded in existing applications that need a programmable interface. This design of a small core language with a large standard library and an easily extensible interpreter was intended by Van Rossum from the start because of his frustrations with ABC, which espoused the opposite mindset.[31]\n\nWhile offering choice in coding methodology, the Python philosophy rejects exuberant syntax, such as in Perl, in favor of a sparser, less-cluttered grammar. As Alex Martelli put it: \"To describe something as clever is not considered a compliment in the Python culture.\"[46] Python's philosophy rejects the Perl \"there is more than one way to do it\" approach to language design in favor of \"there should be one—and preferably only one—obvious way to do it\".[45]\n\nPython's developers strive to avoid premature optimization, and moreover, reject patches to non-critical parts of CPython that would offer a marginal increase in speed at the cost of clarity.[47] When speed is important, a Python programmer can move time-critical functions to extension modules written in languages such as C, or try using PyPy, a just-in-time compiler. Cython is also available, which translates a Python script into C and makes direct C-level API calls into the Python interpreter.\n\nAn important goal of Python's developers is making it fun to use. This is reflected in the origin of the name, which comes from Monty Python,[48] and in an occasionally playful approach to tutorials and reference materials, such as using examples that refer to spam and eggs instead of the standard foo and bar.[49][50]\n\nA common neologism in the Python community is pythonic, which can have a wide range of meanings related to program style. To say that code is pythonic is to say that it uses Python idioms well, that it is natural or shows fluency in the language, that it conforms with Python's minimalist philosophy and emphasis on readability. In contrast, code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.\n\nUsers and admirers of Python, especially those considered knowledgeable or experienced, are often referred to as Pythonists, Pythonistas, and Pythoneers.[51][52]\n\nSyntax and semantics[edit]\nMain article: Python syntax and semantics\nPython is intended to be a highly readable language. It is designed to have an uncluttered visual layout, often using English keywords where other languages use punctuation. Further, Python has fewer syntactic exceptions and special cases than C or Pascal.[53]\n\nIndentation[edit]\nMain article: Python syntax and semantics § Indentation\nPython uses whitespace indentation to delimit blocks - rather than curly braces or keywords. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[54] This feature is also sometimes termed the off-side rule.\n\nStatements and control flow[edit]\nPython's statements include (among others):\n\nThe assignment statement (token '=', the equals sign). This operates differently than in traditional imperative programming languages, and this fundamental mechanism (including the nature of Python's version of variables) illuminates many other features of the language. Assignment in C, e.g., x = 2, translates to \"typed variable name x receives a copy of numeric value 2\". The (right-hand) value is copied into an allocated storage location for which the (left-hand) variable name is the symbolic address. The memory allocated to the variable is large enough (potentially quite large) for the declared type. In the simplest case of Python assignment, using the same example, x = 2, translates to \"(generic) name x receives a reference to a separate, dynamically allocated object of numeric (int) type of value 2.\" This is termed binding the name to the object. Since the name's storage location doesn't contain the indicated value, it is improper to call it a variable. Names may be subsequently rebound at any time to objects of greatly varying types, including strings, procedures, complex objects with data and methods, etc. Successive assignments of a common value to multiple names, e.g., x = 2; y = 2; z = 2 result in allocating storage to (at most) three names and one numeric object, to which all three names are bound. Since a name is a generic reference holder it is unreasonable to associate a fixed data type with it. However at a given time a name will be bound to some object, which will have a type; thus there is dynamic typing.\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if).\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block.\nThe while statement, which executes a block of code as long as its condition is true.\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses; it also ensures that clean-up code in a finally block will always be run regardless of how the block exits.\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming.\nThe def statement, which defines a function or method.\nThe with statement (from Python 2.5), which encloses a code block within a context manager (for example, acquiring a lock before the block of code is run and releasing the lock afterwards, or opening a file and then closing it), allowing Resource Acquisition Is Initialization (RAII)-like behavior.\nThe pass statement, which serves as a NOP. It is syntactically needed to create an empty code block.\nThe assert statement, used during debugging to check for conditions that ought to apply.\nThe yield statement, which returns a value from a generator function. From Python 2.5, yield is also an operator. This form is used to implement coroutines.\nThe import statement, which is used to import modules whose functions or variables can be used in the current program.\nThe print statement was changed to the print() function in Python 3.[55]\nPython does not support tail call optimization or first-class continuations, and, according to Guido van Rossum, it never will.[56][57] However, better support for coroutine-like functionality is provided in 2.5, by extending Python's generators.[58] Before 2.5, generators were lazy iterators; information was passed unidirectionally out of the generator. As of Python 2.5, it is possible to pass information back into a generator function, and as of Python 3.3, the information can be passed through multiple stack levels.[59]\n\nExpressions[edit]\nSome Python expressions are similar to languages such as C and Java, while some are not:\n\nAddition, subtraction, and multiplication are the same, but the behavior of division differs (see Mathematics for details). Python also added the ** operator for exponentiation.\nAs of Python 3.5, it supports matrix multiplication directly with the @ operator, versus C and Java, which implement these as library functions. Earlier versions of Python also used methods instead of an infix operator.[60][61]\nIn Python, == compares by value, versus Java, which compares numerics by value[62] and objects by reference.[63] (Value comparisons in Java on objects can be performed with the equals() method.) Python's is operator may be used to compare object identities (comparison by reference). In Python, comparisons may be chained, for example a <= b <= c.\nPython uses the words and, or, not for its boolean operators rather than the symbolic &&, ||, ! used in Java and C.\nPython has a type of expression termed a list comprehension. Python 2.4 extended list comprehensions into a more general expression termed a generator expression.[43]\nAnonymous functions are implemented using lambda expressions; however, these are limited in that the body can only be one expression.\nConditional expressions in Python are written as x if c else y[64] (different in order of operands from the c ? x : y operator common to many other languages).\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples are written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided all elements of the tuple are immutable. The parentheses around the tuple are optional in some contexts. Tuples can appear on the left side of an equal sign; hence a statement like x, y = y, x can be used to swap two variables.\nPython has a \"string format\" operator %. This functions analogous to printf format strings in C, e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python 3 and 2.6+, this was supplemented by the format() method of the str class, e.g. \"spam={0} eggs={1}\".format(\"blah\", 2).\nPython has various kinds of string literals:\nStrings delimited by single or double quote marks. Unlike in Unix shells, Perl and Perl-influenced languages, single quote marks and double quote marks function identically. Both kinds of string use the backslash (\\) as an escape character and there is no implicit string interpolation such as \"$spam\".\nTriple-quoted strings, which begin and end with a series of three single or double quote marks. They may span multiple lines and function like here documents in shells, Perl and Ruby.\nRaw string varieties, denoted by prefixing the string literal with an r. No escape sequences are interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. Compare \"@-quoting\" in C#.\nPython has array index and array slicing expressions on lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter, called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted, for example a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.\nIn Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\n\nList comprehensions vs. for-loops\nConditional expressions vs. if blocks\nThe eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements.\nStatements cannot be a part of an expression, so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case of this is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is syntactically valid (but probably unintended) C code but if c = 1: ... causes a syntax error in Python.\n\nMethods[edit]\nMethods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, or Ruby).[65]\n\nTyping[edit]\nPython uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that the given object is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.\n\nPython allows programmers to define their own types using classes, which are most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.\n\nBefore version 3.0, Python had two kinds of classes: old-style and new-style.[66] Old-style classes were eliminated in Python 3.0, making all classes new-style. In versions between 2.2 and 3.0, both kinds of classes could be used. The syntax of both styles is the same, the difference being whether the class object is inherited from, directly or indirectly (all new-style classes inherit from object and are instances of type).\n\nSummary of Python 3's built-in types\nType\tMutable\tDescription\tSyntax example\nstr\tImmutable\tA character string: sequence of Unicode codepoints\t'Wikipedia'\n\"Wikipedia\"\n\"\"\"Spanning\nmultiple\nlines\"\"\"\nbytearray\tMutable\tSequence of bytes\tbytearray(b'Some ASCII')\nbytearray(b\"Some ASCII\")\nbytearray([119, 105, 107, 105])\nbytes\tImmutable\tSequence of bytes\tb'Some ASCII'\nb\"Some ASCII\"\nbytes([119, 105, 107, 105])\nlist\tMutable\tList, can contain mixed types\t[4.0, 'string', True]\ntuple\tImmutable\tCan contain mixed types\t(4.0, 'string', True)\nset\tMutable\tUnordered set, contains no duplicates; can contain mixed types if hashable\t{4.0, 'string', True}\nfrozenset\tImmutable\tUnordered set, contains no duplicates; can contain mixed types if hashable\tfrozenset([4.0, 'string', True])\ndict\tMutable\tAssociative array (or dictionary) of key and value pairs; can contain mixed types (keys and values), keys must be a hashable type\t{'key1': 1.0, 3: False}\nint\tImmutable\tInteger of unlimited magnitude[67]\t42\nfloat\tImmutable\tFloating point number, system-defined precision\t3.1415927\ncomplex\tImmutable\tComplex number with real and imaginary parts\t3+2.7j\nbool\tImmutable\tBoolean value\tTrue\nFalse\nellipsis\t\tAn ellipsis placeholder to be used as an index in NumPy arrays\t...\nMathematics[edit]\nPython has the usual C arithmetic operators (+, -, *, /, %). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a new matrix multiply @ operator is included in version 3.5.[68]\n\nThe behavior of division has changed significantly over time:[69]\n\nPython 2.1 and earlier use the C division behavior. The / operator is integer division if both operands are integers, and floating-point division otherwise. Integer division rounds towards 0, e.g. 7 / 3 == 2 and -7 / 3 == -2.\nPython 2.2 changes integer division to round towards negative infinity, e.g. 7 / 3 == 2 and -7 / 3 == -3. The floor division // operator is introduced. So 7 // 3 == 2, -7 // 3 == -3, 7.5 // 3 == 2.0 and -7.5 // 3 == -3.0. Adding from __future__ import division causes a module to use Python 3.0 rules for division (see next).\nPython 3.0 changes / to be always floating-point division. In Python terms, the pre-3.0 / is classic division, the version-3.0 / is real division, and // is floor division.\nRounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a+b) // b == a // b + 1 is always true. It also means that the equation b * (a // b) + a % b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a % b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.[70]\n\nPython provides a round function for rounding a float to the nearest integer. For tie-breaking, versions before 3 use round-away-from-zero: round(0.5) is 1.0, round(-0.5) is −1.0.[71] Python 3 uses round to even: round(1.5) is 2, round(2.5) is 2.[72]\n\nPython allows boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[73][page needed]\n\nPython has extensive built-in support for arbitrary precision arithmetic. Integers are transparently switched from the machine-supported maximum fixed-precision (usually 32 or 64 bits), belonging to the python type int, to arbitrary precision, belonging to the python type long, where needed. The latter have an \"L\" suffix in their textual representation.[74] The Decimal type/class in module decimal (since version 2.4) provides decimal floating point numbers to arbitrary precision and several rounding modes.[75] The Fraction type in module fractions (since version 2.6) provides arbitrary precision for rational numbers.[76]\n\nDue to Python's extensive mathematics library, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.\n\nLibraries[edit]\nPython has a large standard library, commonly cited as one of Python's greatest strengths,[77] providing tools suited to many tasks. This is deliberate and has been described as a \"batteries included\"[29] Python philosophy. For Internet-facing applications, many standard formats and protocols (such as MIME and HTTP) are supported. Modules for creating graphical user interfaces, connecting to relational databases, pseudorandom number generators, arithmetic with arbitrary precision decimals,[78] manipulating regular expressions, and doing unit testing are also included.\n\nSome parts of the standard library are covered by specifications (for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[79]), but most modules are not. They are specified by their code, internal documentation, and test suite (if supplied). However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.\n\nThe standard library is not needed to run Python or embed it in an application. For example, Blender 2.49 omits most of the standard library.\n\nAs of November, 2016, the Python Package Index, the official repository containing third-party software for Python, contains over 92,000[80] packages offering a wide range of functionality, including:\n\ngraphical user interfaces, web frameworks, multimedia, databases, networking and communications\ntest frameworks, automation and web scraping, documentation tools, system administration\nscientific computing, text processing, image processing\nDevelopment environments[edit]\nSee also: Comparison of integrated development environments § Python\nMost Python implementations (including CPython) can function as a command line interpreter, for which the user enters statements sequentially and receives the results immediately (read–eval–print loop (REPL)). In short, Python acts as a command-line interface or shell.\n\nOther shells add abilities beyond those in the basic interpreter, including IDLE and IPython. While generally following the visual style of the Python shell, they implement features like auto-completion, session state retention, and syntax highlighting.\n\nIn addition to standard desktop integrated development environments (Python IDEs), there are also web browser-based IDEs, SageMath (intended for developing science and math-related Python programs), and a browser-based IDE and hosting environment, PythonAnywhere. Additionally, the Canopy IDE is also an option for creating programs written in Python.[81]\n\nImplementations[edit]\nSee also: List of Python software § Python implementations\nThe main Python implementation, named CPython, is written in C meeting the C89 standard.[82] It compiles Python programs into intermediate bytecode,[83] which is executed by the virtual machine.[84] CPython is distributed with a large standard library written in a mixture of C and Python. It is available in versions for many platforms, including Windows and most modern Unix-like systems. CPython was intended from almost its very conception to be cross-platform.[85]\n\nPyPy is a fast, compliant[86] interpreter of Python 2.7 and 3.2. Its just-in-time compiler brings a significant speed improvement over CPython.[87] A version taking advantage of multi-core processors using software transactional memory is being created.[88]\n\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the C memory stack, thus allowing massively concurrent programs. PyPy also has a stackless version.[89]\n\nMicroPython is a lean, fast Python 3 variant that is optimised to run on microcontrollers.\n\nOther just-in-time compilers have been developed in the past, but are now unsupported:\n\nGoogle began a project named Unladen Swallow in 2009 with the aims of speeding up the Python interpreter by 5 times, by using the LLVM, and of improving its multithreading ability to scale to thousands of cores.[90]\nPsyco is a just-in-time specialising compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialised for certain data types and is faster than standard Python code.\nIn 2005, Nokia released a Python interpreter for the Series 60 mobile phones named PyS60. It includes many of the modules from the CPython implementations and some added modules to integrate with the Symbian operating system. This project has been kept up to date to run on all variants of the S60 platform and there are several third party modules available. The Nokia N900 also supports Python with GTK widget libraries, with the feature that programs can be both written and run on the target device.[91]\n\nThere are several compilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\n\nJython compiles into Java byte code, which can then be executed by every Java virtual machine implementation. This also enables the use of Java class library functions from the Python program.\nIronPython follows a similar approach in order to run Python programs on the .NET Common Language Runtime.\nThe RPython language can be compiled to C, Java bytecode, or Common Intermediate Language, and is used to build the PyPy interpreter of Python.\nPyjamas compiles Python to JavaScript.\nShed Skin compiles Python to C++.\nCython and Pyrex compile to C.\nA performance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13.[92]\n\nDevelopment[edit]\nPython's development is conducted largely through the Python Enhancement Proposal (PEP) process. The PEP process is the primary mechanism for proposing major new features, for collecting community input on an issue, and for documenting the design decisions that have gone into Python.[93] Outstanding PEPs are reviewed and commented upon by the Python community and by Van Rossum, the Python project's benevolent dictator for life.[93]\n\nEnhancement of the language goes along with development of the CPython reference implementation. The mailing list python-dev is the primary forum for discussion about the language's development; specific issues are discussed in the Roundup bug tracker maintained at python.org.[94] Development takes place on a self-hosted source code repository running Mercurial.[95]\n\nCPython's public releases come in three types, distinguished by which part of the version number is incremented:\n\nBackwards-incompatible versions, where code is expected to break and must be manually ported. The first part of the version number is incremented. These releases happen infrequently—for example, version 3.0 was released 8 years after 2.0.\nMajor or \"feature\" releases, which are largely compatible but introduce new features. The second part of the version number is incremented. These releases are scheduled to occur roughly every 18 months, and each major version is supported by bugfixes for several years after its release.[96]\nBugfix releases, which introduce no new features but fix bugs. The third and final part of the version number is incremented. These releases are made whenever a sufficient number of bugs have been fixed upstream since the last release, or roughly every 3 months. Security vulnerabilities are also patched in bugfix releases.[97]\nMany alpha, beta, and release-candidates are also released as previews, and for testing before final releases. Although there is a rough schedule for each release, this is often pushed back if the code is not ready. The development team monitors the state of the code by running the large unit test suite during development, and using the BuildBot continuous integration system.[98]\n\nThe community of Python developers has also contributed over 86,000[99] software modules (as of August 20, 2016) to the Python Package Index (PyPI), the official repository of third-party libraries for Python.\n\nThe major academic conference on Python is named PyCon. There are special mentoring programmes like the Pyladies.\n\nNaming[edit]\nPython's name is derived from the television series Monty Python's Flying Circus,[100] and it is common to use Monty Python references in example code.[101] For example, the metasyntactic variables often used in Python literature are spam and eggs, instead of the traditional foo and bar.[101][102] Also, the official Python documentation often contains various obscure Monty Python references.\n\nThe prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyS60, an implementation for the Symbian S60 operating system; PyQt and PyGTK, which bind Qt and GTK, respectively, to Python; and PyPy, a Python implementation originally written in Python.\n\nUses[edit]\nMain article: List of Python software\nSince 2003, Python has consistently ranked in the top ten most popular programming languages as measured by the TIOBE Programming Community Index. As of August 2016, it is the fifth most popular language.[103] It was ranked as Programming Language of the Year for the year 2007 and 2010.[24] It is the third most popular language whose grammatical syntax is not predominantly based on C, e.g. C++, Objective-C (note, C# and Java only have partial syntactic similarity to C, such as the use of curly braces, and are closer in similarity to each other than C).\n\nAn empirical study found scripting languages (such as Python) more productive than conventional languages (such as C and Java) for a programming problem involving string manipulation and search in a dictionary. Memory consumption was often \"better than Java and not much worse than C or C++\".[104]\n\nLarge organizations that make use of Python include Wikipedia, Google,[105] Yahoo!,[106] CERN,[107] NASA,[108] and some smaller ones like ILM,[109] and ITA.[110] The social news networking site, Reddit, is written entirely in Python.\n\nPython can serve as a scripting language for web applications, e.g., via mod_wsgi for the Apache web server.[111] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle and Zope support developers in the design and maintenance of complex applications. Pyjamas and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.\n\nLibraries like NumPy, SciPy and Matplotlib allow the effective use of Python in scientific computing,[112][113] with specialized libraries such as BioPython and Astropy providing domain-specific functionality. SageMath is a mathematical software with a \"notebook\" programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus. The Python language re-implemented in Java platform is used for numeric and statistical calculations with 2D/3D visualization by the DMelt project. [114] [115]\n\nPython has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modeler like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[116] Inkscape, Scribus and Paint Shop Pro,[117] and musical notation program or scorewriter capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[118] It has also been used in several video games,[119][120] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[121] Python is also used in algorithmic trading and quantitative finance.[122] Python can also be implemented in APIs of online brokerages that run on other languages by using wrappers.[123]\n\nPython has been used in artificial intelligence tasks.[124][125][126][127] As a scripting language with module architecture, simple syntax and rich text processing tools, Python is often used for natural language processing tasks.[128]\n\nMany operating systems include Python as a standard component; the language ships with most Linux distributions, AmigaOS 4, FreeBSD, NetBSD, OpenBSD and OS X, and can be used from the terminal. Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.\n\nPython has also seen extensive use in the information security industry, including in exploit development.[129][130]\n\nMost of the Sugar software for the One Laptop per Child XO, now developed at Sugar Labs, is written in Python.[131]\n\nThe Raspberry Pi single-board computer project has adopted Python as its main user-programming language.\n\nLibreOffice includes Python and intends to replace Java with Python. Python Scripting Provider is a core feature[132] since Version 4.0 from 7 February 2013.\n\nLanguages influenced by Python[edit]\nPython's design and philosophy have influenced several programming languages, including:\n\nBoo uses indentation, a similar syntax, and a similar object model. However, Boo uses static typing (and optional duck typing) and is closely integrated with the .NET Framework.[133]\nCobra uses indentation and a similar syntax. Cobra's \"Acknowledgements\" document lists Python first among languages that influenced it.[134] However, Cobra directly supports design-by-contract, unit tests, and optional static typing.[135]\nECMAScript borrowed iterators, generators, and list comprehensions from Python.[136]\nGo is described as incorporating the \"development speed of working in a dynamic language like Python\".[137]\nGroovy was motivated by the desire to bring the Python design philosophy to Java.[138]\nJulia was designed \"with true macros [.. and to be] as usable for general programming as Python [and] should be as fast as C\".[21] Calling to or from Julia is possible; to with PyCall.jl and a Python package pyjulia allows calling, in the other direction, from Python.\nOCaml has an optional syntax, named twt (The Whitespace Thing), inspired by Python and Haskell.[139]\nRuby's creator, Yukihiro Matsumoto, has said: \"I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That's why I decided to design my own language.\"[140]\nCoffeeScript is a programming language that cross-compiles to JavaScript; it has Python-inspired syntax.\nSwift is a programming language invented by Apple; it has some Python-inspired syntax.[141]\nPython's development practices have also been emulated by other languages. The practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python's case, a PEP) is also used in Tcl[142] and Erlang[143] because of Python's influence.\n\nPython has been awarded a TIOBE Programming Language of the Year award twice (in 2007 and 2010), which is given to the language with the greatest growth in popularity over the course of a year, as measured by the TIOBE index.[144]\n\nSee also[edit]\n",
          "type": "intepreted",
          "plid": 28
        },
        {
          "name": "Ruby",
          "details": "Ruby is a dynamic, reflective, object-oriented, general-purpose programming language. It was designed and developed in the mid-1990s by Yukihiro \"Matz\" Matsumoto in Japan.\n\nAccording to its creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, and Lisp.[11] It supports multiple programming paradigms, including functional, object-oriented, and imperative. It also has a dynamic type system and automatic memory management.\n\nContents  [hide] \n1\tHistory\n1.1\tEarly concept\n1.2\tThe name \"Ruby\"\n1.3\tFirst publication\n1.4\tEarly releases\n1.5\tRuby 1.8\n1.6\tRuby 1.9\n1.7\tRuby 2.0\n1.8\tRuby 2.1\n1.9\tRuby 2.2\n1.10\tRuby 2.3\n2\tTable of versions\n3\tPhilosophy\n4\tFeatures\n5\tSemantics\n6\tSyntax\n7\tDifferences from other languages\n8\tInteraction\n9\tExamples\n9.1\tStrings\n9.2\tCollections\n9.3\tControl structures\n9.4\tBlocks and iterators\n9.5\tClasses\n9.5.1\tOpen classes\n9.6\tExceptions\n9.7\tMetaprogramming\n9.8\tMore examples\n10\tImplementations\n10.1\tMatz's Ruby Interpreter\n10.2\tAlternate implementations\n10.3\tPlatform support\n11\tRepositories and libraries\n12\tSee also\n13\tReferences\n14\tFurther reading\n15\tExternal links\nHistory[edit]\nEarly concept[edit]\nRuby was conceived on February 24, 1993. In a 1999 post to the ruby-talk mailing list, Ruby author Yukihiro Matsumoto describes some of his early ideas about the language:[12]\n\nI was talking with my colleague about the possibility of an object-oriented scripting language. I knew Perl (Perl4, not Perl5), but I didn't like it really, because it had the smell of a toy language (it still has). The object-oriented language seemed very promising. I knew Python then. But I didn't like it, because I didn't think it was a true object-oriented language — OO features appeared to be add-on to the language. As a language maniac and OO fan for 15 years, I really wanted a genuine object-oriented, easy-to-use scripting language. I looked for but couldn't find one. So I decided to make it.\n\nMatsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.[13]\n\nThe name \"Ruby\"[edit]\nThe name \"Ruby\" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language.[14] Initially two names were proposed: \"Coral\" and \"Ruby\". Matsumoto chose the latter in a later e-mail to Ishitsuka.[15] Matsumoto later noted a factor in choosing the name \"Ruby\" – it was the birthstone of one of his colleagues.[16][17]\n\nFirst publication[edit]\nThe first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995.[18][19] Subsequently, three more versions of Ruby were released in two days.[14] The release coincided with the launch of the Japanese-language ruby-list mailing list, which was the first mailing list for the new language.\n\nAlready present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.[20]\n\nEarly releases[edit]\nFollowing the release of Ruby 0.95 in 1995, several stable versions of Ruby were released in the following years:\n\nRuby 1.0: December 25, 1996[14]\nRuby 1.2: December 1998\nRuby 1.4: August 1999\nRuby 1.6: September 2000\nIn 1997, the first article about Ruby was published on the Web. In the same year, Matsumoto was hired by netlab.jp to work on Ruby as a full-time developer.[14]\n\nIn 1998, the Ruby Application Archive was launched by Matsumoto, along with a simple English-language homepage for Ruby.[14]\n\nIn 1999, the first English language mailing list ruby-talk began, which signaled a growing interest in the language outside Japan.[21] In this same year, Matsumoto and Keiju Ishitsuka wrote the first book on Ruby, The Object-oriented Scripting Language Ruby (オブジェクト指向スクリプト言語 Ruby), which was published in Japan in October 1999. It would be followed in the early 2000s by around 20 books on Ruby published in Japanese.[14]\n\nBy 2000, Ruby was more popular than Python in Japan.[22] In September 2000, the first English language book Programming Ruby was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language ruby-talk mailing list was receiving more messages than the Japanese-language ruby-list, demonstrating Ruby's increasing popularity in the English-speaking world.\n\nRuby 1.8[edit]\nRuby 1.8 was initially released in August 2003, was stable for a long time, and was retired June 2013.[23] Although deprecated, there is still code based on it. Ruby 1.8 is only partially compatible with Ruby 1.9.\n\nRuby 1.8 has been the subject of several industry standards. The language specifications for Ruby were developed by the Open Standards Promotion Center of the Information-Technology Promotion Agency (a Japanese government agency) for submission to the Japanese Industrial Standards Committee (JISC) and then to the International Organization for Standardization (ISO). It was accepted as a Japanese Industrial Standard (JIS X 3017) in 2011[24] and an international standard (ISO/IEC 30170) in 2012.[25]\n\nAround 2005, interest in the Ruby language surged in tandem with Ruby on Rails, a web framework written in Ruby. Rails is frequently credited with increasing awareness of Ruby.[26]\n\nRuby 1.9[edit]\nRuby 1.9 was released in December 2007. Effective with Ruby 1.9.3, released October 31, 2011,[27] Ruby switched from being dual-licensed under the Ruby License and the GPL to being dual-licensed under the Ruby License and the two-clause BSD license.[28] Adoption of 1.9 was slowed by changes from 1.8 that required many popular third party gems to be rewritten.\n\nRuby 1.9 introduces many significant changes over the 1.8 series.[29] Examples:\n\nblock local variables (variables that are local to the block in which they are declared)\nan additional lambda syntax: f = ->(a,b) { puts a + b }\nper-string character encodings are supported\nnew socket API (IPv6 support)\nrequire_relative import security\nRuby 1.9 has been obsolete since February 23, 2015,[30] and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.\n\nRuby 2.0[edit]\nRuby 2.0 added several new features, including:\n\nmethod keyword arguments,\na new method, Module#prepend, for extending a class,\na new literal for creating an array of symbols,\nnew API for the lazy evaluation of Enumerables, and\na new convention of using #to_h to convert objects to Hashes.[31]\nRuby 2.0 is intended to be fully backward compatible with Ruby 1.9.3. As of the official 2.0.0 release on February 24, 2013, there were only five known (minor) incompatibilities.[32]\n\nIt has been obsolete since February 22, 2016 [1] and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.\n\nRuby 2.1[edit]\nRuby 2.1.0 was released on Christmas Day in 2013.[33] The release includes speed-ups, bugfixes, and library updates.\n\nStarting with 2.1.0, Ruby's versioning policy is more like semantic versioning.[34] Although similar, Ruby's versioning policy is not compatible with semantic versioning:\n\nRuby\tSemantic versioning\nMAJOR: Increased when incompatible change which can’t be released in MINOR. Reserved for special events.\tMAJOR: Increased when you make incompatible API changes.\nMINOR: increased every Christmas, may be API incompatible.\tMINOR: increased when you add functionality in a backwards-compatible manner.\nTEENY: security or bug fix which maintains API compatibility. May be increased more than 10 (such as 2.1.11), and will be released every 2–3 months.\tPATCH: increased when you make backwards-compatible bug fixes.\nPATCH: number of commits since last MINOR release (will be reset at 0 when releasing MINOR).\t-\nSemantic versioning also provides additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format, not available at Ruby.\n\nRuby 2.2[edit]\nRuby 2.2.0 was released on Christmas Day in 2014.[35] The release includes speed-ups, bugfixes, and library updates and removes some deprecated APIs. Most notably, Ruby 2.2.0 introduces changes to memory handling – an incremental garbage collector, support for garbage collection of symbols and the option to compile directly against jemalloc. It also contains experimental support for using vfork(2) with system() and spawn(), and added support for the Unicode 7.0 specification.\n\nFeatures that were made obsolete or removed include callcc, the DL library, Digest::HMAC, lib/rational.rb, lib/complex.rb, GServer, Logger::Application as well as various C API functions.[36]\n\nPowerPC64 performance\nSince version 2.2.1,[37] Ruby MRI performance on PowerPC64 was improved.[38][39][40]\nRuby 2.3[edit]\nRuby 2.3.0 was released on Christmas Day in 2015. A few notable changes include:\n\nThe ability to mark all strings literals as frozen by default with consequently large performance increase in string operations.[41]\nHash comparison to allow direct checking of key/value pairs instead of just keys.\nA new safe navigation operator &. that can ease nil handling (e.g. instead of if obj && obj.foo && obj.foo.bar, we can use if obj&.foo&.bar).\nThe did_you_mean gem is now bundled by default and required on startup to automatically suggest similar name matches on a NameError or NoMethodError.\nHash#dig and Array#dig to easily extract deeply nested values (e.g. given profile = { social: { wikipedia: { name: 'Foo Baz' } } }, the value Foo Baz can now be retrieved by profile.dig(:social, :wikipedia, :name)).\n.grep_v(regexp) which will match all negative examples of a given regular expression in addition to other new features.\nThe 2.3 branch also includes many performance improvements, updates, and bugfixes including changes to Proc#call, Socket and IO use of exception keywords, Thread#name handling, default passive Net::FTP connections, and Rake being removed from stdlib.[42]\n\nTable of versions[edit]\nVersion\tLatest teeny version\tInitial release date\tEnd of support phase\tEnd of security maintenance phase\n1.8\t1.8.7-p375[43]\t2003-08-04[44]\t2012-06[45]\t2014-07-01[46]\n1.9\t1.9.3-p551[47]\t2007-12-25[48]\t2014-02-23[49]\t2015-02-23[50]\n2.0\t2.0.0-p648[51]\t2013-02-24[52]\t2015-02-24[51]\t2016-02-24[51]\n2.1\t2.1.10[53]\t2013-12-25[54]\t2016-03-30[55][56]\t2017-03-30[57]\n2.2\t2.2.5[58]\t2014-12-25[59]\tTBA\tTBA\n2.3\t2.3.1[60]\t2015-12-25[61]\tTBA\tTBA\n2.4\t\t2016-12-25\t\t\n3.0\t\tTBA[62]\t\t\nLegend:Old versionOlder version, still supportedLatest versionFuture release\nPhilosophy[edit]\n\nYukihiro Matsumoto, the creator of Ruby\nMatsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[63] At a Google Tech Talk in 2008 Matsumoto further stated, \"I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language.\"[64] He stresses that systems design needs to emphasize human, rather than computer, needs:[65]\n\nOften people, especially computer engineers, focus on the machines. They think, \"By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something.\" They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.\n\nRuby is said to follow the principle of least astonishment (POLA), meaning that the language should behave in such a way as to minimize confusion for experienced users. Matsumoto has said his primary design goal was to make a language that he himself enjoyed using, by minimizing programmer work and possible confusion. He has said that he had not applied the principle of least astonishment to the design of Ruby,[65] but nevertheless the phrase has come to be closely associated with the Ruby programming language. The phrase has itself been a source of surprise, as novice users may take it to mean that Ruby's behaviors try to closely match behaviors familiar from other languages. In a May 2005 discussion on the newsgroup comp.lang.ruby, Matsumoto attempted to distance Ruby from POLA, explaining that because any design choice will be surprising to someone, he uses a personal standard in evaluating surprise. If that personal standard remains consistent, there would be few surprises for those familiar with the standard.[66]\n\nMatsumoto defined it this way in an interview:[65]\n\nEveryone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least my surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.\n\nFeatures[edit]\nThoroughly object-oriented with inheritance, mixins and metaclasses[67]\nDynamic typing and duck typing\nEverything is an expression (even statements) and everything is executed imperatively (even declarations)\nSuccinct and flexible syntax[68] that minimizes syntactic noise and serves as a foundation for domain-specific languages[69]\nDynamic reflection and alteration of objects to facilitate metaprogramming[70]\nLexical closures, iterators and generators, with a unique block syntax[71]\nLiteral notation for arrays, hashes, regular expressions and symbols\nEmbedding code in strings (interpolation)\nDefault arguments\nFour levels of variable scope (global, class, instance, and local) denoted by sigils or the lack thereof\nGarbage collection\nFirst-class continuations\nStrict boolean coercion rules (everything is true except false and nil)\nException handling\nOperator overloading\nBuilt-in support for rational numbers, complex numbers and arbitrary-precision arithmetic\nCustom dispatch behavior (through method_missing and const_missing)\nNative threads and cooperative fibers (fibers are a 1.9/YARV feature)\nInitial support for Unicode and multiple character encodings (no ICU support)[72]\nNative plug-in API in C\nInteractive Ruby Shell (a REPL)\nCentralized package management through RubyGems\nImplemented on all major platforms\nLarge standard library, including modules for YAML, JSON, XML, CGI, OpenSSL, HTTP, FTP, RSS, curses, zlib, and Tk[73]\nSemantics[edit]\nRuby is object-oriented: every value is an object, including classes and instances of types that many other languages designate as primitives (such as integers, booleans, and \"null\"). Variables always hold references to objects. Every function is a method and methods are always called on an object. Methods defined at the top level scope become methods of the Object class. Since this class is an ancestor of every other class, such methods can be called on any object. They are also visible in all scopes, effectively serving as \"global\" procedures. Ruby supports inheritance with dynamic dispatch, mixins and singleton methods (belonging to, and defined for, a single instance rather than being defined on the class). Though Ruby does not support multiple inheritance, classes can import modules as mixins.\n\nRuby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflection and metaprogramming, as well as support for interpreter-based[74] threads. Ruby features dynamic typing, and supports parametric polymorphism.\n\nAccording to the Ruby FAQ, the syntax is similar to Perl and the semantics are similar to Smalltalk but it differs greatly from Python.[75]\n\nSyntax[edit]\nThe syntax of Ruby is broadly similar to that of Perl and Python. Class and method definitions are signaled by keywords, whereas code blocks can be both defined by keywords or braces. In contrast to Perl, variables are not obligatorily prefixed with a sigil. When used, the sigil changes the semantics of scope of the variable. For practical purposes there is no distinction between expressions and statements.[76] Line breaks are significant and taken as the end of a statement; a semicolon may be equivalently used. Unlike Python, indentation is not significant.\n\nOne of the differences of Ruby compared to Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (attr_writer, attr_reader, etc.). Unlike the \"getter\" and \"setter\" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of calling code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.\n\nPython's property descriptors are similar, but come with a tradeoff in the development process. If one begins in Python by using a publicly exposed instance variable, and later changes the implementation to use a private instance variable exposed through a property descriptor, code internal to the class may need to be adjusted to use the private variable rather than the public property. Ruby’s design forces all instance variables to be private, but also provides a simple way to declare set and get methods. This is in keeping with the idea that in Ruby, one never directly accesses the internal members of a class from outside the class; rather, one passes a message to the class and receives a response.\n\nSee the Examples section below for samples of code demonstrating Ruby syntax.\n\nDifferences from other languages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2016) (Learn how and when to remove this template message)\nSome features that differ notably from languages such as C or Perl:\n\nThe language syntax is sensitive to the capitalization of identifiers, in all cases treating capitalized variables as constants. Class and module names are constants and refer to objects derived from Class and Module.\nThe sigils $ and @ do not indicate variable data type as in Perl, but rather function as scope resolution operators.\nFloating point literals must have digits on both sides of the decimal point: neither .5 nor 2. are valid floating point literals, but 0.5 and 2.0 are.\n(In Ruby, integer literals are objects that can have methods apply to them, so requiring a digit after a decimal point helps to clarify whether 1.e5 should be parsed analogously to 1.to_f or as the exponential-format floating literal 1.0e5. The reason for requiring a digit before the decimal point is less clear; it might relate either to method invocation again, or perhaps to the .. and ... operators, for example in the fragment 0.1...3.)\nBoolean non-boolean datatypes are permitted in boolean contexts (unlike in e.g. Smalltalk and Java), but their mapping to boolean values differs markedly from some other languages: 0 and \"empty\" (e.g. empty list, string or associative array) all evaluate to true, thus changing the meaning of some common idioms in related or similar languages such as Lisp, Perl and Python.\nA consequence of this rule is that Ruby methods by convention — for example, regular-expression searches — return numbers, strings, lists, or other non-false values on success, but nil on failure.\nVersions prior to 1.9 use plain integers to represent single characters, much like C. This may cause surprises when slicing strings: \"abc\"[0] yields 97 (the ASCII code of the first character in the string); to obtain \"a\" use \"abc\"[0,1] (a substring of length 1) or \"abc\"[0].chr.\nThe notation statement until expression does not run the statement if the expression is already true. (The behavior is like Perl, but unlike other languages' equivalent statements, e.g. do { statement } while (!(expression)); in C/C++/...). This is because statement until expression is actually syntactic sugar over until expression; statement; end, the equivalent of which in C/C++ is while (!(expression)) { statement; }, just as statement if expression is equivalent to if (expression) { statement; }. However, the notation begin statement end until expression in Ruby will in fact run the statement once even if the expression is already true, acting similarly to the do-while of other languages. (Matsumoto has expressed a desire to remove the special behavior of begin statement end until expression,[77] but it still exists as of Ruby 2.0.)\nBecause constants are references to objects, changing what a constant refers to generates a warning, but modifying the object itself does not. For example, Greeting << \" world!\" if Greeting == \"Hello\" does not generate an error or warning. This is similar to final variables in Java or a const pointer to a non-const object in C++.\nRuby provides the functionality to freeze an object.\nThe usual conjunctive and disjunctive operators for conditional expressions have the same precedence, so and does not bind tighter than or in Ruby, a behaviour similar to languages such as APL, Ada, VHDL, Mathematica, zkl and others. However, Ruby also has C-like operators || and && that work as in C-like languages.\nA list of so-called gotchas may be found in Hal Fulton's book The Ruby Way, 2nd ed (ISBN 0-672-32884-4), Section 1.5. A similar list in the 1st edition pertained to an older version of Ruby (version 1.6), some problems of which have been fixed in the meantime. For example, retry now works with while, until, and for, as well as with iterators.\n\nInteraction[edit]\nSee also: Interactive Ruby Shell\nThe Ruby official distribution also includes irb, an interactive command-line interpreter that can be used to test code quickly. The following code fragment represents a sample session using irb:\n\n$ irb\nirb(main):001:0> puts 'Hello, World'\nHello, World\n => nil\nirb(main):002:0> 1+2\n => 3\nExamples[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2014) (Learn how and when to remove this template message)\nThe following examples can be run in a Ruby shell such as Interactive Ruby Shell, or saved in a file and run from the command line by typing ruby <filename>.\n\nClassic Hello world example:\n\nputs 'Hello World!'\nSome basic Ruby code:\n\n# Everything, including a literal, is an object, so this works:\n-199.abs                                                 # => 199\n'ice is nice'.length                                     # => 11\n'ruby is cool.'.index('u')                               # => 1\n\"Nice Day Isn't It?\".downcase.split('').uniq.sort.join   # => \" '?acdeinsty\"\nInput:\n\nprint 'Please type name >'\nname = gets.chomp\nputs \"Hello #{name}.\"\nConversions:\n\nputs 'Give me a number'\nnumber = gets.chomp\nputs number.to_i\noutput_number = number.to_i + 1\nputs output_number.to_s + ' is a bigger number.'\nStrings[edit]\nThere are a variety of ways to define strings in Ruby.\n\nThe following assignments are equivalent:\n\na = \"\\nThis is a double-quoted string\\n\"\na = %Q{\\nThis is a double-quoted string\\n}\na = %{\\nThis is a double-quoted string\\n}\na = %/\\nThis is a double-quoted string\\n/\na = <<-BLOCK\n\nThis is a double-quoted string\nBLOCK\nStrings support variable interpolation:\n\nvar = 3.14159\n\"pi is #{var}\"\n=> \"pi is 3.14159\"\nThe following assignments are equivalent and produce raw strings:\n\na = 'This is a single-quoted string'\na = %q{This is a single-quoted string}\nCollections[edit]\nConstructing and using an array:\n\na = [1, 'hi', 3.14, 1, 2, [4, 5]]\n\na[2]             # => 3.14\na.[](2)          # => 3.14\na.reverse        # => [[4, 5], 2, 1, 3.14, 'hi', 1]\na.flatten.uniq   # => [1, 'hi', 3.14, 2, 4, 5]\nConstructing and using an associative array (in Ruby, called a hash):\n\nhash = Hash.new # equivalent to hash = {}\nhash = { :water => 'wet', :fire => 'hot' } # makes the previous line redundant as we are now\n                                           # assigning hash to a new, separate hash object\nputs hash[:fire] # prints \"hot\"\n\nhash.each_pair do |key, value|   # or: hash.each do |key, value|\n  puts \"#{key} is #{value}\"\nend\n# returns {:water=>\"wet\", :fire=>\"hot\"} and prints:\n# water is wet\n# fire is hot\n\nhash.delete :water                            # deletes the pair :water => 'wet' and returns \"wet\"\nhash.delete_if {|key,value| value == 'hot'}   # deletes the pair :fire => 'hot' and returns {}\nControl structures[edit]\nIf statement:\n\n# Generate a random number and print whether it's even or odd.\nif rand(100) % 2 == 0\n  puts \"It's even\"\nelse\n  puts \"It's odd\"\nend\nBlocks and iterators[edit]\nThe two syntaxes for creating a code block:\n\n{ puts 'Hello, World!' } # note the braces\n# or:\ndo\n  puts 'Hello, World!'\nend\nA code block can be passed to a method as an optional block argument. Many built-in methods have such arguments:\n\nFile.open('file.txt', 'w') do |file| # 'w' denotes \"write mode\"\n  file.puts 'Wrote some text.'\nend                                  # file is automatically closed here\n\nFile.readlines('file.txt').each do |line|\n  puts line\nend\n# => Wrote some text.\nParameter-passing a block to be a closure:\n\n# In an object instance variable (denoted with '@'), remember a block.\ndef remember(&a_block)\n  @block = a_block\nend\n\n# Invoke the preceding method, giving it a block that takes a name.\nremember {|name| puts \"Hello, #{name}!\"}\n\n# Call the closure (note that this happens not to close over any free variables):\n@block.call('Jon')   # => \"Hello, Jon!\"\nCreating an anonymous function:\n\nproc {|arg| puts arg}\nProc.new {|arg| puts arg}\nlambda {|arg| puts arg}\n->(arg) {puts arg}         # introduced in Ruby 1.9\nReturning closures from a method:\n\ndef create_set_and_get(initial_value=0) # note the default value of 0\n  closure_value = initial_value\n  [ Proc.new {|x| closure_value = x}, Proc.new { closure_value } ]\nend\n\nsetter, getter = create_set_and_get  # returns two values\nsetter.call(21)\ngetter.call      # => 21\n\n# Parameter variables can also be used as a binding for the closure,\n# so the preceding can be rewritten as:\n\ndef create_set_and_get(closure_value=0)\n  [ proc {|x| closure_value = x } , proc { closure_value } ]\nend\nYielding the flow of program control to a block that was provided at calling time:\n\ndef use_hello\n  yield \"hello\"\nend\n\n# Invoke the preceding method, passing it a block.\nuse_hello {|string| puts string}  # => 'hello'\nIterating over enumerations and arrays using blocks:\n\narray = [1, 'hi', 3.14]\narray.each {|item| puts item }\n# prints:\n# 1\n# 'hi'\n# 3.14\n\narray.each_index {|index| puts \"#{index}: #{array[index]}\" }\n# prints:\n# 0: 1\n# 1: 'hi'\n# 2: 3.14\n\n# The following uses a (a..b) Range\n(3..6).each {|num| puts num }\n# prints:\n# 3\n# 4\n# 5\n# 6\n\n# The following uses a (a...b) Range\n(3...6).each {|num| puts num }\n# prints:\n# 3\n# 4\n# 5\nA method such as inject can accept both a parameter and a block. The inject method iterates over each member of a list, performing some function on it while retaining an aggregate. This is analogous to the foldl function in functional programming languages. For example:\n\n[1,3,5].inject(10) {|sum, element| sum + element}   # => 19\nOn the first pass, the block receives 10 (the argument to inject) as sum, and 1 (the first element of the array) as element. This returns 11, which then becomes sum on the next pass. It is added to 3 to get 14, which is then added to 5 on the third pass, to finally return 19.\n\nUsing an enumeration and a block to square the numbers 1 to 10 (using a range):\n\n(1..10).collect {|x| x*x}  # => [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nOr invoke a method on each item (map is a synonym for collect):\n\n(1..5).map(&:to_f)  # => [1.0, 2.0, 3.0, 4.0, 5.0]\nClasses[edit]\nThe following code defines a class named Person. In addition to initialize, the usual constructor to create new objects, it has two methods: one to override the <=> comparison operator (so Array#sort can sort by age) and the other to override the to_s method (so Kernel#puts can format its output). Here, attr_reader is an example of metaprogramming in Ruby: attr_accessor defines getter and setter methods of instance variables, but attr_reader only getter methods. The last evaluated statement in a method is its return value, allowing the omission of an explicit return statement.\n\nclass Person\n  attr_reader :name, :age\n  def initialize(name, age)\n    @name, @age = name, age\n  end\n  def <=>(person) # the comparison operator for sorting\n    @age <=> person.age\n  end\n  def to_s\n    \"#{@name} (#{@age})\"\n  end\nend\n\ngroup = [\n  Person.new(\"Bob\", 33),\n  Person.new(\"Chris\", 16),\n  Person.new(\"Ash\", 23)\n]\n\nputs group.sort.reverse\nThe preceding code prints three names in reverse age order:\n\nBob (33)\nAsh (23)\nChris (16)\nPerson is a constant and is a reference to a Class object.\n\nOpen classes[edit]\nIn Ruby, classes are never closed: methods can always be added to an existing class. This applies to all classes, including the standard, built-in classes. All that is needed to do is open up a class definition for an existing class, and the new contents specified will be added to the existing contents. A simple example of adding a new method to the standard library's Time class:\n\n# re-open Ruby's Time class\nclass Time\n  def yesterday\n    self - 86400\n  end\nend\n\ntoday = Time.now               # => 2013-09-03 16:09:37 +0300\nyesterday = today.yesterday    # => 2013-09-02 16:09:37 +0300\nAdding methods to previously defined classes is often called monkey-patching. If performed recklessly, the practice can lead to both behavior collisions with subsequent unexpected results and code scalability problems.\n\nExceptions[edit]\nAn exception is raised with a raise call:\n\nraise\nAn optional message can be added to the exception:\n\nraise \"This is a message\"\nExceptions can also be specified by the programmer:\n\nraise ArgumentError, \"Illegal arguments!\"\nAlternatively, an exception instance can be passed to the raise method:\n\nraise ArgumentError.new(\"Illegal arguments!\")\nThis last construct is useful when raising an instance of a custom exception class featuring a constructor that takes more than one argument:\n\nclass ParseError < Exception\n  def initialize input, line, pos\n    super \"Could not parse '#{input}' at line #{line}, position #{pos}\"\n  end\nend\n\nraise ParseError.new(\"Foo\", 3, 9)\nExceptions are handled by the rescue clause. Such a clause can catch exceptions that inherit from StandardError. Other flow control keywords that can be used when handling exceptions are else and ensure:\n\nbegin\n  # do something\nrescue\n  # handle exception\nelse\n  # do this if no exception was raised\nensure\n  # do this whether or not an exception was raised\nend\nIt is a common mistake to attempt to catch all exceptions with a simple rescue clause. To catch all exceptions one must write:\n\nbegin\n  # do something\nrescue Exception\n  # Exception handling code here.\n  # Don't write only \"rescue\"; that only catches StandardError, a subclass of Exception.\nend\nOr catch particular exceptions:\n\nbegin\n  # do something\nrescue RuntimeError\n  # handle only RuntimeError and its subclasses\nend\nIt is also possible to specify that the exception object be made available to the handler clause:\n\nbegin\n  # do something\nrescue RuntimeError => e\n  # handling, possibly involving e, such as \"puts e.to_s\"\nend\nAlternatively, the most recent exception is stored in the magic global $!.\n\nSeveral exceptions can also be caught:\n\nbegin\n  # do something\nrescue RuntimeError, Timeout::Error => e\n  # handling, possibly involving e\nend\nMetaprogramming[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2014) (Learn how and when to remove this template message)\nRuby code can programmatically modify, at runtime, aspects of its own structure that would be fixed in more rigid languages, such as class and method definitions. This sort of metaprogramming can be used to write more concise code and effectively extend the language.\n\nFor example, the following Ruby code generates new methods for the built-in String class, based on a list of colors. The methods wrap the contents of the string with an HTML tag styled with the respective color.\n\nCOLORS = { black:   \"000\",\n           red:     \"f00\",\n           green:   \"0f0\",\n           yellow:  \"ff0\",\n           blue:    \"00f\",\n           magenta: \"f0f\",\n           cyan:    \"0ff\",\n           white:   \"fff\" }\n\nclass String\n  COLORS.each do |color,code|\n    define_method \"in_#{color}\" do\n      \"<span style=\\\"color: ##{code}\\\">#{self}</span>\"\n    end\n  end\nend\nThe generated methods could then be used like this:\n\n\"Hello, World!\".in_blue\n => \"<span style=\\\"color: #00f\\\">Hello, World!</span>\"\nTo implement the equivalent in many other languages, the programmer would have to write each method (in_black, in_red, in_green, etc.) separately.\n\nSome other possible uses for Ruby metaprogramming include:\n\nintercepting and modifying method calls\nimplementing new inheritance models\ndynamically generating classes from parameters\nautomatic object serialization\ninteractive help and debugging\nMore examples[edit]\nMore sample Ruby code is available as algorithms in the following article:\n\nExponentiating by squaring\nImplementations[edit]\nSee also: Ruby MRI § Operating systems\nMatz's Ruby Interpreter[edit]\nThe official Ruby interpreter often referred to as the Matz's Ruby Interpreter or MRI. This implementation is written in C and uses its own Ruby-specific virtual machine.\n\nThe standardized and retired Ruby 1.8 implementation was written in C, as a single-pass interpreted language.[23]\n\nStarting with Ruby 1.9, and continuing with Ruby 2.x and above, the official Ruby interpreter has been YARV (\"Yet Another Ruby VM\"), and this implementation has superseded the slower virtual machine used in previous releases of MRI.\n\nAlternate implementations[edit]\nAs of 2010, there are a number of alternative implementations of Ruby, including JRuby, Rubinius, MagLev, IronRuby, MacRuby (and its iOS counterpart, RubyMotion), mruby, HotRuby, Topaz and Opal. Each takes a different approach, with IronRuby, JRuby, MacRuby and Rubinius providing just-in-time compilation and MacRuby and mruby also providing ahead-of-time compilation.\n\nRuby has two major alternate implementations:\n\nJRuby, a Java implementation that runs on the Java virtual machine. JRuby currently targets Ruby 2.2,\nRubinius, a C++ bytecode virtual machine that uses LLVM to compile to machine code at runtime. The bytecode compiler and most core classes are written in pure Ruby. Rubinius currently targets Ruby 2.1,\nOther Ruby implementations include:\n\nMagLev, a Smalltalk implementation that runs on GemTalk Systems' GemStone/S VM\nmruby, an implementation designed to be embedded into C code, in a similar vein to Lua. It is currently being developed by Yukihiro Matsumoto and others\nOpal, a web-based interpreter that compiles Ruby to JavaScript\nRGSS, or Ruby Game Scripting System, a proprietary implementation that is used by the RPG Maker series of software for game design and modification of the RPG Maker engine\nA transpiler (partial) from Ruby to Julia, julializer is available. It can be used for a large speedup over e.g. Ruby or JRuby implementations (may only be useful for numerical code).[78]\nOther now defunct Ruby implementations were:\n\nMacRuby, an OS X implementation on the Objective-C runtime\nIronRuby an implementation on the .NET Framework\nCardinal, an implementation for the Parrot virtual machine\nRuby Enterprise Edition, often shortened to ree, an implementation optimized to handle large-scale Ruby on Rails projects\nThe maturity of Ruby implementations tends to be measured by their ability to run the Ruby on Rails (Rails) framework, because it is complex to implement and uses many Ruby-specific features. The point when a particular implementation achieves this goal is called \"the Rails singularity\". The reference implementation (MRI), JRuby, and Rubinius[79] are all able to run Rails unmodified in a production environment.\n\nPlatform support[edit]\nMatsumoto originally did Ruby development on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.[80][81]\n\nBy 1999, Ruby was known to work across many different operating systems, including NEWS-OS, SunOS, AIX, SVR4, Solaris, NEC UP-UX, NeXTSTEP, BSD, Linux, Mac OS, DOS, Windows, and BeOS.[82]\n\nModern Ruby versions and implementations are available on many operating systems, such as Linux, BSD, Solaris, AIX, OS X, Windows, Windows Phone,[83] Windows CE, Symbian OS, BeOS, and IBM i.\n\nRepositories and libraries[edit]\nRubyGems is Ruby's package manager. A Ruby package is called a \"gem\" and can easily be installed via the command line. Most gems are libraries, though a few exist that are applications, such as IDEs.[84] There are over 124,000 Ruby gems hosted on RubyGems.org.\n\nMany new and existing Ruby libraries are hosted on GitHub, a service that offers version control repository hosting for Git.\n\nThe Ruby Application Archive, which hosted applications, documentation, and libraries for Ruby programming, was maintained until 2013, when its function was transferred to RubyGems.[85]\n\nSee also[edit]\n\tFree software portal\nicon\tComputer programming portal\nComparison of programming languages\nWhy's (poignant) Guide to Ruby — an online ruby textbook in graphic novel format\nMetasploit Project — the world's largest Ruby project, with over 700,000 lines of code\nXRuby",
          "type": "intepreted",
          "plid": 29
        },
        {
          "name": "Scala",
          "details": "Scala (/ˈskɑːlɑː/ skah-lah)[9] is a general-purpose programming language. Scala has full support for functional programming and a strong static type system. Designed to be concise,[10] many of Scala's design decisions were inspired by criticism of Java's shortcomings.[8]\n\nScala source code is intended to be compiled to Java bytecode, so that the resulting executable code runs on a Java virtual machine. Java libraries may be used directly in Scala code and vice versa (language interoperability).[11] Like Java, Scala is object-oriented, and uses a curly-brace syntax reminiscent of the C programming language. Unlike Java, Scala has many features of functional programming languages like Scheme, Standard ML and Haskell, including currying, type inference, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), and anonymous types. Other features of Scala not present in Java include operator overloading, optional parameters, named parameters, raw strings, and no checked exceptions.\n\nThe name Scala is a portmanteau of scalable and language, signifying that it is designed to grow with the demands of its users.[12]\n\nContents  [hide] \n1\tHistory\n2\tPlatforms and license\n3\tExamples\n3.1\t\"Hello World\" example\n3.2\tBasic example\n3.3\tExample with classes\n4\tFeatures (with reference to Java)\n4.1\tSyntactic flexibility\n4.2\tUnified type system\n4.3\tFor-expressions\n4.4\tFunctional tendencies\n4.4.1\tEverything is an expression\n4.4.2\tType inference\n4.4.3\tAnonymous functions\n4.4.4\tImmutability\n4.4.5\tLazy (non-strict) evaluation\n4.4.6\tTail recursion\n4.4.7\tCase classes and pattern matching\n4.4.8\tPartial functions\n4.5\tObject-oriented extensions\n4.6\tExpressive type system\n4.7\tType enrichment\n5\tConcurrency\n6\tCluster computing\n7\tTesting\n8\tVersions\n9\tComparison with other JVM languages\n10\tAdoption\n10.1\tLanguage rankings\n10.2\tCompanies\n11\tCriticism\n12\tSee also\n13\tReferences\n14\tFurther reading\n15\tExternal links\nHistory[edit]\nThe design of Scala started in 2001 at the École Polytechnique Fédérale de Lausanne (EPFL) by Martin Odersky. It followed on from work on Funnel, a programming language combining ideas from functional programming and Petri nets.[13] Odersky formerly worked on Generic Java, and javac, Sun's Java compiler.[13]\n\nAfter an internal release in late 2003, Scala was released publicly in early 2004 on the Java platform,[14] and on the .NET Framework in June 2004.[8][13][15] A second version (v2.0) followed in March 2006.[8] The .NET support was officially dropped in 2012.[16]\n\nAlthough Scala had extensive support for functional programming from its inception, Java remained a mostly object oriented language until the inclusion of lambda expressions with Java 8 in 2014.\n\nOn 17 January 2011 the Scala team won a five-year research grant of over €2.3 million from the European Research Council.[17] On 12 May 2011, Odersky and collaborators launched Typesafe Inc. (renamed Lightbend Inc., February 2016; 9 months ago), a company to provide commercial support, training, and services for Scala. Typesafe received a $3 million investment in 2011 from Greylock Partners.[18][19][20][21]\n\nPlatforms and license[edit]\nScala runs on the Java platform (Java virtual machine) and is compatible with existing Java programs.[14] As Android applications are typically written in Java and translated from Java bytecode into Dalvik bytecode (which may be further translated to native machine code during installation) when packaged, Scala's Java compatibility makes it well suited to Android development, more so when a functional approach is preferred.[22] Scala also can compile to JavaScript, making it possible to write Scala programs that can run in web browsers.[23]\n\nThe Scala software distribution, including compiler and libraries, is released under a BSD license.[24]\n\nExamples[edit]\n\"Hello World\" example[edit]\nThe Hello World program written in Scala has this form:\n\n object HelloWorld extends App {\n   println(\"Hello, World!\")\n }\nUnlike the stand-alone Hello World application for Java, there is no class declaration and nothing is declared to be static; a singleton object created with the object keyword is used instead.\n\nWith the program saved in a file named HelloWorld.scala, it can be compiled from the command line:\n\n$ scalac HelloWorld.scala\nTo run it:\n\n$ scala HelloWorld\n(You may need to use the \"-cp\" option to set the classpath as in Java).\n\nThis is analogous to the process for compiling and running Java code. Indeed, Scala's compiling and executing model is identical to that of Java, making it compatible with Java build tools such as Apache Ant.\n\nA shorter version of the \"Hello World\" Scala program is:\n\nprintln(\"Hello, World!\")\nScala includes interactive shell and scripting support.[25] Saved in a file named HelloWorld2.scala, this can be run as a script with no prior compiling using:\n\n$ scala HelloWorld2.scala\nCommands can also be entered directly into the Scala interpreter, using the option -e:\n\n$ scala -e 'println(\"Hello, World!\")'\nFinally, commands can be entered interactively in the REPL:\n\n$ scala\nWelcome to Scala version 2.10.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_51).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> println(\"Hello, World!\")\nHello, World!\n\nscala>\nBasic example[edit]\nThe following example shows the differences between Java and Scala syntax:\n\n// Java:\nint mathFunction(int num) {\n    int numSquare = num*num;\n    return (int) (Math.cbrt(numSquare) +\n      Math.log(numSquare));\n}\n// Scala: Direct conversion from Java\n\n// no import needed; scala.math\n// already imported as `math`\ndef mathFunction(num: Int): Int = {\n  var numSquare: Int = num*num\n  return (math.cbrt(numSquare) + math.log(numSquare)).\n    asInstanceOf[Int]\n}\n// Scala: More idiomatic\n// Uses type inference, omits `return` statement,\n// uses `toInt` method, declares numSquare immutable\n\nimport math._\ndef intRoot23(num: Int) = {\n  val numSquare = num*num\n  (cbrt(numSquare) + log(numSquare)).toInt\n}\nSome syntactic differences in this code are:\n\nScala does not require semicolons to end statements.\nValue types are capitalized: Int, Double, Boolean instead of int, double, boolean.\nParameter and return types follow, as in Pascal, rather than precede as in C.\nMethods must be preceded by def.\nLocal or class variables must be preceded by val (indicates an immutable variable) or var (indicates a mutable variable).\nThe return operator is unnecessary in a function (although allowed); the value of the last executed statement or expression is normally the function's value.\nInstead of the Java cast operator (Type) foo, Scala uses foo.asInstanceOf[Type], or a specialized function such as toDouble or toInt.\nInstead of Java's import foo.*;, Scala uses import foo._.\nFunction or method foo() can also be called as just foo; method thread.send(signo) can also be called as just thread send signo; and method foo.toString() can also be called as just foo toString.\nThese syntactic relaxations are designed to allow support for domain-specific languages.\n\nSome other basic syntactic differences:\n\nArray references are written like function calls, e.g. array(i) rather than array[i]. (Internally in Scala, both arrays and functions are conceptualized as kinds of mathematical mappings from one object to another.)\nGeneric types are written as e.g. List[String] rather than Java's List<String>.\nInstead of the pseudo-type void, Scala has the actual singleton class Unit (see below).\nExample with classes[edit]\nThe following example contrasts the definition of classes in Java and Scala.\n\n// Java:\npublic class Point {\n  private final double x, y;\n\n  public Point(final double x, final double y) {\n    this.x = x;\n    this.y = y;\n  }\n\n  public Point(\n    final double x, final double y,\n    final boolean addToGrid\n  ) {\n    this(x, y);\n  \n    if (addToGrid)\n      grid.add(this);\n  }\n\n  public Point() {\n    this(0.0, 0.0);\n  }\n\n  public double getX() {\n    return x;\n  }\n\n  public double getY() {\n    return y;\n  }\n\n  double distanceToPoint(final Point other) {\n    return distanceBetweenPoints(x, y,\n      other.x, other.y);\n  }\n\n  private static Grid grid = new Grid();\n\n  static double distanceBetweenPoints(\n      final double x1, final double y1,\n      final double x2, final double y2\n  ) {\n    return Math.hypot(x1 - x2, y1 - y2);\n  }\n}\n// Scala\nclass Point(\n    val x: Double, val y: Double,\n    addToGrid: Boolean = false\n) {\n  import Point._\n\n  if (addToGrid)\n    grid.add(this)\n\n  def this() = this(0.0, 0.0)\n\n  def distanceToPoint(other: Point) =\n    distanceBetweenPoints(x, y, other.x, other.y)\n}\n\nobject Point {\n  private val grid = new Grid()\n\n  def distanceBetweenPoints(x1: Double, y1: Double,\n      x2: Double, y2: Double) = {\n    math.hypot(x1 - x2, y1 - y2)\n  }\n}\nThe above code shows some of the conceptual differences between Java and Scala's handling of classes:\n\nScala has no static variables or methods. Instead, it has singleton objects, which are essentially classes with only one object in the class. Singleton objects are declared using object instead of class. It is common to place static variables and methods in a singleton object with the same name as the class name, which is then known as a companion object.[14] (The underlying class for the singleton object has a $ appended. Hence, for class Foo with companion object object Foo, under the hood there's a class Foo$ containing the companion object's code, and one object of this class is created, using the singleton pattern.)\nIn place of constructor parameters, Scala has class parameters, which are placed on the class, similar to parameters to a function. When declared with a val or var modifier, fields are also defined with the same name, and automatically initialized from the class parameters. (Under the hood, external access to public fields always goes through accessor (getter) and mutator (setter) methods, which are automatically created. The accessor function has the same name as the field, which is why it's unnecessary in the above example to explicitly declare accessor methods.) Note that alternative constructors can also be declared, as in Java. Code that would go into the default constructor (other than initializing the member variables) goes directly at class level.\nDefault visibility in Scala is public.\nFeatures (with reference to Java)[edit]\nScala has the same compiling model as Java and C#, namely separate compiling and dynamic class loading, so that Scala code can call Java libraries.\n\nScala's operational characteristics are the same as Java's. The Scala compiler generates byte code that is nearly identical to that generated by the Java compiler.[14] In fact, Scala code can be decompiled to readable Java code, with the exception of certain constructor operations. To the Java virtual machine (JVM), Scala code and Java code are indistinguishable. The only difference is one extra runtime library, scala-library.jar.[26]\n\nScala adds a large number of features compared with Java, and has some fundamental differences in its underlying model of expressions and types, which make the language theoretically cleaner and eliminate several corner cases in Java. From the Scala perspective, this is practically important because several added features in Scala are also available in C#. Examples include:\n\nSyntactic flexibility[edit]\nAs mentioned above, Scala has a good deal of syntactic flexibility, compared with Java. The following are some examples:\n\nSemicolons are unnecessary; lines are automatically joined if they begin or end with a token that cannot normally come in this position, or if there are unclosed parentheses or brackets.\nAny method can be used as an infix operator, e.g. \"%d apples\".format(num) and \"%d apples\" format num are equivalent. In fact, arithmetic operators like + and << are treated just like any other methods, since function names are allowed to consist of sequences of arbitrary symbols (with a few exceptions made for things like parens, brackets and braces that must be handled specially); the only special treatment that such symbol-named methods undergo concerns the handling of precedence.\nMethods apply and update have syntactic short forms. foo()—where foo is a value (singleton object or class instance)—is short for foo.apply(), and foo() = 42 is short for foo.update(42). Similarly, foo(42) is short for foo.apply(42), and foo(4) = 2 is short for foo.update(4, 2). This is used for collection classes and extends to many other cases, such as STM cells.\nScala distinguishes between no-parens (def foo = 42) and empty-parens (def foo() = 42) methods. When calling an empty-parens method, the parentheses may be omitted, which is useful when calling into Java libraries that do not know this distinction, e.g., using foo.toString instead of foo.toString(). By convention, a method should be defined with empty-parens when it performs side effects.\nMethod names ending in colon (:) expect the argument on the left-hand-side and the receiver on the right-hand-side. For example, the 4 :: 2 :: Nil is the same as Nil.::(2).::(4), the first form corresponding visually to the result (a list with first element 4 and second element 2).\nClass body variables can be transparently implemented as separate getter and setter methods. For trait FooLike { var bar: Int }, an implementation may be object Foo extends FooLike { private var x = 0; def bar = x; def bar_=(value: Int) { x = value }} } }. The call site will still be able to use a concise foo.bar = 42.\nThe use of curly braces instead of parentheses is allowed in method calls. This allows pure library implementations of new control structures.[27] For example, breakable { ... if (...) break() ... } looks as if breakable was a language defined keyword, but really is just a method taking a thunk argument. Methods that take thunks or functions often place these in a second parameter list, allowing to mix parentheses and curly braces syntax: Vector.fill(4) { math.random } is the same as Vector.fill(4)(math.random). The curly braces variant allows the expression to span multiple lines.\nFor-expressions (explained further down) can accommodate any type that defines monadic methods such as map, flatMap and filter.\nBy themselves, these may seem like questionable choices, but collectively they serve the purpose of allowing domain-specific languages to be defined in Scala without needing to extend the compiler. For example, Erlang's special syntax for sending a message to an actor, i.e. actor ! message can be (and is) implemented in a Scala library without needing language extensions.\n\nUnified type system[edit]\nJava makes a sharp distinction between primitive types (e.g. int and boolean) and reference types (any class). Only reference types are part of the inheritance scheme, deriving from java.lang.Object. In Scala, however, all types inherit from a top-level class Any, whose immediate children are AnyVal (value types, such as Int and Boolean) and AnyRef (reference types, as in Java). This means that the Java distinction between primitive types and boxed types (e.g. int vs. Integer) is not present in Scala; boxing and unboxing is completely transparent to the user. Scala 2.10 allows for new value types to be defined by the user.\n\nFor-expressions[edit]\nInstead of the Java \"foreach\" loops for looping through an iterator, Scala has a much more powerful concept of for-expressions. These are similar to list comprehensions in languages such as Haskell, or a combination of list comprehensions and generator expressions in Python. For-expressions using the yield keyword allow a new collection to be generated by iterating over an existing one, returning a new collection of the same type. They are translated by the compiler into a series of map, flatMap and filter calls. Where yield is not used, the code approximates to an imperative-style loop, by translating to foreach.\n\nA simple example is:\n\nval s = for (x <- 1 to 25 if x*x > 50) yield 2*x\nThe result of running it is the following vector:\n\nVector(16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50)\n(Note that the expression 1 to 25 is not special syntax. The method to is rather defined in the standard Scala library as an extension method on integers, using a technique known as implicit conversions[28] that allows new methods to be added to existing types.)\n\nA more complex example of iterating over a map is:\n\n// Given a map specifying Twitter users mentioned in a set of tweets,\n// and number of times each user was mentioned, look up the users\n// in a map of known politicians, and return a new map giving only the\n// Democratic politicians (as objects, rather than strings).\nval dem_mentions = for {\n    (mention, times) <- mentions\n    account          <- accounts.get(mention)\n    if account.party == \"Democratic\"\n  } yield (account, times)\nExpression (mention, times) <- mentions is an example of pattern matching (see below). Iterating over a map returns a set of key-value tuples, and pattern-matching allows the tuples to easily be destructured into separate variables for the key and value. Similarly, the result of the comprehension also returns key-value tuples, which are automatically built back up into a map because the source object (from the variable mentions) is a map. Note that if mentions instead held a list, set, array or other collection of tuples, exactly the same code above would yield a new collection of the same type.\n\nFunctional tendencies[edit]\nWhile supporting all of the object-oriented features available in Java (and in fact, augmenting them in various ways), Scala also provides a large number of capabilities that are normally found only in functional programming languages. Together, these features allow Scala programs to be written in an almost completely functional style, and also allow functional and object-oriented styles to be mixed.\n\nExamples are:\n\nNo distinction between statements and expressions\nType inference\nAnonymous functions with capturing semantics (i.e., closures)\nImmutable variables and objects\nLazy evaluation\nDelimited continuations (since 2.8)\nHigher-order functions\nNested functions\nCurrying\nPattern matching\nAlgebraic data types (through case classes)\nTuples\nEverything is an expression[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2013) (Learn how and when to remove this template message)\nUnlike C or Java, but similar to languages such as Lisp, Scala makes no distinction between statements and expressions. All statements are in fact expressions that evaluate to some value. Functions that would be declared as returning void in C or Java, and statements like while that logically do not return a value, are in Scala considered to return the type Unit, which is a singleton type, with only one object of that type. Functions and operators that never return at all (e.g. the throw operator or a function that always exits non-locally using an exception) logically have return type Nothing, a special type containing no objects; that is, a bottom type, i.e. a subclass of every possible type. (This in turn makes type Nothing compatible with every type, allowing type inference to function correctly.)\n\nSimilarly, an if-then-else \"statement\" is actually an expression, which produces a value, i.e. the result of evaluating one of the two branches. This means that such a block of code can be inserted wherever an expression is desired, obviating the need for a ternary operator in Scala:\n\n// Java:\nint hexDigit = x >= 10 ? x + 'A' - 10 : x + '0';\n// Scala:\nval hexDigit = if (x >= 10) x + 'A' - 10 else x + '0'\nFor similar reasons, return statements are unnecessary in Scala, and in fact are discouraged. As in Lisp, the last expression in a block of code is the value of that block of code, and if the block of code is the body of a function, it will be returned by the function.\n\nTo make it clear that all expressions are functions, even methods that return Unit are written with an equals sign\n\ndef printValue(x: String): Unit = {\n  println(\"I ate a %s\".format(x))\n}\nor equivalently (with type inference, and omitting the unnecessary braces):\n\ndef printValue(x: String) = println(\"I ate a %s\" format x)\nType inference[edit]\nDue to type inference, the type of variables, function return values, and many other expressions can typically be omitted, as the compiler can deduce it. Examples are val x = \"foo\" (for an immutable, constant variable or immutable object) or var x = 1.5 (for a variable whose value can later be changed). Type inference in Scala is essentially local, in contrast to the more global Hindley-Milner algorithm used in Haskell, ML and other more purely functional languages. This is done to facilitate object-oriented programming. The result is that certain types still need to be declared (most notably, function parameters, and the return types of recursive functions), e.g.\n\ndef formatApples(x: Int) = \"I ate %d apples\".format(x)\nor (with a return type declared for a recursive function)\n\ndef factorial(x: Int): Int =\n  if (x == 0)\n    1\n  else\n    x*factorial(x - 1)\nAnonymous functions[edit]\nIn Scala, functions are objects, and a convenient syntax exists for specifying anonymous functions. An example is the expression x => x < 2, which specifies a function with one parameter, that compares its argument to see if it is less than 2. It is equivalent to the Lisp form (lambda (x) (< x 2)). Note that neither the type of x nor the return type need be explicitly specified, and can generally be inferred by type inference; but they can be explicitly specified, e.g. as (x: Int) => x < 2 or even (x: Int) => (x < 2): Boolean.\n\nAnonymous functions behave as true closures in that they automatically capture any variables that are lexically available in the environment of the enclosing function. Those variables will be available even after the enclosing function returns, and unlike in the case of Java's anonymous inner classes do not need to be declared as final. (It is even possible to modify such variables if they are mutable, and the modified value will be available the next time the anonymous function is called.)\n\nAn even shorter form of anonymous function uses placeholder variables: For example, the following:\n\nlist map { x => sqrt(x) }\ncan be written more concisely as\n\nlist map { sqrt(_) }\nor even\n\nlist map sqrt\nImmutability[edit]\nScala enforces a distinction between immutable (unmodifiable, read-only) variables, whose value cannot be changed once assigned, and mutable variables, which can be changed. A similar distinction is made between immutable and mutable objects. The distinction must be made when a variable is declared: Immutable variables are declared with val while mutable variables use var. Similarly, all of the collection objects (container types) in Scala, e.g. linked lists, arrays, sets and hash tables, are available in mutable and immutable variants, with the immutable variant considered the more basic and default implementation. The immutable variants are \"persistent\" data types in that they create a new object that encloses the old object and adds the new member(s); this is similar to how linked lists are built up in Lisp, where elements are prepended by creating a new \"cons\" cell with a pointer to the new element (the \"head\") and the old list (the \"tail\"). This allows for very easy concurrency — no locks are needed as no shared objects are ever modified. Immutable structures are also constructed efficiently, in the sense that modified instances reuses most of old instance data and unused/unreferenced parts are collected by GC.[29]\n\nLazy (non-strict) evaluation[edit]\nEvaluation is strict (\"eager\") by default. In other words, Scala evaluates expressions as soon as they are available, rather than as needed. However, you can declare a variable non-strict (\"lazy\") with the lazy keyword, meaning that the code to produce the variable's value will not be evaluated until the first time the variable is referenced. Non-strict collections of various types also exist (such as the type Stream, a non-strict linked list), and any collection can be made non-strict with the view method. Non-strict collections provide a good semantic fit to things like server-produced data, where the evaluation of the code to generate later elements of a list (that in turn triggers a request to a server, possibly located somewhere else on the web) only happens when the elements are actually needed.\n\nTail recursion[edit]\nFunctional programming languages commonly provide tail call optimization to allow for extensive use of recursion without stack overflow problems. Limitations in Java bytecode complicate tail call optimization on the JVM. In general, a function that calls itself with a tail call can be optimized, but mutually recursive functions cannot. Trampolines have been suggested as a workaround.[30] Trampoline support has been provided by the Scala library with the object scala.util.control.TailCalls since Scala 2.8.0 (released July 14, 2010). A function may optionally be annotated with @tailrec, in which case it will not compile unless it is tail recursive.[31]\n\nCase classes and pattern matching[edit]\nScala has built-in support for pattern matching, which can be thought of as a more sophisticated, extensible version of a switch statement, where arbitrary data types can be matched (rather than just simple types like integers, booleans and strings), including arbitrary nesting. A special type of class known as a case class is provided, which includes automatic support for pattern matching and can be used to model the algebraic data types used in many functional programming languages. (From the perspective of Scala, a case class is simply a normal class for which the compiler automatically adds certain behaviors that could also be provided manually, e.g., definitions of methods providing for deep comparisons and hashing, and destructuring a case class on its constructor parameters during pattern matching.)\n\nAn example of a definition of the quicksort algorithm using pattern matching is this:\n\ndef qsort(list: List[Int]): List[Int] = list match {\n  case Nil => Nil\n  case pivot :: tail =>\n    val (smaller, rest) = tail.partition(_ < pivot)\n    qsort(smaller) ::: pivot :: qsort(rest)\n}\nThe idea here is that we partition a list into the elements less than a pivot and the elements not less, recursively sort each part, and paste the results together with the pivot in between. This uses the same divide-and-conquer strategy of mergesort and other fast sorting algorithms.\n\nThe match operator is used to do pattern matching on the object stored in list. Each case expression is tried in turn to see if it will match, and the first match determines the result. In this case, Nil only matches the literal object Nil, but pivot :: tail matches a non-empty list, and simultaneously destructures the list according to the pattern given. In this case, the associated code will have access to a local variable named pivot holding the head of the list, and another variable tail holding the tail of the list. Note that these variables are read-only, and are semantically very similar to variable bindings established using the let operator in Lisp and Scheme.\n\nPattern matching also happens in local variable declarations. In this case, the return value of the call to tail.partition is a tuple — in this case, two lists. (Tuples differ from other types of containers, e.g. lists, in that they are always of fixed size and the elements can be of differing types — although here they are both the same.) Pattern matching is the easiest way of fetching the two parts of the tuple.\n\nThe form _ < pivot is a declaration of an anonymous function with a placeholder variable; see the section above on anonymous functions.\n\nThe list operators :: (which adds an element onto the beginning of a list, similar to cons in Lisp and Scheme) and ::: (which appends two lists together, similar to append in Lisp and Scheme) both appear. Despite appearances, there is nothing \"built-in\" about either of these operators. As specified above, any string of symbols can serve as function name, and a method applied to an object can be written \"infix\"-style without the period or parentheses. The line above as written:\n\nqsort(smaller) ::: pivot :: qsort(rest)\ncould also be written thus:\n\nqsort(rest).::(pivot).:::(qsort(smaller))\nin more standard method-call notation. (Methods that end with a colon are right-associative and bind to the object to the right.)\n\nPartial functions[edit]\nIn the pattern-matching example above, the body of the match operator is a partial function, which consists of a series of case expressions, with the first matching expression prevailing, similar to the body of a switch statement. Partial functions are also used in the exception-handling portion of a try statement:\n\ntry {\n  ...\n} catch {\n  case nfe:NumberFormatException => { println(nfe); List(0) }\n  case _ => Nil\n}\nFinally, a partial function can be used alone, and the result of calling it is equivalent to doing a match over it. For example, the prior code for quicksort can be written thus:\n\nval qsort: List[Int] => List[Int] = {\n  case Nil => Nil\n  case pivot :: tail =>\n    val (smaller, rest) = tail.partition(_ < pivot)\n    qsort(smaller) ::: pivot :: qsort(rest)\n}\nHere a read-only variable is declared whose type is a function from lists of integers to lists of integers, and bind it to a partial function. (Note that the single parameter of the partial function is never explicitly declared or named.) However, we can still call this variable exactly as if it were a normal function:\n\nscala> qsort(List(6,2,5,9))\nres32: List[Int] = List(2, 5, 6, 9)\nObject-oriented extensions[edit]\nScala is a pure object-oriented language in the sense that every value is an object. Data types and behaviors of objects are described by classes and traits. Class abstractions are extended by subclassing and by a flexible mixin-based composition mechanism to avoid the problems of multiple inheritance.\n\nTraits are Scala's replacement for Java's interfaces. Interfaces in Java versions under 8 are highly restricted, able only to contain abstract function declarations. This has led to criticism that providing convenience methods in interfaces is awkward (the same methods must be reimplemented in every implementation), and extending a published interface in a backwards-compatible way is impossible. Traits are similar to mixin classes in that they have nearly all the power of a regular abstract class, lacking only class parameters (Scala's equivalent to Java's constructor parameters), since traits are always mixed in with a class. The super operator behaves specially in traits, allowing traits to be chained using composition in addition to inheritance. The following example is a simple window system:\n\nabstract class Window {\n  // abstract\n  def draw()\n}\n\nclass SimpleWindow extends Window {\n  def draw() {\n    println(\"in SimpleWindow\")\n    // draw a basic window\n  }\n}\n\ntrait WindowDecoration extends Window { }\n\ntrait HorizontalScrollbarDecoration extends WindowDecoration {\n  // \"abstract override\" is needed here in order for \"super()\" to work because the parent\n  // function is abstract. If it were concrete, regular \"override\" would be enough.\n  abstract override def draw() {\n    println(\"in HorizontalScrollbarDecoration\")\n    super.draw()\n    // now draw a horizontal scrollbar\n  }\n}\n\ntrait VerticalScrollbarDecoration extends WindowDecoration {\n  abstract override def draw() {\n    println(\"in VerticalScrollbarDecoration\")\n    super.draw()\n    // now draw a vertical scrollbar\n  }\n}\n\ntrait TitleDecoration extends WindowDecoration {\n  abstract override def draw() {\n    println(\"in TitleDecoration\")\n    super.draw()\n    // now draw the title bar\n  }\n}\nA variable may be declared thus:\n\nval mywin = new SimpleWindow with VerticalScrollbarDecoration with HorizontalScrollbarDecoration with TitleDecoration\nThe result of calling mywin.draw() is\n\nin TitleDecoration\nin HorizontalScrollbarDecoration\nin VerticalScrollbarDecoration\nin SimpleWindow\nIn other words, the call to draw first executed the code in TitleDecoration (the last trait mixed in), then (through the super() calls) threaded back through the other mixed-in traits and eventually to the code in Window, even though none of the traits inherited from one another. This is similar to the decorator pattern, but is more concise and less error-prone, as it doesn't require explicitly encapsulating the parent window, explicitly forwarding functions whose implementation isn't changed, or relying on run-time initialization of entity relationships. In other languages, a similar effect could be achieved at compile-time with a long linear chain of implementation inheritance, but with the disadvantage compared to Scala that one linear inheritance chain would have to be declared for each possible combination of the mix-ins.\n\nExpressive type system[edit]\nScala is equipped with an expressive static type system that enforces the safe and coherent use of abstractions. In particular, the type system supports:\n\nClasses and abstract types as object members\nStructural types\nPath-dependent types\nCompound types\nExplicitly typed self references\nGeneric classes\nPolymorphic methods\nUpper and lower type bounds\nVariance\nAnnotation\nViews\nScala is able to infer types by usage. This makes most static type declarations optional. Static types need not be explicitly declared unless a compiler error indicates the need. In practice, some static type declarations are included for the sake of code clarity.\n\nType enrichment[edit]\nA common technique in Scala, known as \"enrich my library\"[32] (originally termed as \"pimp my library\" by Martin Odersky in 2006;[28] though concerns were raised about this phrasing due to its negative connotation[33] and immaturity[34]), allows new methods to be used as if they were added to existing types. This is similar to the C# concept of extension methods but more powerful, because the technique is not limited to adding methods and can, for instance, be used to implement new interfaces. In Scala, this technique involves declaring an implicit conversion from the type \"receiving\" the method to a new type (typically, a class) that wraps the original type and provides the additional method. If a method cannot be found for a given type, the compiler automatically searches for any applicable implicit conversions to types that provide the method in question.\n\nThis technique allows new methods to be added to an existing class using an add-on library such that only code that imports the add-on library gets the new functionality, and all other code is unaffected.\n\nThe following example shows the enrichment of type Int with methods isEven and isOdd:\n\nobject MyExtensions {\n  implicit class IntPredicates(i: Int) {\n    def isEven = i % 2 == 0\n    def isOdd  = !isEven\n  }\n}\n\nimport MyExtensions._  // bring implicit enrichment into scope\n4.isEven  // -> true\nImporting the members of MyExtensions brings the implicit conversion to extension class IntPredicates into scope.[35]\n\nConcurrency[edit]\nScala standard library includes support for the actor model, in addition to the standard Java concurrency APIs. Lightbend Inc., provides a platform[36] that includes Akka,[37] a separate open source framework that provides actor-based concurrency. Akka actors may be distributed or combined with software transactional memory (transactors). Alternative communicating sequential processes (CSP) implementations for channel-based message passing are Communicating Scala Objects,[38] or simply via JCSP.\n\nAn Actor is like a thread instance with a mailbox. It can be created by system.actorOf, overriding the receive method to receive messages and using the ! (exclamation point) method to send a message.[39] The following example shows an EchoServer that can receive messages and then print them.\n\nval echoServer = actor(new Act {\n  become {\n    case msg => println(\"echo \" + msg)\n  }\n})\nechoServer ! \"hi\"\nScala also comes with built-in support for data-parallel programming in the form of Parallel Collections[40] integrated into its Standard Library since version 2.9.0.\n\nThe following example shows how to use Parallel Collections to improve performance.[41]\n\nval urls = List(\"http://scala-lang.org\",  \"https://github.com/scala/scala\")\n\ndef fromURL(url: String) = scala.io.Source.fromURL(url)\n  .getLines().mkString(\"\\n\")\n\nval t = System.currentTimeMillis()\nurls.par.map(fromURL(_))\nprintln(\"time: \" + (System.currentTimeMillis - t) + \"ms\")\nBesides actor support and data-parallelism, Scala also supports asynchronous programming with Futures and Promises, software transactional memory, and event streams.[42]\n\nCluster computing[edit]\nThe most well-known open source cluster computing solution, written in Scala, is Apache Spark. Additionally, Apache Kafka, the publish-subscribe message queue popular with Spark and other stream processing technologies, is written in Scala.\n\nTesting[edit]\nThere are several ways to test code in Scala:\n\nScalaTest supports multiple testing styles and can integrate with Java-based testing frameworks[43]\nScalaCheck, a library similar to Haskell's QuickCheck[44]\nspecs2, a library for writing executable software specifications[45]\nScalaMock provides support for testing high-order and curried functions[46]\nJUnit or TestNG, two popular testing frameworks written in Java\nVersions[edit]\nVersion\tReleased\tFeatures\tStatus\tNotes\n2.0[47]\t12-Mar-2006\t_\t_\t_\n2.1.8[48]\t23-Aug-2006\t_\t_\t_\n2.3.0[49]\t23-Nov-2006\t_\t_\t_\n2.4.0[50]\t09-Mar-2007\t_\t_\t_\n2.5.0[51]\t02-May-2007\t_\t_\t_\n2.6.0[52]\t27-Jul-2007\t_\t_\t_\n2.7.0[53]\t07-Feb-2008\t_\t_\t_\n2.8.0[54]\t14-Jul-2010\tRevision the common, uniform, and all-encompassing framework for collection types.\t_\t_\n2.9.0[55]\t12-May-2011\t_\t_\t_\n2.10[56]\t04-Jan-2013\t\nValue Classes[57]\nImplicit Classes[58]\nString Interpolation[59]\nFutures and Promises[60]\nDynamic and applyDynamic[61]\nDependent method types:\ndef identity(x: AnyRef): x.type = x // the return type says we return exactly what we got\nNew ByteCode emitter based on ASM:\nCan target JDK 1.5, 1.6 and 1.7\nEmits 1.6 bytecode by default\nOld 1.5 backend is deprecated\nA new Pattern Matcher: rewritten from scratch to generate more robust code (no more exponential blow-up!)\ncode generation and analyses are now independent (the latter can be turned off with -Xno-patmat-analysis)\nScaladoc Improvements\nImplicits (-implicits flag)\nDiagrams (-diagrams flag, requires graphviz)\nGroups (-groups)\nModularized Language features[62]\nParallel Collections[63] are now configurable with custom thread pools\nAkka Actors now part of the distribution\nscala.actors have been deprecated and the akka implementation is now included in the distribution.\nPerformance Improvements\nFaster inliner\nRange#sum is now O(1)\nUpdate of ForkJoin library\nFixes in immutable TreeSet/TreeMap\nImprovements to PartialFunctions\nAddition of ??? and NotImplementedError\nAddition of IsTraversableOnce + IsTraversableLike type classes for extension methods\nDeprecations and cleanup\nFloating point and octal literal syntax deprecation\nRemoved scala.dbc\nExperimental features\n\nScala Reflection[64]\nMacros[65]\n_\t_\n2.10.2[66]\t06-Jun-2013\t_\t_\t_\n2.10.3[67]\t01-Oct-2013\t_\t_\t_\n2.10.4[68]\t18-Mar-2014\t_\t_\t_\n2.10.5[69]\t05-Mar-2015\t_\t_\t_\n2.11.0[70]\t21-Apr-2014\t_\t_\t_\n2.11.1[71]\t20-May-2014\t_\t_\t_\n2.11.2[72]\t22-Jul-2014\t_\t_\t_\n2.11.4[73]\t31-Oct-2014\t_\t_\t_\n2.11.5[74]\t08-Jan-2015\t_\t_\t_\n2.11.6[75]\t05-Mar-2015\t_\t_\t_\n2.11.7[76]\t23-Jun-2015\t_\t_\t_\n2.11.8[77]\t8-Mar-2016\t_\tCurrent\t_\nComparison with other JVM languages[edit]\nScala is often compared with Groovy and Clojure, two other programming languages also using the JVM. Substantial differences between these languages are found in the type system, in the extent to which each language supports object-oriented and functional programming, and in the similarity of their syntax to the syntax of Java.\n\nScala is statically typed, while both Groovy and Clojure are dynamically typed. This makes the type system more complex and difficult to understand but allows almost all type errors to be caught at compile-time and can result in significantly faster execution. By contrast, dynamic typing requires more testing to ensure program correctness and is generally slower in order to allow greater programming flexibility and simplicity. Regarding speed differences, current versions of Groovy and Clojure allow for optional type annotations to help programs avoid the overhead of dynamic typing in cases where types are practically static. This overhead is further reduced when using recent versions of the JVM, which has been enhanced with an invoke dynamic instruction for methods that are defined with dynamically typed arguments. These advances reduce the speed gap between static and dynamic typing, although a statically typed language, like Scala, is still the preferred choice when execution efficiency is very important.\n\nRegarding programming paradigms, Scala inherits the object-oriented model of Java and extends it in various ways. Groovy, while also strongly object-oriented, is more focused in reducing verbosity. In Clojure, object-oriented programming is deemphasised with functional programming being the main strength of the language. Scala also has many functional programming facilities, including features found in advanced functional languages like Haskell, and tries to be agnostic between the two paradigms, letting the developer choose between the two paradigms or, more frequently, some combination thereof.\n\nRegarding syntax similarity with Java, Scala inherits much of Java's syntax, as is the case with Groovy. Clojure on the other hand follows the Lisp syntax, which is different in both appearance and philosophy. However, learning Scala is also considered difficult because of its many advanced features. This is not the case with Groovy, despite its also being a feature-rich language, mainly because it was designed to be mainly a scripting language.[citation needed]\n\nAdoption[edit]\nLanguage rankings[edit]\nScala was voted the most popular JVM scripting language at the 2012 JavaOne conference.[14]\n\nAs of 2013, all JVM-based languages (Scala, Groovy, Clojure) are significantly less popular than the original Java language, which is usually ranked first or second,[78][79][80] and which is also simultaneously evolving over time.\n\n\nIndeed.com job trends: Scala and related technologies\nThe RedMonk Programming Language Rankings, as of June 2016 placed Scala 14th, based on a position in terms of number of GitHub projects and in terms of number of questions tagged on Stack Overflow.[78] (Groovy and Clojure were both in 20th place.)[78] Here, Scala is shown somewhat between a first-tier group of languages (including, C, Python, PHP, Ruby, etc.), and ahead of a second-tier group.\n\nAnother measure, the Popularity of Programming Language Index[81] which tracks searches for language tutorials ranked Scala 15th in July 2016 with a small upward trend, making it the most popular JVM-based language after Java.\n\nAs of January 2016, the TIOBE index[79] of programming language popularity shows Scala in 30th place (as measured by internet search engine rankings and similar publication-counting), but–as mentioned under \"Bugs & Change Requests\"–TIOBE is aware of issues with its methodology of using search terms which might not be commonly used in some programming language communities. In this ranking Scala is ahead of functional languages Haskell (39th), Erlang (35rd) and Clojure (>50), but below Java (1st).\n\nThe ThoughtWorks Technology Radar, which is an opinion based half-yearly report of a group of senior technologists,[82] recommends Scala adoption in its languages and frameworks category.[83]\n\nAccording to Indeed.com Job Trends, Scala demand has been rapidly increasing since 2010, trending ahead of Clojure and Groovy.[84]\n\nCompanies[edit]\nIn April 2009, Twitter announced that it had switched large portions of its backend from Ruby to Scala and intended to convert the rest.[85]\nGilt uses Scala and Play Framework.[86]\nFoursquare uses Scala and Lift.[87]\nSpinGo uses Scala and Akka.[88]\nCoursera uses Scala and Play Framework.[89]\nApple Inc. uses Scala in certain teams, along with Java and the Play framework.[90][91]\nThe Guardian newspaper's high-traffic website guardian.co.uk[92] announced in April 2011 that it was switching from Java to Scala,[93][94]\nThe New York Times revealed in 2014 that its internal content management system Blackbeard is built using Scala, Akka and Play.[95]\nThe Huffington Post newspaper started to employ Scala as part of its contents delivery system Athena in 2013.[96]\nSwiss bank UBS approved Scala for general production usage.[97]\nThe BitGold platform was built entirely on Scala and Play Framework.[98]\nLinkedIn uses the Scalatra microframework to power its Signal API.[99]\nMeetup uses Unfiltered toolkit for real-time APIs.[100]\nRemember the Milk uses Unfiltered toolkit, Scala and Akka for public API and real time updates.[101]\nVerizon seeking to make \"a next generation framework\" using Scala.[102]\nLeadIQ was built entirely on Scala, Akka and Play Framework.[103]\nAirbnb develops open source machine learning software \"Aerosolve\", written in Java and Scala.[104]\nZalando moved its technology stack from Java to Scala and Play.[105]\nSoundCloud uses Scala for its back-end, employing technologies such as Finagle (micro services),[106] Scalding and Spark (data processing).[107]\nDatabricks uses Scala for the Apache Spark Big Data platform.\nMorgan Stanley uses Scala extensively in their finance and asset-related projects.[108]\nThere are teams within Google/Alphabet Inc. that use Scala, mostly due to acquisitions such as Firebase[109] and Nest.[110]\nWalmart Canada Uses Scala for their back end platform.[111]\nx.ai uses Scala for their AI-driven Personal Assistant.[112]\nCriticism[edit]\nIn March 2015, former VP of the Platform Engineering group at Twitter Raffi Krikorian, stated he would not have chosen Scala in 2011 due to its learning curve.[113] The same month, LinkedIn SVP Kevin Scott stated their decision to \"minimize [their] dependence on Scala.\"[114] In November 2011, Yammer moved away from Scala for reasons that included the learning curve for new team members and incompatibility from one version of the Scala compiler to the next.[115]\n\ndotty is an attempt at creating a simpler, faster Scala compiler based on a formal calculus,[1] that will enable faster language development and future language simplification.[116]",
          "type": "compiled",
          "plid": 30
        },
        {
          "name": "Smalltalk",
          "details": "Smalltalk is an object-oriented, dynamically typed, reflective programming language. Smalltalk was created as the language to underpin the \"new world\" of computing exemplified by \"human–computer symbiosis.\"[2] It was designed and created in part for educational use, more so for constructionist learning, at the Learning Research Group (LRG) of Xerox PARC by Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Scott Wallace, and others during the 1970s.\n\nThe language was first generally released as Smalltalk-80. Smalltalk-like languages are in continuing active development and have gathered loyal communities of users around them. ANSI Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.[3]\n\nContents  [hide] \n1\tHistory\n2\tInfluences\n3\tObject-oriented programming\n4\tReflection\n5\tSyntax\n5.1\tLiterals\n5.2\tVariable declarations\n5.3\tAssignment\n5.4\tMessages\n5.5\tExpressions\n5.6\tCode blocks\n6\tControl structures\n7\tClasses\n7.1\tMethods\n7.2\tInstantiating classes\n8\tHello World example\n9\tImage-based persistence\n10\tLevel of access\n11\tJust-in-time compilation\n12\tList of implementations\n13\tSee also\n14\tReferences\n15\tFurther reading\n16\tExternal links\nHistory[edit]\nThere are a large number of Smalltalk variants.[4] The unqualified word Smalltalk is often used to indicate the Smalltalk-80 language, the first version to be made publicly available and created in 1980.\n\nSmalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, which Dan Ingalls implemented. The first version, known as Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in \"a page of code.\"[2] A later variant actually used for research work is now known as Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.\n\nAfter significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created. This system had a development environment featuring most of the now familiar tools, including a class library code browser/editor. Smalltalk-80 added metaclasses, to help maintain the \"everything is an object\" (except private instance variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways of creating instances).\n\nSmalltalk-80 was the first language variant made available outside of PARC, first as Smalltalk-80 Version 1, given to a small number of firms (Hewlett-Packard, Apple Computer, Tektronix, and DEC) and universities (UC Berkeley) for \"peer review\" and implementation on their platforms. Later (in 1983) a general availability implementation, known as Smalltalk-80 Version 2, was released as an image (platform-independent file with object definitions) and a virtual machine specification. ANSI Smalltalk has been the standard language reference since 1998.[5]\n\nTwo of the currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks.[6] (Dan Ingalls later ported Hobbes to Squeak.)\n\nDuring the late 1980s to mid-1990s, Smalltalk environments—including support, training and add-ons—were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers. While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge/Smalltalk. Easel introduced Enfin at this time on Windows and OS/2. Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge). Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.\n\nIn 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom.[7] VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.\n\nCincom, Gemstone and Object Arts, plus other vendors continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java and it is, as of 2006, supported by Instantiations, Inc.[8] which has renamed the product VA Smalltalk and released several new versions. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented towards research and use in commercial environments.\n\nA significant development, that has spread across all current Smalltalk environments, is the increasing usage of two web frameworks, Seaside and AIDA/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone and Instantiations incorporating and extending it.\n\nInfluences[edit]\nSmalltalk was one of many object-oriented programming languages based on Simula.[9] Smalltalk was also one of the most influential programming languages. Virtually all of the object-oriented languages that came after—Flavors,[10] CLOS, Objective-C, Java, Python, Ruby,[11] and many others—were influenced by Smalltalk. Smalltalk was also one of the most popular languages with the Agile Methods, Rapid Prototyping, and Software Patterns[12] communities. The highly productive environment provided by Smalltalk platforms made them ideal for rapid, iterative development.\n\nSmalltalk emerged from a larger program of ARPA funded research that in many ways defined the modern world of computing. In addition to Smalltalk working prototypes of things such as hypertext, GUIs, multimedia, the mouse, telepresence, and the Internet were developed by ARPA researchers in the 1960s.[13][14] Alan Kay (one of the inventors of Smalltalk) also described a tablet computer he called the Dynabook which resembles modern tablet computers like the iPad.[15]\n\nSmalltalk environments were often the first to develop what are now common object-oriented software design patterns. One of the most popular is the Model–view–controller pattern for User Interface design. The MVC pattern enables developers to have multiple consistent views of the same underlying data. It's ideal for software development environments, where there are various views (e.g., entity-relation, dataflow, object model, etc.) of the same underlying specification. Also, for simulations or games where the underlying model may be viewed from various angles and levels of abstraction.[16]\n\nIn addition to the MVC pattern the Smalltalk language and environment were tremendously influential in the history of the Graphical User Interface (GUI) and the What You See Is What You Get (WYSIWYG) user interface, font editors, and desktop metaphors for UI design. The powerful built-in debugging and object inspection tools that came with Smalltalk environments set the standard for all the Integrated Development Environments, starting with Lisp Machine environments, that came after.[17]\n\nObject-oriented programming[edit]\nMain article: Object-oriented programming\nAs in other object-oriented languages, the central concept in Smalltalk-80 (but not in Smalltalk-72) is that of an object. An object is always an instance of a class. Classes are \"blueprints\" that describe the properties and behavior of their instances. For example, a GUI's window class might declare that windows have properties such as the label, the position and whether the window is visible or not. The class might also declare that instances support operations such as opening, closing, moving and hiding. Each particular window object would have its own values of those properties, and each of them would be able to perform operations defined by its class.\n\nA Smalltalk object can do exactly three things:\n\nHold state (references to other objects).\nReceive a message from itself or another object.\nIn the course of processing a message, send messages to itself or another object.\nThe state an object holds is always private to that object. Other objects can query or change that state only by sending requests (messages) to the object to do so. Any message can be sent to any object: when a message is received, the receiver determines whether that message is appropriate. Alan Kay has commented that despite the attention given to objects, messaging is the most important concept in Smalltalk: \"The big idea is 'messaging'—that is what the kernel of Smalltalk/Squeak is all about (and it's something that was never quite completed in our Xerox PARC phase).\"[18]\n\nSmalltalk is a \"pure\" object-oriented programming language, meaning that, unlike Java and C++, there is no difference between values which are objects and values which are primitive types. In Smalltalk, primitive values such as integers, booleans and characters are also objects, in the sense that they are instances of corresponding classes, and operations on them are invoked by sending messages. A programmer can change or extend (through subclassing) the classes that implement primitive values, so that new behavior can be defined for their instances—for example, to implement new control structures—or even so that their existing behavior will be changed. This fact is summarized in the commonly heard phrase \"In Smalltalk everything is an object\", which may be more accurately expressed as \"all values are objects\", as variables are not.\n\nSince all values are objects, classes themselves are also objects. Each class is an instance of the metaclass of that class. Metaclasses in turn are also objects, and are all instances of a class called Metaclass. Code blocks—Smalltalk's way of expressing anonymous functions—are also objects.[19]\n\nReflection[edit]\nReflection is a term that computer scientists apply to software programs that have the capability to inspect their own structure, for example their parse tree or datatypes of input and output parameters. Reflection was first primarily a feature of interpreted languages such as Smalltalk and Lisp. The fact that statements are interpreted means that the programs have access to information created as they were parsed and can often even modify their own structure.\n\nReflection is also a feature of having a meta-model as Smalltalk does. The meta-model is the model that describes the language itself and developers can use the meta-model to do things like walk through, examine, and modify the parse tree of an object. Or find all the instances of a certain kind of structure (e.g., all the instances of the Method class in the meta-model).\n\nSmalltalk-80 is a totally reflective system, implemented in Smalltalk-80 itself. Smalltalk-80 provides both structural and computational reflection. Smalltalk is a structurally reflective system whose structure is defined by Smalltalk-80 objects. The classes and methods that define the system are themselves objects and fully part of the system that they help define. The Smalltalk compiler compiles textual source code into method objects, typically instances of CompiledMethod. These get added to classes by storing them in a class's method dictionary. The part of the class hierarchy that defines classes can add new classes to the system. The system is extended by running Smalltalk-80 code that creates or defines classes and methods. In this way a Smalltalk-80 system is a \"living\" system, carrying around the ability to extend itself at run time.\n\nSince the classes are themselves objects, they can be asked questions such as \"what methods do you implement?\" or \"what fields/slots/instance variables do you define?\". So objects can easily be inspected, copied, (de)serialized and so on with generic code that applies to any object in the system.[20]\n\nSmalltalk-80 also provides computational reflection, the ability to observe the computational state of the system. In languages derived from the original Smalltalk-80 the current activation of a method is accessible as an object named via a pseudo-variable (one of the six reserved words), thisContext. By sending messages to thisContext a method activation can ask questions like \"who sent this message to me\". These facilities make it possible to implement co-routines or Prolog-like back-tracking without modifying the virtual machine. The exception system is implemented using this facility. One of the more interesting uses of this is in the Seaside web framework which relieves the programmer of dealing with the complexity of a Web Browser's back button by storing continuations for each edited page and switching between them as the user navigates a web site. Programming the web server using Seaside can then be done using a more conventional programming style.[21]\n\nAn example of how Smalltalk can use reflection is the mechanism for handling errors. When an object is sent a message that it does not implement, the virtual machine sends the object the doesNotUnderstand: message with a reification of the message as an argument. The message (another object, an instance of Message) contains the selector of the message and an Array of its arguments. In an interactive Smalltalk system the default implementation of doesNotUnderstand: is one that opens an error window (a Notifier) reporting the error to the user. Through this and the reflective facilities the user can examine the context in which the error occurred, redefine the offending code, and continue, all within the system, using Smalltalk-80's reflective facilities.[22][23]\n\nSyntax[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2014) (Learn how and when to remove this template message)\nSmalltalk-80 syntax is rather minimalist, based on only a handful of declarations and reserved words. In fact, only six \"keywords\" are reserved in Smalltalk: true, false, nil, self, super, and thisContext. These are actually called pseudo-variables, identifiers that follow the rules for variable identifiers but denote bindings that the programmer cannot change. The true, false, and nil pseudo-variables are singleton instances. self and super refer to the receiver of a message within a method activated in response to that message, but sends to super are looked up in the superclass of the method's defining class rather than the class of the receiver, which allows methods in subclasses to invoke methods of the same name in superclasses. thisContext refers to the current activation record. The only built-in language constructs are message sends, assignment, method return and literal syntax for some objects. From its origins as a language for children of all ages, standard Smalltalk syntax uses punctuation in a manner more like English than mainstream coding languages. The remainder of the language, including control structures for conditional evaluation and iteration, is implemented on top of the built-in constructs by the standard Smalltalk class library. (For performance reasons, implementations may recognize and treat as special some of those messages; however, this is only an optimization and is not hardwired into the language syntax.)\n\nThe adage that \"Smalltalk syntax fits on a postcard\" refers to a code snippet by Ralph Johnson, demonstrating all the basic standard syntactic elements of methods:[24]\n\nexampleWithNumber: x\n    | y |\n    true & false not & (nil isNil) ifFalse: [self halt].\n    y := self size + super size.\n    #($a #a \"a\" 1 1.0)\n        do: [ :each |\n            Transcript show: (each class name);\n                       show: ' '].\n    ^x < y\nLiterals[edit]\nThe following examples illustrate the most common objects which can be written as literal values in Smalltalk-80 methods.\n\nNumbers. The following list illustrates some of the possibilities.\n\n42\n-42\n123.45\n1.2345e2\n2r10010010\n16rA000\nThe last two entries are a binary and a hexadecimal number, respectively. The number before the 'r' is the radix or base. The base does not have to be a power of two; for example 36rSMALLTALK is a valid number equal to 80738163270632 decimal.\n\nCharacters are written by preceding them with a dollar sign:\n\n$A\nStrings are sequences of characters enclosed in single quotes:\n\n'Hello, world!'\nTo include a quote in a string, escape it using a second quote:\n\n'I said, ''Hello, world!'' to them.'\nDouble quotes do not need escaping, since single quotes delimit a string:\n\n'I said, \"Hello, world!\" to them.'\nTwo equal strings (strings are equal if they contain all the same characters) can be different objects residing in different places in memory. In addition to strings, Smalltalk has a class of character sequence objects called Symbol. Symbols are guaranteed to be unique—there can be no two equal symbols which are different objects. Because of that, symbols are very cheap to compare and are often used for language artifacts such as message selectors (see below).\n\nSymbols are written as # followed by a string literal. For example:\n\n#'foo'\nIf the sequence does not include whitespace or punctuation characters, this can also be written as:\n\n#foo\nArrays:\n\n#(1 2 3 4)\ndefines an array of four integers.\n\nMany implementations support the following literal syntax for ByteArrays:\n\n#[1 2 3 4]\ndefines a ByteArray of four integers.\n\nAnd last but not least, blocks (anonymous function literals)\n\n[... Some smalltalk code...]\nBlocks are explained in detail further in the text.\n\nMany Smalltalk dialects implement additional syntaxes for other objects, but the ones above are the essentials supported by all.\n\nVariable declarations[edit]\nThe two kinds of variables commonly used in Smalltalk are instance variables and temporary variables. Other variables and related terminology depend on the particular implementation. For example, VisualWorks has class shared variables and namespace shared variables, while Squeak and many other implementations have class variables, pool variables and global variables.\n\nTemporary variable declarations in Smalltalk are variables declared inside a method (see below). They are declared at the top of the method as names separated by spaces and enclosed by vertical bars. For example:\n\n| index |\ndeclares a temporary variable named index. Multiple variables may be declared within one set of bars:\n\n| index vowels |\ndeclares two variables: index and vowels.\n\nAssignment[edit]\nA variable is assigned a value via the ':=' syntax. So:\n\nvowels := 'aeiou'\nAssigns the string 'aeiou' to the previously declared vowels variable. The string is an object (a sequence of characters between single quotes is the syntax for literal strings), created by the compiler at compile time.\n\nIn the original Parc Place image, the glyph of the underscore character (_) appeared as a left-facing arrow (like in the 1963 version of the ASCII code). Smalltalk originally accepted this left-arrow as the only assignment operator. Some modern code still contains what appear to be underscores acting as assignments, hearkening back to this original usage. Most modern Smalltalk implementations accept either the underscore or the colon-equals syntax.\n\nMessages[edit]\nThe message is the most fundamental language construct in Smalltalk. Even control structures are implemented as message sends. Smalltalk adopts by default a synchronous, single dynamic message dispatch strategy (as contrasted to the asynchronous, multiple dispatch strategy adopted by some other object-oriented languages).\n\nThe following example sends the message 'factorial' to number 42:\n\n42 factorial\nIn this situation 42 is called the message receiver, while 'factorial' is the message selector. The receiver responds to the message by returning a value (presumably in this case the factorial of 42). Among other things, the result of the message can be assigned to a variable:\n\naRatherBigNumber := 42 factorial\n\"factorial\" above is what is called a unary message because only one object, the receiver, is involved. Messages can carry additional objects as arguments, as follows:\n\n2 raisedTo: 4\nIn this expression two objects are involved: 2 as the receiver and 4 as the message argument. The message result, or in Smalltalk parlance, the answer is supposed to be 16. Such messages are called keyword messages. A message can have more arguments, using the following syntax:\n\n'hello world' indexOf: $o startingAt: 6\nwhich answers the index of character 'o' in the receiver string, starting the search from index 6. The selector of this message is \"indexOf:startingAt:\", consisting of two pieces, or keywords.\n\nSuch interleaving of keywords and arguments is meant to improve readability of code, since arguments are explained by their preceding keywords. For example, an expression to create a rectangle using a C++ or Java-like syntax might be written as:\n\nnew Rectangle(100, 200);\nIt's unclear which argument is which. By contrast, in Smalltalk, this code would be written as:\n\nRectangle width: 100 height: 200\nThe receiver in this case is \"Rectangle\", a class, and the answer will be a new instance of the class with the specified width and height.\n\nFinally, most of the special (non-alphabetic) characters can be used as what are called binary messages. These allow mathematical and logical operators to be written in their traditional form:\n\n3 + 4\nwhich sends the message \"+\" to the receiver 3 with 4 passed as the argument (the answer of which will be 7). Similarly,\n\n3 > 4\nis the message \">\" sent to 3 with argument 4 (the answer of which will be false).\n\nNotice, that the Smalltalk-80 language itself does not imply the meaning of those operators. The outcome of the above is only defined by how the receiver of the message (in this case a Number instance) responds to messages \"+\" and \">\".\n\nA side effect of this mechanism is operator overloading. A message \">\" can also be understood by other objects, allowing the use of expressions of the form \"a > b\" to compare them.\n\nExpressions[edit]\nAn expression can include multiple message sends. In this case expressions are parsed according to a simple order of precedence. Unary messages have the highest precedence, followed by binary messages, followed by keyword messages. For example:\n\n3 factorial + 4 factorial between: 10 and: 100\nis evaluated as follows:\n\n3 receives the message \"factorial\" and answers 6\n4 receives the message \"factorial\" and answers 24\n6 receives the message \"+\" with 24 as the argument and answers 30\n30 receives the message \"between:and:\" with 10 and 100 as arguments and answers true\nThe answer of the last message sent is the result of the entire expression.\n\nParentheses can alter the order of evaluation when needed. For example,\n\n(3 factorial + 4) factorial between: 10 and: 100\nwill change the meaning so that the expression first computes \"3 factorial + 4\" yielding 10. That 10 then receives the second \"factorial\" message, yielding 3628800. 3628800 then receives \"between:and:\", answering false.\n\nNote that because the meaning of binary messages is not hardwired into Smalltalk-80 syntax, all of them are considered to have equal precedence and are evaluated simply from left to right. Because of this, the meaning of Smalltalk expressions using binary messages can be different from their \"traditional\" interpretation:\n\n3 + 4 * 5\nis evaluated as \"(3 + 4) * 5\", producing 35. To obtain the expected answer of 23, parentheses must be used to explicitly define the order of operations:\n\n3 + (4 * 5)\nUnary messages can be chained by writing them one after another:\n\n3 factorial factorial log\nwhich sends \"factorial\" to 3, then \"factorial\" to the result (6), then \"log\" to the result (720), producing the result 2.85733.\n\nA series of expressions can be written as in the following (hypothetical) example, each separated by a period. This example first creates a new instance of class Window, stores it in a variable, and then sends two messages to it.\n\n | window |\n  window := Window new.\n  window label: 'Hello'.\n  window open\nIf a series of messages are sent to the same receiver as in the example above, they can also be written as a cascade with individual messages separated by semicolons:\n\n  Window new\n    label: 'Hello';\n    open\nThis rewrite of the earlier example as a single expression avoids the need to store the new window in a temporary variable. According to the usual precedence rules, the unary message \"new\" is sent first, and then \"label:\" and \"open\" are sent to the answer of \"new\".\n\nCode blocks[edit]\nA block of code (an anonymous function) can be expressed as a literal value (which is an object, since all values are objects.) This is achieved with square brackets:\n\n[ :params | <message-expressions> ]\nWhere :params is the list of parameters the code can take. This means that the Smalltalk code:\n\n[:x | x + 1]\ncan be understood as:\n\n{\\displaystyle f} f : {\\displaystyle f(x)=x+1} f(x)=x+1\nor expressed in lambda terms as:\n\n{\\displaystyle \\lambda x} \\lambda x : {\\displaystyle x+1} x+1\nand\n\n[:x | x + 1] value: 3\ncan be evaluated as\n\n{\\displaystyle f(3)=3+1} f(3)=3+1\nOr in lambda terms as:\n\n{\\displaystyle (\\lambda x:x+1)3_{\\beta }\\rightarrow 4} (\\lambda x:x+1)3_{\\beta }\\rightarrow 4\nThe resulting block object can form a closure: it can access the variables of its enclosing lexical scopes at any time. Blocks are first-class objects.\n\nBlocks can be executed by sending them the value message (compound variations exist in order to provide parameters to the block e.g. 'value:value:' and 'valueWithArguments:').\n\nThe literal representation of blocks was an innovation which on the one hand allowed certain code to be significantly more readable; it allowed algorithms involving iteration to be coded in a clear and concise way. Code that would typically be written with loops in some languages can be written concisely in Smalltalk using blocks, sometimes in a single line. But more importantly blocks allow control structure to be expressed using messages and polymorphism, since blocks defer computation and polymorphism can be used to select alternatives. So if-then-else in Smalltalk is written and implemented as\n\nexpr ifTrue: [statements to evaluate if expr] ifFalse: [statements to evaluate if not expr]\n\nTrue methods for evaluation\nifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock\n     ^trueAlternativeBlock value\n\nFalse methods for evaluation\nifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock\n    ^falseAlternativeBlock value\npositiveAmounts := allAmounts select: [:anAmount | anAmount isPositive]\nNote that this is related to functional programming, wherein patterns of computation (here selection) are abstracted into higher-order functions. For example, the message select: on a Collection is equivalent to the higher-order function filter on an appropriate functor.[25]\n\nControl structures[edit]\nControl structures do not have special syntax in Smalltalk. They are instead implemented as messages sent to objects. For example, conditional execution is implemented by sending the message ifTrue: to a Boolean object, passing as an argument the block of code to be executed if and only if the Boolean receiver is true.\n\nThe following code demonstrates this:\n\nresult := a > b\n    ifTrue:[ 'greater' ]\n    ifFalse:[ 'less or equal' ]\nBlocks are also used to implement user-defined control structures, enumerators, visitors, pluggable behavior and many other patterns. For example:\n\n| aString vowels |\naString := 'This is a string'.\nvowels := aString select: [:aCharacter | aCharacter isVowel].\nIn the last line, the string is sent the message select: with an argument that is a code block literal. The code block literal will be used as a predicate function that should answer true if and only if an element of the String should be included in the Collection of characters that satisfy the test represented by the code block that is the argument to the \"select:\" message.\n\nA String object responds to the \"select:\" message by iterating through its members (by sending itself the message \"do:\"), evaluating the selection block (\"aBlock\") once with each character it contains as the argument. When evaluated (by being sent the message \"value: each\"), the selection block (referenced by the parameter \"aBlock\", and defined by the block literal \"[:aCharacter | aCharacter isVowel]\"), answers a boolean, which is then sent \"ifTrue:\". If the boolean is the object true, the character is added to a string to be returned. Because the \"select:\" method is defined in the abstract class Collection, it can also be used like this:\n\n| rectangles aPoint collisions |\nrectangles := OrderedCollection \n  with: (Rectangle left: 0 right: 10 top: 100 bottom: 200)\n  with: (Rectangle left: 10 right: 10 top: 110 bottom: 210).\naPoint := Point x: 20 y: 20.\ncollisions := rectangles select: [:aRect | aRect containsPoint: aPoint].\nClasses[edit]\nThis is a stock class definition:[26]\n\nObject subclass: #MessagePublisher\n    instanceVariableNames: ''\n    classVariableNames: ''\n    poolDictionaries: ''\n    category: 'Smalltalk Examples'\nOften, most of this definition will be filled in by the environment. Notice that this is actually a message to the \"Object\"-class to create a subclass called \"MessagePublisher\". In other words: classes are first-class objects in Smalltalk which can receive messages just like any other object and can be created dynamically at execution time.\n\nMethods[edit]\nWhen an object receives a message, a method matching the message name is invoked. The following code defines a method publish, and so defines what will happen when this object receives the 'publish' message.\n\npublish\n    Transcript show: 'Hello World!'\nThe following method demonstrates receiving multiple arguments and returning a value:\n\nquadMultiply: i1 and: i2 \n    \"This method multiplies the given numbers by each other and the result by 4.\"\n    | mul |\n    mul := i1 * i2.\n    ^mul * 4\nThe method's name is #quadMultiply:and:. The return value is specified with the ^ operator.\n\nNote that objects are responsible for determining dynamically at runtime which method to execute in response to a message—while in many languages this may be (sometimes, or even always) determined statically at compile time.\n\nInstantiating classes[edit]\nThe following code:\n\nMessagePublisher new\ncreates (and returns) a new instance of the MessagePublisher class. This is typically assigned to a variable:\n\npublisher := MessagePublisher new\nHowever, it is also possible to send a message to a temporary, anonymous object:\n\nMessagePublisher new publish\nHello World example[edit]\nThe Hello world program is used by virtually all texts to new programming languages as the first program learned to show the most basic syntax and environment of the language. For Smalltalk, the program is extremely simple to write. The following code, the message \"show:\" is sent to the object \"Transcript\" with the String literal 'Hello, world!' as its argument. Invocation of the \"show:\" method causes the characters of its argument (the String literal 'Hello, world!') to be displayed in the transcript (\"terminal\") window.\n\nTranscript show: 'Hello, world!'.\nNote that a Transcript window would need to be open in order to see the results of this example.\n\nImage-based persistence[edit]\nMost popular programming systems separate static program code (in the form of class definitions, functions or procedures) from dynamic, or run time, program state (such as objects or other forms of program data). They load program code when a program starts, and any prior program state must be recreated explicitly from configuration files or other data sources. Any settings the program (and programmer) does not explicitly save must be set up again for each restart. A traditional program also loses much useful document information each time a program saves a file, quits, and reloads. This loses details such as undo history or cursor position. Image based systems don't force losing all that just because a computer is turned off, or an OS updates.\n\nMany Smalltalk systems, however, do not differentiate between program data (objects) and code (classes). In fact, classes are objects themselves. Therefore, most Smalltalk systems store the entire program state (including both Class and non-Class objects) in an image file. The image can then be loaded by the Smalltalk virtual machine to restore a Smalltalk-like system to a prior state.[27] This was inspired by FLEX, a language created by Alan Kay and described in his M.Sc. thesis.[28]\n\nSmalltalk images are similar to (restartable) core dumps and can provide the same functionality as core dumps, such as delayed or remote debugging with full access to the program state at the time of error. Other languages that model application code as a form of data, such as Lisp, often use image-based persistence as well. This method of persistence is powerful for rapid development because all the development information (e.g. parse trees of the program) is saved which facilitates debugging. However, it also has serious drawbacks as a true persistence mechanism. For one thing, developers may often want to hide implementation details and not make them available in a run time environment. For legal reasons as well as for maintenance reasons, allowing anyone to modify the program at run time inevitably introduces complexity and potential errors that would not be possible with a compiled system that does not expose source code in the run time environment. Also, while the persistence mechanism is easy to use it lacks the true persistence capabilities needed for most multi-user systems. The most obvious is the ability to do transactions with multiple users accessing the same database in parallel.[29]\n\nLevel of access[edit]\nEverything in Smalltalk-80 is available for modification from within a running program. This means that, for example, the IDE can be changed in a running system without restarting it. In some implementations, the syntax of the language or the garbage collection implementation can also be changed on the fly. Even the statement true become: false is valid in Smalltalk, although executing it is not recommended. When used judiciously, this level of flexibility allows for one of the shortest required times for new code to enter a production system.[citation needed]\n\nJust-in-time compilation[edit]\nMain article: Just-in-time compilation\nSmalltalk programs are usually compiled to bytecode, which is then interpreted by a virtual machine or dynamically translated into machine-native code.\n\nList of implementations[edit]\nAmber Smalltalk Smalltalk running atop JavaScript\nAthena, Smalltalk scripting engine for Java ≥ 1.6\nBistro\nCincom has the following Smalltalk products: ObjectStudio, VisualWorks and WebVelocity.\nVisual Smalltalk Enterprise, and family, including Smalltalk/V\nCuis Smalltalk, open source, modern Smalltalk-80 [3]\nF-Script\nGemTalk Systems, GemStone/s\nGNU Smalltalk\nÉtoilé Pragmatic Smalltalk, Smalltalk for Étoilé, a GNUstep-based user environment\nStepTalk, GNUstep scripting framework uses Smalltalk language on an Objective-C runtime\nGravel Smalltalk, a Smalltalk implementation for the JVM\nInstantiations, VA Smalltalk being the follow-on to IBM VisualAge Smalltalk\nVisualAge Smalltalk\nLittle Smalltalk\nObject Arts, Dolphin Smalltalk\nObject Connect, Smalltalk MT Smalltalk for Windows\nObjective-Smalltalk, Smalltalk on Objective-C runtime with extensions for Software Architecture\nLSW Vision-Smalltalk have partnered with Object Arts\nPanda Smalltalk, open source engine, written in C, has no dependencies except libc\nPharo Smalltalk, Pharo Project's open-source multi-platform Smalltalk\nPocket Smalltalk, runs on Palm Pilot\nRedline Smalltalk, runs on the Java virtual machine[30]\nRefactory, produces #Smalltalk\nSmalltalk YX\nSmalltalk/X[31]\nSqueak, open source Smalltalk\nCog, JIT VM written in Squeak Smalltalk\nCogDroid, port of non-JIT variant of Cog VM to Android\neToys, eToys visual programming system for learning\niSqueak, Squeak interpreter port for iOS devices, iPhone/iPad\nJSqueak, Squeak interpreter written in Java\nPotato, Squeak interpreter written in Java, a direct derivative of JSqueak\nRoarVM, RoarVM is a multi- and manycore interpreter for Squeak and Pharo\nStrongtalk, for Windows, offers optional strong typing\nVista Smalltalk\nCalmoSoft Project for Vista Smalltalk\nSee also[edit]\nObjective-C\nGLASS (software bundle)\nDistributed Data Management Architecture",
          "type": "compiled",
          "plid": 31
        }
      ],
      "havings": [
        {
          "plid": 1,
          "pdid": 3
        },
        {
          "plid": 11,
          "pdid": 21
        },
        {
          "plid": 13,
          "pdid": 16
        },
        {
          "plid": 14,
          "pdid": 16
        },
        {
          "plid": 14,
          "pdid": 6
        },
        {
          "plid": 15,
          "pdid": 6
        },
        {
          "plid": 15,
          "pdid": 34
        },
        {
          "plid": 15,
          "pdid": 67
        },
        {
          "plid": 16,
          "pdid": 64
        },
        {
          "plid": 16,
          "pdid": 62
        },
        {
          "plid": 16,
          "pdid": 67
        },
        {
          "plid": 16,
          "pdid": 34
        },
        {
          "plid": 16,
          "pdid": 45
        },
        {
          "plid": 17,
          "pdid": 34
        },
        {
          "plid": 17,
          "pdid": 33
        },
        {
          "plid": 17,
          "pdid": 67
        },
        {
          "plid": 18,
          "pdid": 33
        },
        {
          "plid": 18,
          "pdid": 16
        },
        {
          "plid": 18,
          "pdid": 64
        },
        {
          "plid": 20,
          "pdid": 16
        },
        {
          "plid": 21,
          "pdid": 64
        },
        {
          "plid": 21,
          "pdid": 34
        },
        {
          "plid": 22,
          "pdid": 64
        },
        {
          "plid": 22,
          "pdid": 60
        },
        {
          "plid": 22,
          "pdid": 34
        },
        {
          "plid": 22,
          "pdid": 16
        },
        {
          "plid": 23,
          "pdid": 34
        },
        {
          "plid": 23,
          "pdid": 16
        },
        {
          "plid": 23,
          "pdid": 64
        },
        {
          "plid": 25,
          "pdid": 45
        },
        {
          "plid": 25,
          "pdid": 64
        },
        {
          "plid": 25,
          "pdid": 16
        },
        {
          "plid": 25,
          "pdid": 34
        },
        {
          "plid": 27,
          "pdid": 33
        },
        {
          "plid": 27,
          "pdid": 34
        },
        {
          "plid": 27,
          "pdid": 16
        },
        {
          "plid": 27,
          "pdid": 64
        },
        {
          "plid": 27,
          "pdid": 45
        },
        {
          "plid": 28,
          "pdid": 45
        },
        {
          "plid": 28,
          "pdid": 64
        },
        {
          "plid": 28,
          "pdid": 34
        },
        {
          "plid": 28,
          "pdid": 16
        },
        {
          "plid": 28,
          "pdid": 33
        },
        {
          "plid": 29,
          "pdid": 64
        },
        {
          "plid": 29,
          "pdid": 34
        },
        {
          "plid": 29,
          "pdid": 16
        },
        {
          "plid": 29,
          "pdid": 45
        },
        {
          "plid": 30,
          "pdid": 16
        },
        {
          "plid": 30,
          "pdid": 64
        },
        {
          "plid": 30,
          "pdid": 34
        },
        {
          "plid": 31,
          "pdid": 64
        },
        {
          "plid": 26,
          "pdid": 34
        },
        {
          "plid": 26,
          "pdid": 16
        },
        {
          "plid": 26,
          "pdid": 64
        },
        {
          "plid": 26,
          "pdid": 62
        },
        {
          "plid": 26,
          "pdid": 45
        },
        {
          "plid": 26,
          "pdid": 33
        }
      ]
    }
  ]
}