{
  "libraries": [
    {
      "id": 1,
      "email": "abc@yahoo.sample.com",
      "name": "Sample library",
      "public": true,
      "paradigms": [
        {
          "pdid": 1,
          "name": "Action",
          "details": "In computer science, an action language is a language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world.[1] Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning. The best known action language is PDDL.[2]\n\nAction languages fall into two classes: action description languages and action query languages. Examples of the former include STRIPS, PDDL, Language A (a generalization of STRIPS; the propositional part of Pednault's ADL), Language B (an extension of A adding indirect effects, distinguishing static and dynamic laws) and Language C (which adds indirect effects also, and does not assume that every fluent is automatically \"inertial\"). There are also the Action Query Languages P, Q and R. Several different algorithms exist for converting action languages, and in particular, action language C, to answer set programs.[3][4] Since modern answer-set solvers make use of boolean SAT algorithms to very rapidly ascertain satisfiability, this implies that action languages can also enjoy the progress being made in the domain of boolean SAT solving.\n\nFormal definition[edit]\nAll action languages supplement the definition of a state transition system with a set F of fluents, a set V of values that fluents may take, and a function mapping S × F to V, where S is the set of states of a state transition system.",
          "subparadigms": []
        },
        {
          "pdid": 2,
          "name": "Agent-oriented",
          "details": "Agent-oriented programming (AOP) is a programming paradigm where the construction of the software is centered on the concept of software agents. In contrast to object-oriented programming which has objects (providing methods with variable parameters) at its core, AOP has externally specified agents (with interfaces and messaging capabilities) at its core. They can be thought of as abstractions of objects. Exchanged messages are interpreted by receiving \"agents\", in a way specific to its class of agents.\nHistory[edit]\nHistorically the concept of Agent-oriented programming and the idea of centering your software around the concept of agent was first used by Yoav Shoham within his Artificial Intelligence studies, in 1990.[1][2] His agents are specific to his own paradigm as they have just one method, with a single parameter. To quote Yoav Shoham from his paper in 1990 for a basic difference between of AOP against OOP:\n\n...agent-oriented programming (AOP), which can be viewed as a specialization of object-oriented programming. ...\nOOP\tAOP\nBasic unit\tobject\tagent\nParameters defining state of basic unit\tunconstrained\tbeliefs, commitments, capabilities, choices....\nProcess of computation\tmessage passing and response methods\tmessage passing and response methods\nTypes of message\tunconstrained\tinform, request, offer, promise, decline....\nFrameworks[edit]\nThere are multiple AOP 'frameworks' also called Agent Platforms that implement Shoham's programming paradigm. The following examples illustrate how a basic agent is programmed as a Hello World Program.\n\nJADE[edit]\nFor the Java-platform one of the frameworks is JADE [3] (http://jade.tilab.com/). Here is a very basic example [1] of an Agent that runs code\n\npackage helloworld;\nimport jade.core.Agent;\n\npublic class Hello extends Agent {\n\t\n\tprotected void setup() { \n\t\tSystem.out.println(\"Hello World. \");\n\t\tSystem.out.println(\"My name is \"+ getLocalName()); \n\t}\n\t\n\tpublic Hello() {\n\t\tSystem.out.println(\"Constructor called\");\n\t}\n\n}\nAt the core of JADE's AOP model is that its API supports the standard FIPA Agent Communication Language\n\nAgent Speak (Jason)[edit]\nFor a literal translation of Agent Oriented concepts into a scheme unobfuscated as is JADE, behind Java and Object Orientedness, Agent Speak [4] (Jason) provides a \"natural\" language for agents.\n\n\t\n\tstarted.\n\n\t+started <- .print(\"Hello World. \").\nSARL Language[edit]\nSARL[5] (SARL website) provides the fundamental abstractions for coding multiagent systems. It uses a script-like syntax (inspired by Scala and Ruby).\n\npackage helloworld\nimport io.sarl.core.Initialize\nagent HelloWorldAgent {\n        on Initialize {\t\n             println(\"Hello World.\")\n        }\n}\nMiddleware[edit]\nOne way to implement modular or extensible AOP support is to define standard AOP APIs to middleware functions that are themselves implemented as software agents. For example, a directory service can be implemented as a FIPA directory facilitator or DF software agent; life-cycle management to start, stop, suspend and resume agents can be implemented as a FIPA Agent Management Service or AMS agent.[6] A benefit of the AOP approach is that it support more dynamic roles between different users and providers of applications, services and networks. For example, traditionally, networks and services were usually managed by the network and service provider on behalf of the customer and offered as a single virtual network service but customers themselves are becoming more empowered to integrate and manage their own services. This can be achieved via AOP and APIs to middleware agents that can flexibly and dynamically manage communication.[7]",
          "subparadigms": []
        },
        {
          "pdid": 3,
          "name": "Array-oriented",
          "details": "In computer science, array programming languages (also known as vector or multidimensional languages) generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.\n\nArray programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon to find array programming language one-liners that require more than a couple of pages of Java code.[1]\n\nModern programming languages that support array programming are commonly used in scientific and engineering settings; these include Fortran 90, Mata, MATLAB, Analytica, TK Solver (as lists), Octave, R, Cilk Plus, Julia, and the NumPy extension to Python. In these languages, an operation that operates on entire arrays can be called a vectorized operation,[2] regardless of whether it is executed on a vector processor or not.\nContents  [hide] \n1\tConcepts\n2\tUses\n3\tLanguages\n3.1\tScalar languages\n3.2\tArray languages\n3.2.1\tAda\n3.2.2\tAnalytica\n3.2.3\tBASIC\n3.2.4\tMata\n3.2.5\tMATLAB\n3.2.6\trasql\n3.2.7\tR\n4\tMathematical reasoning and language notation\n5\tThird-party libraries\n6\tSee also\n7\tReferences\n8\tExternal links\nConcepts[edit]\nThe fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\n\nIverson described the rationale behind array programming (actually referring to APL) as follows:[3]\n\nmost programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician. [...]\n\nThe thesis [...] is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. [...] it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\n[...] Users of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted, since a clear statement of an algorithm can usually be used as a basis from which one may easily derive more efficient algorithm.\n\nThe basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\n\nFunction rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.\n\nUses[edit]\nArray programming is very well suited to implicit parallelization; a topic of much research nowadays. Further, Intel and compatible CPUs developed and produced after 1997 contained various instruction set extensions, starting from MMX and continuing through SSSE3 and 3DNow!, which include rudimentary SIMD array capabilities. Array processing is distinct from parallel processing in that one physical processor performs operations on a group of items simultaneously while parallel processing aims to split a larger problem into smaller ones (MIMD) to be solved piecemeal by numerous processors. Processors with two or more cores are increasingly common today.\n\nLanguages[edit]\nThe canonical examples of array programming languages are APL, J, and Fortran. Others include: D, A+, Analytica, Chapel, IDL, Julia, K, Q, Mata, Mathematica, MATLAB, MOLSF, NumPy, GNU Octave, PDL, R, S-Lang, SAC, Nial and ZPL.\n\nScalar languages[edit]\nIn scalar languages such as C and Pascal, operations apply only to single values, so a+b expresses the addition of two numbers. In such languages, adding one array to another requires indexing and looping, the coding of which is tedious and error-prone[citation needed].\n\nfor (i = 0; i < n; i++)\n    for (j = 0; j < n; j++)\n        a[i][j] += b[i][j];\nArray languages[edit]\nIn array languages, operations are generalized to apply to both scalars and arrays. Thus, a+b expresses the sum of two scalars if a and b are scalars, or the sum of two arrays if they are arrays.\n\nAn array language simplifies programming but possibly at a cost known as the abstraction penalty.[4][5][6] Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).\n\nAda[edit]\nThe previous C code would become the following in the Ada language,[7] which supports array-programming syntax.\n\n A := A + B;\nAnalytica[edit]\nAnalytica provides the same economy of expression as Ada.\n\n A := A + B;\nThis operation works whether operands, A or B, are scalar or arrays with one more dimensions. Each dimension is identified by an index variable, which controls the nature of the operation. The result has the union of the dimensions of the operands. If A and B have the same dimensions (indexes), the result has those same dimensions. If A and B are vectors with different dimensions, the resulting A is 2-dimensional, containing both dimensions, with each element the sum of the corresponding values of A and B. Variable A must be a local variable; Analytica, as a declarative language, avoids side effects by disallowing assignment to global variables.\n\nBASIC[edit]\nDartmouth BASIC had MAT statements for matrix and array manipulation in its third edition (1966).\n\n DIM A(4),B(4),C(4)\n MAT A = 1\n MAT B = 2*A\n MAT C = A + B\n MAT PRINT A,B,C\nMata[edit]\nStata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.\n\n. mata:\n\n: A = (1,2,3) \\(4,5,6)\n\n: A\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n: B = (2..4) \\(1..3)\n\n: B\n       1   2   3\n    +-------------+\n  1 |  2   3   4  |\n  2 |  1   2   3  |\n    +-------------+\n\n: C = J(3,2,1)           // A 3 by 2 matrix of ones\n\n: C\n       1   2\n    +---------+\n  1 |  1   1  |\n  2 |  1   1  |\n  3 |  1   1  |\n    +---------+\n\n: D = A + B\n\n: D\n       1   2   3\n    +-------------+\n  1 |  3   5   7  |\n  2 |  5   7   9  |\n    +-------------+\n\n: E = A*C\n\n: E\n        1    2\n    +-----------+\n  1 |   6    6  |\n  2 |  15   15  |\n    +-----------+\n\n: F = A:*B\n\n: F\n        1    2    3\n    +----------------+\n  1 |   2    6   12  |\n  2 |   4   10   18  |\n    +----------------+\n\n: G = E :+ 3\n\n: G\n        1    2\n    +-----------+\n  1 |   9    9  |\n  2 |  18   18  |\n    +-----------+\n\n: H = F[(2\\1), (1, 2)]    // Subscripting to get a submatrix of F and\n\n:                         // switch row 1 and 2\n: H\n        1    2\n    +-----------+\n  1 |   4   10  |\n  2 |   2    6  |\n    +-----------+\n\n: I = invsym(F'*F)        // Generalized inverse (F*F^(-1)F=F) of a\n\n:                         // symmetric positive semi-definite matrix\n: I\n[symmetric]\n                 1             2             3\n    +-------------------------------------------+\n  1 |            0                              |\n  2 |            0          3.25                |\n  3 |            0         -1.75   .9444444444  |\n    +-------------------------------------------+\n\n: end\nMATLAB[edit]\nThe implementation in MATLAB allows the same economy allowed by using the Ada language.\n\nA = A + B;\nA variant of the MATLAB language is the GNU Octave language, which extends the original language with augmented assignments:\n\nA += B;\nBoth MATLAB and GNU Octave natively support linear algebra operations such as matrix multiplication, matrix inversion, and the numerical solution of system of linear equations, even using the Moore–Penrose pseudoinverse.[8][9]\n\nThe Nial example of the inner product of two arrays can be implemented using the native matrix multiplication operator. If a is a row vector of size [1 n] and b is a corresponding column vector of size [n 1].\n\na * b;\nThe inner product between two matrices having the same number of elements can be implemented with the auxiliary operator (:), which reshapes a given matrix into a column vector, and the transpose operator ':\n\nA(:)' * B(:);\nrasql[edit]\nThe rasdaman query language is a database-oriented array-programming language. For example, two arrays could be added with the following query:\n\nSELECT A + B\nFROM   A, B\nR[edit]\nThe R language supports array paradigm by default. The following example illustrates a process of multiplication of two matrices followed by an addition of a scalar (which is, in fact, a one-element vector) and a vector:\n\n> A <- matrix(1:6, nrow=2)                              !!this has nrow=2 ... and A has 2 rows\n> A\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n> B <- t( matrix(6:1, nrow=2) )  # t() is a transpose operator                           !!this has nrow=2 ... and B has 3 rows --- a clear contradiction to the definition of A\n> B\n     [,1] [,2]\n[1,]    6    5\n[2,]    4    3\n[3,]    2    1\n> C <- A %*% B\n> C\n     [,1] [,2]\n[1,]   28   19\n[2,]   40   28\n> D <- C + 1\n> D\n     [,1] [,2]\n[1,]   29   20\n[2,]   41   29\n> D + c(1, 1)  # c() creates a vector\n     [,1] [,2]\n[1,]   30   21\n[2,]   42   30\nMathematical reasoning and language notation[edit]\nThe matrix left-division operator concisely expresses some semantic properties of matrices. As in the scalar equivalent, if the (determinant of the) coefficient (matrix) A is not null then it is possible to solve the (vectorial) equation A * x = b by left-multiplying both sides by the inverse of A: A−1 (in both MATLAB and GNU Octave languages: A^-1). The following mathematical statements hold when A is a full rank square matrix:\n\nA^-1 *(A * x)==A^-1 * (b)\n(A^-1 * A)* x ==A^-1 * b       (matrix-multiplication associativity)\nx = A^-1 * b\nwhere == is the equivalence relational operator. The previous statements are also valid MATLAB expressions if the third one is executed before the others (numerical comparisons may be false because of round-off errors).\n\nIf the system is overdetermined - so that A has more rows than columns - the pseudoinverse A+ (in MATLAB and GNU Octave languages: pinv(A)) can replace the inverse A−1, as follows:\n\npinv(A) *(A * x)==pinv(A) * (b)\n(pinv(A) * A)* x ==pinv(A) * b       (matrix-multiplication associativity)\nx = pinv(A) * b\nHowever, these solutions are neither the most concise ones (e.g. still remains the need to notationally differentiate overdetermined systems) nor the most computationally efficient. The latter point is easy to understand when considering again the scalar equivalent a * x = b, for which the solution x = a^-1 * b would require two operations instead of the more efficient x = b / a. The problem is that generally matrix multiplications are not commutative as the extension of the scalar solution to the matrix case would require:\n\n(a * x)/ a ==b / a\n(x * a)/ a ==b / a       (commutativity does not hold for matrices!)\nx * (a / a)==b / a       (associativity also holds for matrices)\nx = b / a\nThe MATLAB language introduces the left-division operator \\ to maintain the essential part of the analogy with the scalar case, therefore simplifying the mathematical reasoning and preserving the conciseness:\n\nA \\ (A * x)==A \\ b\n(A \\ A)* x ==A \\ b       (associativity also holds for matrices, commutativity is no more required)\nx = A \\ b\nThis is not only an example of terse array programming from the coding point of view but also from the computational efficiency perspective, which in several array programming languages benefits from quite efficient linear algebra libraries such as ATLAS or LAPACK.[10][11]\n\nReturning to the previous quotation of Iverson, the rationale behind it should now be evident:\n\nit is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\nThird-party libraries[edit]\nThe use of specialized and efficient libraries to provide more terse abstractions is also common in other programming languages. In C++ several linear algebra libraries exploit the language ability to overload operators. In some cases a very terse abstraction in those languages is explicitly influenced by the array programming paradigm, as the Armadillo and Blitz++ libraries do.[12][13]",
          "subparadigms": []
        },
        {
          "pdid": 4,
          "name": "Automata-based",
          "details": "Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). Sometimes a potentially infinite set of possible states is introduced, and such a set can have a complicated structure, not just an enumeration.\n\nFSM-based programming is generally the same, but, formally speaking, doesn't cover all possible variants, as FSM stands for finite state machine, and automata-based programming doesn't necessarily employ FSMs in the strict sense.\n\nThe following properties are key indicators for automata-based programming:\n\nThe time period of the program's execution is clearly separated down to the steps of the automaton. Each of the steps is effectively an execution of a code section (same for all the steps), which has a single entry point. Such a section can be a function or other routine, or just a cycle body. The step section might be divided down to subsections to be executed depending on different states, although this is not necessary.\nAny communication between the steps is only possible via the explicitly noted set of variables named the state. Between any two steps, the program (or its part created using the automata-based technique) can not have implicit components of its state, such as local (stack) variables' values, return addresses, the current instruction pointer, etc. That is, the state of the whole program, taken at any two moments of entering the step of the automaton, can only differ in the values of the variables being considered as the state of the automaton.\nThe whole execution of the automata-based code is a (possibly explicit) cycle of the automaton's steps.\n\nAnother reason for using the notion of automata-based programming is that the programmer's style of thinking about the program in this technique is very similar to the style of thinking used to solve mathematical tasks using Turing machines, Markov algorithms, etc.\n\nContents  [hide] \n1\tExample\n1.1\tTraditional (imperative) program in C\n1.2\tAutomata-based style program\n1.3\tA separate function for the automation step\n1.4\tExplicit state transition table\n1.5\tAutomation and Automata\n1.5.1\tExample Program\n1.5.2\tAutomation & Events\n1.6\tUsing object-oriented capabilities\n2\tApplications\n3\tHistory\n4\tCompared against imperative and procedural programming\n5\tObject-oriented programming relationship\n6\tSee also\n7\tReferences\n8\tExternal links\nExample[edit]\nConsider a program in C that reads a text from standard input stream, line by line, and prints the first word of each line. It is clear we need first to read and skip the leading spaces, if any, then read characters of the first word and print them until the word ends, and then read and skip all the remaining characters until the end-of-line character is encountered. Upon reaching the end of line character (regardless of the stage), we restart the algorithm from the beginning, and upon encountering the end of file condition (regardless of the stage), we terminate the program.\n\nTraditional (imperative) program in C[edit]\nThe program which solves the example task in traditional (imperative) style can look something like this:\n\n#include <stdio.h>\n#include <ctype.h>\nint main(void)\n{\n    int c;\n    do {\n        do\n            c = getchar();\n        while(c == ' ');\n        while(c != EOF && !isspace(c) && c != '\\n') {\n            putchar(c);\n            c = getchar();\n        }\n        putchar('\\n');\n        while(c != EOF && c != '\\n')\n            c = getchar();\n    } while(c != EOF);\n    return 0;\n}\nAutomata-based style program[edit]\nThe same task can be solved by thinking in terms of finite state machines. Note that line parsing has three stages: skipping the leading spaces, printing the word and skipping the trailing characters. Let's call them states before, inside and after. The program may now look like this:\n\n#include <stdio.h>\n#include <ctype.h>\nint main(void)\n{\n    enum states {\n        before, inside, after\n    } state;\n    int c;\n    state = before;\n    while((c = getchar()) != EOF) {\n        switch(state) {\n            case before:\n                if(c != ' ') {\n                    putchar(c);\n                    if(c != '\\n')\n                        state = inside;\n                }\n                break; \n            case inside:\n                if(!isspace(c))\n                    putchar(c);\n                else {\n                    putchar('\\n');\n                    if(c == '\\n')\n                        state = before;\n                    else\n                        state = after;\n                }\n                break;\n            case after:\n                if(c == '\\n')\n                    state = before;\n        }\n    }\n    return 0;\n}\nAlthough the code now looks longer, it has at least one significant advantage: there's only one reading (that is, call to the getchar() function) instruction in the program. Besides that, there's only one loop instead of the four the previous versions had.\n\nIn this program, the body of the while loop is the automaton step, and the loop itself is the cycle of the automaton's work.\n\nAutomaton's diagram\nThe program implements (models) the work of a finite state machine shown on the picture. The N denotes the end of line character, the S denotes spaces, and the A stands for all the other characters. The automaton follows exactly one arrow on each step depending on the current state and the encountered character. Some state switches are accompanied with printing the character; such arrows are marked with asterisks.\n\nIt is not absolutely necessary to divide the code down to separate handlers for each unique state. Furthermore, in some cases the very notion of the state can be composed of several variables' values, so that it could be impossible to handle each possible state explicitly. In the discussed program it is possible to reduce the code length by noticing that the actions taken in response to the end of line character are the same for all the possible states. The following program is equal to the previous one but is a bit shorter:\n\n#include <stdio.h>\n#include <ctype.h>\nint main(void)\n{\n    enum states {\n        before, inside, after\n    } state;\n    int c;\n    state = before;\n    while((c = getchar()) != EOF) {\n        if(c == '\\n') {\n            putchar('\\n');\n            state = before;\n        } else\n        switch(state) {\n            case before:\n                if(c != ' ') {\n                    putchar(c);\n                    state = inside;\n                }\n                break;\n            case inside:\n                if(c == ' ') {\n                    state = after;\n                } else {\n                    putchar(c);\n                }\n                break;\n            case after:\n                break;\n        }\n    }\n    if(state != before)\n        putchar('\\n');\n    return 0;\n}\nA separate function for the automation step[edit]\nThe most important property of the previous program is that the automaton step code section is clearly localized. With a separate function for it, we can better demonstrate this property:\n\n#include <stdio.h>\nenum states { before, inside, after };\nvoid step(enum states *state, int c)\n{\n    if(c == '\\n') {\n        putchar('\\n');\n        *state = before;\n    } else\n    switch(*state) {\n        case before:\n            if(c != ' ') {\n                putchar(c);\n                *state = inside;\n            }\n            break;\n        case inside:\n            if(c == ' ') {\n                *state = after;\n            } else {\n                putchar(c);\n            }\n            break;\n        case after:\n            break;\n    }\n} \nint main(void)\n{\n    int c;\n    enum states state = before;\n    while((c = getchar()) != EOF) {\n        step(&state, c);\n    }\n    if(state != before)\n        putchar('\\n');\n    return 0;\n}\nThis example clearly demonstrates the basic properties of automata-based code:\n\ntime periods of automaton step executions may not overlap\nthe only information passed from the previous step to the next is the explicitly specified automaton state\nExplicit state transition table[edit]\nA finite automaton can be defined by an explicit state transition table. Generally speaking, an automata-based program code can naturally reflect this approach. In the program below there's an array named the_table, which defines the table. The rows of the table stand for three states, while columns reflect the input characters (first for spaces, second for the end of line character, and the last is for all the other characters).\n\nFor every possible combination, the table contains the new state number and the flag, which determines whether the automaton must print the symbol. In a real life task, this could be more complicated; e.g., the table could contain pointers to functions to be called on every possible combination of conditions.\n\n#include <stdio.h>\nenum states { before = 0, inside = 1, after = 2 };\nstruct branch {\n    unsigned char new_state:2;\n    unsigned char should_putchar:1;\n};\nstruct branch the_table[3][3] = {\n                 /* ' '         '\\n'        others */\n    /* before */ { {before,0}, {before,1}, {inside,1} },\n    /* inside */ { {after, 1}, {before,1}, {inside,1} },\n    /* after  */ { {after, 0}, {before,1}, {after, 0} }\n};\nvoid step(enum states *state, int c)\n{\n    int idx2 = (c == ' ') ? 0 : (c == '\\n') ? 1 : 2;\n    struct branch *b = & the_table[*state][idx2];\n    *state = (enum states)(b->new_state);\n    if(b->should_putchar) putchar(c);\n}\nAutomation and Automata[edit]\nAutomata-based programming indeed closely matches the programming needs found in the field of automation.\n\nA production cycle is commonly modelled as:\n\nA sequence of stages stepping according to input data (from captors).\nA set of actions performed depending on the current stage.\nVarious dedicated programming languages allow expressing such a model in more or less sophisticated ways.\n\nExample Program[edit]\nThe example presented above could be expressed according to this view like in the following program. Here pseudo-code uses such conventions:\n\n'set' and 'reset' respectively activate & inactivate a logic variable (here a stage)\n':' is assignment, '=' is equality test\nSPC : ' '\nEOL : '\\n'\n\nstates : (before, inside, after, end, endplusnl)\n\nsetState(c) {\n    if c=EOF then if inside or after then set endplusnl else set end\n    if before and (c!=SPC and c!=EOL) then set inside\n    if inside and (c=SPC or c=EOL) then set after\n    if after and c=EOL then set before\n}\n\ndoAction(c) {\n    if inside then write(c)\n    else if c=EOL or endplusnl then write(EOL)\n}\n\ncycle {\n    set before\n    loop {\n        c : readCharacter\n        setState(c)\n        doAction(c)\n    }\n    until end or endplusnl\n}\nThe separation of routines expressing cycle progression on one side, and actual action on the other (matching input & output) allows clearer and simpler code.\n\nAutomation & Events[edit]\nIn the field of automation, stepping from step to step depends on input data coming from the machine itself. This is represented in the program by reading characters from a text. In reality, those data inform about position, speed, temperature, etc. of critical elements of a machine.\n\nLike in GUI programming, changes in the machine state can thus be considered as events causing the passage from a state to another, until the final one is reached. The combination of possible states can generate a wide variety of events, thus defining a more complex production cycle. As a consequence, cycles are usually far to be simple linear sequences. There are commonly parallel branches running together and alternatives selected according to different events, schematically represented below:\n\n   s:stage   c:condition\n   \n   s1\n   |\n   |-c2\n   |\n   s2\n   |\n   ----------\n   |        |\n   |-c31    |-c32\n   |        |\n  s31       s32\n   |        |\n   |-c41    |-c42\n   |        |\n   ----------\n   |\n   s4\nUsing object-oriented capabilities[edit]\nIf the implementation language supports object-oriented programming, a simple refactoring is to encapsulate the automaton into an object, thus hiding its implementation details. For example, an object-oriented version in C++ of the same program is below. A more sophisticated refactoring could employ the State pattern.\n\n#include <stdio.h>\nclass StateMachine {\n    enum states { before = 0, inside = 1, after = 2 } state;\n    struct branch {\n        unsigned char new_state:2;\n        unsigned char should_putchar:1;\n    };\n    static struct branch the_table[3][3];\npublic:\n    StateMachine() : state(before) {}\n    void FeedChar(int c) {\n        int idx2 = (c == ' ') ? 0 : (c == '\\n') ? 1 : 2;\n        struct branch *b = & the_table[state][idx2];\n        state = (enum states)(b->new_state);\n        if(b->should_putchar) putchar(c);\n    }\n};\nstruct StateMachine::branch StateMachine::the_table[3][3] = {\n                 /* ' '         '\\n'        others */\n    /* before */ { {before,0}, {before,1}, {inside,1} },\n    /* inside */ { {after, 0}, {before,1}, {inside,1} },\n    /* after  */ { {after, 0}, {before,1}, {after, 0} }\n};\nint main(void)\n{\n    int c;\n    StateMachine machine;\n    while((c = getchar()) != EOF)\n        machine.FeedChar(c);\n    return 0;\n}\nNote: To minimize changes not directly related to the subject of the article, the input/output functions from the standard library of C are being used. Note the use of the ternary operator, which could also be implemented as if-else.\n\nApplications[edit]\nAutomata-based programming is widely used in lexical and syntactic analyses.[1]\n\nBesides that, thinking in terms of automata (that is, breaking the execution process down to automaton steps and passing information from step to step through the explicit state) is necessary for event-driven programming as the only alternative to using parallel processes or threads.\n\nThe notions of states and state machines are often used in the field of formal specification. For instance, UML-based software architecture development uses state diagrams to specify the behaviour of the program. Also various communication protocols are often specified using the explicit notion of state (see, e.g., RFC 793[2]).\n\nThinking in terms of automata (steps and states) can also be used to describe semantics of some programming languages. For example, the execution of a program written in the Refal language is described as a sequence of steps of a so-called abstract Refal machine; the state of the machine is a view (an arbitrary Refal expression without variables).\n\nContinuations in the Scheme language require thinking in terms of steps and states, although Scheme itself is in no way automata-related (it is recursive). To make it possible the call/cc feature to work, implementation needs to be able to catch a whole state of the executing program, which is only possible when there's no implicit part in the state. Such a caught state is the very thing called continuation, and it can be considered as the state of a (relatively complicated) automaton. The step of the automaton is deducing the next continuation from the previous one, and the execution process is the cycle of such steps.\n\nAlexander Ollongren in his book[3] explains the so-called Vienna method of programming languages semantics description which is fully based on formal automata.\n\nThe STAT system [1] is a good example of using the automata-based approach; this system, besides other features, includes an embedded language called STATL which is purely automata-oriented.\n\nHistory[edit]\nAutomata-based techniques were used widely in the domains where there are algorithms based on automata theory, such as formal language analyses.[1]\n\nOne of the early papers on this is by Johnson et al., 1968.[4]\n\nOne of the earliest mentions of automata-based programming as a general technique is found in the paper by Peter Naur, 1963.[5] The author calls the technique Turing machine approach, however no real Turing machine is given in the paper; instead, the technique based on states and steps is described.\n\nCompared against imperative and procedural programming[edit]\nThe notion of state is not exclusive property of automata-based programming.[6] Generally speaking, state (or program state) appears during execution of any computer program, as a combination of all information that can change during the execution. For instance, a state of a traditional imperative program consists of\n\nvalues of all variables and the information stored within dynamic memory\nvalues stored in registers\nstack contents (including local variables' values and return addresses)\ncurrent value of the instruction pointer\nThese can be divided to the explicit part (such as values stored in variables) and the implicit part (return addresses and the instruction pointer).\n\nHaving said this, an automata-based program can be considered as a special case of an imperative program, in which implicit part of the state is minimized. The state of the whole program taken at the two distinct moments of entering the step code section can differ in the automaton state only. This simplifies the analysis of the program.\n\nObject-oriented programming relationship[edit]\nIn the theory of object-oriented programming an object is said to have an internal state and is capable of receiving messages, responding to them, sending messages to other objects and changing the internal state during message handling. In more practical terminology, to call an object's method is considered the same as to send a message to the object.\n\nThus, on the one hand, objects from object-oriented programming can be considered as automata (or models of automata) whose state is the combination of internal fields, and one or more methods are considered to be the step. Such methods must not call each other nor themselves, neither directly nor indirectly, otherwise the object can not be considered to be implemented in an automata-based manner.\n\nOn the other hand, it is obvious that object is good for implementing a model of an automaton. When the automata-based approach is used within an object-oriented language, an automaton model is usually implemented by a class, the state is represented with internal (private) fields of the class, and the step is implemented as a method; such a method is usually the only non-constant public method of the class (besides constructors and destructors). Other public methods could query the state but don't change it. All the secondary methods (such as particular state handlers) are usually hidden within the private part of the class.",
          "subparadigms": []
        },
        {
          "pdid": 5,
          "name": "Relativistic programming",
          "details": "Relativistic programming (RP) is a style of concurrent programming where instead of trying to avoid conflicts between readers and writers (or writers and writers in some cases) the algorithm is designed to tolerate them and get a correct result regardless of the order of events. Also, relativistic programming algorithms are designed to work without the presences of a global order of events. That is, there may be some cases where one thread sees two events in a different order than another thread (hence the term relativistic because in Einstein's theory of special relativity[citation needed] the order of events is not always the same to different viewers).\n\nRelativistic programming provides advantages in performance compared to other concurrency paradigms because it does not require one thread to wait for another nearly as often. Because of this, forms of it (Read-Copy-Update for instance) are now used extensively in the Linux kernel (over 9,000 times as of March 2014 and has grown from nothing to 8% of all locking primitives in about a decade).[1]",
          "subparadigms": []
        },
        {
          "pdid": 6,
          "name": "Concurrent computing",
          "details": "Concurrent computing is a form of computing in which several computations are executed during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or \"thread of control\" for each computation (\"process\"). A concurrent system is one where a computation can advance without waiting for all other computations to complete; where more than one computation can advance at the same time.[1]\n\nAs a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.\n\nContents  [hide] \n1\tIntroduction\n1.1\tCoordinating access to shared resources\n1.2\tAdvantages\n2\tModels\n3\tImplementation\n3.1\tInteraction and communication\n4\tHistory\n5\tPrevalence\n6\tLanguages supporting it\n7\tSee also\n8\tNotes\n9\tReferences\n10\tFurther reading\n11\tExternal links\nIntroduction[edit]\nSee also: Parallel computing\nConcurrent computing is related to but distinct from parallel computing, though these concepts are frequently confused,[2][3] and both can be described as \"multiple processes executing during the same period of time\". In parallel computing, execution occurs at the same physical instant, for example on separate processors of a multi-processor machine, with the goal of speeding up computations—parallel computing is impossible on a (one-core) single processor, as only one computation can occur at any instant (during any single clock cycle).[a] By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.[4]:1\n\nFor example, concurrent processes can be executed on one core by interleaving the execution steps of each process via time-sharing slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.\n\nConcurrent computations may be executed in parallel,[2][5] for example by assigning each process to a separate processor or processor core, or distributing a computation across a network, but in general, the languages, tools and techniques for parallel programming may not be suitable for concurrent programming, and vice versa.\n\nThe exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:\n\nT1 may be executed and finished before T2 or vice versa (serial and sequential);\nT1 and T2 may be executed alternately (serial and concurrent);\nT1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent).\nThe word \"sequential\" is used as an antonym for both \"concurrent\" and \"parallel\"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs.[6] A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called a serial schedule. A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control.\n\nCoordinating access to shared resources[edit]\nThe main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.[5] Potential problems include race conditions, deadlocks, and resource starvation. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource balance:\n\n1 bool withdraw(int withdrawal)\n2 {\n3 \n    if (balance >= withdrawal)\n4     {\n5 \n        balance -= withdrawal;\n6         return true;\n7     } \n8     return false;\n9 }\nSuppose balance = 500, and two concurrent threads make the calls withdraw(300) and withdraw(350). If line 3 in both operations executes before line 5 both operations will find that balance >= withdrawal evaluates to true, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources need the use of concurrency control, or non-blocking algorithms.\n\nBecause concurrent systems rely on the use of shared resources (including communication media), concurrent computing in general needs the use of some form of arbiter somewhere in the implementation to mediate access to these resources.\n\nUnfortunately, while many solutions exist to the problem of a conflict over one resource, many of those \"solutions\" have their own concurrency problems such as deadlock when more than one resource is involved.\n\nAdvantages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)\nIncreased program throughput—parallel execution of a concurrent program allows the number of tasks completed in a given time to increase.\nHigh responsiveness for input/output—input/output-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.\nMore appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes.\nModels[edit]\nThere are several models of concurrent computing, which can be used to understand and analyze concurrent systems. These models include:\n\nActor model\nObject-capability model for security\nPetri nets\nProcess calculi such as\nAmbient calculus\nCalculus of communicating systems (CCS)\nCommunicating sequential processes (CSP)\nπ-calculus\nJoin-calculus\nInput/output automaton\nImplementation[edit]\n[icon]\tThis section needs expansion. You can help by adding to it. (February 2014)\nA number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process.\n\nInteraction and communication[edit]\nIn some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. Explicit communication can be divided into two classes:\n\nShared memory communication\nConcurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). This style of concurrent programming usually needs the use of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. A program that properly implements any of these is said to be thread-safe.\nMessage passing communication\nConcurrent components communicate by exchanging messages (exemplified by Scala, Erlang and occam). The exchange of messages may be carried out asynchronously, or may use a synchronous \"rendezvous\" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as \"send and pray\"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.[citation needed] A wide variety of mathematical theories to understand and analyze message-passing systems are available, including the actor model, and various process calculi. Message passing can be efficiently implemented via symmetric multiprocessing, with or without shared memory cache coherence.\nShared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors.\n\nHistory[edit]\nConcurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via time-division multiplexing (1870s).\n\nThe academic study of concurrent algorithms started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.[7]\n\nPrevalence[edit]\nConcurrency is pervasive in computing, occurring from low-level hardware on a single chip to world-wide networks. Examples follow.\n\nAt the programming language level:\n\nChannel\nCoroutine\nFutures and promises\nAt the operating system level:\n\nComputer multitasking, including both cooperative multitasking and preemptive multitasking\nTime-sharing, which replaced sequential batch processing of jobs with concurrent use of a system\nProcess\nThread\nAt the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.\n\nLanguages supporting it[edit]\nConcurrent programming languages are programming languages that use language constructs for concurrency. These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. Such languages are sometimes described as Concurrency Oriented Languages or Concurrency Oriented Programming Languages (COPL).[8]\n\nToday, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.[citation needed]\n\nMany concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include:\n\nAda—general purpose, with native support for message passing and monitor based concurrency\nAlef—concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs\nAlice—extension to Standard ML, adds support for concurrency via futures\nAteji PX—extension to Java with parallel primitives inspired from π-calculus\nAxum—domain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntax\nC++—std::thread\nCω (C omega)—for research, extends C#, uses asynchronous communication\nC#—supports concurrent computing since version 5.0 using lock, yield, async and await keywords\nClojure—modern Lisp for the JVM\nConcurrent Clean—functional programming, similar to Haskell\nConcurrent Collections (CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and control\nConcurrent Haskell—lazy, pure functional language operating concurrent processes on shared memory\nConcurrent ML—concurrent extension of Standard ML\nConcurrent Pascal—by Per Brinch Hansen\nCurry\nD—multi-paradigm system programming language with explicit support for concurrent programming (actor model)\nE—uses promises to preclude deadlocks\nECMAScript—promises available in various libraries, proposed for inclusion in standard in ECMAScript 6\nEiffel—through its SCOOP mechanism based on the concepts of Design by Contract\nElixir—dynamic and functional meta-programming aware language running on the Erlang VM.\nErlang—uses asynchronous message passing with nothing shared\nFAUST—real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler\nFortran—coarrays and do concurrent are part of Fortran 2008 standard\nGo—for system programming, with a concurrent programming model based on CSP\nHume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing\nIo—actor-based concurrency\nJanus—features distinct askers and tellers to logical variables, bag channels; is purely declarative\nJava—Thread class or Runnable interface.\nJavaScript—via web workers, in a browser environment, promises, and callbacks.\nJoCaml—concurrent and distributed channel based, extension of OCaml, implements the Join-calculus of processes\nJoin Java—concurrent, based on Java language\nJoule—dataflow-based, communicates by message passing\nJoyce—concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen\nLabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language\nLimbo—relative of Alef, for system programming in Inferno (operating system)\nMultiLisp—Scheme variant extended to support parallelism\nModula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutines\nModula-3—modern member of Algol family with extensive support for threads, mutexes, condition variables\nNewsqueak—for research, with channels as first-class values; predecessor of Alef\nNode.js—a server-side runtime environment for JavaScript\noccam—influenced heavily by communicating sequential processes (CSP)\noccam-π—a modern variant of occam, which incorporates ideas from Milner's π-calculus\nOrc—heavily concurrent, nondeterministic, based on Kleene algebra\nOz-Mozart—multiparadigm, supports shared-state and message-passing concurrency, and futures\nParaSail—object-oriented, parallel, free of pointers, race conditions\nPict—essentially an executable implementation of Milner's π-calculus\nPerl with AnyEvent and Coro\nPython with Twisted, greenlet and gevent\nReia—uses asynchronous message passing between shared-nothing objects\nRed/System—for system programming, based on Rebol\nRuby with Concurrent Ruby and Celluloid\nRust—for system programming, focus on massive concurrency, using message-passing with move semantics, shared immutable memory, and shared mutable memory that is provably free of race conditions.[9]\nSALSA—actor-based with token-passing, join, and first-class continuations for distributed computing over the Internet\nScala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way\nSequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions\nSR—for research\nStackless Python\nStratifiedJS—combinator-based concurrency, based on JavaScript\nSuperPascal—concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen\nUnicon—for research\nTermite Scheme—adds Erlang-like concurrency to Scheme\nTNSDL—for developing telecommunication exchanges, uses asynchronous message passing\nVHSIC Hardware Description Language (VHDL)—IEEE STD-1076\nXC—concurrency-extended subset of C language developed by XMOS, based on communicating sequential processes, built-in constructs for programmable I/O\nMany other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.",
          "subparadigms": [
            5
          ]
        },
        {
          "pdid": 7,
          "name": "Concurrent constraint logic programming",
          "details": "Concurrent constraint logic programming is a version of constraint logic programming aimed primarily at programming concurrent processes rather than (or in addition to) solving constraint satisfaction problems. Goals in constraint logic programming are evaluated concurrently; a concurrent process is therefore programmed as the evaluation of a goal by the interpreter.\n\nSyntactically, concurrent constraints logic programs are similar to non-concurrent programs, the only exception being that clauses include guards, which are constraints that may block the applicability of the clause under some conditions. Semantically, concurrent constraint logic programming differs from its non-concurrent versions because a goal evaluation is intended to realize a concurrent process rather than finding a solution to a problem. Most notably, this difference affects how the interpreter behaves when more than one clause is applicable: non-concurrent constraint logic programming recursively tries all clauses; concurrent constraint logic programming chooses only one. This is the most evident effect of an intended directionality of the interpreter, which never revise a choice it has previously taken. Other effects of this are the semantical possibility of having a goal that cannot be proved while the whole evaluation does not fail, and a particular way for equating a goal and a clause head.\n\nConstraint handling rules can be seen as a form of concurrent constraint logic programming, but are used for programming a constraint simplifier or solver rather than concurrent processes.\n\nContents  [hide] \n1\tDescription\n2\tHistory\n3\tSee also\n4\tReferences\nDescription[edit]\nIn constraint logic programming, the goals in the current goal are evaluated sequentially, usually proceeding in a LIFO order in which newer goals are evaluated first. The concurrent version of logic programming allows for evaluating goals in parallel: every goal is evaluated by a process, and processes run concurrently. These processes interact via the constraint store: a process can add a constraint to the constraint store while another one checks whether a constraint is entailed by the store.\n\nAdding a constraint to the store is done like in regular constraint logic programming. Checking entailment of a constraint is done via guards to clauses. Guards require a syntactic extension: a clause of concurrent constraint logic programming is written as H :- G | B where G is a constraint called the guard of the clause. Roughly speaking, a fresh variant of this clause can be used to replace a literal in the goal only if the guard is entailed by the constraint store after the equation of the literal and the clause head is added to it. The precise definition of this rule is more complicated, and is given below.\n\nThe main difference between non-concurrent and concurrent constraint logic programming is that the first is aimed at search, while the second is aimed at implementing concurrent processes. This difference affects whether choices can be undone, whether processes are allowed not to terminate, and how goals and clause heads are equated.\n\nThe first semantical difference between regular and concurrent constraint logic programming is about the condition when more than one clause can be used for proving a goal. Non-concurrent logic programming tries all possible clauses when rewriting a goal: if the goal cannot be proved while replacing it with the body of a fresh variant of a clause, another clause is proved, if any. This is because the aim is to prove the goal: all possible ways to prove the goal are tried. On the other hand, concurrent constraint logic programming aims at programming parallel processes. In general concurrent programming, if a process makes a choice, this choice cannot be undone. The concurrent version of constraint logic programming implements processes by allowing them to take choices, but committing to them once they have been taken. Technically, if more than one clause can be used to rewrite a literal in the goal, the non-concurrent version tries in turn all clauses, while the concurrent version chooses a single arbitrary clause: contrary to the non-concurrent version, the other clauses will never be tried. These two different ways for handling multiple choices are often called \"don't know nondeterminism\" and \"don't care nondeterminism\".\n\nWhen rewriting a literal in the goal, the only considered clauses are those whose guard is entailed by the union of the constraint store and the equation of the literal with the clause head. The guards provide a way for telling which clauses are not to be considered at all. This is particularly important given the commitment to a single clause of concurrent constraint logic programming: once a clause has been chosen, this choice will be never reconsidered. Without guards, the interpreter could choose a \"wrong\" clause to rewrite a literal, while other \"good\" clauses exist. In non-concurrent programming, this is less important, as the interpreter always tries all possibilities. In concurrent programming, the interpreter commits to a single possibility without trying the other ones.\n\nA second effect of the difference between the non-concurrent and the concurrent version is that concurrent constraint logic programming is specifically designed to allow processes to run without terminating. Non-terminating processes are common in general in concurrent processing; the concurrent version of constraint logic programming implements them by not using the condition of failure: if no clause is applicable for rewriting a goal, the process evaluating this goal stops instead of making the whole evaluation fail like in non-concurrent constraint logic programming. As a result, the process evaluating a goal may be stopped because no clause is available to proceed, but at the same time the other processes keep running.\n\nSynchronization among processes that are solving different goals is achieved via the use of guards. If a goal cannot be rewritten because all clauses that could be used have a guard that is not entailed by the constraint store, the process solving this goal is blocked until the other processes add the constraints that are necessary to entail the guard of at least one of the applicable clauses. This synchronization is subject to deadlocks: if all goals are blocked, no new constraints will be added and therefore no goal will ever be unblocked.\n\nA third effect of the difference between concurrent and non-concurrent logic programming is in the way a goal is equated to the head of a fresh variant of a clause. Operationally, this is done by checking whether the variables in the head can be equated to terms in such a way the head is equal to the goal. This rule differs from the corresponding rule for constraint logic programming in that it only allows adding constraints in the form variable=term, where the variable is one of the head. This limitation can be seen as a form of directionality, in that the goal and the clause head are treated differently.\n\nPrecisely, the rule telling whether a fresh variant H:-G|B of a clause can be used to rewrite a goal A is as follows. First, it is checked whether A and H have the same predicate. Second, it is checked whether there exists a way for equating {\\displaystyle A} A with {\\displaystyle H} H given the current constraint store; contrary to regular logic programming, this is done under one-sided unification, which only allows a variable of the head to be equal to a term. Third, the guard is checked for entailment from the constraint store and the equations generated in the second step; the guard may contain variables that are not mentioned in the clause head: these variables are interpreted existentially. This method for deciding the applicability of a fresh variant of a clause for replacing a goal can be compactly expressed as follows: the current constraint store entails that there exists an evaluation of the variables of the head and the guard such that the head is equal to the goal and the guard is entailed. In practice, entailment may be checked with an incomplete method.\n\nAn extension to the syntax and semantics of concurrent logic programming is the atomic tell. When the interpreter uses a clause, its guard is added to the constraint store. However, also added are the constraints of the body. Due to commitment to this clause, the interpreter does not backtrack if the constraints of the body are inconsistent with the store. This condition can be avoided by the use of atomic tell, which is a variant in which the clause contain a sort of \"second guard\" that is only checked for consistency. Such a clause is written H :- G:D|B. This clause is used to rewrite a literal only if G is entailed by the constraint store and D is consistent with it. In this case, both G and D are added to the constraint store.\n\nHistory[edit]\nThe study of concurrent constraint logic programming started at the end of the 1980s, when some of the principles of concurrent logic programming were integrated into constraint logic programming by Michael J. Maher. The theoretical properties of concurrent constraint logic programming were later studied by various authors, such as Vijay A. Saraswat.",
          "subparadigms": []
        },
        {
          "pdid": 8,
          "name": "Constraint logic programming",
          "details": "Constraint logic programming is a form of constraint programming, in which logic programming is extended to include concepts from constraint satisfaction. A constraint logic program is a logic program that contains constraints in the body of clauses. An example of a clause including a constraint is A(X,Y) :- X+Y>0, B(X), C(Y). In this clause, X+Y>0 is a constraint; A(X,Y), B(X), and C(Y) are literals as in regular logic programming. This clause states one condition under which the statement A(X,Y) holds: X+Y is greater than zero and both B(X) and C(Y) are true.\n\nAs in regular logic programming, programs are queried about the provability of a goal, which may contain constraints in addition to literals. A proof for a goal is composed of clauses whose bodies are satisfiable constraints and literals that can in turn be proved using other clauses. Execution is performed by an interpreter, which starts from the goal and recursively scans the clauses trying to prove the goal. Constraints encountered during this scan are placed in a set called constraint store. If this set is found out to be unsatisfiable, the interpreter backtracks, trying to use other clauses for proving the goal. In practice, satisfiability of the constraint store may be checked using an incomplete algorithm, which does not always detect inconsistency.\n\nContents  [hide] \n1\tOverview\n2\tSemantics\n3\tTerms and conditions\n3.1\tTree terms\n3.2\tReals\n3.3\tFinite domains\n4\tThe constraint store\n5\tLabeling\n6\tProgram reformulations\n7\tConstraint handling rules\n8\tBottom-up evaluation\n9\tConcurrent constraint logic programming\n10\tApplications\n11\tHistory\n12\tReferences\n13\tSee also\nOverview[edit]\nFormally, constraint logic programs are like regular logic programs, but the body of clauses can contain constraints, in addition to the regular logic programming literals. As an example, X>0 is a constraint, and is included in the last clause of the following constraint logic program.\n\n B(X,1):- X<0.\n B(X,Y):- X=1, Y>0.\n A(X,Y):- X>0, B(X,Y).\nLike in regular logic programming, evaluating a goal such as A(X,1) requires evaluating the body of the last clause with Y=1. Like in regular logic programming, this in turn requires proving the goal B(X,1). Contrary to regular logic programming, this also requires a constraint to be satisfied: X>0, the constraint in the body of the last clause (in regular logic programming, X>0 cannot be proved unless X is bound to a fully-ground term and execution of the program will fail if that is not the case).\n\nWhether a constraint is satisfied cannot always be determined when the constraint is encountered. In this case, for example, the value of X is not determined when the last clause is evaluated. As a result, the constraint X>0 is not satisfied nor violated at this point. Rather than proceeding in the evaluation of B(X,1) and then checking whether the resulting value of X is positive afterwards, the interpreter stores the constraint X>0 and then proceeds in the evaluation of B(X,1); this way, the interpreter can detect violation of the constraint X>0 during the evaluation of B(X,1), and backtrack immediately if this is the case, rather than waiting for the evaluation of B(X,1) to conclude.\n\nIn general, the evaluation of a constraint logic program proceeds like for a regular logic program, but constraints encountered during evaluation are placed in a set called constraint store. As an example, the evaluation of the goal A(X,1) proceeds by evaluating the body of the first clause with Y=1; this evaluation adds X>0 to the constraint store and requires the goal B(X,1) to be proved. While trying to prove this goal, the first clause is applicable, but its evaluation adds X<0 to the constraint store. This addition makes the constraint store unsatisfiable, and the interpreter backtracks, removing the last addition from the constraint store. The evaluation of the second clause adds X=1 and Y>0 to the constraint store. Since the constraint store is satisfiable and no other literal is left to prove, the interpreter stops with the solution X=1, Y=1.\n\nSemantics[edit]\nThe semantics of constraint logic programs can be defined in terms of a virtual interpreter that maintains a pair {\\displaystyle \\langle G,S\\rangle } \\langle G,S\\rangle  during execution. The first element of this pair is called current goal; the second element is called constraint store. The current goal contains the literals the interpreter is trying to prove and may also contain some constraints it is trying to satisfy; the constraint store contains all constraints the interpreter has assumed satisfiable so far.\n\nInitially, the current goal is the goal and the constraint store is empty. The interpreter proceeds by removing the first element from the current goal and analyzing it. The details of this analysis are explained below, but in the end this analysis may produce a successful termination or a failure. This analysis may involve recursive calls and addition of new literals to the current goal and new constraint to the constraint store. The interpreter backtracks if a failure is generated. A successful termination is generated when the current goal is empty and the constraint store is satisfiable.\n\nThe details of the analysis of a literal removed from the goal is as follows. After having removed this literal from the front of the goal, it is checked whether it is a constraint or a literal. If it is a constraint, it is added to the constraint store. If it is a literal, a clause whose head has the same predicate of the literal is chosen; the clause is rewritten by replacing its variables with new variables (variables not occurring in the goal): the result is called a fresh variant of the clause; the body of the fresh variant of the clause is then placed in front of the goal; the equality of each argument of the literal with the corresponding one of the fresh variant head is placed in front of the goal as well.\n\nSome checks are done during these operations. In particular, the constraint store is checked for consistency every time a new constraint is added to it. In principle, whenever the constraint store is unsatisfiable the algorithm could backtrack. However, checking unsatisfiability at each step would be inefficient. For this reason, an incomplete satisfiability checker may be used instead. In practice, satisfiability is checked using methods that simplify the constraint store, that is, rewrite it into an equivalent but simpler-to-solve form. These methods can sometimes but not always prove unsatisfiability of an unsatisfiable constraint store.\n\nThe interpreter has proved the goal when the current goal is empty and the constraint store is not detected unsatisfiable. The result of execution is the current set of (simplified) constraints. This set may include constraints such as {\\displaystyle X=2} X=2 that force variables to a specific value, but may also include constraints like {\\displaystyle X>2} X>2 that only bound variables without giving them a specific value.\n\nFormally, the semantics of constraint logic programming is defined in terms of derivations. A transition is a pair of pairs goal/store, noted {\\displaystyle \\langle G,S\\rangle \\rightarrow \\langle G',S'\\rangle } \\langle G,S\\rangle \\rightarrow \\langle G',S'\\rangle . Such a pair states the possibility of going from state {\\displaystyle \\langle G,S\\rangle } \\langle G,S\\rangle  to state {\\displaystyle \\langle G',S'\\rangle } \\langle G',S'\\rangle . Such a transition is possible in three possible cases:\n\nan element of {\\displaystyle G} G is a constraint {\\displaystyle C} C, {\\displaystyle G'=G\\backslash \\{C\\}} G'=G\\backslash \\{C\\} and {\\displaystyle S'=S\\cup \\{C\\}} S'=S\\cup \\{C\\}; in other words, a constraint can be moved from the goal to the constraint store\nan element of {\\displaystyle G} G is a literal {\\displaystyle L(t_{1},\\ldots ,t_{n})} L(t_{1},\\ldots ,t_{n}), there exists a clause that, rewritten using new variables, is {\\displaystyle L(t_{1}',\\ldots ,t_{n}'):-B} L(t_{1}',\\ldots ,t_{n}'):-B, {\\displaystyle G'} G' is {\\displaystyle G} G with {\\displaystyle L(t_{1},\\ldots ,t_{n})} L(t_{1},\\ldots ,t_{n}) replaced by {\\displaystyle t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}',B} t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}',B, and {\\displaystyle S'=S} S'=S; in other words, a literal can be replaced by the body of a fresh variant of a clause having the same predicate in the head, adding the body of the fresh variant and the above equalities of terms to the goal\n{\\displaystyle S} S and {\\displaystyle S'} S' are equivalent according to the specific constraint semantics\nA sequence of transitions is a derivation. A goal {\\displaystyle G} G can be proved if there exists a derivation from {\\displaystyle \\langle G,\\emptyset \\rangle } \\langle G,\\emptyset \\rangle  to {\\displaystyle \\langle \\emptyset ,S\\rangle } \\langle \\emptyset ,S\\rangle  for some satisfiable constraint store {\\displaystyle S} S. This semantics formalizes the possible evolutions of an interpreter that arbitrarily chooses the literal of the goal to process and the clause to replace literals. In other words, a goal is proved under this semantics if there exists a sequence of choices of literals and clauses, among the possibly many ones, that lead to an empty goal and satisfiable store.\n\nActual interpreters process the goal elements in a LIFO order: elements are added in the front and processed from the front. They also choose the clause of the second rule according to the order in which they are written, and rewrite the constraint store when it is modified.\n\nThe third possible kind of transition is a replacement of the constraint store with an equivalent one. This replacement is limited to those done by specific methods, such as constraint propagation. The semantics of constraint logic programming is parametric not only to the kind of constraints used but also to the method for rewriting the constraint store. The specific methods used in practice replace the constraint store with one that is simpler to solve. If the constraint store is unsatisfiable, this simplification may detect this unsatisfiability sometimes, but not always.\n\nThe result of evaluating a goal against a constraint logic program is defined if the goal is proved. In this case, there exists a derivation from the initial pair to a pair where the goal is empty. The constraint store of this second pair is considered the result of the evaluation. This is because the constraint store contains all constraints assumed satisfiable to prove the goal. In other words, the goal is proved for all variable evaluations that satisfy these constraints.\n\nThe pairwise equality of terms of two literals is often compactly denoted by {\\displaystyle L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}')} L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}'): this is a shorthand for the constraints {\\displaystyle t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}'} t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}'. A common variant of the semantics for constraint logic programming adds {\\displaystyle L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}')} L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}') directly to the constraint store rather than to the goal.\n\nTerms and conditions[edit]\nDifferent definitions of terms are used, generating different kinds of constraint logic programming: over trees, reals, or finite domains. A kind of constraint that is always present is the equality of terms. Such constraints are necessary because the interpreter adds t1=t2 to the goal whenever a literal P(...t1...) is replaced with the body of a clause fresh variant whose head is P(...t2...).\n\nTree terms[edit]\nConstraint logic programming with tree terms emulates regular logic programming by storing substitutions as constraints in the constraint store. Terms are variables, constants, and function symbols applied to other terms. The only constraints considered are equalities and disequalities between terms. Equality is particularly important, as constraints like t1=t2 are often generated by the interpreter. Equality constraints on terms can be simplified, that is solved, via unification:\n\nA constraint t1=t2 can be simplified if both terms are function symbols applied to other terms. If the two function symbols are the same and the number of subterms is also the same, this constraint can be replaced with the pairwise equality of subterms. If the terms are composed of different function symbols or the same functor but on different number of terms, the constraint is unsatisfiable.\n\nIf one of the two terms is a variable, the only allowed value the variable can take is the other term. As a result, the other term can replace the variable in the current goal and constraint store, thus practically removing the variable from consideration. In the particular case of equality of a variable with itself, the constraint can be removed as always satisfied.\n\nIn this form of constraint satisfaction, variable values are terms.\n\nReals[edit]\nConstraint logic programming with real numbers uses real expressions as terms. When no function symbols are used, terms are expressions over reals, possibly including variables. In this case, each variable can only take a real number as a value.\n\nTo be precise, terms are expressions over variables and real constants. Equality between terms is a kind of constraint that is always present, as the interpreter generates equality of terms during execution. As an example, if the first literal of the current goal is A(X+1) and the interpreter has chosen a clause that is A(Y-1):-Y=1 after rewriting is variables, the constraints added to the current goal are X+1=Y-1 and {\\displaystyle Y=1} Y=1. The rules of simplification used for function symbols are obviously not used: X+1=Y-1 is not unsatisfiable just because the first expression is built using + and the second using -.\n\nReals and function symbols can be combined, leading to terms that are expressions over reals and function symbols applied to other terms. Formally, variables and real constants are expressions, as any arithmetic operator over other expressions. Variables, constants (zero-arity-function symbols), and expressions are terms, as any function symbol applied to terms. In other words, terms are built over expressions, while expressions are built over numbers and variables. In this case, variables ranges over real numbers and terms. In other words, a variable can take a real number as a value, while another takes a term.\n\nEquality of two terms can be simplified using the rules for tree terms if none of the two terms is a real expression. For example, if the two terms have the same function symbol and number of subterms, their equality constraint can be replaced with the equality of subterms.\n\nFinite domains[edit]\nSee also: Constraint satisfaction problem\nThe third class of constraints used in constraint logic programming is that of finite domains. Values of variables are in this case taken from a finite domain, often that of integer numbers. For each variable, a different domain can be specified: X::[1..5] for example means that the value of X is between 1 and 5. The domain of a variable can also be given by enumerating all values a variable can take; therefore, the above domain declaration can be also written X::[1,2,3,4,5]. This second way of specifying a domain allows for domains that are not composed of integers, such as X::[george,mary,john]. If the domain of a variable is not specified, it is assumed to be the set of integers representable in the language. A group of variables can be given the same domain using a declaration like [X,Y,Z]::[1..5].\n\nThe domain of a variable may be reduced during execution. Indeed, as the interpreter adds constraints to the constraint store, it performs constraint propagation to enforce a form of local consistency, and these operations may reduce the domain of variables. If the domain of a variable becomes empty, the constraint store is inconsistent, and the algorithm backtracks. If the domain of a variable becomes a singleton, the variable can be assigned the unique value in its domain. The forms of consistency typically enforced are arc consistency, hyper-arc consistency, and bound consistency. The current domain of a variable can be inspected using specific literals; for example, dom(X,D) finds out the current domain D of a variable X.\n\nAs for domains of reals, functors can be used with domains of integers. In this case, a term can be an expression over integers, a constant, or the application of a functor over other terms. A variable can take an arbitrary term as a value, if its domain has not been specified to be a set of integers or constants.\n\nThe constraint store[edit]\nThe constraint store contains the constraints that are currently assumed satisfiable. It can be considered what the current substitution is for regular logic programming. When only tree terms are allowed, the constraint store contains constraints in the form t1=t2; these constraints are simplified by unification, resulting in constraints of the form variable=term; such constraints are equivalent to a substitution.\n\nHowever, the constraint store may also contain constraints in the form t1!=t2, if the difference != between terms is allowed. When constraints over reals or finite domains are allowed, the constraint store may also contain domain-specific constraints like X+2=Y/2, etc.\n\nThe constraint store extends the concept of current substitution in two ways. First, it does not only contain the constraints derived from equating a literal with the head of a fresh variant of a clause, but also the constraints of the body of clauses. Second, it does not only contain constraints of the form variable=value but also constraints on the considered constraint language. While the result of a successful evaluation of a regular logic program is the final substitution, the result for a constraint logic program is the final constraint store, which may contain constraint of the form variable=value but in general may contain arbitrary constraints.\n\nDomain-specific constraints may come to the constraint store both from the body of a clauses and from equating a literal with a clause head: for example, if the interpreter rewrites the literal A(X+2) with a clause whose fresh variant head is A(Y/2), the constraint X+2=Y/2 is added to the constraint store. If a variable appears in a real or finite domain expression, it can only take a value in the reals or the finite domain. Such a variable cannot take a term made of a functor applied to other terms as a value. The constraint store is unsatisfiable if a variable is bound to take both a value of the specific domain and a functor applied to terms.\n\nAfter a constraint is added to the constraint store, some operations are performed on the constraint store. Which operations are performed depends on the considered domain and constraints. For example, unification is used for finite tree equalities, variable elimination for polynomial equations over reals, constraint propagation to enforce a form of local consistency for finite domains. These operations are aimed at making the constraint store simpler to be checked for satisfiability and solved.\n\nAs a result of these operations, the addition of new constraints may change the old ones. It is essential that the interpreter is able to undo these changes when it backtracks. The simplest case method is for the interpreter to save the complete state of the store every time it makes a choice (it chooses a clause to rewrite a goal). More efficient methods for allowing the constraint store to return to a previous state exist. In particular, one may just save the changes to the constraint store made between two points of choice, including the changes made to the old constraints. This can be done by simply saving the old value of the constraints that have been modified; this method is called trailing. A more advanced method is to save the changes that have been done on the modified constraints. For example, a linear constraint is changed by modifying its coefficient: saving the difference between the old and new coefficient allows reverting a change. This second method is called semantic backtracking, because the semantics of the change is saved rather than the old version of the constraints only.\n\nLabeling[edit]\nThe labeling literals are used on variables over finite domains to check satisfiability or partial satisfiability of the constraint store and to find a satisfying assignment. A labeling literal is of the form labeling([variables]), where the argument is a list of variables over finite domains. Whenever the interpreter evaluates such a literal, it performs a search over the domains of the variables of the list to find an assignment that satisfies all relevant constraints. Typically, this is done by a form of backtracking: variables are evaluated in order, trying all possible values for each of them, and backtracking when inconsistency is detected.\n\nThe first use of the labeling literal is to actual check satisfiability or partial satisfiability of the constraint store. When the interpreter adds a constraint to the constraint store, it only enforces a form of local consistency on it. This operation may not detect inconsistency even if the constraint store is unsatisfiable. A labeling literal over a set of variables enforces a satisfiability check of the constraints over these variables. As a result, using all variables mentioned in the constraint store results in checking satisfiability of the store.\n\nThe second use of the labeling literal is to actually determine an evaluation of the variables that satisfies the constraint store. Without the labeling literal, variables are assigned values only when the constraint store contains a constraint of the form X=value and when local consistency reduces the domain of a variable to a single value. A labeling literal over some variables forces these variables to be evaluated. In other words, after the labeling literal has been considered, all variables are assigned a value.\n\nTypically, constraint logic programs are written in such a way labeling literals are evaluated only after as many constraints as possible have been accumulated in the constraint store. This is because labeling literals enforce search, and search is more efficient if there are more constraints to be satisfied. A constraint satisfaction problem is typical solved by a constraint logic program having the following structure:\n\nsolve(X):-constraints(X), labeling(X)\nconstraints(X):- (all constraints of the CSP)\nWhen the interpreter evaluates the goal solve(args), it places the body of a fresh variant of the first clause in the current goal. Since the first goal is constraints(X'), the second clause is evaluated, and this operation moves all constraints in the current goal and eventually in the constraint store. The literal labeling(X') is then evaluated, forcing a search for a solution of the constraint store. Since the constraint store contains exactly the constraints of the original constraint satisfaction problem, this operation searches for a solution of the original problem.\n\nProgram reformulations[edit]\nA given constraint logic program may be reformulated to improve its efficiency. A first rule is that labeling literals should be placed after as much constraints on the labeled literals are accumulated in the constraint store. While in theory A(X):-labeling(X),X>0 is equivalent to A(X):-X>0,labeling(X), the search that is performed when the interpreter encounters the labeling literal is on a constraint store that does not contain the constraint X>0. As a result, it may generate solutions, such as X=-1, that are later found out not to satisfy this constraint. On the other hand, in the second formulation the search is performed only when the constraint is already in the constraint store. As a result, search only returns solutions that are consistent with it, taking advantage of the fact that additional constraints reduce the search space.\n\nA second reformulation that can increase efficiency is to place constraints before literals in the body of clauses. Again, A(X):-B(X),X>0 and A(X):-X>0,B(X) are in principle equivalent. However, the first may require more computation. For example, if the constraint store contains the constraint X<-2, the interpreter recursively evaluates B(X) in the first case; if it succeeds, it then finds out that the constraint store is inconsistent when adding X>0. In the second case, when evaluating that clause, the interpreter first adds X>0 to the constraint store and then possibly evaluates B(X). Since the constraint store after the addition of X>0 turns out to be inconsistent, the recursive evaluation of B(X) is not performed at all.\n\nA third reformulation that can increase efficiency is the addition of redundant constrains. If the programmer knows (by whatever means) that the solution of a problem satisfies a specific constraint, they can include that constraint to cause inconsistency of the constraint store as soon as possible. For example, if it is known beforehand that the evaluation of B(X) will result in a positive value for X, the programmer may add X>0 before any occurrence of B(X). As an example, A(X,Y):-B(X),C(X) will fail on the goal A(-2,Z), but this is only found out during the evaluation of the subgoal B(X). On the other hand, if the above clause is replaced by A(X,Y):-X>0,A(X),B(X), the interpreter backtracks as soon as the constraint X>0 is added to the constraint store, which happens before the evaluation of B(X) even starts.\n\nConstraint handling rules[edit]\nConstraint handling rules were initially defined as a stand-alone formalism for specifying constraint solvers, and were later embedded in logic programming. There are two kinds of constraint handling rules. The rules of the first kind specify that, under a given condition, a set of constraints is equivalent to another one. The rules of the second kind specify that, under a given condition, a set of constraints implies another one. In a constraint logic programming language supporting constraint handling rules, a programmer can use these rules to specify possible rewritings of the constraint store and possible additions of constraints to it. The following are example rules:\n\nA(X) <=> B(X) | C(X)\nA(X) ==> B(X) | C(X)\nThe first rule tells that, if B(X) is entailed by the store, the constraint A(X) can be rewritten as C(X). As an example, N*X>0 can be rewritten as X>0 if the store implies that N>0. The symbol <=> resembles equivalence in logic, and tells that the first constraint is equivalent to the latter. In practice, this implies that the first constraint can be replaced with the latter.\n\nThe second rule instead specifies that the latter constraint is a consequence of the first, if the constraint in the middle is entailed by the constraint store. As a result, if A(X) is in the constraint store and B(X) is entailed by the constraint store, then C(X) can be added to the store. Differently from the case of equivalence, this is an addition and not a replacement: the new constraint is added but the old one remains.\n\nEquivalence allows for simplifying the constraint store by replacing some constraints with simpler ones; in particular, if the third constraint in an equivalence rule is true, and the second constraint is entailed, the first constraint is removed from the constraint store. Inference allows for the addition of new constraints, which may lead to proving inconsistency of the constraint store, and may generally reduce the amount of search needed to establish its satisfiability.\n\nLogic programming clauses in conjunction with constraint handling rules can be used to specify a method for establishing the satisfiability of the constraint store. Different clauses are used to implement the different choices of the method; the constraint handling rules are used for rewriting the constraint store during execution. As an example, one can implement backtracking with unit propagation this way. Let holds(L) represents a propositional clause, in which the literals in the list L are in the same order as they are evaluated. The algorithm can be implemented using clauses for the choice of assigning a literal to true or false, and constraint handling rules to specify propagation. These rules specify that holds([l|L]) can be removed if l=true follows from the store, and it can be rewritten as holds(L) if l=false follows from the store. Similarly, holds([l]) can be replaced by l=true. In this example, the choice of value for a variable is implemented using clauses of logic programming; however, it can be encoded in constraint handling rules using an extension called disjunctive constraint handling rules or CHR∨.\n\nBottom-up evaluation[edit]\nThe standard strategy of evaluation of logic programs is top-down and depth-first: from the goal, a number of clauses are identified as being possibly able to prove the goal, and recursion over the literals of their bodies is performed. An alternative strategy is to start from the facts and use clauses to derive new facts; this strategy is called bottom-up. It is considered better than the top-down one when the aim is that of producing all consequences of a given program, rather than proving a single goal. In particular, finding all consequences of a program in the standard top-down and depth-first manner may not terminate while the bottom-up evaluation strategy terminates.\n\nThe bottom-up evaluation strategy maintains the set of facts proved so far during evaluation. This set is initially empty. With each step, new facts are derived by applying a program clause to the existing facts, and are added to the set. For example, the bottom up evaluation of the following program requires two steps:\n\nA(q).\nB(X):-A(X).\nThe set of consequences is initially empty. At the first step, A(q) is the only clause whose body can be proved (because it is empty), and A(q) is therefore added to the current set of consequences. At the second step, since A(q) is proved, the second clause can be used and B(q) is added to the consequences. Since no other consequence can be proved from {A(q),B(q)}, execution terminates.\n\nThe advantage of the bottom-up evaluation over the top-down one is that cycles of derivations do not produce an infinite loop. This is because adding a consequence to the current set of consequences that already contains it has no effect. As an example, adding a third clause to the above program generates a cycle of derivations in the top-down evaluation:\n\nA(q).\nB(X):-A(X).\nA(X):-B(X).\nFor example, while evaluating all answers to the goal A(X), the top-down strategy would produce the following derivations:\n\nA(q)\nA(q):-B(q), B(q):-A(q), A(q)\nA(q):-B(q), B(q):-A(q), A(q):-B(q), B(q):-A(q), A(q)\nIn other words, the only consequence A(q) is produced first, but then the algorithm cycles over derivations that do not produce any other answer. More generally, the top-down evaluation strategy may cycle over possible derivations, possibly when other ones exist.\n\nThe bottom-up strategy does not have the same drawback, as consequences that were already derived has no effect. On the above program, the bottom-up strategy starts adding A(q) to the set of consequences; in the second step, B(X):-A(X) is used to derive B(q); in the third step, the only facts that can be derived from the current consequences are A(q) and B(q), which are however already in the set of consequences. As a result, the algorithm stops.\n\nIn the above example, the only used facts were ground literals. In general, every clause that only contains constraints in the body is considered a fact. For example, a clause A(X):-X>0,X<10 is considered a fact as well. For this extended definition of facts, some facts may be equivalent while not syntactically equal. For example, A(q) is equivalent to A(X):-X=q and both are equivalent to A(X):-X=Y, Y=q. To solve this problem, facts are translated into a normal form in which the head contains a tuple of all-different variables; two facts are then equivalent if their bodies are equivalent on the variables of the head, that is, their sets of solutions are the same when restricted to these variables.\n\nAs described, the bottom-up approach has the advantage of not considering consequences that have already been derived. However, it still may derive consequences that are entailed by those already derived while not being equal to any of them. As an example, the bottom up evaluation of the following program is infinite:\n\nA(0).\nA(X):-X>0.\nA(X):-X=Y+1, A(Y).\nThe bottom-up evaluation algorithm first derives that A(X) is true for X=0 and X>0. In the second step, the first fact with the third clause allows for the derivation of A(1). In the third step, A(2) is derived, etc. However, these facts are already entailed by the fact that A(X) is true for any nonnegative X. This drawback can be overcome by checking for entailment facts that are to be added to the current set of consequences. If the new consequence is already entailed by the set, it is not added to it. Since facts are stored as clauses, possibly with \"local variables\", entailment is restricted over the variables of their heads.\n\nConcurrent constraint logic programming[edit]\nMain article: Concurrent constraint logic programming\nThe concurrent versions of constraint logic programming are aimed at programming concurrent processes rather than solving constraint satisfaction problems. Goals in constraint logic programming are evaluated concurrently; a concurrent process is therefore programmed as the evaluation of a goal by the interpreter.\n\nSyntactically, concurrent constraints logic programs are similar to non-concurrent programs, the only exception being that clauses includes guards, which are constraints that may block the applicability of the clause under some conditions. Semantically, concurrent constraint logic programming differs from its non-concurrent versions because a goal evaluation is intended to realize a concurrent process rather than finding a solution to a problem. Most notably, this difference affects how the interpreter behaves when more than one clause is applicable: non-concurrent constraint logic programming recursively tries all clauses; concurrent constraint logic programming chooses only one. This is the most evident effect of an intended directionality of the interpreter, which never revises a choice it has previously taken. Other effects of this are the semantical possibility of having a goal that cannot be proved while the whole evaluation does not fail, and a particular way for equating a goal and a clause head.\n\nApplications[edit]\nConstraint logic programming has been applied to a number of fields, such as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, finance, and others.\n\nHistory[edit]\nConstraint logic programming was introduced by Jaffar and Lassez in 1987. They generalized the observation that the term equations and disequations of Prolog II were a specific form of constraints, and generalized this idea to arbitrary constraint languages. The first implementations of this concept were Prolog III, CLP(R), and CHIP.",
          "subparadigms": [
            7
          ]
        },
        {
          "pdid": 9,
          "name": "Constraint programming",
          "details": "In computer science, constraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints. Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found. This makes constraint programming a form of declarative programming. The constraints used in constraint programming are of various kinds: those used in constraint satisfaction problems (e.g. \"A or B is true\"), those solved by the simplex algorithm (e.g. \"x ≤ 5\"), and others. Constraints are usually embedded within a programming language or provided via separate software libraries.\n\nConstraint programming can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. This variant of logic programming is due to Jaffar and Lassez, who extended in 1987 a specific class of constraints that were introduced in Prolog II. The first implementations of constraint logic programming were Prolog III, CLP(R), and CHIP.\n\nInstead of logic programming, constraints can be mixed with functional programming, term rewriting, and imperative languages. Programming languages with built-in support for constraints include Oz (functional programming) and Kaleidoscope (imperative programming). Mostly, constraints are implemented in imperative languages via constraint solving toolkits, which are separate libraries for an existing imperative language.\n\nContents  [hide] \n1\tConstraint logic programming\n2\tPerturbation vs refinement models\n3\tDomains\n4\tConstraint programming libraries for imperative programming languages\n5\tSome languages that support constraint programming\n5.1\tLogic programming based constraint logic languages\n6\tSee also\n7\tReferences\n8\tExternal links\nConstraint logic programming[edit]\nMain article: Constraint logic programming\nConstraint programming is an embedding of constraints in a host language. The first host languages used were logic programming languages, so the field was initially called constraint logic programming. The two paradigms share many important features, like logical variables and backtracking. Today most Prolog implementations include one or more libraries for constraint logic programming.\n\nThe difference between the two is largely in their styles and approaches to modeling the world. Some problems are more natural (and thus, simpler) to write as logic programs, while some are more natural to write as constraint programs.\n\nThe constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time. A problem is typically stated as a state of the world containing a number of unknown variables. The constraint program searches for values for all the variables.\n\nTemporal concurrent constraint programming (TCC) and non-deterministic temporal concurrent constraint programming (MJV) are variants of constraint programming that can deal with time.\n\nPerturbation vs refinement models[edit]\nLanguages for constraint-based programming follow one of two approaches:[1]\n\nRefinement model: variables in the problem are initially unassigned, and each variable is assumed to be able to contain any value included in its range or domain. As computation progresses, values in the domain of a variable are pruned if they are shown to be incompatible with the possible values of other variables, until a single value is found for each variable.\nPerturbation model: variables in the problem are assigned a single initial value. At different times one or more variables receive perturbations (changes to their old value), and the system propagates the change trying to assign new values to other variables that are consistent with the perturbation.\nConstraint propagation in constraint satisfaction problems is a typical example of a refinement model, and spreadsheets are a typical example of a perturbation model.\n\nThe refinement model is more general, as it does not restrict variables to have a single value, it can lead to several solutions to the same problem. However, the perturbation model is more intuitive for programmers using mixed imperative constraint object-oriented languages.[2]\n\nDomains[edit]\nThe constraints used in constraint programming are typically over some specific domains. Some popular domains for constraint programming are:\n\nboolean domains, where only true/false constraints apply (SAT problem)\ninteger domains, rational domains\nlinear domains, where only linear functions are described and analyzed (although approaches to non-linear problems do exist)\nfinite domains, where constraints are defined over finite sets\nmixed domains, involving two or more of the above\nFinite domains is one of the most successful domains of constraint programming. In some areas (like operations research) constraint programming is often identified with constraint programming over finite domains.\n\nAll of the above examples are commonly solved by satisfiability modulo theories (SMT) solvers.\n\nFinite domain solvers are useful for solving constraint satisfaction problems, and are often based on arc consistency or one of its approximations.\n\nThe syntax for expressing constraints over finite domains depends on the host language. The following is a Prolog program that solves the classical alphametic puzzle SEND+MORE=MONEY in constraint logic programming:\n\n% This code works in both YAP and SWI-Prolog using the environment-supplied\n% CLPFD constraint solver library.  It may require minor modifications to work\n% in other Prolog environments or using other constraint solvers.\n:- use_module(library(clpfd)).\nsendmore(Digits) :-\n   Digits = [S,E,N,D,M,O,R,Y],     % Create variables\n   Digits ins 0..9,                % Associate domains to variables\n   S #\\= 0,                        % Constraint: S must be different from 0\n   M #\\= 0,\n   all_different(Digits),          % all the elements must take different values\n                1000*S + 100*E + 10*N + D     % Other constraints\n              + 1000*M + 100*O + 10*R + E\n   #= 10000*M + 1000*O + 100*N + 10*E + Y,\n   label(Digits).                  % Start the search\nThe interpreter creates a variable for each letter in the puzzle. The operator ins is used to specify the domains of these variables, so that they range over the set of values {0,1,2,3, ..., 9}. The constraints S#\\=0 and M#\\=0 means that these two variables cannot take the value zero. When the interpreter evaluates these constraints, it reduces the domains of these two variables by removing the value 0 from them. Then, the constraint all_different(Digits) is considered; it does not reduce any domain, so it is simply stored. The last constraint specifies that the digits assigned to the letters must be such that \"SEND+MORE=MONEY\" holds when each letter is replaced by its corresponding digit. From this constraint, the solver infers that M=1. All stored constraints involving variable M are awakened: in this case, constraint propagation on the all_different constraint removes value 1 from the domain of all the remaining variables. Constraint propagation may solve the problem by reducing all domains to a single value, it may prove that the problem has no solution by reducing a domain to the empty set, but may also terminate without proving satisfiability or unsatisfiability. The label literals are used to actually perform search for a solution.\n\nConstraint programming libraries for imperative programming languages[edit]\nConstraint programming is often realized in imperative programming via a separate library. Some popular libraries for constraint programming are:\n\nArtelys Kalis (C++, Java, Python library, FICO Xpress module, proprietary)\nCassowary (Smalltalk, C++, Java, Python, JavaScript, Ruby library, free software: LGPL, no longer maintained)\nCHIP V5 C++ and C libraries (proprietary)\nChoco (Java library, free software: X11 style)\nComet (C style language for constraint programming, constraint-based local search and mathematical programming, free binaries available for academic use)\nCream (Java library, free software: LGPL)\nDisolver (C++ library, proprietary)\nGecode (C++ library, Python bindings, free software: X11 style)\nGoogle or-tools (Python, Java, C++ and .NET library, Apache license)\nGurobi\nIBM ILOG CP (C++ library, proprietary) and CP Optimizer (C++, Java, .NET libraries, proprietary) successor[3] of ILOG Solver, which was considered the market leader in commercial constraint programming software as of 2006[4]\nJaCoP (Java library, open source) available here\nJOpt (Java library, free software)\nJSR-331 (Java Constraint Programming API, JCP standard)\nKoalog Constraint Solver (Java library, proprietary)\nNumberjack (Python platform, free software: LGPL)\nMinion (C++ program, GPL)\npython-constraint (Python library, GPL)\nOscaR (Scala library, LGPL)\nScarab (Scala library, BSD license)\nSMOCS (Scala Monadic library, BSD license)\nOptaPlanner (Java library, Apache license)\nZ3 (C++ solver with C, Java, C#, and Python bindings, MIT license)\nSome languages that support constraint programming[edit]\nAIMMS, an algebraic modeling language with support for constraint programming.[5]\nAlma-0 a small, strongly typed, constraint language with a limited number of features inspired by logic programming, supporting imperative programming.\nAMPL, an algebraic modeling language with support for constraint programming.[6]\nBabelsberg a family of object-constraint programming languages for Ruby, JavaScript, Squeak, and Python.[7]\nBertrand a language for building constraint programming systems.\nCommon Lisp via Screamer (a free software library which provides backtracking and CLP(R), CHiP features).\nConstraint Handling Rules\nMiniZinc (a high-level constraint programming system, BSD-style license)\nKaleidoscope, an object-oriented imperative constraint programming language.\nOz\nClaire\nCurry (Haskell based, with free implementations)\nSystemVerilog Computer hardware simulation language has built in constraint solver.\nWolfram Language\nLogic programming based constraint logic languages[edit]\nB-Prolog (Prolog-based, proprietary)\nCHIP V5[8] (Prolog-based, also includes C++ and C libraries, proprietary)\nCiao (Prolog-based, Free software: GPL/LGPL)\nECLiPSe (Prolog-based, open source)\nSICStus (Prolog-based, proprietary)\nGNU Prolog (free software)\nPicat (open C source)\nYAP Prolog[1]\nSWI Prolog a free Prolog system containing several libraries for constraint solving\nJekejeke Minlog (Prolog-based, proprietary)\nF1 Compiler (proprietary no-cost software)",
          "subparadigms": [
            8
          ]
        },
        {
          "pdid": 10,
          "name": "Flow-based programming",
          "details": "In computer programming, flow-based programming (FBP) is a programming paradigm that defines applications as networks of \"black box\" processes, which exchange data across predefined connections by message passing, where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. FBP is thus naturally component-oriented.\n\nFBP is a particular form of dataflow programming based on bounded buffers, information packets with defined lifetimes, named ports, and separate definition of connections.\n\nContents  [hide] \n1\tIntroduction\n2\tHistory\n3\tConcepts\n4\tExamples\n4.1\t\"Telegram Problem\"\n4.2\tBatch update\n4.3\tMultiplexing processes\n4.4\tSimple interactive network\n5\tComparison with other paradigms and methodologies\n5.1\tJackson Structured Programming (JSP) and Jackson System Development (JSD)\n5.2\tApplicative programming\n5.3\tLinda\n5.4\tObject-oriented programming\n6\tSee also\n7\tReferences\n8\tExternal links\nIntroduction[edit]\nFlow-based programming defines applications using the metaphor of a \"data factory\". It views an application not as a single, sequential process, which starts at a point in time, and then does one thing at a time until it is finished, but as a network of asynchronous processes communicating by means of streams of structured data chunks, called \"information packets\" (IPs). In this view, the focus is on the application data and the transformations applied to it to produce the desired outputs. The network is defined externally to the processes, as a list of connections which is interpreted by a piece of software, usually called the \"scheduler\".\n\nThe processes communicate by means of fixed-capacity connections. A connection is attached to a process by means of a port, which has a name agreed upon between the process code and the network definition. More than one process can execute the same piece of code. At any point in time, a given IP can only be \"owned\" by a single process, or be in transit between two processes. Ports may either be simple, or array-type, as used e.g. for the input port of the Collate component described below. It is the combination of ports with asynchronous processes that allows many long-running primitive functions of data processing, such as Sort, Merge, Summarize, etc., to be supported in the form of software black boxes.\n\nBecause FBP processes can continue executing as long they have data to work on and somewhere to put their output, FBP applications generally run in less elapsed time than conventional programs, and make optimal use of all the processors on a machine, with no special programming required to achieve this.[citation needed]\n\nThe network definition is usually diagrammatic, and is converted into a connection list in some lower-level language or notation. FBP is thus a visual programming language at this level. More complex network definitions have a hierarchical structure, being built up from subnets with \"sticky\" connections.\n\nFBP has much in common with the Linda[1] language in that it is, in Gelernter and Carriero's terminology, a \"coordination language\":[2] it is essentially language-independent. Indeed, given a scheduler written in a sufficiently low-level language, components written in different languages can be linked together in a single network. FBP thus lends itself to the concept of domain-specific languages or \"mini-languages\".\n\nFBP exhibits \"data coupling\", described in the article on coupling as the loosest type of coupling between components. The concept of loose coupling is in turn related to that of service-oriented architectures, and FBP fits a number of the criteria for such an architecture, albeit at a more fine-grained level than most examples of this architecture.\n\nFBP promotes high-level, functional style of specifications that simplify reasoning about system behavior. An example of this is the distributed data flow model for constructively specifying and analyzing the semantics of distributed multi-party protocols.\n\nHistory[edit]\nFlow-Based Programming was invented by J. Paul Morrison in the early 1970s, and initially implemented in software for a Canadian bank.[3] FBP at its inception was strongly influenced by some IBM simulation languages of the period, in particular GPSS, but its roots go all the way back to Conway's seminal paper on what he called coroutines.[4]\n\nFBP has undergone a number of name changes over the years: the original implementation was called AMPS (Advanced Modular Processing System). One large application in Canada went live in 1975, and, as of 2013, has been in continuous production use, running daily, for almost 40 years. Because IBM considered the ideas behind FBP \"too much like a law of nature\" to be patentable they instead put the basic concepts of FBP into the public domain, by means of a Technical Disclosure Bulletin, \"Data Responsive Modular, Interleaved Task Programming System\",[5] in 1971.[3] An article describing its concepts and experience using it was published in 1978 in the IBM Research IBM Systems Journal under the name DSLM.[6] A second implementation was done as a joint project of IBM Canada and IBM Japan, under the name \"Data Flow Development Manager\" (DFDM), and was briefly marketed in Japan in the late '80s under the name \"Data Flow Programming Manager\".\n\nGenerally the concepts were referred to within IBM as \"Data Flow\", but this term was felt to be too general, and eventually the name flow-based programming was adopted.\n\nFrom the early '80s to 1993 J. Paul Morrison and IBM architect Wayne Stevens refined and promoted the concepts behind FBP. Stevens wrote several articles describing and supporting the FBP concept, and included material about it in several of his books.[7][8][non-primary source needed][9][non-primary source needed]. In 1994 Morrison published a book describing FBP, and providing empirical evidence that FBP led to reduced development times.[10]\n\nIn 2013, a European FBP-based project called NoFlo was implemented in Javascript. It is being used mainly in the area of web site design incorporating a number of AI concepts.[11]\n\nConcepts[edit]\nThe following diagram shows the major entities of an FBP diagram (apart from the Information Packets). Such a diagram can be converted directly into a list of connections, which can then be executed by an appropriate engine (software or hardware).\n\n\nSimple FBP diagram\nA, B and C are processes executing code components. O1, O2, and the two INs are ports connecting the connections M and N to their respective processes. It is permitted for processes B and C to be executing the same code, so each process must have its own set of working storage, control blocks, etc. Whether or not they do share code, B and C are free to use the same port names, as port names only have meaning within the components referencing them (and at the network level, of course).\n\nM and N are what are often referred to as \"bounded buffers\", and have a fixed capacity in terms of the number of IPs that they can hold at any point in time.\n\nThe concept of ports is what allows the same component to be used at more than one place in the network. In combination with a parametrization ability, called Initial Information Packets (IIPs), ports provide FBP with a component reuse ability, making FBP a component-based architecture. FBP thus exhibits what Raoul de Campo and Nate Edwards of IBM Research have termed configurable modularity.\n\nInformation Packets or IPs are allocated in what might be called \"IP space\" (just as Linda's tuples are allocated in \"tuple space\"), and have a well-defined lifetime until they are disposed of and their space is reclaimed - in FBP this must be an explicit action on the part of an owning process. IPs traveling across a given connection (actually it is their \"handles\" that travel) constitute a \"stream\", which is generated and consumed asynchronously - this concept thus has similarities to the lazy cons concept described in the 1976 article by Friedman and Wise.[12]\n\nIPs are usually structured chunks of data - some IPs, however, may not contain any real data, but are used simply as signals. An example of this is \"bracket IPs\", which can be used to group data IPs into sequential patterns within a stream, called \"substreams\". Substreams may in turn be nested. IPs may also be chained together to form \"IP trees\", which travel through the network as single objects.\n\nThe system of connections and processes described above can be \"ramified\" to any size. During the development of an application, monitoring processes may be added between pairs of processes, processes may be \"exploded\" to subnets, or simulations of processes may be replaced by the real process logic. FBP therefore lends itself to rapid prototyping.\n\nThis is really an assembly line image of data processing: the IPs travelling through a network of processes may be thought of as widgets travelling from station to station in an assembly line. \"Machines\" may easily be reconnected, taken off line for repair, replaced, and so on. Oddly enough, this image is very similar to that of unit record equipment that was used to process data before the days of computers, except that decks of cards had to be hand-carried from one machine to another.\n\nImplementations of FBP may be non-preemptive or preemptive - the earlier implementations tended to be non-preemptive (mainframe and C language), whereas the latest Java implementation (see below) uses Java Thread class and is preemptive.\n\nExamples[edit]\n\"Telegram Problem\"[edit]\nFBP components often form complementary pairs. This example uses two such pairs. The problem described seems very simple as described in words, but in fact is surprisingly difficult to accomplish using conventional procedural logic. The task, called the \"Telegram Problem\", originally described by Peter Naur, is to write a program which accepts lines of text and generates output lines containing as many words as possible, where the number of characters in each line does not exceed a certain length. The words may not be split and we assume no word is longer than the size of the output lines. This is analogous to the word-wrapping problem in text editors.[13]\n\nIn conventional logic, the programmer rapidly discovers that neither the input nor the output structures can be used to drive the call hierarchy of control flow. In FBP, on the other hand, the problem description itself suggests a solution:\n\n\"words\" are mentioned explicitly in the description of the problem, so it is reasonable for the designer to treat words as information packets (IPs)\nin FBP there is no single call hierarchy, so the programmer is not tempted to force a sub-pattern of the solution to be the top level.\nHere is the most natural solution in FBP (there is no single \"correct\" solution in FBP, but this seems like a natural fit):\n\n\nPeter Naur's \"Telegram problem\"\nwhere DC and RC stand for \"DeCompose\" and \"ReCompose\", respectively.\n\nAs mentioned above, Initial Information Packets (IIPs) can be used to specify parametric information such as the desired output record length (required by the rightmost two components), or file names. IIPs are data chunks associated with a port in the network definition which become \"normal\" IPs when a \"receive\" is issued for the relevant port.\n\nBatch update[edit]\nThis type of program involves passing a file of \"details\" (changes, adds and deletes) against a \"master file\", and producing (at least) an updated master file, and one or more reports. Update programs are generally quite hard to code using synchronous, procedural code, as two (sometimes more) input streams have to be kept synchronized, even though there may be masters without corresponding details, or vice versa.\n\n\nCanonical \"batch update\" structure\nIn FBP, a reusable component (Collate), based on the unit record idea of a Collator, makes writing this type of application much easier as Collate merges the two streams and inserts bracket IPs to indicate grouping levels, significantly simplifying the downstream logic. Suppose that one stream (\"masters\" in this case) consists of IPs with key values of 1, 2 and 3, and the second stream IPs (\"details\") have key values of 11, 12, 21, 31, 32, 33 and 41, where the first digit corresponds to the master key values. Using bracket characters to represent \"bracket\" IPs, the collated output stream will be as follows:\n\n( m1 d11 d12 ) ( m2 d21 ) ( m3 d31 d32 d33 ) (d41)\nAs there was no master with a value of 4, the last group consists of a single detail (plus brackets).\n\nThe structure of the above stream can be described succinctly using a BNF-like notation such as\n\n{ ( [m] d* ) }*\nCollate is a reusable black box which only needs to know where the control fields are in its incoming IPs (even this is not strictly necessary as transformer processes can be inserted upstream to place the control fields in standard locations), and can in fact be generalized to any number of input streams, and any depth of bracket nesting. Collate uses an array-type port for input, allowing a variable number of input streams.\n\nMultiplexing processes[edit]\nFlow-based programming supports process multiplexing in a very natural way. Since components are read-only, any number of instances of a given component (\"processes\") can run asynchronously with each other.\n\n\nExample of multiplexing\nWhen computers usually had a single processor, this was useful when a lot of I/O was going on; now that machines usually have multiple processors, this is starting to become useful when processes are CPU-intensive as well. The diagram in this section shows a single \"Load Balancer\" process distributing data between 3 processes, labeled S1, S2 and S3, respectively, which are instances of a single component, which in turn feed into a single process on a \"first-come, first served\" basis.\n\nSimple interactive network[edit]\n\nSchematic of general interactive application\nIn this general schematic, requests (transactions) coming from users enter the diagram at the upper left, and responses are returned at the lower left. The \"back ends\" (on the right side) communicate with systems at other sites, e.g. using CORBA, MQSeries, etc. The cross-connections represent requests that do not need to go to the back ends, or requests that have to cycle through the network more than once before being returned to the user.\n\nAs different requests may use different back-ends, and may require differing amounts of time for the back-ends (if used) to process them, provision must be made to relate returned data to the appropriate requesting transactions, e.g. hash tables or caches.\n\nThe above diagram is schematic in the sense that the final application may contain many more processes: processes may be inserted between other processes to manage caches, display connection traffic, monitor throughput, etc. Also the blocks in the diagram may represent \"subnets\" - small networks with one or more open connections.\n\nComparison with other paradigms and methodologies[edit]\nJackson Structured Programming (JSP) and Jackson System Development (JSD)[edit]\nMain articles: Jackson Structured Programming and Jackson System Development\nThis methodology assumes that a program must be structured as a single procedural hierarchy of subroutines. Its starting point is to describe the application as a set of \"main lines\", based on the input and output data structures. One of these \"main lines\" is then chosen to drive the whole program, and the others are required to be \"inverted\" to turn them into subroutines (hence the name \"Jackson inversion\"). This sometimes results in what is called a \"clash\", requiring the program to be split into multiple programs or coroutines. When using FBP, this inversion process is not required, as every FBP component can be considered a separate \"main line\".\n\nFBP and JSP share the concept of treating a program (or some components) as a parser of an input stream.\n\nIn Jackson's later work, Jackson System Development (JSD), the ideas were developed further.[14][15]\n\nIn JSD the design is maintained as a network design until the final implementation stage. The model is then transformed into a set of sequential processes to the number of available processors. Jackson discusses the possibility of directly executing the network model that exists prior to this step, in section 1.3 of his book (italics added):\n\nThe specification produced at the end of the System Timing step is, in principle, capable of direct execution. The necessary environment would contain a processor for each process, a device equivalent to an unbounded buffer for each data stream, and some input and output devices where the system is connected to the real world. Such an environment could, of course, be provided by suitable software running on a sufficiently powerful machine. Sometimes, such direct execution of the specification will be possible, and may even be a reasonable choice.[15]\nFBP was recognized by M A Jackson as an approach that follows his method of \"Program decomposition into sequential processes communicating by a coroutine-like mechanism\" [16]\n\nApplicative programming[edit]\nMain article: Applicative programming language\nW.B. Ackerman defines an applicative language as one which does all of its processing by means of operators applied to values.[17] The earliest known applicative language was LISP.\n\nAn FBP component can be regarded as a function transforming its input stream(s) into its output stream(s). These functions are then combined to make more complex transformations, as shown here:\n\n\nTwo functions feeding one\nIf we label streams, as shown, with lower case letters, then the above diagram can be represented succinctly as follows:\n\nc = G(F(a),F(b));\nJust as in functional notation F can be used twice because it only works with values, and therefore has no side effects, in FBP two instances of a given component may be running concurrently with each other, and therefore FBP components must not have side-effects either. Functional notation could clearly be used to represent at least a part of an FBP network.\n\nThe question then arises whether FBP components can themselves be expressed using functional notation. W.H. Burge showed how stream expressions can be developed using a recursive, applicative style of programming, but this work was in terms of (streams of) atomic values.[18] In FBP, it is necessary to be able to describe and process structured data chunks (FBP IPs).\n\nFurthermore, most applicative systems assume that all the data is available in memory at the same time, whereas FBP applications need to be able to process long-running streams of data while still using finite resources. Friedman and Wise suggested a way to do this by adding the concept of \"lazy cons\" to Burge's work. This removed the requirement that both of the arguments of \"cons\" be available at the same instant of time. \"Lazy cons\" does not actually build a stream until both of its arguments are realized - before that it simply records a \"promise\" to do this. This allows a stream to be dynamically realized from the front, but with an unrealized back end. The end of the stream stays unrealized until the very end of the process, while the beginning is an ever-lengthening sequence of items.\n\nLinda[edit]\nMain article: Linda\nMany of the concepts in FBP seem to have been discovered independently in different systems over the years. Linda, mentioned above, is one such. The difference between the two techniques is illustrated by the Linda \"school of piranhas\" load balancing technique - in FBP, this requires an extra \"load balancer\" component which routes requests to the component in a list which has the smallest number of IPs waiting to be processed. Clearly FBP and Linda are closely related, and one could easily be used to simulate the other.\n\nObject-oriented programming[edit]\nMain article: Object-oriented programming\nAn object in OOP can be described as a semi-autonomous unit comprising both information and behaviour. Objects communicate by means of \"method calls\", which are essentially subroutine calls, done indirectly via the class to which the receiving object belongs. The object's internal data can only be accessed by means of method calls, so this is a form of information hiding or \"encapsulation\". Encapsulation, however, predates OOP - David Parnas wrote one of the seminal articles on it in the early 70s [19] - and is a basic concept in computing. Encapsulation is the very essence of an FBP component, which may be thought of as a black box, performing some conversion of its input data into its output data. In FBP, part of the specification of a component is the data formats and stream structures that it can accept, and those it will generate. This constitutes a form of design by contract. In addition, the data in an IP can only be accessed directly by the currently owning process. Encapsulation can also be implemented at the network level, by having outer processes protect inner ones.\n\nA paper by C. Ellis and S. Gibbs distinguishes between active objects and passive objects.[20] Passive objects comprise information and behaviour, as stated above, but they cannot determine the timing of this behaviour. Active objects on the other hand can do this. In their article Ellis and Gibbs state that active objects have much more potential for the development of maintainable systems than do passive objects. An FBP application can be viewed as a combination of these two types of object, where FBP processes would correspond to active objects, while IPs would correspond to passive objects.",
          "subparadigms": []
        },
        {
          "pdid": 11,
          "name": "Spreadsheet",
          "details": "A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form.[1][2][3] Spreadsheets are developed as computerized simulations of paper accounting worksheets.[4] The program operates on data entered in cells of a table. Each cell may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells. A spreadsheet may also refer to one such electronic document.[5][6][7]\n\nSpreadsheet users can adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for \"what-if\" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.\n\nBesides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.\n\nSpreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.\n\nLANPAR, available in 1969,[8] was the first electronic spreadsheet on mainframe and time sharing computers. LANPAR was an acronym: LANguage for Programming Arrays at Random.[8] VisiCalc was the first electronic spreadsheet on a microcomputer,[9] and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system.[10] Excel now has the largest market share on the Windows and Macintosh platforms.[11][12][13] A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.\n\nContents  [hide] \n1\tUsage\n2\tHistory\n2.1\tPaper spreadsheets\n2.2\tEarly implementations\n2.2.1\tBatch spreadsheet report generator\n2.2.2\tLANPAR spreadsheet compiler\n2.2.3\tAutoplan/Autotab spreadsheet programming language\n2.2.4\tIBM Financial Planning and Control System\n2.2.5\tAPLDOT modeling language\n2.3\tVisiCalc\n2.4\tSuperCalc\n2.5\tLotus 1-2-3 and other MS-DOS spreadsheets\n2.6\tMicrosoft Excel\n2.7\tOpen source software\n2.8\tWeb based spreadsheets\n2.9\tOther spreadsheets\n2.10\tOther products\n3\tConcepts\n3.1\tCells\n3.1.1\tValues\n3.1.2\tAutomatic recalculation\n3.1.3\tReal-time update\n3.1.4\tLocked cell\n3.1.5\tData format\n3.1.6\tCell formatting\n3.1.7\tNamed cells\n3.1.7.1\tCell reference\n3.1.7.2\tCell ranges\n3.2\tSheets\n3.3\tFormulas\n3.4\tFunctions\n3.5\tSubroutines\n3.6\tRemote spreadsheet\n3.7\tCharts\n3.8\tMulti-dimensional spreadsheets\n3.9\tLogical spreadsheets\n4\tProgramming issues\n4.1\tEnd-user development\n4.2\tSpreadsheet programs\n5\tShortcomings\n6\tSpreadsheet risk\n7\tSee also\n8\tReferences\n9\tExternal links\nUsage[edit]\nbasic spreadsheet with toolbar\nOpenOffice.org Calc spreadsheet\nA spreadsheet consists of a table of cells arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, \"A\", \"B\", \"C\", etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, \"C10\" for instance. This system of cell references was introduced in VisiCalc, and known as \"A1 notation\". Additionally, spreadsheets have the concept of a range, a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range \"A1:A10\".\n\nIn modern spreadsheet applications, several spreadsheets, often known as worksheets or simply sheets, are gathered together to form a workbook. A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, \"Sheet 1!C10\". Some systems extend this syntax to allow cell references to different workbooks.\n\nUsers interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text hello world, the number 5 or the date 16-Dec-91. A formula would begin with the equals sign, =5*3, but this would normally be invisible because the display shows the result of the calculation, 15 in this case, not the formula itself. This may lead to confusion in some cases.\n\nThe key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula =5*C10 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value 3 the result will be 15. But C10 might also hold its own formula referring to other cells, and so on.\n\nThe ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical steps, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the SUM function that adds up all the numbers within a range.\n\nSpreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable—sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.\n\nA spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.\n\nHistory[edit]\nPaper spreadsheets[edit]\nThe word \"spreadsheet\" came from \"spread\" in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word \"spread-sheet\" came to mean the format used to present book-keeping ledgers—with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect—which were, traditionally, a \"spread\" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed \"analysis paper\") ruled into rows and columns in that format and approximately twice as wide as ordinary paper.[14]\n\nEarly implementations[edit]\nBatch spreadsheet report generator[edit]\nA batch \"spreadsheet\" is indistinguishable from a batch compiler with added input data, producing an output report, i.e., a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper \"Budgeting Models and System Simulation\" by Richard Mattessich.[15] The subsequent work by Mattessich (1964a, Chpt. 9, Accounting and Analytical Methods) and its companion volume, Mattessich (1964b, Simulation of the Firm through a Budget Computer Program) applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual cells.\n\nIn 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled Business Computer Language was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed[16]\n\nApplied Data Resources had a FORTRAN preprocessor called Empires.\n\nIn the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.\n\nLANPAR spreadsheet compiler[edit]\nA key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 U.S. Patent 4,398,249 on spreadsheet automatic natural order recalculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the CCPA (Predecessor Court of the Federal Circuit) overturning the Patent Office in 1983—establishing that \"something does not cease to become patentable merely because the point of novelty is in an algorithm.\" However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.[17]\n\nThe actual software was called LANPAR — LANguage for Programming Arrays at Random.[18] This was conceived and entirely developed in the summer of 1969 following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having computer calculating results in the right order (\"Forward Referencing/Natural Order Calculation\"). Pardo and Landau developed and implemented the software in 1969.[19]\n\nLANPAR was used by Bell Canada, AT&T and the 18 operating telcos nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first \"non-procedural\" computer languages) [20] as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, Supercalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward Referencing/Natural Order Calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.\n\nThe LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions including logical comparisons and \"if/then\" statements could be used in any cell, and cells could be presented in any order.\n\nAutoplan/Autotab spreadsheet programming language[edit]\nIn 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. \"AutoPlan\" ran on GE’s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name AutoTab. (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.)\n\nAutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column. In 1975, Autotab-II was advertised as extending the original to a maximum of \"1,500 rows and columns, combined in any proportion the user requires...\"[21]\n\nIBM Financial Planning and Control System[edit]\nThe IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.\n\nAPLDOT modeling language[edit]\nAn example of an early \"industrial weight\" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD.[22] The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a \"spreadsheet\" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.\n\nVisiCalc[edit]\n\nVisiCalc running on an Apple II\nBecause of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time PC World magazine called VisiCalc the first electronic spreadsheet.[23]\n\nBricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.\n\nVisiCalc went on to become the first killer app,[24][25] an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.\n\nSuperCalc[edit]\nSuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M and was ported to MS-DOS in 1982.\n\nLotus 1-2-3 and other MS-DOS spreadsheets[edit]\nThe acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.[26]\n\nLotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.\n\nMicrosoft Excel[edit]\nMicrosoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported[27] it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3,[14] and in 2013, IBM discontinued Lotus-1-2-3 altogether.[28]\n\nOpen source software[edit]\nGnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the very closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.\n\nWeb based spreadsheets[edit]\nMain article: List of online spreadsheets\nWith the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as EditGrid, Google Sheets, Microsoft Excel Online, Smartsheet, ZK Spreadsheet, or Zoho Office Suite also have strong multi-user collaboration features and/or offer real time updates from remote sources such as stock prices and currency exchange rates.\n\nOther spreadsheets[edit]\nNotable current spreadsheet software:\n\nCalligra Sheets (formerly KCalc)\nCorel Quattro Pro (WordPerfect Office)\nKingsoft Spreadsheets\nMariner Calc and Calc XLS are Mariner Software's spreadsheet applications for Mac OS X and iOS.\nNeoOffice\nNumbers is Apple Inc.'s spreadsheet software, part of iWork.\nPyspread\nZCubes-Calc\nDiscontinued spreadsheet software:\n\n3D-Calc for Atari ST computers\nFramework by Forefront Corporation/Ashton-Tate (1983/84)\nGNU Oleo – A traditional terminal mode spreadsheet for UNIX/UNIX-like systems\nIBM Lotus Symphony (2007)\nJavelin Software\nKCells\nLotus Improv[29]\nLotus Jazz for Macintosh\nLotus Symphony (1984)\nMultiPlan\nClaris' Resolve (Macintosh)\nResolver One\nBorland's Quattro Pro\nSIAG\nSuperCalc\nT/Maker\nTarget Planner Calc for CP/M and TRS-DOS[30][31]\nTrapeze for Macintosh[32]\nWingz for Macintosh\nOther products[edit]\nA number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day.\n\nSpreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.\n\nConcepts[edit]\nThe main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.\n\nCells[edit]\nA \"cell\" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).\n\nMy Spreadsheet\nA\tB\tC\tD\n01\tvalue1\tvalue2\tadded\tmultiplied\n02\t10\t20\t30\t200\nAn array of cells is called a sheet or worksheet. It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).\n\nA cell may contain a value or a formula, or it may simply be left empty. By convention, formulas usually begin with = sign.\n\nValues[edit]\nA value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.\n\nThe Spreadsheet Value Rule\n\nComputer scientist Alan Kay used the term value rule to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.[33] The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.[34]\n\nAutomatic recalculation[edit]\nA standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.\n\nRecalculation generally requires that there are no circular dependencies in a spreadsheet. A dependency graph is a graph that has a vertex for each object to be updated, and an edge connecting two objects whenever one of them needs to be updated earlier than the other. Dependency graphs without circular dependencies form directed acyclic graphs, representations of partial orderings (in this case, across a spreadsheet) that can be relied upon to give a definite result.[35]\n\nReal-time update[edit]\nThis feature refers to updating a cell's contents periodically with a value from an external source—such as a cell in a \"remote\" spreadsheet. For shared, Web-based spreadsheets, it applies to \"immediately\" updating cells another user has updated. All dependent cells must be updated also.\n\nLocked cell[edit]\nOnce entered, selected cells (or the entire spreadsheet) can optionally be \"locked\" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing \"constants\" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.\n\nData format[edit]\nA cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example \"31/12/2007\" or \"31 Dec 2007\" would default to the cell format of date. Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.\n\nSome cell formats such as \"numeric\" or \"currency\" can also specify the number of decimal places.\n\nThis can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.\n\nCell formatting[edit]\nDepending on the capability of the spreadsheet application, each cell (like its counterpart the \"style\" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.\n\nA cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.\n\nNamed cells[edit]\n\nUse of named column variables x & y in Microsoft Excel. Formula for y=x2 resembles Fortran, and Name Manager shows the definitions of x & y.\nIn most implementations, a cell, or group of cells in a column or row, can be \"named\" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.\n\nCell reference[edit]\nIn place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.\n\nA typical cell reference in \"A1\" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A–Z and AA–IV) followed by a row number (e.g., in the range 1–65536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative \"R1C1\" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.\n\nWhen the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).\n\nA cell on the same \"sheet\" is usually addressed as:\n\n=A1\nA cell on a different sheet of the same spreadsheet is usually addressed as:\n\n=SHEET2!A1             (that is; the first cell in sheet 2 of same spreadsheet).\nSome spreadsheet implementations in Excel allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:\n\n='C:\\Documents and Settings\\Username\\My spreadsheets\\[main sheet]Sheet1!A1\nIn a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values—which they often do not.\n\nA circular reference occurs when the formula in one cell refers—directly, or indirectly through a chain of cell references—to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.\n\nCell ranges[edit]\nLikewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as \"=SUM(A1:A6)\" would add all the cells specified and put the result in the cell containing the formula itself.\n\nSheets[edit]\nIn the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.\n\nFormulas[edit]\n\nAnimation of a simple spreadsheet that multiplies values in the left column by 2, then sums the calculated values from the right column to the bottom-most cell. In this example, only the values in the A column are entered (10, 20, 30), and the remainder of cells are formulas. Formulas in the B column multiply values from the A column using relative references, and the formula in B4 uses the SUM() function to find the sum of values in the B1:B3 range.\nA formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by \"clicking\" the mouse over a particular cell; otherwise it contains the result of the calculation.\n\nA formula assigns values to a cell or range of cells, and typically has the format:\n\n=expression\nwhere the expression consists of:\n\nvalues, such as 2, 9.14 or 6.67E-11;\nreferences to other cells, such as, e.g., A1 for a single cell or B1:B3 for a range;\narithmetic operators, such as +, -, *, /, and others;\nrelational operators, such as >=, <, and others; and,\nfunctions, such as SUM(), TAN(), and many others.\nWhen a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., A1, or B1:B3), absolute (e.g., $A$1, or $B$1:$B$3) or mixed row– or column-wise absolute/relative (e.g., $A1 is column-wise absolute and A$1 is row-wise absolute).\n\nThe available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.\n\nA formula may contain a condition (or nested conditions)—with or without an actual calculation—and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.\n\n=IF(SUM(A1:A6) > 100, \"More than 100%\", SUM(A1:A6))\nFurther examples:\n\n=IF(AND(A1<>\"\",B1<>\"\"),A1/B1,\"\") means that if both cells A1 and B1 are not <> empty \"\", then divide A1 by B1 and display, other do not display anything.\n=IF(AND(A1<>\"\",B1<>\"\"),IF(B1<>0,A1/B1,\"Division by zero\"),\"\") means that if cells A1 and B1 are not empty, and B1 is not zero, then divide A1 by B1, if B1 is zero, then display \"Division by zero, and do not display anything if either A1 and B1 are empty.\n=IF(OR(A1<>\"\",B1<>\"\"),\"Either A1 or B1 show text\",\"\") means to display the text if either cells A1 or B1 are not empty.\nThe best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.\n\nA spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.\n\nFunctions[edit]\n\nUse of user-defined function sq(x) in Microsoft Excel.\nSpreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for user-defined functions. In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name sq is user-assigned, and function sq is introduced using the Visual Basic editor supplied with Excel. Name Manager displays the spreadsheet definitions of named variables x & y.\n\nSubroutines[edit]\n\nSubroutine in Microsoft Excel writes values calculated using x into y.\nFunctions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable x, calculates its square, and writes this value into the corresponding element of named column variable y. The y column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.\n\nRemote spreadsheet[edit]\nWhenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a \"remote\" spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.\n\nCharts[edit]\n\nGraph made using Microsoft Excel\nMany spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.\n\nMulti-dimensional spreadsheets[edit]\nIn the late 1980s and early 1990s, first Javelin Software and Lotus Improv appeared. Unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin—the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation.\n\nIn these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets—variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, the program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.\n\nTrapeze,[32] a spreadsheet on the Mac, went further and explicitly supported not just table columns, but also matrix operators.\n\nLogical spreadsheets[edit]\nSpreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.\n\nProgramming issues[edit]\nJust as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.\n\nEnd-user development[edit]\nSpreadsheets are a popular End-user development tool.[36] EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.\n\nThey use spatial relationships to define program relationships. Humans have highly developed intuitions about spaces, and of dependencies between items. Sequential programming usually requires typing line after line of text, which must be read slowly and carefully to be understood and changed.\nThey are forgiving, allowing partial results and functions to work. One or more parts of a program can work correctly, even if other parts are unfinished or broken. This makes writing and debugging programs easier, and faster. Sequential programming usually needs every program line and character to be correct for a program to run. One error usually stops the whole program and prevents any result.\nModern spreadsheets allow for secondary notation. The program can be annotated with colors, typefaces, lines, etc. to provide visual cues about the meaning of elements in the program.\nExtensions that allow users to create new functions can provide the capabilities of a functional language.[37]\nExtensions that allow users to build and apply models from the domain of machine learning.[38][39]\nSpreadsheets are versatile. With their boolean logic and graphics capabilities, even electronic circuit design is possible.[40]\nSpreadsheets can store relational data and spreadsheet formulas can express all queries of SQL. There exists a query translator, which automatically generates the spreadsheet implementation from the SQL code.[41]\nSpreadsheet programs[edit]\nA \"spreadsheet program\" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.\n\nIt is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.\n\nSpreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.\n\nMany of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).\n\nSpreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.\n\nShortcomings[edit]\nWhile spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.[42]\n\nResearch by ClusterSeven has shown huge discrepancies in the way financial institutions and corporate entities understand, manage and police their often vast estates of spreadsheets and unstructured financial data (including comma separated variable (CSV) files and Microsoft Access Databases). One study in early 2011 of nearly 1,500 people in the UK found that 57% of spreadsheet users have never received formal training on the spreadsheet package they use. 72% said that no internal department checks their spreadsheets for accuracy. Only 13% said that Internal Audit reviews their spreadsheets, while a mere 1% receive checks from their risk department.[43]\nSpreadsheets have significant reliability problems. Research studies estimate that roughly 94% of spreadsheets deployed in the field contain errors, and 5.2% of cells in unaudited spreadsheets contain errors.[44]\nDespite the high error risks often associated with spreadsheet authorship and use, specific steps can be taken to significantly enhance control and reliability by structurally reducing the likelihood of error occurrence at their source.[45]\nThe practical expressiveness of spreadsheets can be limited unless their modern features are used. Several factors contribute to this limitation. Implementing a complex model on a cell-at-a-time basis requires tedious attention to detail. Authors have difficulty remembering the meanings of hundreds or thousands of cell addresses that appear in formulas.\nThese drawbacks are mitigated by the use of named variables for cell designations, and employing variables in formulas rather than cell locations and cell-by-cell manipulations. Graphs can be used to show instantly how results are changed by changes in parameter values. In fact, the spreadsheet can be made invisible except for a transparent user interface that requests pertinent input from the user, displays results requested by the user, creates reports, and has built-in error traps to prompt correct input.[46]\nSimilarly, formulas expressed in terms of cell addresses are hard to keep straight and hard to audit. Research shows that spreadsheet auditors who check numerical results and cell formulas find no more errors than auditors who only check numerical results.[44] That is another reason to use named variables and formulas employing named variables.\nThe alteration of a dimension demands major surgery. When rows (or columns) are added to or deleted from a table, one has to adjust the size of many downstream tables that depend on the table being changed. In the process, it is often necessary to move other cells around to make room for the new columns or rows, and to adjust graph data sources. In large spreadsheets, this can be extremely time consuming.[47][48]\nAdding or removing a dimension is so difficult, one generally has to start over. The spreadsheet as a paradigm really forces one to decide on dimensionality right of the beginning of one's spreadsheet creation, even though it is often most natural to make these choices after one's spreadsheet model has matured. The desire to add and remove dimensions also arises in parametric and sensitivity analyses.[47][48]\nMulti-dimensional spreadsheets and tools such as Analytica avoid this important pitfall by generalizing the 2-D paradigm of the classical spreadsheet to a multi-dimensional representation.\nCollaboration in authoring spreadsheet formulas can be difficult when such collaboration occurs at the level of cells and cell addresses.\nHowever, like programming languages, spreadsheets are capable of using aggregate cells with similar meaning and indexed variables with names that indicate meaning. Some spreadsheets have good collaboration features, and it is inadvisable to author at the level of cells and cell formulas to avoid obstacles to collaboration, where many people cooperate on data entry and many people use the same spreadsheet. In collaborative authoring, it is advisable to use the range-protection feature of spreadsheets that prevents the contents of specific parts of a worksheet from being inadvertently altered.\nOther problems associated with spreadsheets include:[49][50]\n\nSome sources advocate the use of specialized software instead of spreadsheets for some applications (budgeting, statistics)[51][52][53]\nMany spreadsheet software products, such as Microsoft Excel[54] (versions prior to 2007) and OpenOffice.org Calc[55] (versions prior to 2008), have a capacity limit of 65,536 rows by 256 columns (216 and 28 respectively). This can present a problem for people using very large datasets, and may result in lost data.\nLack of auditing and revision control. This makes it difficult to determine who changed what and when. This can cause problems with regulatory compliance. Lack of revision control greatly increases the risk of errors due the inability to track, isolate and test changes made to a document.\nLack of security. Spreadsheets lack controls on who can see and modify particular data. This, combined with the lack of auditing above, can make it easy for someone to commit fraud.[56]\nBecause they are loosely structured, it is easy for someone to introduce an error, either accidentally or intentionally, by entering information in the wrong place or expressing dependencies among cells (such as in a formula) incorrectly.[47][57][58]\nThe results of a formula (example \"=A1*B1\") applies only to a single cell (that is, the cell the formula is actually located in—in this case perhaps C1), even though it can \"extract\" data from many other cells, and even real time dates and actual times. This means that to cause a similar calculation on an array of cells, an almost identical formula (but residing in its own \"output\" cell) must be repeated for each row of the \"input\" array. This differs from a \"formula\" in a conventional computer program, which typically makes one calculation that it applies to all the input in turn. With current spreadsheets, this forced repetition of near identical formulas can have detrimental consequences from a quality assurance standpoint and is often the cause of many spreadsheet errors. Some spreadsheets have array formulas to address this issue.\nTrying to manage the sheer volume of spreadsheets that may exist in an organization without proper security, audit trails, unintentional introduction of errors, and other items listed above can become overwhelming.\nWhile there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals \"don't know\" how their spreadsheets are audited; only 6% invest in a third-party solution[59]\n\nSpreadsheet risk[edit]\nFurther information: Financial modeling § Accounting\nSpreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g., out of date exchange rates). Some single-instance errors have exceeded US$1 billion.[60][61] Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.\n\nIn the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.\n\nDespite this, research[62] carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over £50m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.[62][63]\n\nIn 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010–13 European austerity programs. [64]",
          "subparadigms": []
        },
        {
          "pdid": 12,
          "name": "Reactive programming",
          "details": "In computing, reactive programming is a programming paradigm oriented around data flows and the propagation of change. This means that it should be possible to express static or dynamic data flows with ease in the programming languages used, and that the underlying execution model will automatically propagate changes through the data flow.\n\nFor example, in an imperative programming setting, {\\displaystyle a:=b+c} a:=b+c would mean that {\\displaystyle a} a is being assigned the result of {\\displaystyle b+c} b+c in the instant the expression is evaluated, and later, the values of {\\displaystyle b} b and {\\displaystyle c} c can be changed with no effect on the value of {\\displaystyle a} a. However, in reactive programming, the value of {\\displaystyle a} a would be automatically updated whenever the values of {\\displaystyle b} b and {\\displaystyle c} c change, without the program executing the sentence {\\displaystyle a:=b+c} a:=b+c again.\n\nAnother example is a hardware description language such as Verilog. In this case, reactive programming allows changes to be modeled as they propagate through a circuit.\n\nReactive programming has foremost been proposed as a way to simplify the creation of interactive user interfaces, animations in real time systems, but is essentially a general programming paradigm.\n\nFor example, in a model–view–controller architecture, reactive programming can allow changes in the underlying model to automatically be reflected in the view, and vice versa.[1]\n\nContents  [hide] \n1\tDefinition of Reactive Programming\n2\tApproaches to Creating Reactive Programming Languages\n3\tProgramming Models and Semantics\n4\tImplementation Techniques and Challenges\n4.1\tEssence of Implementations\n4.1.1\tChange Propagation Algorithms\n4.1.2\tWhat to push?\n4.2\tImplementation Challenges in Reactive Programming\n4.2.1\tGlitches\n4.2.2\tCyclic Dependencies\n4.2.3\tInteraction with Mutable State\n4.2.4\tDynamic Updating of the Graph of Dependencies\n5\tConcepts\n5.1\tDegrees of explicitness\n5.2\tStatic or Dynamic\n5.3\tHigher-order reactive programming\n5.4\tData flow differentiation\n5.5\tEvaluation models of reactive programming\n5.5.1\tSimilarities with observer pattern\n6\tApproaches\n6.1\tImperative\n6.2\tObject-oriented\n6.3\tFunctional\n7\tExamples\n7.1\tSpreadsheets\n8\tSee also\n9\tReferences\n10\tExternal links\nDefinition of Reactive Programming[edit]\nQuoting Gérard Berry:[2]\n\nIt is convenient to distinguish roughly between three kinds of computer programs. Transformational programs compute results from a given set of inputs; typical examples are compilers or numerical computation programs. Interactive programs interact at their own speed with users or with other programs; from a user point of view, a time-sharing system is interactive. Reactive programs also maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not the program itself. Interactive programs work at their own pace and mostly deal with communication, while reactive programs only work in respond to external demands and mostly deal with accurate interrupt handling. Real-time programs are usually reactive. However, there are reactive programs that are not usually considered as being real-time, such as protocols, system drivers, or man-machine interface handlers.\n\nApproaches to Creating Reactive Programming Languages[edit]\nThere are several popular approaches to creating reactive programming languages. Some are dedicated languages that are specific to some domain constraints (such as real-time or embedded computing or hardware description). Some are general-purpose languages that support reactivity. Finally, some are libraries or embedded domain-specific languages that enable reactivity alongside or on top of an existing general-purpose programming language. These different approaches result in trade-offs in the languages; in general, the more restricted the language, the more compilers and analysis tools can inform programmers (e.g., in performing analysis for whether programs can be executed in real time), while trading off general applicability.\n\nProgramming Models and Semantics[edit]\nA variety of models and semantics govern the family of reactive programming. We can loosely split them along the following dimensions:\n\nSynchrony: is the underlying model of time synchronous versus asynchronous?\nDeterminism: Deterministic versus non-deterministic in both evaluation process and results (the former does not necessarily imply the latter)\nUpdate process: callbacks versus dataflow versus actors\nImplementation Techniques and Challenges[edit]\nEssence of Implementations[edit]\nThe runtime of reactive programming languages usually relies on a graph that captures the dependencies among the reactive values. In the graph, nodes represent computations and edges model dependency relationships. The language runtime uses the graph to keep track of which computations must be executed again when one of the inputs changes.\n\nChange Propagation Algorithms[edit]\nThere are numerous implementation techniques used by reactive programming systems that represent the data flow graph explicitly. The most common algorithms are:\n\npull\npush\nhybrid push-pull\nWhat to push?[edit]\nAt the implementation level, reacting to an event consists of propagating across the graph the information that a change has happened. As a consequence, computations that are affected by the change and may be outdated are re-executed. These computations are usually in the transitive closure of the changed source. Change propagation may lead to an update of the sinks of the graph.\n\nThe information propagated in the graph can consist of the complete state of a node, i.e., the result of the computation of that node. In this case the previous output of the node is ignored. Another option is that changes are propagated incrementally. In this case, the information propagated along edges consists only of a delta that describes how the previous node has changed. The latter approach is especially important when nodes hold a large amount of state which would be expensive to recompute from scratch. Propagating deltas is essentially an optimization and has been extensively studied in incremental computing. This approach requires a solution to the view-update problem, which is well-known from databases maintaining views of changing data. Another common optimization is to accumulate changes and propagate a batch of them instead of a single one. This solution can be faster because it reduces communication among nodes and optimization strategies can reason about a batch of changes - for example two changes in the batch can cancel each other and can be simply ignored. Finally, it is also possible to propagate notifications of invalidity, causing nodes with invalid inputs to pull updates in order to update their own outputs.\n\nThere are two principal ways in which the dependency graph is built:\n\nThe graph of dependencies is maintained implicitly by an event loop. In this case, the registration of explicit callbacks creates implicit dependencies. This means that the inversion of control induced by callbacks is left in place; however, by making the callbacks functional (returning a state value instead of a unit value) callbacks become compositional.\nThe graph of dependencies is program-specific and given by the programmer. This approach enables addressing the inversion of control of callbacks in two ways: either the graph is specified explicitly (typically using a DSL which may be embedded), or the graph is implicitly defined by expressions and generated by \"the language\".\nImplementation Challenges in Reactive Programming[edit]\nGlitches[edit]\nWhen propagating changes, it is possible to pick propagation orders such that the value of an expression is not a natural consequence of the source program. We can illustrate this easily with an example. Suppose seconds is a reactive value that changes every second to represent the current time (in seconds). Consider this expression:\n\nt = seconds + 1\ng = (t > seconds)\nBecause t should always be greater than seconds, this expression should always evaluate to a true value. Unfortunately, this can depend on the order of evaluation. When seconds changes, two expressions have to update: seconds + 1 and the conditional. If the first evaluates before the second, then this invariant will hold. If, however, the conditional updates first, using the old value of t and the new value of seconds, then the expression will evaluate to a false value. This is called a glitch.\n\nSome reactive languages are glitch-free, and prove this property[citation needed]. This is usually achieved by topologically sorting expressions and updating values in topological order. This can, however, have performance implications, such as delaying the delivery of values (due to the order of propagation). In some cases, therefore, reactive languages permit glitches, and developers must be aware of the possibility that values may temporarily fail to correspond to the program source, and that some expressions may evaluate multiple times (for instance, t > seconds may evaluate twice: once when the new value of seconds arrives, and once more when t updates).\n\nCyclic Dependencies[edit]\nTopological sorting of dependencies depends on the dependency graph being a directed acyclic graph (DAG). In practice, a program may define a dependency graph that has cycles. Usually, reactive programming languages expect such cycles to be \"broken\" by placing some element along a \"back edge\" to permit reactive updating to terminate. Typically, languages provide an operator like delay that is used by the update mechanism for this purpose, since a delay implies that what follows must be evaluated in the \"next time step\" (allowing the current evaluation to terminate).\n\nInteraction with Mutable State[edit]\nReactive languages typically assume that their expressions are purely functional. This allows an update mechanism to choose different orders in which to perform updates, and leave the specific order unspecified (thereby enabling optimizations). When a reactive language is embedded in a programming language with state, however, it may be possible for programmers to perform mutable operations. How to make this interaction smooth remains an open problem.\n\nIn some cases, it is possible to have principled partial solutions. Two such solutions include:\n\nA language might offer a notion of \"mutable cell\". A mutable cell is one that the reactive update system is aware of, so that changes made to the cell propagate to the rest of the reactive program. This enables the non-reactive part of the program to perform a traditional mutation while enabling reactive code to be aware of and respond to this update, thus maintaining the consistency of the relationship between values in the program. An example of a reactive language that provides such a cell is FrTime.[3]\nProperly encapsulated object-oriented libraries offer an encapsulated notion of state. In principle, it is therefore possible for such a library to interact smoothly with the reactive portion of a language. For instance, callbacks can be installed in the getters of the object-oriented library to notify the reactive update engine about state changes, and changes in the reactive component can be pushed to the object-oriented library through getters. FrTime employs such a strategy.[4]\nDynamic Updating of the Graph of Dependencies[edit]\nIn some reactive languages, the graph of dependencies is static, i.e., the graph is fixed throughout the program's execution. In other languages, the graph can be \"dynamic\", i.e., it can change as the program executes. For a simple example, consider this illustrative example (where seconds is a reactive value):\n\nt =\n  if ((seconds mod 2) == 0):\n    seconds + 1\n  else:\n    seconds - 1\n  end\nt + 1\nEvery second, the value of this expression changes to a different reactive expression, which t + 1 then depends on. Therefore, the graph of dependencies updates every second.\n\nPermitting dynamic updating of dependencies provides significant expressive power (for instance, dynamic dependencies routinely occur in graphical user interface (GUI) programs). However, the reactive update engine must decide whether to reconstruct expressions each time, or to keep an expression's node constructed but inactive; in the latter case, ensure that they do not participate in the computation when they are not supposed to be active.\n\nConcepts[edit]\nDegrees of explicitness[edit]\nReactive programming languages can range from very explicit ones where data flows are set up by using arrows, to implicit where the data flows are derived from language constructs that look similar to those of imperative or functional programming. For example, in implicitly lifted functional reactive programming (FRP) a function call might implicitly cause a node in a data flow graph to be constructed. Reactive programming libraries for dynamic languages (such as the Lisp \"Cells\" and Python \"Trellis\" libraries) can construct a dependency graph from runtime analysis of the values read during a function's execution, allowing data flow specifications to be both implicit and dynamic.\n\nSometimes the term reactive programming refers to the architectural level of software engineering, where individual nodes in the data flow graph are ordinary programs that communicate with each other.\n\nStatic or Dynamic[edit]\nReactive programming can be purely static where the data flows are set up statically, or be dynamic where the data flows can change during the execution of a program.\n\nThe use of data switches in the data flow graph could to some extent make a static data flow graph appear as dynamic, and blur the distinction slightly. True dynamic reactive programming however could use imperative programming to reconstruct the data flow graph.\n\nHigher-order reactive programming[edit]\nReactive programming could be said to be of higher order if it supports the idea that data flows could be used to construct other data flows. That is, the resulting value out of a data flow is another data flow graph that is executed using the same evaluation model as the first.\n\nData flow differentiation[edit]\nIdeally all data changes are propagated instantly, but this cannot be assured in practice. Instead it might be necessary to give different parts of the data flow graph different evaluation priorities. This can be called differentiated reactive programming.[5]\n\nFor example, in a word processor the marking of spelling errors need not be totally in sync with the inserting of characters. Here differentiated reactive programming could potentially be used to give the spell checker lower priority, allowing it to be delayed while keeping other data-flows instantaneous.\n\nHowever, such differentiation introduces additional design complexity. For example, deciding how to define the different data flow areas, and how to handle event passing between different data flow areas.\n\nEvaluation models of reactive programming[edit]\nEvaluation of reactive programs is not necessarily based on how stack based programming languages are evaluated. Instead, when some data is changed, the change is propagated to all data that is derived partially or completely from the data that was changed. This change propagation could be achieved in a number of ways, where perhaps the most natural way is an invalidate/lazy-revalidate scheme.\n\nIt could be problematic simply to naively propagate a change using a stack, because of potential exponential update complexity if the data structure has a certain shape. One such shape can be described as \"repeated diamonds shape\", and has the following structure: An→Bn→An+1, An→Cn→An+1, where n=1,2... This problem could be overcome by propagating invalidation only when some data is not already invalidated, and later re-validate the data when needed using lazy evaluation.\n\nOne inherent problem for reactive programming is that most computations that would be evaluated and forgotten in a normal programming language, needs to be represented in the memory as data-structures.[citation needed] This could potentially make RP highly memory consuming. However, research on what is called lowering could potentially overcome this problem.[6]\n\nOn the other side, reactive programming is a form of what could be described as \"explicit parallelism\", and could therefore be beneficial for utilizing the power of parallel hardware.\n\nSimilarities with observer pattern[edit]\nReactive programming has principal similarities with the observer pattern commonly used in object-oriented programming. However, integrating the data flow concepts into the programming language would make it easier to express them and could therefore increase the granularity of the data flow graph. For example, the observer pattern commonly describes data-flows between whole objects/classes, whereas object-oriented reactive programming could target the members of objects/classes.\n\nThe stack-based evaluation model of common object orientation is also not entirely suitable for data-flow propagation, as occurrences of \"tree feedback edges\" in the data structures could make the program face exponential complexities. But because of its relatively limited use and low granularity, this is rarely a problem for the observer pattern in practice.\n\nApproaches[edit]\nImperative[edit]\nIt is possible to fuse reactive programming with ordinary imperative programming. In such a paradigm, imperative programs operate upon reactive data structures.[7] Such a set-up is analogous to constraint imperative programming; however, while constraint imperative programming manages bidirectional constraints, reactive imperative programming manages one-way dataflow constraints.\n\nObject-oriented[edit]\nObject-oriented reactive programming (OORP) is a combination of object oriented programming and reactive programming. Perhaps the most natural way to make such a combination is as follows: Instead of methods and fields, objects have reactions that automatically re-evaluate when the other reactions they depend on have been modified.[citation needed]\n\nBelow is an illustration of the A=X+Y introductory example using JavaScript and jQuery:[8]\n\n    X: <input type=\"text\" id=\"X\" /> <br />\n    Y: <input type=\"text\" id=\"Y\" /> <br />\n    A: <span id=\"A\"></span>\nfunction setA() {  // A=X+Y as integers\n   var A = parseInt($('#X').val()) + parseInt($('#Y').val());\n   $('#A').text(A);\n}\nsetA();  // for initial value of A\n$('#X,#Y').css('cursor', 'pointer').click(function () {\n   // by reaction to a click at X or at Y...\n   var obj = $(this);\n   obj.val(parseInt(obj.val()) + 1);  // updates X or Y \n   setA();  // updates A\n});\nIf an OORP programming language maintains its imperative methods, it would also fall under the category of imperative reactive programming.\n\nFunctional[edit]\nFunctional reactive programming (FRP) is a programming paradigm for reactive programming on functional programming.\n\nExamples[edit]\nSpreadsheets[edit]\nA modern spreadsheet is often cited as an example of reactive programming. This is problematic because the unqualified term \"spreadsheet\" may refer to either:\n\nThe underlying collection of cells, where each cell contains either a literal value or a formula that refers to other cells such as \"=B1+C1\". This table of cells is effectively a computer program that determines how a set of output cells are computed from a set of input cells. This may be saved to a file, and this is often referred to as a \"spreadsheet\" (e.g. \"the budget spreadsheet\").\nThe interactive application program with a graphical user interface that is used to edit and evaluate the underlying table of cells from (1). In virtually all spreadsheet applications, interactively changing any one cell on the sheet will result in immediately re-evaluating all formulas that directly or indirectly depend on that cell and updating the display to reflect these re-evaluations.\nConfusion arises because the spreadsheet application (2) is an example of a reactive program, while the program effectively defined by the underlying spreadsheet (1) is typically not itself a reactive program.[citation needed] Semantically, the underlying spreadsheet (1) simply determines a calculation from a set of input cells to a set of output cells, and thus could be directly translated to a simple transformational calculation (i.e. function) in a traditional programming language.",
          "subparadigms": []
        },
        {
          "pdid": 13,
          "name": "Dataflow programming",
          "details": "In computer programming, dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Dataflow programming languages share some features of functional languages, and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing. Some authors use the term Datastream instead of Dataflow to avoid confusion with Dataflow Computing or Dataflow architecture, based on an indeterministic machine paradigm. Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.\n\nContents  [hide] \n1\tProperties of dataflow programming languages\n1.1\tState\n1.2\tRepresentation\n2\tHistory\n3\tLanguages\n4\tApplication programming interfaces\n5\tSee also\n6\tReferences\n7\tExternal links\nProperties of dataflow programming languages[edit]\nTraditionally, a program is modeled as a series of operations happening in a specific order; this may be referred to as sequential,[1]:p.3 procedural,[2] Control flow[2] (indicating that the program chooses a specific path), or imperative programming. The program focuses on commands, in line with the von Neumann[1]:p.3 vision of sequential programming, where data is normally \"at rest\"[2]:p.7\n\nIn contrast, dataflow programming emphasizes the movement of data and models programs as a series of connections. Explicitly defined inputs and outputs connect operations, which function like black boxes.[2]:p.2 An operation runs as soon as all of its inputs become valid.[3] Thus, dataflow languages are inherently parallel and can work well in large, decentralized systems.[1]:p.3[4] [5]\n\nState[edit]\nOne of the key concepts in computer programming is the idea of state, essentially a snapshot of various conditions in the system. Most programming languages require a considerable amount of state information, which is generally hidden from the programmer. Often, the computer itself has no idea which piece of information encodes the enduring state. This is a serious problem, as the state information needs to be shared across multiple processors in parallel processing machines. Most languages force the programmer to add extra code to indicate which data and parts of the code are important to the state. This code tends to be both expensive in terms of performance, as well as difficult to read or debug. Explicit parallelism is one of the main reasons for the poor performance of Enterprise Java Beans when building data-intensive, non-OLTP applications.\n\nWhere a linear program can be imagined as a single worker moving between tasks (operations), a dataflow program is more like a series of workers on an assembly line, each doing a specific task whenever materials are available. Since the operations are only concerned with the availability of data inputs, they have no hidden state to track, and are all \"ready\" at the same time.\n\nRepresentation[edit]\nDataflow programs are represented in different ways. A traditional program is usually represented as a series of text instructions, which is reasonable for describing a serial system which pipes data between small, single-purpose tools that receive, process, and return. Dataflow programs start with an input, perhaps the command line parameters, and illustrate how that data is used and modified. The flow of data is explicit, often visually illustrated as a line or pipe.\n\nIn terms of encoding, a dataflow program might be implemented as a hash table, with uniquely identified inputs as the keys, used to look up pointers to the instructions. When any operation completes, the program scans down the list of operations until it finds the first operation where all inputs are currently valid, and runs it. When that operation finishes, it will typically output data, thereby making another operation become valid.\n\nFor parallel operation, only the list needs to be shared; it is the state of the entire program. Thus the task of maintaining state is removed from the programmer and given to the language's runtime. On machines with a single processor core where an implementation designed for parallel operation would simply introduce overhead, this overhead can be removed completely by using a different runtime.\n\nHistory[edit]\nA pioneer dataflow language was BLODI (BLOck DIagram), developed by John Larry Kelly, Jr., Carol Lochbaum and Victor A. Vyssotsky for specifying sampled data systems.[6] A BLODI specification of functional units (amplifiers, adders, delay lines, etc.) and their interconnections was compiled into a single loop that updated the entire system for one clock tick.\n\nMore conventional dataflow languages were originally developed in order to make parallel programming easier. In Bert Sutherland's 1966 Ph.D. thesis, The On-line Graphical Specification of Computer Procedures,[7] Sutherland created one of the first graphical dataflow programming frameworks. Subsequent dataflow languages were often developed at the large supercomputer labs. One of the most popular was SISAL, developed at Lawrence Livermore National Laboratory. SISAL looks like most statement-driven languages, but variables should be assigned once. This allows the compiler to easily identify the inputs and outputs. A number of offshoots of SISAL have been developed, including SAC, Single Assignment C, which tries to remain as close to the popular C programming language as possible.\n\nThe United States Navy funded development of ACOS and SPGN (signal processing graph notation) starting in early 1980's. This is in use on a number of platforms in the field today.[8]\n\nA more radical concept is Prograph, in which programs are constructed as graphs onscreen, and variables are replaced entirely with lines linking inputs to outputs. Incidentally, Prograph was originally written on the Macintosh, which remained single-processor until the introduction of the DayStar Genesis MP in 1996.\n\nThere are many hardware architectures oriented toward the efficient implementation of dataflow programming models. MIT's tagged token dataflow architecture was designed by Greg Papadopoulos.\n\nData flow has been proposed as an abstraction for specifying the global behavior of distributed system components: in the live distributed objects programming model, distributed data flows are used to store and communicate state, and as such, they play the role analogous to variables, fields, and parameters in Java-like programming languages.\n\nLanguages[edit]\n\nIt has been suggested that List of dataflow programming languages be merged into this section. (Discuss) Proposed since April 2015.\nAgilent VEE\nAlteryx\nANI\nDARTS\nASCET\nAviSynth scripting language, for video processing\nBLODI\nBMDFM Binary Modular Dataflow Machine\nCAL\nSPGN - Signal Processing Graph Notation\nCOStream\nCassandra-vision - A Visual programming language with OpenCV support and C++ extension API\nСorezoid - cloud OS with visual development environment for business process management within any company\nCuneiform, a functional workflow language.\nCurin\nCMS Pipelines\nHume\nJoule (programming language)\nJuttle\nKNIME\nKorduene\nLabVIEW, G[3]\nLinda\nLucid[2]\nLustre\nM, used as the backend of Microsoft Excel's ETL plugin Power Query.\nMaxCompiler - Designed by Maxeler Technologies and is compatible with OpenSPL\nMax/MSP\nMicrosoft Visual Programming Language - A component of Microsoft Robotics Studio designed for Robotics programming\nNextflow - A data-driven toolkit for computational pipelines based on the dataflow programming model\nOpenWire - adds visual dataflow programming capabilities to Delphi via VCL or FireMonkey components and a graphical editor (homonymous binary protocol is unrelated)\nOpenWire Studio - A visual development environment which allows the development of software prototypes by non developers.\nOrange - An open-source, visual programming tool for data mining, statistical data analysis, and machine learning.\nOz now also distributed since 1.4.0\nPifagor\nPipeline Pilot\nPOGOL\nPrograph\nPure Data\nPythonect\nQuartz Composer - Designed by Apple; used for graphic animations and effects\nRaftLib - Massive multi-threading with C++ IOStream-like Operators\nSAC Single Assignment C\nScala (with a library SynapseGrid)\nSIGNAL (a dataflow-oriented synchronous language enabling multi-clock specifications)\nSimulink\nSIMPL\nSISAL\nStreamIt - A programming language and a compilation infrastructure, specifically engineered for modern streaming systems.\nstromx - A visual programming environment focused on industrial vision (open source)\nSPL - Data Flow description language of IBM InfoSphere Streams streaming engine\nSWARM\nSystemVerilog - A hardware description language\nTersus - Visual programming platform (open source)\nVerilog - A hardware description language absorbed into the SystemVerilog standard in 2009\nVHDL - A hardware description language\nVignette's VBIS language for business processes integration\nvvvv\nVSXu\nWidget Workshop, a \"game\" designed for children which is technically a simplified dataflow programming language.\nXEE (Starlight) XML Engineering Environment\nXProc\nAnandamideScript, flexible data-flow scripting language for C++ and QT [9]\nApplication programming interfaces[edit]\nApache Airflow: Library for pything aimed at coordinating large computational workflows.\nDC: Library that allows the embedding of one-way dataflow constraints in a C/C++ program.\nSystemC: Library for C++, mainly aimed at hardware design.\nMDF: Library for python aimed at financial models",
          "subparadigms": [
            10,
            11,
            12
          ]
        },
        {
          "pdid": 14,
          "name": "Functional logic programming",
          "details": "Functional logic programming is the combination, in a single programming language, of the paradigms of functional programming (including higher-order programming) and logic programming (Nondeterministic programming, unification). This style of programming is embodied by various programming languages, including Curry and Mercury.\n\nA journal devoted to the integration of functional and logic programming was published by MIT Press and the European Association for Programming Languages and Systems between 1995 and 2008.[1]",
          "subparadigms": []
        },
        {
          "pdid": 15,
          "name": "Purely functional programming",
          "details": "In computer science, purely functional programming usually designates a programming paradigm—a style of building the structure and elements of computer programs—that treats all computation as the evaluation of mathematical functions. Purely functional programing may also be defined by forbidding changing-state and mutable data.\n\nPurely functional programing consists in restricting programming to its functional paradigm.\n\nContents  [hide] \n1\tDifference between pure and not-pure functional programming\n2\tProperties of purely functional program\n2.1\tStrict versus non-strict evaluation\n2.2\tParallel computing\n2.3\tData structures\n3\tPurely functional language\n4\tReferences\nDifference between pure and not-pure functional programming[edit]\nThe exact difference between pure and impure functional programing is a matter of controversy.[1]\n\nA program is usually said to be functional when it uses some concepts of functional programming, such as first-class functions and higher-order functions.[2] However, a first-class function may use techniques from the imperative paradigm, such as arrays or input/output methods are not purely functional programs, therefore the first-class function needs not be purely functional. In fact, the earliest programming languages cited as being functional, IPL and Lisp,[3][4] were both \"impure\" functional languages by the current definition.\n\nPurely functional data structures are persistent. Persistency is required for functional programming; without it, the same computation could return different results. Functional programming may use persistent non-purely functional data structures, while those data structures may not be used in purely functional programs.\n\nProperties of purely functional program[edit]\nStrict versus non-strict evaluation[edit]\nMain article: Evaluation strategy\nAll evaluation strategy which ends on a purely functional programs returns the same result. In particular, it ensures that the programmer does not have to consider in which order programs are evaluated, since eager evaluation will return the same result than lazy evaluation. However, it is still possible that an eager evaluation may not terminate while the lazy evaluation of the same program halts.\n\nParallel computing[edit]\nPurely functional programing simplifies parallel computing[5] since two purely functional parts of the evaluation never interact.\n\nData structures[edit]\nMain article: Purely functional data structure\nPurely functional data structures are often represented in a different way than their imperative counterparts.[6] For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nPurely functional language[edit]\nMain article: List of programming languages by type § Pure\nA purely functional language is a language which only admits purely functional programming. Purely functional programs can however be written in languages which are not purely functional.",
          "subparadigms": []
        },
        {
          "pdid": 16,
          "name": "Functional programming",
          "details": "In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions[1] or declarations[2] instead of statements. In functional code, the output value of a function depends only on the arguments that are input to the function, so calling a function f twice with the same value for an argument x will produce the same result f(x) each time. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.\n\nFunctional programming has its roots in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, logic programming, is based on relations.[3]\n\nIn contrast, imperative programming changes state with commands in the source language, the most simple example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.[3]\n\nFunctional programming languages, especially purely functional ones such as Hope, have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme,[4][5][6][7] Clojure,[8][9] Wolfram Language[10] (also known as Mathematica), Racket,[11] Erlang,[12][13][14] OCaml,[15][16] Haskell,[17][18] and F#[19][20] have been used in industrial and commercial applications by a wide variety of organizations. Functional programming is also supported in some domain-specific programming languages like R (statistics),[21] J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML),[22][23] and Opal.[24] Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.[25]\n\nProgramming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts.[26] This is also true of the PHP programming language.[27] C++11, Java 8, and C# 3.0 all added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala[28] – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.\n\nContents  [hide] \n1\tHistory\n2\tConcepts\n2.1\tFirst-class and higher-order functions\n2.2\tPure functions\n2.3\tRecursion\n2.4\tStrict versus non-strict evaluation\n2.5\tType systems\n2.6\tReferential transparency\n2.7\tFunctional programming in non-functional languages\n2.8\tData structures\n3\tComparison to imperative programming\n3.1\tSimulating state\n3.2\tEfficiency issues\n3.3\tCoding styles\n3.3.1\tPython\n3.3.2\tHaskell\n3.3.3\tPerl 6\n3.3.4\tErlang\n3.3.5\tElixir\n3.3.6\tLisp\n3.3.7\tClojure\n3.3.8\tD\n3.3.9\tR\n3.3.10\tSequenceL\n4\tUse in industry\n5\tIn education\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory[edit]\nLambda calculus provides a theoretical framework for describing functions and their evaluation. Although it is a mathematical abstraction rather than a programming language, it forms the basis of almost all functional programming languages today. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.[29]\n\nAn early functional-flavored language was Lisp, developed in the late 1950s for the IBM 700/7000 series scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT).[30] Lisp introduced many features now found in functional languages, though Lisp is technically a multi-paradigm language. Scheme and Dylan were later attempts to simplify and improve Lisp.\n\nInformation Processing Language (IPL) is sometimes cited as the first computer-based functional programming language.[31] It is an assembly-style language for manipulating lists of symbols. It does have a notion of \"generator\", which amounts to a function accepting a function as an argument, and, since it is an assembly-level language, code can be used as data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.\n\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\n\nJohn Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\".[32] He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style which has come to be associated with functional programming.\n\nIn the 1970s, ML was created by Robin Milner at the University of Edinburgh, and David Turner initially developed the language SASL at the University of St Andrews and later the language Miranda at the University of Kent. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL.[33] NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation.[34] Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope.[35] ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML. Meanwhile, the development of Scheme (a partly functional dialect of Lisp), as described in the influential Lambda Papers and the 1985 textbook Structure and Interpretation of Computer Programs, brought awareness of the power of functional programming to the wider programming-languages community.\n\nIn the 1980s, Per Martin-Löf developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs of arbitrarily complex mathematical propositions expressed as dependent types. This led to powerful new approaches to interactive theorem proving and has influenced the development of many subsequent functional programming languages.\n\nThe Haskell language began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\n\nConcepts[edit]\nA number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages are often hybrids of several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.[36]\n\nFirst-class and higher-order functions[edit]\nMain articles: First-class function and Higher-order function\nHigher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator {\\displaystyle d/dx} d/dx, which returns the derivative of a function {\\displaystyle f} f.\n\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term that describes programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\n\nHigher-order functions enable partial application or currying, a technique in which a function is applied to its arguments one at a time, with each application returning a new function that accepts the next argument. This allows one to succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\n\nPure functions[edit]\nPure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\n\nIf the result of a pure expression is not used, it can be removed without affecting other expressions.\nIf a pure function is called with arguments that cause no side-effects, the result is constant with respect to that argument list (sometimes called referential transparency), i.e. if the pure function is again called with the same arguments, the same result will be returned (this can enable caching optimizations such as memoization).\nIf there is no data dependency between two pure expressions, then their order can be reversed, or they can be performed in parallel and they cannot interfere with one another (in other terms, the evaluation of any pure expression is thread-safe).\nIf the entire language does not allow side-effects, then any evaluation strategy can be used; this gives the compiler freedom to reorder or combine the evaluation of expressions in a program (for example, using deforestation).\nWhile most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also allows functions to be designated \"pure\".\n\nRecursion[edit]\nMain article: Recursion (computer science)\nIteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Though some recursion requires maintaining a stack, tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. The Scheme language standard requires implementations to recognize and optimize tail recursion. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\n\nCommon patterns of recursion can be factored out using higher order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples. Such higher order functions play a role analogous to built-in control structures such as loops in imperative languages.\n\nMost general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.[37]\n\nStrict versus non-strict evaluation[edit]\nMain article: Evaluation strategy\nFunctional languages can be categorized by whether they use strict (eager) or non-strict (lazy) evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm will itself fail. For example, the expression:\n\nprint length([2+1, 3*2, 1/0, 5-4])\nwill fail under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function will return the value 4 (i.e., the number of items in the list), since evaluating it will not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\n\nThe usual implementation strategy for lazy evaluation in functional languages is graph reduction.[38] Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.\n\nHughes 1984 argues for lazy evaluation as a mechanism for improving program modularity through separation of concerns, by easing independent implementation of producers and consumers of data streams.[39] Launchbury 1993 describes some difficulties that lazy evaluation introduces, particularly in analyzing a program's storage requirements, and proposes an operational semantics to aid in such analysis.[40] Harper 2009 proposes including both strict and lazy evaluation in the same language, using the language's type system to distinguish them.[41]\n\nType systems[edit]\nEspecially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), although they reject all invalid programs at runtime, when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.\n\nSome research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which allows types to depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with[citation needed]. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.[42]\n\nA limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience.[43] GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as \"case classes\"), and have been proposed as additions to other languages including Java and C#.[44]\n\nReferential transparency[edit]\nMain article: Referential transparency\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.[45]\n\nConsider C assignment statement x = x * 10, this changes the value assigned to the variable x. Let us say that the initial value of x was 1, then two consecutive evaluations of the variable x will yield 10 and 100 respectively. Clearly, replacing x = x * 10 with either 10 or 100 gives a program with different meaning, and so the expression is not referentially transparent. In fact, assignment statements are never referentially transparent.\n\nNow, consider another function such as int plusone(int x) {return x+1;} is transparent, as it will not implicitly change the input x and thus has no such side effects. Functional programs exclusively use this type of function and are therefore referentially transparent.\n\nFunctional programming in non-functional languages[edit]\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages.[46] For example, both D and Fortran 95 explicitly support pure functions.[47]\n\nJavaScript, Lua[48] and Python had first class functions from their inception.[49] Amrit Prem added support to Python for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2,[50] though Python 3 relegated \"reduce\" to the functools standard library module.[51] First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, and C++11.[citation needed]\n\nIn Java, anonymous classes can sometimes be used to simulate closures;[52] however, anonymous classes are not always proper replacements to closures because they have more limited capabilities.[53] Java 8 supports lambda expressions as a replacement for some anonymous classes.[54] However, the presence of checked exceptions in Java can make functional programming inconvenient, because it can be necessary to catch checked exceptions and then rethrow them—a problem that does not occur in other JVM languages that do not have checked exceptions, such as Scala.[citation needed]\n\nIn C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\n\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\n\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages,[55] for example the tuple in Python, which is an immutable array.\n\nData structures[edit]\nMain article: Purely functional data structure\nPurely functional data structures are often represented in a different way than their imperative counterparts.[56] For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nComparison to imperative programming[edit]\nFunctional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\n\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.\n\nSimulating state[edit]\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\n\nThe pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).[57]\n\nAnother way in which functional languages can simulate state is by passing around a data structure that represents the current state as a parameter to function calls. On each function call, a copy of this data structure is created with whatever differences are the result of the function. This is referred to as 'state-passing style'.\n\nImpure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.\n\nAlternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.\n\nEfficiency issues[edit]\nFunctional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal.[58] This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware (which is a highly evolved Turing machine). Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree).[59] However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C.[60] For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\n\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.[61]\n\nLazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993[40] discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan et al. 2008[62] give some practical advice for analyzing and fixing them. However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles)[citation needed].\n\nCoding styles[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2013) (Learn how and when to remove this template message)\nImperative programs have the environment and a sequence of steps manipulating the environment. Functional programs have an expression that is successively substituted until it reaches normal form. An example illustrates this with different solutions to the same programming goal (calculating Fibonacci numbers).\n\nPython[edit]\nPrinting first 10 Fibonacci numbers, iterative\n\ndef fibonacci(n, first=0, second=1):\n    while n != 0:\n        print(first, end=\"\\n\") # side-effect\n        n, first, second = n - 1, second, first + second # assignment\nfibonacci(10)\nPrinting first 10 Fibonacci numbers, functional expression style\n\nfibonacci = (lambda n, first=0, second=1:\n    \"\" if n == 0 else\n    str(first) + \"\\n\" + fibonacci(n - 1, second, first + second))\nprint(fibonacci(10), end=\"\")\nPrinting a list with first 10 Fibonacci numbers, with generators\n\ndef fibonacci(n, first=0, second=1):\n    while n != 0:\n        yield first\n        n, first, second = n - 1, second, first + second # assignment\nprint(list(fibonacci(10)))\nPrinting a list with first 10 Fibonacci numbers, functional expression style\n\nfibonacci = (lambda n, first=0, second=1:\n    [] if n == 0 else\n    [first] + fibonacci(n - 1, second, first + second))\nprint(fibonacci(10))\nHaskell[edit]\nPrinting first 10 fibonacci numbers, functional expression style[1]\n\nfibonacci_aux = \\n first second->\n    if n == 0 then \"\" else\n    show first ++ \"\\n\" ++ fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStr (fibonacci 10)\nPrinting a list with first 10 fibonacci numbers, functional expression style[1]\n\nfibonacci_aux = \\n first second->\n    if n == 0 then [] else\n    [first] ++ fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1]\n\nfibonacci = \\n-> if n == 0 then 0\n                 else if n == 1 then 1\n                      else fibonacci(n - 1) + fibonacci(n - 2)\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style,[1] tail recursive\n\nfibonacci_aux = \\n first second->\n    if n == 0 then first else\n    fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with recursive lists\n\nfibonacci_aux = \\first second-> first : fibonacci_aux second (first + second)\nselect = \\n zs-> if n==0 then head zs\n                 else select (n - 1) (tail zs)\nfibonacci = \\n-> select n (fibonacci_aux 0 1)\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with primitives for recursive lists\n\nfibonacci_aux = \\first second-> first : fibonacci_aux second (first + second)\nfibonacci = \\n-> (fibonacci_aux 0 1) !! n\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with primitives for recursive lists, more concisely\n\nfibonacci_aux = 0:1:zipWith (+) fibonacci_aux (tail fibonacci_aux)\nfibonacci = \\n-> fibonacci_aux !! n\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional declaration style,[2] tail recursive\n\nfibonacci_aux 0 first _ = first\nfibonacci_aux n first second = fibonacci_aux (n - 1) second (first + second)\nfibonacci n = fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional declaration style, using lazy infinite lists and primitives\n\nfibs = 1 : 1 : zipWith (+) fibs (tail fibs) \n-- an infinite list of the fibonacci numbers\n-- fibs is defined in terms of fibs\nfibonacci = (fibs !!)\nmain = putStrLn $ show $ fibonacci 11\nPerl 6[edit]\nAs influenced by Haskell and others, Perl 6 has several functional and declarative approaches to problems. For example, you can declaratively build up a well-typed recursive version (the type constraints are optional) through signature pattern matching:\n\nsubset NonNegativeInt of Int where * >= 0;\n\nproto fib (|) is cached returns NonNegativeInt {*}\nmulti fib (0) { 0 }\nmulti fib (1) { 1 }\nmulti fib (NonNegativeInt $n) { fib($n - 1) + fib($n - 2) }\n\nfor ^10 -> $n { say fib($n) }\nAn alternative to this is to construct a lazy iterative sequence, which appears as an almost direct illustration of the sequence:\n\nmy @fib = 0, 1, *+* ... *; # Each additional entry is the sum of the previous two\n                           # and this sequence extends lazily indefinitely\nsay @fib[^10];             # Display the first 10 entries\nErlang[edit]\nErlang is a functional, concurrent, general-purpose programming language. A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. Use other algorithms for fast performance[63]):\n\n-module(fib).    % This is the file 'fib.erl', the module and the filename must match\n-export([fib/1]). % This exports the function 'fib' of arity 1\n\nfib(1) -> 1; % If 1, then return 1, otherwise (note the semicolon ; meaning 'else')\nfib(2) -> 1; % If 2, then return 1, otherwise\nfib(N) -> fib(N - 2) + fib(N - 1).\nElixir[edit]\nElixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM).\n\nThe Fibonacci function can be written in Elixir as follows:\n\ndefmodule Fibonacci do\n   def fib(0), do: 0\n   def fib(1), do: 1\n   def fib(n), do: fib(n-1) + fib(n-2)\nend\nLisp[edit]\nThe Fibonacci function can be written in Common Lisp as follows:\n\n(defun fib (n &optional (a 0) (b 1))\n  (if (= n 0)\n      a\n      (fib (- n 1) b (+ a b))))\nThe program can then be called as\n\n(fib 10)\nClojure[edit]\nThe Fibonacci function can be written in Clojure as follows:\n\n(defn fib\n  [n]\n  (loop [a 0 b 1 i n]\n    (if (zero? i)\n      a\n      (recur b (+ a b) (dec i)))))\nThe program can then be called as\n\n(fib 7)\nD[edit]\nD has support for functional programming[clarification needed][citation needed]:\n\nimport std.stdio;\nimport std.range;\n\nvoid main()\n{\n    /* 'f' is a range representing the first 10 Fibonacci numbers */\n    auto f = recurrence!((seq, i) => seq[0] + seq[1])(0, 1)\n             .take(10);\n\n    writeln(f);\n}\nR[edit]\nR is an environment for statistical computing and graphics. It is also a functional programming language.\n\nThe Fibonacci function can be written in R as a recursive function as follows:\n\nfib <- function(n) {\n if (n <= 2) 1\n else fib(n - 1) + fib(n - 2)\n}\nOr it can be written as a singly recursive function:\n\nfib <- function(n,a=1,b=1) { \n if (n == 1) a \n else fib(n-1,b,a+b) \n}\nOr it can be written as an iterative function:\n\nfib <- function(n) {\n if (n == 1) 1\n else if (n == 2) 1\n else {\n  fib<-c(1,1)\n  for (i in 3:n) fib<-c(0,fib[1])+fib[2]\n  fib[2]\n }\n}\nThe function can then be called as\n\nfib(10)\nSequenceL[edit]\nSequenceL is a functional, concurrent, general-purpose programming language. The Fibonacci function can be written in SequenceL as follows:\n\nfib(n) := n when n < 2 else\n          fib(n - 1) + fib(n - 2);\nThe function can then be called as\n\nfib(10)\nTo reduce the memory consumed by the call stack when computing a large Fibonacci term, a tail-recursive version can be used. A tail-recursive function is implemented by the SequenceL compiler as a memory-efficient looping structure:\n\nfib(n) := fib_Helper(0, 1, n);\n\nfib_Helper(prev, next, n) :=\n    prev when n < 1 else\n    next when n = 1 else\n    fib_Helper(next, next + prev, n - 1);\nUse in industry[edit]\nFunctional programming has long been popular in academia, but with few industrial applications.[64]:page 11 However, recently several prominent functional programming languages have been used in commercial or industrial systems. For example, the Erlang programming language, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems.[13] It has since become popular for building a range of applications at companies such as T-Mobile, Nortel, Facebook, Électricité de France and WhatsApp.[12][14][65][66][67] The Scheme dialect of Lisp was used as the basis for several applications on early Apple Macintosh computers,[4][5] and has more recently been applied to problems such as training simulation software[6] and telescope control.[7] OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis,[15] driver verification, industrial robot programming, and static analysis of embedded software.[16] Haskell, although initially intended as a research language,[18] has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.[17][18]\n\nOther functional programming languages that have seen use in industry include Scala,[68] F#,[19][20] (both being functional-OO hybrids with support for both purely functional and imperative programming) Wolfram Language,[10] Lisp,[69] Standard ML[70][71] and Clojure[72]\n\nIn education[edit]\nFunctional programming is being used as a method to teach problem solving, algebra and geometric concepts.[73] It has also been used as a tool to teach classical mechanics in Structure and Interpretation of Classical Mechanics.\n",
          "subparadigms": [
            14,
            15
          ]
        },
        {
          "pdid": 17,
          "name": "Abductive logic programming",
          "details": "Abductive logic programming (ALP) is a high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates. Problem solving is effected by deriving hypotheses on these abducible predicates (abductive hypotheses) as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abduction) or goals to be achieved (as in normal logic programming). It can be used to solve problems in diagnosis, planning, natural language and machine learning. It has also been used to interpret negation as failure as a form of abductive reasoning.\n\nContents  [hide] \n1\tSyntax\n2\tInformal meaning and problem solving\n2.1\tExample 1\n2.2\tExample 2\n2.3\tExample 3\n3\tFormal semantics\n4\tImplementation and systems\n5\tSee also\n6\tNotes\n7\tReferences\n8\tExternal links\nSyntax[edit]\nAbductive logic programs have three components, {\\displaystyle \\langle P,A,IC\\rangle ,} \\langle P,A,IC\\rangle, where:\n\nP is a logic program of exactly the same form as in logic programming\nA is a set of predicate names, called the abducible predicates\nIC is a set of first-order classical formulae.\nNormally, the logic program P does not contain any clauses whose head (or conclusion) refers to an abducible predicate. (This restriction can be made without loss of generality.) Also in practice, many times, the integrity constraints in IC are often restricted to the form of denials, i.e. clauses of the form:\n\n   false:- A1,...,An, not B1, ..., not Bm.\nSuch a constraint means that it is not possible for all A1,...,An to be true and at the same time all of B1,...,Bm to be false.\n\nInformal meaning and problem solving[edit]\nThe clauses in P define a set of non-abducible predicates and through this they provide a description (or model) of the problem domain. The integrity constraints in IC specify general properties of the problem domain that need to be respected in any solution of a problem.\n\nA problem, G, which expresses either an observation that needs to be explained or a goal that is desired, is represented by a conjunction of positive and negative (NAF) literals. Such problems are solved by computing \"abductive explanations\" of G.\n\nAn abductive explanation of a problem G is a set of positive (and sometimes also negative) ground instances of the abducible predicates, such that, when these are added to the logic program P, the problem G and the integrity constraints IC both hold. Thus abductive explanations extend the logic program P by the addition of full or partial definitions of the abducible predicates. In this way, abductive explanations form solutions of the problem according to the description of the problem domain in P and IC. The extension or completion of the problem description given by the abductive explanations provides new information, hitherto not contained in the solution to the problem. Quality criteria to prefer one solution over another, often expressed via integrity constraints, can be applied to select specific abductive explanations of the problem G.\n\nComputation in ALP combines the backwards reasoning of normal logic programming (to reduce problems to sub-problems) with a kind of integrity checking to show that the abductive explanations satisfy the integrity constraints.\n\nThe following two examples, written in simple structured English rather than in the strict syntax of ALP, illustrate the notion of abductive explanation in ALP and its relation to problem solving.\n\nExample 1[edit]\nThe abductive logic program, {\\displaystyle \\langle P,A,{\\mathit {IC}}\\rangle } \\langle P,A,\\mathit{IC} \\rangle, has in {\\displaystyle P} P the following sentences:\n\n  Grass is wet if it rained.\n  Grass is wet if the sprinkler was on.\n  The sun was shining.\nThe abducible predicates in {\\displaystyle A} A are \"it rained\" and \"the sprinkler was on\" and the only integrity constraint in {\\displaystyle {\\mathit {IC}}} \\mathit{IC} is:\n\n  false if it rained and the sun was shining.\nThe observation that the grass is wet has two potential explanations, \"it rained\" and \"the sprinkler was on\", which entail the observation. However, only the second potential explanation, \"the sprinkler was on\", satisfies the integrity constraint.\n\nExample 2[edit]\nConsider the abductive logic program consisting of the following (simplified) clauses:\n\n  X is a citizen if X is born in the USA.\n  X is a citizen if X is born outside the USA and X is a resident of the USA and X is naturalized.\n  X is a citizen if X is born outside the USA and Y is the mother of X and Y is a citizen and X is registered.\n  Mary is the mother of John.\n  Mary is a citizen.\ntogether with the five abducible predicates, \"is born in the USA\", \"is born outside the USA\", \"is a resident of the USA\", \"is naturalized\" and \"is registered\" and the integrity constraint:\n\n  false if John is a resident of the USA.\nThe goal \"John is citizen\" has two abductive solutions, one of which is \"John is born in the USA\", the other of which is \"John is born outside the USA\" and \"John is registered\". The potential solution of becoming a citizen by residence and naturalization fails because it violates the integrity constraint.\n\nA more complex example that is also written in the more formal syntax of ALP is the following.\n\nExample 3[edit]\nThe abductive logic program below describes a simple model of the lactose metabolism of the bacterium E. coli. The program, P, describes (in its first rule) that E. coli can feed on the sugar lactose if it makes two enzymes permease and galactosidase. Like all enzymes, these are made if they are coded by a gene (Gene) that is expressed (described by the second rule). The two enzymes of permease and galactosidase are coded by two genes, lac(y) and lac(z) respectively (stated in the fifth and sixth rule of the program), in a cluster of genes (lac(X)) – called an operon – that is expressed when the amounts (amt) of glucose are low and lactose are high or when they are both at medium level (see the fourth and fifth rule). The abducibles, A, declare all ground instances of the predicates \"amount\" as assumable. This reflects that in the model the amounts at any time of the various substances are unknown. This is incomplete information that is to be determined in each problem case. The integrity constraints, IC, state that the amount of any substance (S) can only take one value.\n\nDomain knowledge (P)\n\n   feed(lactose):-make(permease),make(galactosidase).\n   make(Enzyme):-code(Gene,Enzyme),express(Gene).\n   express(lac(X)):-amount(glucose,low),amount(lactose,hi).\n   express(lac(X)):-amount(glucose,medium),amount(lactose,medium).\n   code(lac(y),permease).\n   code(lac(z),galactosidase).\n   temperature(low):-amount(glucose,low).\nIntegrity constraints (IC)\n\n  false :- amount(S,V1), amount(S,V2), V1 ≠ V2.\nAbducibles (A)\n\n  abducible_predicate(amount).\nThe problem goal is {\\displaystyle G={\\text{feed(lactose)}}} G=\\text{feed(lactose)}. This can arise either as an observation to be explained or as a state of affairs to be achieved by finding a plan. This goal has two abductive explanations:\n\n{\\displaystyle \\Delta _{1}=\\{{\\text{amount(lactose, hi), amount(glucose, low)}}\\}} \\Delta_1=\\{\\text{amount(lactose, hi), amount(glucose, low)}\\}\n{\\displaystyle \\Delta _{2}=\\{{\\text{amount(lactose, medium), amount(glucose, medium)}}\\}} \\Delta_2=\\{\\text{amount(lactose, medium), amount(glucose, medium)}\\}\nThe decision which of the two to adopt could depend on addition information that is available, e.g. it may be known that when the level of glucose is low then the organism exhibits a certain behaviour – in the model such additional information is that the temperature of the organism is low – and by observing the truth or falsity of this it is possible to choose the first or second explanation respectively.\n\nOnce an explanation has been chosen, then this becomes part of the theory, which can be used to draw new conclusions. The explanation and more generally these new conclusions form the solution of the problem.\n\nFormal semantics[edit]\nThe formal semantics of the central notion of an abductive explanation in ALP can be defined in the following way.\n\nGiven an abductive logic program, {\\displaystyle \\langle P,A,{\\mathit {IC}}\\rangle } \\langle P,A,\\mathit{IC}\\rangle, an abductive explanation for a problem {\\displaystyle G} G is a set {\\displaystyle \\Delta } \\Delta  of ground atoms on abducible predicates such that:\n\n{\\displaystyle P\\cup \\Delta \\models G} P \\cup \\Delta \\models G\n{\\displaystyle P\\cup \\Delta \\models IC} P \\cup \\Delta \\models IC\n{\\displaystyle P\\cup \\Delta } P \\cup \\Delta is consistent\nThis definition leaves open the choice of the underlying semantics of logic programming through which we give the exact meaning of the entailment relation {\\displaystyle \\models } \\models  and the notion of consistency of the (extended) logic programs. Any of the different semantics of logic programming such as the completion, stable or well-founded semantics can (and have been used in practice) to give different notions of abductive explanations and thus different forms of ALP frameworks.\n\nThe above definition takes a particular view on the formalization of the role of the integrity constraints {\\displaystyle {\\mathit {IC}}} \\mathit{IC} as restrictions on the possible abductive solutions. It requires that these are entailed by the logic program extended with an abductive solution, thus meaning that in any model of the extended logic program (which one can think of as an ensuing world given {\\displaystyle \\Delta } \\Delta ) the requirements of the integrity constraints are met. In some cases this may be unnecessarily strong and the weaker requirement of consistency, namely that {\\displaystyle P\\cup {\\mathit {IC}}\\cup \\Delta } P \\cup \\mathit{IC} \\cup \\Delta is consistent, can be sufficient, meaning that there exists at least one model (possible ensuing world) of the extended program where the integrity constraints hold. In practice, in many cases these two ways of formalizing the role of the integrity constraints coincide as the logic program and its extensions always have a unique model. Many of the ALP systems use the entailment view of the integrity constraints as this can be easily implemented without the need for any extra specialized procedures for the satisfaction of the integrity constraints since this view treats the constraints in the same way as the problem goal. Note also that in many practical cases the third condition in this formal definition of an abductive explanation in ALP is either trivially satisfied or it is contained in the second condition via the use of specific integrity constraints that capture consistency.\n\nImplementation and systems[edit]\nMost of the implementations of ALP extend the SLD resolution-based computational model of logic programming. ALP can also be implemented by means of its link with Answer Set Programming (ASP), where the ASP systems can be employed. Examples of systems of the former approach are ACLP, A-system, CIFF, SCIFF, ABDUAL and ProLogICA.\n\nSee also[edit]\nAbductive reasoning\nAnswer set programming\nInductive logic programming\nNegation as failure\nArgumentation",
          "subparadigms": []
        },
        {
          "pdid": 18,
          "name": "Answer set programming",
          "details": "Answer set programming (ASP) is a form of declarative programming oriented towards difficult (primarily NP-hard) search problems. It is based on the stable model (answer set) semantics of logic programming. In ASP, search problems are reduced to computing stable models, and answer set solvers — programs for generating stable models—are used to perform search. The computational process employed in the design of many answer set solvers is an enhancement of the DPLL algorithm and, in principle, it always terminates (unlike Prolog query evaluation, which may lead to an infinite loop).\n\nIn a more general sense, ASP includes all applications of answer sets to knowledge representation[1][2] and the use of Prolog-style query evaluation for solving problems arising in these applications.\n\nContents  [hide] \n1\tHistory\n2\tAnswer set programming language AnsProlog\n3\tGenerating stable models\n4\tExamples of ASP programs\n4.1\tGraph coloring\n4.2\tLarge clique\n4.3\tHamiltonian cycle\n4.4\tDependency parsing\n5\tComparison of implementations\n6\tSee also\n7\tReferences\n8\tExternal links\nHistory[edit]\nThe planning method proposed in 1993 by Dimopoulos, Nebel and Köhler[3] is an early example of answer set programming. Their approach is based on the relationship between plans and stable models.[4] Soininen and Niemelä[5] applied what is now known as answer set programming to the problem of product configuration. The use of answer set solvers for search was identified as a new programming paradigm by Marek and Truszczyński in a paper that appeared in a 25-year perspective on the logic programming paradigm published in 1999 [6] and in [Niemelä 1999].[7] Indeed, the new terminology of \"answer set\" instead of \"stable model\" was first proposed by Lifschitz[8] in a paper appearing in the same retrospective volume as the Marek-Truszczynski paper.\n\nAnswer set programming language AnsProlog[edit]\nLparse is the name of the program that was originally created as a grounding tool (front-end) for the answer set solver smodels. The language that Lparse accepts is now commonly called AnsProlog*,[9] short for Answer Set Programming in Logic.[10] It is now used in the same way in many other answer set solvers, including assat, clasp, cmodels, gNt, nomore++ and pbmodels. (dlv is an exception; the syntax of ASP programs written for dlv is somewhat different.)\n\nAn AnsProlog program consists of rules of the form\n\n<head> :- <body> .\nThe symbol :- (\"if\") is dropped if <body> is empty; such rules are called facts. The simplest kind of Lparse rules are rules with constraints.\n\nOne other useful construct included in this language is choice. For instance, the choice rule\n\n{p,q,r}.\nsays: choose arbitrarily which of the atoms {\\displaystyle p,q,r} p,q,r to include in the stable model. The lparse program that contains this choice rule and no other rules has 8 stable models—arbitrary subsets of {\\displaystyle \\{p,q,r\\}} \\{p,q,r\\}. The definition of a stable model was generalized to programs with choice rules.[11] Choice rules can be treated also as abbreviations for propositional formulas under the stable model semantics.[12] For instance, the choice rule above can be viewed as shorthand for the conjunction of three \"excluded middle\" formulas:\n\n{\\displaystyle (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r).} (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r).\nThe language of lparse allows us also to write \"constrained\" choice rules, such as\n\n1{p,q,r}2.\nThis rule says: choose at least 1 of the atoms {\\displaystyle p,q,r} p,q,r, but not more than 2. The meaning of this rule under the stable model semantics is represented by the propositional formula\n\n{\\displaystyle (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r)} (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r)\n{\\displaystyle \\land \\,(p\\lor q\\lor r)\\land \\neg (p\\land q\\land r).} \\land \\,(p\\lor q\\lor r)\\land \\neg (p\\land q\\land r).\nCardinality bounds can be used in the body of a rule as well, for instance:\n\n:- 2{p,q,r}.\nAdding this constraint to an Lparse program eliminates the stable models that contain at least 2 of the atoms {\\displaystyle p,q,r} p,q,r. The meaning of this rule can be represented by the propositional formula\n\n{\\displaystyle \\neg ((p\\land q)\\lor (p\\land r)\\lor (q\\land r)).} \\neg ((p\\land q)\\lor (p\\land r)\\lor (q\\land r)).\nVariables (capitalized, as in Prolog) are used in Lparse to abbreviate collections of rules that follow the same pattern, and also to abbreviate collections of atoms within the same rule. For instance, the Lparse program\n\np(a). p(b). p(c).\nq(X) :- p(X), X!=a.\nhas the same meaning as\n\np(a). p(b). p(c).\nq(b). q(c).\nThe program\n\np(a). p(b). p(c).\n{q(X):-p(X)}2.\nis shorthand for\n\np(a). p(b). p(c).\n{q(a),q(b),q(c)}2.\nA range is of the form:\n\n<Predicate>(start..end)\nwhere start and end are constant valued arithmetic expressions. A range is a notational shortcut that is mainly used to define numerical domains in a compatible way. For example, the fact\n\na(1..3).\nis a shortcut for\n\na(1). a(2). a(3).\nRanges can also be used in rule bodies with the same semantics.\n\nA conditional literal is of the form:\n\np(X):q(X)\nIf the extension of q is {q(a1); q(a2); ... ; q(aN)}, the above condition is semantically equivalent to writing p(a1), p(a2), ... , p(aN) in the place of the condition. For example\n\nq(1..2).\na :- 1 {p(X):q(X)}.\nis a shorthand for\n\nq(1). q(2).\na :- 1 {p(1), p(2)}.\nGenerating stable models[edit]\nTo find a stable model of the Lparse program stored in file ${filename} we use the command\n\n% lparse ${filename} | smodels\nOption 0 instructs smodels to find all stable models of the program. For instance, if file test contains the rules\n\n1{p,q,r}2.\ns :- not p.\nthen the command produces the output\n\n% lparse test | smodels 0\nAnswer: 1\nStable Model: q p \nAnswer: 2\nStable Model: p \nAnswer: 3\nStable Model: r p \nAnswer: 4\nStable Model: q s \nAnswer: 5\nStable Model: r s \nAnswer: 6\nStable Model: r q s\nExamples of ASP programs[edit]\nGraph coloring[edit]\nAn {\\displaystyle n} n-coloring of a graph {\\displaystyle G=\\left\\langle V,E\\right\\rangle } {\\displaystyle G=\\left\\langle V,E\\right\\rangle } is a function {\\displaystyle color:V\\to \\{1,\\dots ,n\\}} {\\displaystyle color:V\\to \\{1,\\dots ,n\\}} such that {\\displaystyle color(x)\\neq color(y)} color(x)\\neq color(y) for every pair of adjacent vertices {\\displaystyle (x,y)\\in E} {\\displaystyle (x,y)\\in E}. We would like to use ASP to find an {\\displaystyle n} n-coloring of a given graph (or determine that it does not exist).\n\nThis can be accomplished using the following Lparse program:\n\nc(1..n).                                           \n1 {color(X,I) : c(I)} 1 :- v(X).             \n:- color(X,I), color(Y,I), e(X,Y), c(I).\nLine 1 defines the numbers {\\displaystyle 1,\\dots ,n} 1,\\dots ,n to be colors. According to the choice rule in Line 2, a unique color {\\displaystyle i} i should be assigned to each vertex {\\displaystyle x} x. The constraint in Line 3 prohibits assigning the same color to vertices {\\displaystyle x} x and {\\displaystyle y} y if there is an edge connecting them.\n\nIf we combine this file with a definition of {\\displaystyle G} G, such as\n\nv(1..100). % 1,...,100 are vertices\ne(1,55). % there is an edge from 1 to 55\n. . .\nand run smodels on it, with the numeric value of {\\displaystyle n} n specified on the command line, then the atoms of the form {\\displaystyle color(\\dots ,\\dots )} color(\\dots ,\\dots ) in the output of smodels will represent an {\\displaystyle n} n-coloring of {\\displaystyle G} G.\n\nThe program in this example illustrates the \"generate-and-test\" organization that is often found in simple ASP programs. The choice rule describes a set of \"potential solutions\" — a simple superset of the set of solutions to the given search problem. It is followed by a constraint, which eliminates all potential solutions that are not acceptable. However, the search process employed by smodels and other answer set solvers is not based on trial and error.\n\nLarge clique[edit]\nA clique in a graph is a set of pairwise adjacent vertices. The following lparse program finds a clique of size {\\displaystyle \\geq n} \\geq n in a given graph, or determines that it does not exist:\n\nn {in(X) : v(X)}.\n:- in(X), in(Y), v(X), v(Y), X!=Y, not e(X,Y), not e(Y,X).\nThis is another example of the generate-and-test organization. The choice rule in Line 1 \"generates\" all sets consisting of {\\displaystyle \\geq n} \\geq n vertices. The constraint in Line 2 \"weeds out\" the sets that are not cliques.\n\nHamiltonian cycle[edit]\nA Hamiltonian cycle in a directed graph is a cycle that passes through each vertex of the graph exactly once. The following Lparse program can be used to find a Hamiltonian cycle in a given directed graph if it exists; we assume that 0 is one of the vertices.\n\n{in(X,Y)} :- e(X,Y).\n\n:- 2 {in(X,Y) : e(X,Y)}, v(X).\n:- 2 {in(X,Y) : e(X,Y)}, v(Y).\n\nr(X) :- in(0,X), v(X).\nr(Y) :- r(X), in(X,Y), e(X,Y).\n\n:- not r(X), v(X).\nThe choice rule in Line 1 \"generates\" all subsets of the set of edges. The three constraints \"weed out\" the subsets that are not Hamiltonian cycles. The last of them uses the auxiliary predicate {\\displaystyle r(x)} r(x) (\" {\\displaystyle x} x is reachable from 0\") to prohibit the vertices that do not satisfy this condition. This predicate is defined recursively in Lines 4 and 5.\n\nThis program is an example of the more general \"generate, define and test\" organization: it includes the definition of an auxiliary predicate that helps us eliminate all \"bad\" potential solutions.\n\nDependency parsing[edit]\nIn natural language processing, dependency-based parsing can be formulated as an ASP problem.[13] The following code parses the Latin sentence Puella pulchra in villa linguam latinam discit \"the pretty girl is learning Latin in the villa\". The syntax tree is expressed by the arc predicates which represent the dependencies between the words of the sentence. The computed structure is a linearly ordered rooted tree.\n\n% ********** input sentence **********\nword(1, puella). word(2, pulchra). word(3, in). word(4, villa). word(5, linguam). word(6, latinam). word(7, discit).\n% ********** lexicon **********\n1{ node(X, attr(pulcher, a, fem, nom, sg));\n   node(X, attr(pulcher, a, fem, nom, sg)) }1 :- word(X, pulchra).\nnode(X, attr(latinus, a, fem, acc, sg)) :- word(X, latinam).\n1{ node(X, attr(puella, n, fem, nom, sg));\n   node(X, attr(puella, n, fem, abl, sg)) }1 :- word(X, puella).\n1{ node(X, attr(villa, n, fem, nom, sg));\n   node(X, attr(villa, n, fem, abl, sg)) }1 :- word(X, villa).\nnode(X, attr(linguam, n, fem, acc, sg)) :- word(X, linguam).\nnode(X, attr(discere, v, pres, 3, sg)) :- word(X, discit).\nnode(X, attr(in, p)) :- word(X, in).\n% ********** syntactic rules **********\n0{ arc(X, Y, subj) }1 :- node(X, attr(_, v, _, 3, sg)), node(Y, attr(_, n, _, nom, sg)).\n0{ arc(X, Y, dobj) }1 :- node(X, attr(_, v, _, 3, sg)), node(Y, attr(_, n, _, acc, sg)).\n0{ arc(X, Y, attr) }1 :- node(X, attr(_, n, Gender, Case, Number)), node(Y, attr(_, a, Gender, Case, Number)).\n0{ arc(X, Y, prep) }1 :- node(X, attr(_, p)), node(Y, attr(_, n, _, abl, _)), X < Y.\n0{ arc(X, Y, adv) }1 :- node(X, attr(_, v, _, _, _)), node(Y, attr(_, p)), not leaf(Y).\n% ********** guaranteeing the treeness of the graph **********\n1{ root(X):node(X, _) }1.\n:- arc(X, Z, _), arc(Y, Z, _), X != Y.\n:- arc(X, Y, L1), arc(X, Y, L2), L1 != L2.\npath(X, Y) :- arc(X, Y, _).\npath(X, Z) :- arc(X, Y, _), path(Y, Z).\n:- path(X, X).\n:- root(X), node(Y, _), X != Y, not path(X, Y).\nleaf(X) :- node(X, _), not arc(X, _, _).\nComparison of implementations[edit]\nEarly systems, such as Smodels, used backtracking to find solutions. As the theory and practice of Boolean SAT solvers evolved, a number of ASP solvers were built on top of SAT solvers, including ASSAT and Cmodels. These converted ASP formula into SAT propositions, applied the SAT solver, and then converted the solutions back to ASP form. More recent systems, such as Clasp, use a hybrid approach, using conflict-driven algorithms inspired by SAT, without full converting into a boolean-logic form. These approaches allow for significant improvements of performance, often by an order of magnitude, over earlier backtracking algorithms.\n\nThe Potassco project acts as an umbrella for many of the systems below, including clasp, grounding systems (gringo), incremental systems (iclingo), constraint solvers (clingcon), action language to ASP compilers (coala), distributed MPI implementations (claspar), and many others.\n\nMost systems support variables, but only indirectly, by forcing grounding, by using a grounding system such as Lparse or gringo as a front end. The need for grounding can cause a combinatorial explosion of clauses; thus, systems that perform on-the-fly grounding might have an advantage.\n\nPlatform\tFeatures\tMechanics\nName\tOS\tLicence\tVariables\tFunction symbols\tExplicit sets\tExplicit lists\tDisjunctive (choice rules) support\t\nASPeRiX\tLinux\tGPL\tYes\t\t\t\tNo\ton-the-fly grounding\nASSAT\tSolaris\tFreeware\t\t\t\t\t\tSAT-solver based\nClasp Answer Set Solver\tLinux, macOS, Windows\tGPL\tYes, in Clingo\tYes\tNo\tNo\tYes\tincremental, SAT-solver inspired (nogood, conflict-driven)\nCmodels\tLinux, Solaris\tGPL\tRequires grounding\t\t\t\tYes\tincremental, SAT-solver inspired (nogood, conflict-driven)\nDLV\tLinux, macOS, Windows[14]\tfree for academic and non-commercial educational use, and for non-profit organizations[14]\tYes\tYes\tNo\tNo\tYes\tnot Lparse compatible\nDLV-Complex\tLinux, macOS, Windows\tGPL\t\tYes\tYes\tYes\tYes\tbuilt on top of DLV — not Lparse compatible\nGnT\tLinux\tGPL\tRequires grounding\t\t\t\tYes\tbuilt on top of smodels\nnomore++\tLinux\tGPL\t\t\t\t\t\tcombined literal+rule-based\nPlatypus\tLinux, Solaris, Windows\tGPL\t\t\t\t\t\tdistributed, multi-threaded nomore++, smodels\nPbmodels\tLinux\t?\t\t\t\t\t\tpseudo-boolean solver based\nSmodels\tLinux, macOS, Windows\tGPL\tRequires grounding\tNo\tNo\tNo\tNo\t\nSmodels-cc\tLinux\t?\tRequires grounding\t\t\t\t\tSAT-solver based; smodels w/conflict clauses\nSup\tLinux\t?\t\t\t\t\t\tSAT-solver based\nSee also[edit]\nDefault logic\nLogic programming\nNon-monotonic logic\nProlog\nStable model semantics",
          "subparadigms": []
        },
        {
          "pdid": 19,
          "name": "Concurrent logic programming",
          "details": "Concurrent logic programming is a variant of logic programming in which programs are sets of guarded Horn clauses of the form:\n\nH :- G1, …, Gn | B1, …, Bn.\nThe conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator.\n\nDeclaratively, guarded Horn clauses are read as ordinary logical implications:\n\nH if G1 and … and Gn and B1 and … and Bn.\nHowever, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of \"don't care nondeterminism\", rather than \"don't know nondeterminism\".\n\nHistory[edit]\nThe first concurrent logic programming language was the Relational Language of Clark and Gregory, which was an offshoot of IC-Prolog. Later versions of concurrent logic programming include Shapiro's Concurrent Prolog and Ueda's Guarded Horn Clause language .\n\nThe development of concurrent logic programming was given an impetus when GHC was used to implement KL1, the systems programming language of the Japanese Fifth Generation Project (FGCS). The FGCS Project was a $400M initiative by Japan's Ministry of International Trade and Industry, begun in 1982, to use massively parallel computing/processing for artificial intelligence applications. The choice of concurrent logic programming as the “missing link” between the hardware and the applications was influenced by a visit to the FGCS Project in 1982 by Ehud Shapiro, who invented Concurrent Prolog.",
          "subparadigms": []
        },
        {
          "pdid": 20,
          "name": "Inductive logic programming",
          "details": "Inductive logic programming (ILP) is a subfield of machine learning which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\n\nSchema: positive examples + negative examples + background knowledge ⇒ hypothesis.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[1][2][3] Shapiro built its first implementation (Model Inference System) in 1981:[4] a Prolog program that inductively inferred logic programs from positive and negative examples. The term Inductive Logic Programming was first introduced[5] in a paper by Stephen Muggleton in 1991.[6] Muggleton also founded the annual international conference on Inductive Logic Programming, introduced the theoretical ideas of Predicate Invention, Inverse resolution,[7] and Inverse entailment,.[8] Muggleton implemented Inverse entailment first in the PROGOL system. The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction.\n\nContents  [hide] \n1\tFormal definition\n2\tExample\n3\tInductive Logic Programming system\n3.1\tHypothesis search\n3.2\tImplementations\n4\tSee also\n5\tReferences\n6\tFurther reading\nFormal definition[edit]\nThe background knowledge is given as a logic theory B, commonly in the form of Horn clauses used in logic programming. The positive and negative examples are given as a conjunction {\\displaystyle E^{+}} E^{+} and {\\displaystyle E^{-}} E^{-} of unnegated and negated ground literals, respectively. A correct hypothesis h is a logic proposition satisfying the following requirements.[9]\n\n{\\displaystyle {\\begin{array}{llll}{\\text{Necessity:}}&B&\\not \\models &E^{+}\\\\{\\text{Sufficiency:}}&B\\land h&\\color {blue}{\\models }&E^{+}\\\\{\\text{Weak consistency:}}&B\\land h&\\not \\models &{\\textit {false}}\\\\{\\text{Strong consistency:}}&B\\land h\\land E^{-}&\\not \\models &{\\textit {false}}\\end{array}}} {\\displaystyle {\\begin{array}{llll}{\\text{Necessity:}}&B&\\not \\models &E^{+}\\\\{\\text{Sufficiency:}}&B\\land h&\\color {blue}{\\models }&E^{+}\\\\{\\text{Weak consistency:}}&B\\land h&\\not \\models &{\\textit {false}}\\\\{\\text{Strong consistency:}}&B\\land h\\land E^{-}&\\not \\models &{\\textit {false}}\\end{array}}}\n\"Necessity\" does not impose a restriction on h, but forbids any generation of a hypothesis as long as the positive facts are explainable without it. \"Sufficiency\" requires any generated hypothesis h to explain all positive examples {\\displaystyle E^{+}} E^{+}. \"Weak consistency\" forbids generation of any hypothesis h that contradicts the background knowledge B. \"Strong consistency\" also forbids generation of any hypothesis h that is inconsistent with the negative examples {\\displaystyle E^{-}} E^{-}, given the background knowledge B; it implies \"Weak consistency\"; if no negative examples are given, both requirements coincide. Džeroski [10] requires only \"Sufficiency\" (called \"Completeness\" there) and \"Strong consistency\".\n\nExample[edit]\n\nAssumed family relations in section \"Example\"\nThe following well-known example about learning definitions of family relations uses the abbreviations\n\n{\\displaystyle {\\textit {par}}:{\\textit {parent}}} {\\textit  {par}}:{\\textit  {parent}}, {\\displaystyle {\\textit {fem}}:{\\textit {female}}} {\\textit  {fem}}:{\\textit  {female}}, {\\displaystyle {\\textit {dau}}:{\\textit {daughter}}} {\\textit  {dau}}:{\\textit  {daughter}}, {\\displaystyle g:{\\textit {George}}} g:{\\textit  {George}}, {\\displaystyle h:{\\textit {Helen}}} h:{\\textit  {Helen}}, {\\displaystyle m:{\\textit {Mary}}} m:{\\textit  {Mary}}, {\\displaystyle t:{\\textit {Tom}}} t:{\\textit  {Tom}}, {\\displaystyle n:{\\textit {Nancy}}} n:{\\textit  {Nancy}}, and {\\displaystyle e:{\\textit {Eve}}} e:{\\textit  {Eve}}.\nIt starts from the background knowledge (cf. picture)\n\n{\\displaystyle {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)} {\\textit  {par}}(h,m)\\land {\\textit  {par}}(h,t)\\land {\\textit  {par}}(g,m)\\land {\\textit  {par}}(t,e)\\land {\\textit  {par}}(n,e)\\land {\\textit  {fem}}(h)\\land {\\textit  {fem}}(m)\\land {\\textit  {fem}}(n)\\land {\\textit  {fem}}(e),\nthe positive examples\n\n{\\displaystyle {\\textit {dau}}(m,h)\\land {\\textit {dau}}(e,t)} {\\textit  {dau}}(m,h)\\land {\\textit  {dau}}(e,t),\nand the trivial proposition {\\displaystyle {\\textit {true}}} {\\textit  {true}} to denote the absence of negative examples.\n\nPlotkin's [11][12] \"relative least general generalization (rlgg)\" approach to inductive logic programming shall be used to obtain a suggestion about how to formally define the daughter relation {\\displaystyle {\\textit {dau}}} {\\textit  {dau}}.\n\nThis approach uses the following steps.\n\nRelativize each positive example literal with the complete background knowledge:\n{\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\end{aligned}}} {\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\end{aligned}}},\nConvert into clause normal form:\n{\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\end{aligned}}} {\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\end{aligned}}},\nAnti-unify each compatible [13] pair [14] of literals:\n{\\displaystyle {\\textit {dau}}(x_{me},x_{ht})} {\\textit  {dau}}(x_{{me}},x_{{ht}}) from {\\displaystyle {\\textit {dau}}(m,h)} {\\textit  {dau}}(m,h) and {\\displaystyle {\\textit {dau}}(e,t)} {\\textit  {dau}}(e,t),\n{\\displaystyle \\lnot {\\textit {par}}(x_{ht},x_{me})} \\lnot {\\textit  {par}}(x_{{ht}},x_{{me}}) from {\\displaystyle \\lnot {\\textit {par}}(h,m)} \\lnot {\\textit  {par}}(h,m) and {\\displaystyle \\lnot {\\textit {par}}(t,e)} \\lnot {\\textit  {par}}(t,e),\n{\\displaystyle \\lnot {\\textit {fem}}(x_{me})} \\lnot {\\textit  {fem}}(x_{{me}}) from {\\displaystyle \\lnot {\\textit {fem}}(m)} \\lnot {\\textit  {fem}}(m) and {\\displaystyle \\lnot {\\textit {fem}}(e)} \\lnot {\\textit  {fem}}(e),\n{\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m) from {\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m) and {\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m), similar for all other background-knowledge literals\n{\\displaystyle \\lnot {\\textit {par}}(x_{gt},x_{me})} \\lnot {\\textit  {par}}(x_{{gt}},x_{{me}}) from {\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m) and {\\displaystyle \\lnot {\\textit {par}}(t,e)} \\lnot {\\textit  {par}}(t,e), and many more negated literals\nDelete all negated literals containing variables that don't occur in a positive literal:\nafter deleting all negated literals containing other variables than {\\displaystyle x_{me},x_{ht}} x_{{me}},x_{{ht}}, only {\\displaystyle {\\textit {dau}}(x_{me},x_{ht})\\lor \\lnot {\\textit {par}}(x_{ht},x_{me})\\lor \\lnot {\\textit {fem}}(x_{me})} {\\textit  {dau}}(x_{{me}},x_{{ht}})\\lor \\lnot {\\textit  {par}}(x_{{ht}},x_{{me}})\\lor \\lnot {\\textit  {fem}}(x_{{me}}) remains, together with all ground literals from the background knowledge\nConvert clauses back to Horn form:\n{\\displaystyle {\\textit {dau}}(x_{me},x_{ht})\\leftarrow {\\textit {par}}(x_{ht},x_{me})\\land {\\textit {fem}}(x_{me})\\land ({\\text{all background knowledge facts}})} {\\textit  {dau}}(x_{{me}},x_{{ht}})\\leftarrow {\\textit  {par}}(x_{{ht}},x_{{me}})\\land {\\textit  {fem}}(x_{{me}})\\land ({\\text{all background knowledge facts}})\nThe resulting Horn clause is the hypothesis h obtained by the rlgg approach. Ignoring the background knowledge facts, the clause informally reads \" {\\displaystyle x_{me}} x_{{me}} is called a daughter of {\\displaystyle x_{ht}} x_{{ht}} if {\\displaystyle x_{ht}} x_{{ht}} is the parent of {\\displaystyle x_{me}} x_{{me}} and {\\displaystyle x_{me}} x_{{me}} is female\", which is a commonly accepted definition.\n\nConcerning the above requirements, \"Necessity\" was satisfied because the predicate {\\displaystyle {\\textit {dau}}} {\\textit  {dau}} doesn't appear in the background knowledge, which hence cannot imply any property containing this predicate, such as the positive examples are. \"Sufficiency\" is satisfied by the computed hypothesis h, since it, together with {\\displaystyle {\\textit {par}}(h,m)\\land {\\textit {fem}}(m)} {\\textit  {par}}(h,m)\\land {\\textit  {fem}}(m) from the background knowledge, implies the first positive example {\\displaystyle {\\textit {dau}}(m,h)} {\\textit  {dau}}(m,h), and similarly h and {\\displaystyle {\\textit {par}}(t,e)\\land {\\textit {fem}}(e)} {\\textit  {par}}(t,e)\\land {\\textit  {fem}}(e) from the background knowledge implies the second positive example {\\displaystyle {\\textit {dau}}(e,t)} {\\textit  {dau}}(e,t). \"Weak consistency\" is satisfied by h, since h holds in the (finite) Herbrand structure described by the background knowledge; similar for \"Strong consistency\".\n\nThe common definition of the grandmother relation, viz. {\\displaystyle {\\textit {gra}}(x,z)\\leftarrow {\\textit {fem}}(x)\\land {\\textit {par}}(x,y)\\land {\\textit {par}}(y,z)} {\\textit  {gra}}(x,z)\\leftarrow {\\textit  {fem}}(x)\\land {\\textit  {par}}(x,y)\\land {\\textit  {par}}(y,z), cannot be learned using the above approach, since the variable y occurs in the clause body only; the corresponding literals would have been deleted in the 4th step of the approach. To overcome this flaw, that step has to be modified such that it can be parametrized with different literal post-selection heuristics. Historically, the GOLEM implementation is based on the rlgg approach.\n\nInductive Logic Programming system[edit]\nInductive Logic Programming system is a program that takes as an input logic theories {\\displaystyle B,E^{+},E^{-}} B,E^{+},E^{-} and outputs a correct hypothesis H wrt theories {\\displaystyle B,E^{+},E^{-}} B,E^{+},E^{-} An algorithm of an ILP system consists of two parts: hypothesis search and hypothesis selection. First a hypothesis is searched with an inductive logic programming procedure, then a subset of the found hypotheses (in most systems one hypothesis) is chosen by a selection algorithm. A selection algorithm scores each of the found hypotheses and returns the ones with the highest score. An example of score function include minimal compression length where a hypothesis with a lowest Kolmogorov complexity has the highest score and is returned. An ILP system is complete iff for any input logic theories {\\displaystyle B,E^{+},E^{-}} B,E^{+},E^{-} any correct hypothesis H wrt to these input theories can be found with its hypothesis search procedure.\n\nHypothesis search[edit]\nModern ILP systems like Progol,[6] Hail [15] and Imparo [16] find a hypothesis H using the principle of the inverse entailment[6] for theories B, E, H: {\\displaystyle B\\land H\\models E\\iff B\\land \\neg E\\models \\neg H} B\\land H\\models E\\iff B\\land \\neg E\\models \\neg H. First they construct an intermediate theory F called a bridge theory satisfying the conditions {\\displaystyle B\\land \\neg E\\models F} B\\land \\neg E\\models F and {\\displaystyle F\\models \\neg H} F\\models \\neg H. Then as {\\displaystyle H\\models \\neg F} H\\models \\neg F, they generalize the negation of the bridge theory F with the anti-entailment.[17] However, the operation of the anti-entailment since being highly non-deterministic is computationally more expensive. Therefore, an alternative hypothesis search can be conducted using the operation of the inverse subsumption (anti-subsumption) instead which is less non-deterministic than anti-entailment.\n\nQuestions of completeness of a hypothesis search procedure of specific ILP system arise. For example, Progol's hypothesis search procedure based on the inverse entailment inference rule is not complete by Yamamoto's example.[18] On the other hand, Imparo is complete by both anti-entailment procedure [19] and its extended inverse subsumption [20] procedure.\n\nImplementations[edit]\n1BC and 1BC2: first-order naive Bayesian classifiers: (http://www.cs.bris.ac.uk/Research/MachineLearning/1BC/)\nACE (A Combined Engine) (http://dtai.cs.kuleuven.be/ACE/)\nAleph (http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/)\nAtom (http://www.ahlgren.info/research/atom/)\nClaudien (http://dtai.cs.kuleuven.be/claudien/)\nDL-Learner (http://dl-learner.org)\nDMax (http://dtai.cs.kuleuven.be/dmax/)\nFOIL (ftp://ftp.cs.su.oz.au/pub/foil6.sh)\nGolem (ILP) (http://www.doc.ic.ac.uk/~shm/Software/golem)\nImparo[19]\nInthelex (INcremental THEory Learner from EXamples) (http://lacam.di.uniba.it:8000/systems/inthelex/)\nLime (http://cs.anu.edu.au/people/Eric.McCreath/lime.html)\nMetagol (http://github.com/metagol/metagol)\nMio (http://libra.msra.cn/Publication/3392493/mio-user-s-manual)\nMIS (Model Inference System) by Ehud Shapiro\nPROGOL (http://www.doc.ic.ac.uk/~shm/Software/progol5.0)\nRSD (http://labe.felk.cvut.cz/~zelezny/rsd/)\nTertius (http://www.cs.bris.ac.uk/publications/Papers/1000545.pdf)\nWarmr (now included in ACE)\nProGolem (http://ilp.doc.ic.ac.uk/ProGolem/) [21][22]\nSee also[edit]\nCommonsense reasoning\nFormal concept analysis\nInductive inference\nInductive reasoning\nInductive programming\nInductive probability\nStatistical relational learning\nVersion space learning",
          "subparadigms": []
        },
        {
          "pdid": 21,
          "name": "Logic programming",
          "details": "Logic programming is a programming paradigm based on formal logic. A program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n\nH :- B1, …, Bn.\nand are read declaratively as logical implications:\n\nH if B1 and … and Bn.\nH is called the head of the rule and B1, …, Bn is called the body. Facts are rules that have no body, and are written in the simplified form:\n\nH.\nIn the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.\n\nIn ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:\n\nto solve H, solve B1, and ... and solve Bn.\nConsider, for example, the following clause:\n\nfallible(X) :- human(X).\nbased on an example used by Terry Winograd [1] to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:\n\nhuman(socrates).\ncan be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by \"assigning\" socrates to X.\n\nThe declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.\n\nContents  [hide] \n1\tHistory\n2\tConcepts\n2.1\tLogic and control\n2.2\tProblem solving\n2.3\tNegation as failure\n2.4\tKnowledge representation\n3\tVariants and extensions\n3.1\tProlog\n3.2\tAbductive logic programming\n3.3\tMetalogic programming\n3.4\tConstraint logic programming\n3.5\tConcurrent logic programming\n3.6\tConcurrent constraint logic programming\n3.7\tInductive logic programming\n3.8\tHigher-order logic programming\n3.9\tLinear logic programming\n3.10\tObject-oriented logic programming\n3.11\tTransaction logic programming\n4\tSee also\n5\tReferences\n5.1\tGeneral introductions\n5.2\tOther sources\n6\tFurther reading\n7\tExternal links\nHistory[edit]\nThe use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.[2] This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.[3]\n\nLogic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.[citation needed]\n\nAlthough it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm.[4] Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time.[1] To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.[citation needed]\n\nHayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover.[5] Kowalski, on the other hand, developed SLD resolution,[6] a variant of SL-resolution,[7] and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.\n\nThe Association for Logic Programming was founded to promote Logic Programming in 1986.\n\nProlog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages,[8] constraint logic programming languages and datalog.[citation needed]\n\nConcepts[edit]\nLogic and control[edit]\nMain article: Declarative programming\nLogic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan\n\nAlgorithm = Logic + Control\nwhere \"Logic\" represents a logic program and \"Control\" represents different theorem-proving strategies.[9]\n\nProblem solving[edit]\nIn the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an \"and\". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an \"or\".\n\nAny search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.\n\nIn the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.\n\nNegation as failure[edit]\nMain article: Negation as failure\nFor most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:\n\nH :- A1, …, An, not B1, …, not Bn.\nand is read declaratively as a logical implication:\n\nH if A1 and … and An and not B1 and … and not Bn.\nwhere H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as \"negation as failure\", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:\n\ncanfly(X) :- bird(X), not abnormal(X).\nabnormal(X) :-  wounded(X).\nbird(john).\nbird(mary).\nwounded(john).\nGiven the goal of finding something that can fly:\n\n:- canfly(X).\nthere are two candidate solutions, which solve the first subgoal bird(X), namely X = john and X = mary. The second subgoal not abnormal(john) of the first candidate solution fails, because wounded(john) succeeds and therefore abnormal(john) succeeds. However, The second subgoal not abnormal(mary) of the second candidate solution succeeds, because wounded(mary) fails and therefore abnormal(mary) fails. Therefore, X = mary is the only solution of the goal.\n\nMicro-Planner had a construct, called \"thnot\", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as not(Goal) or \\+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \\+ X == 1 fails when the variable X has been bound to the atom 1, but it succeeds in all other cases, including when X is unbound. This makes Prolog's reasoning non-monotonic: X = 1, \\+ X == 1 always fails, while \\+ X == 1, X = 1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).\n\nThe logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say\n\nH :- Body1.\n…\nH :- Bodyk.\nas a definition of the predicate\n\nH iff (Body1 or … or Bodyk)\nwhere \"iff\" means \"if and only if\". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.\n\nFor example, the completion of the program above is:\n\ncanfly(X) iff bird(X), not abnormal(X).\nabnormal(X) iff wounded(X).\nbird(X) iff X = john or X = mary.\nX = X.\nnot john = mary.\nnot mary = john.\nThe notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.\n\nAs an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in \"extended logic programming\", to formalise such phrases as \"the contrary can not be shown\", where \"contrary\" is classical negation and \"can not be shown\" is the epistemic interpretation of negation as failure.\n\nKnowledge representation[edit]\nThe fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.\n\nDespite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor [10] credit the representation of the British Nationality Act as a logic program [11] with being \"hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences\".\n\nVariants and extensions[edit]\nProlog[edit]\nMain article: Prolog\nThe programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.\n\nIt was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation\n\nH :- B1, …, Bn.\nwhich can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.[6]\n\nColmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.\n\nAbductive logic programming[edit]\nAbductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be \"open\" or undefined. A clause in an abductive logic program has the form:\n\nH :- B1, …, Bn, A1, …, An.\nwhere H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:\n\nfalse :- B1, …, Bn.\nwhere the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:\n\ncanfly(X) :- bird(X), normal(X).\nfalse :-  normal(X), wounded(X).\nbird(john).\nbird(mary).\nwounded(john).\nwhere the predicate normal is abducible.\n\nProblem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:\n\n:- canfly(X).\nAbductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.\n\nMetalogic programming[edit]\nBecause mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called \"vanilla\" meta-interpreter:\n\n    solve(true).\n    solve((A,B)):- solve(A),solve(B).\n    solve(A):- clause(A,B),solve(B).\nwhere true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.\n\nMetalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.\n\nConstraint logic programming[edit]\nMain article: Constraint logic programming\nConstraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:\n\nH :- C1, …, Cn {\\displaystyle \\Diamond } \\Diamond  B1, …, Bn.\nwhere H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:\n\nH if C1 and … and Cn and B1 and … and Bn.\nHowever, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.\n\nProcedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.\n\nThe following constraint logic program represents a toy temporal database of john's history as a teacher:\n\nteaches(john, hardware, T) :- 1990 ≤ T, T < 1999.\nteaches(john, software, T) :- 1999 ≤ T, T < 2005.\nteaches(john, logic, T) :- 2005 ≤ T, T ≤ 2012.\nrank(john, instructor, T) :- 1990 ≤ T, T < 2010.\nrank(john, professor, T) :- 2010 ≤ T, T < 2014.\nHere ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:\n\n:- teaches(john, logic, T), rank(john, professor, T).\nThe solution is 2010 ≤ T, T ≤ 2012.\n\nConstraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.\n\nConcurrent logic programming[edit]\nMain article: Concurrent logic programming\nConcurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).[12]\n\nA concurrent logic program is a set of guarded Horn clauses of the form:\n\nH :- G1, …, Gn | B1, …, Bn.\nThe conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:\n\nH if G1 and … and Gn and B1 and … and Bn.\nHowever, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of \"don't care nondeterminism\", rather than \"don't know nondeterminism\".\n\nFor example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:\n\nshuffle([], [], []).\nshuffle(Left, Right, Merge) :-\n    Left = [First | Rest] |\n    Merge = [First | ShortMerge],\n    shuffle(Rest, Right, ShortMerge).\nshuffle(Left, Right, Merge) :-\n    Right = [First | Rest] |\n    Merge = [First | ShortMerge],\n    shuffle(Left, Rest, ShortMerge).\nHere, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:\n\nshuffle([ace, queen, king], [1, 4, 2], Merge).\nThe program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].\n\nArguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced.[13] However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.[neutrality is disputed]\n\nConcurrent constraint logic programming[edit]\nMain article: Concurrent constraint logic programming\nConcurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.\n\nInductive logic programming[edit]\nMain article: Inductive logic programming\nInductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.\n\nHigher-order logic programming[edit]\nSeveral researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.\n\nLinear logic programming[edit]\nBasing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli,[14] ACL,[15] and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.\n\nObject-oriented logic programming[edit]\nF-logic extends logic programming with objects and the frame syntax. A number of systems are based on F-logic, including Flora-2, FLORID, and a highly scalable commercial system Ontobroker.\n\nLogtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. Highly portable, it supports most standard-complaint Prolog systems as backend compilers.\n\nTransaction logic programming[edit]\nTransaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.\n\nSee also[edit]\nBoolean satisfiability problem\nConstraint logic programming\nDatalog\nFril\nFunctional programming\nFuzzy logic\nInductive logic programming\nLogic in computer science (includes Formal methods)\nLogic programming languages\nProgramming paradigm\nR++\nReasoning system\nRule-based machine learning\nSatisfiability",
          "subparadigms": [
            16,
            17,
            18,
            19
          ]
        },
        {
          "pdid": 22,
          "name": "Declarative programming",
          "details": "In computer science, declarative programming is a programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.[1]\n\nMany languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives[2] (the how being left up to the language's implementation). This is in contrast with imperative programming, which implements algorithms in explicit steps.\n\nDeclarative programming often considers programs as theories of a formal logic, and computations as deductions in that logic space. Declarative programming may greatly simplify writing parallel programs.[3]\n\nCommon declarative languages include those of database query languages (e.g., SQL, XQuery), regular expressions, logic programming, functional programming, and configuration management systems.\n\nContents  [hide] \n1\tDefinition\n2\tSubparadigms\n2.1\tConstraint programming\n2.2\tDomain-specific languages\n2.3\tFunctional programming\n2.4\tHybrid languages\n2.5\tLogic programming\n2.6\tModeling\n3\tSee also\n4\tReferences\n5\tExternal links\nDefinition[edit]\nDeclarative programming is often defined as any style of programming that is not imperative. A number of other common definitions exist that attempt to give the term a definition other than simply contrasting it with imperative programming. For example:\n\nA program that describes what computation should be performed and not how to compute it\nAny programming language that lacks side effects (or more specifically, is referentially transparent)\nA language with a clear correspondence to mathematical logic.[4]\nThese definitions overlap substantially.\n\nDeclarative programming contrasts with imperative and procedural programming. Declarative programming is a non-imperative style of programming in which programs describe their desired results without explicitly listing commands or steps that must be performed. Functional and logical programming languages are characterized by a declarative programming style. In logical programming languages, programs consist of logical statements, and the program executes by searching for proofs of the statements.\n\nIn a pure functional language, such as Haskell, all functions are without side effects, and state changes are only represented as functions that transform the state, which is explicitly represented as a first class object in the program. Although pure functional languages are non-imperative, they often provide a facility for describing the effect of a function as a series of steps. Other functional languages, such as Lisp, OCaml and Erlang, support a mixture of procedural and functional programming.\n\nSome logical programming languages, such as Prolog, and database query languages, such as SQL, while declarative in principle, also support a procedural style of programming.\n\nSubparadigms[edit]\nDeclarative programming is an umbrella term that includes a number of better-known programming paradigms.\n\nConstraint programming[edit]\nConstraint programming states relations between variables in the form of constraints that specify the properties of the target solution. The set of constraints is solved by giving a value to each variable so that the solution is consistent with the maximum number of constraints. Constraint programming often complements other paradigms: functional, logical, or even imperative programming.\n\nDomain-specific languages[edit]\nWell-known examples of declarative domain-specific languages (DSLs) include the yacc parser generator input language, the Make build specification language, Puppet's configuration management language, regular expressions, and a subset of SQL (SELECT queries, for example). DSLs have the advantage of being useful while not necessarily needing to be Turing-complete, which makes it easier for a language to be purely declarative.\n\nMany markup languages such as HTML, MXML, XAML, XSLT or other user-interface markup languages are often declarative. HTML, for example, only describes what should appear on a webpage - it specifies neither control flow rendering a page nor its possible interactions with a user.\n\nAs of 2013 some software systems combine traditional user-interface markup languages (such as HTML) with declarative markup that defines what (but not how) the back-end server systems should do to support the declared interface. Such systems, typically using a domain-specific XML namespace, may include abstractions of SQL database syntax or parameterised calls to web services using representational state transfer (REST) and SOAP.\n\nFunctional programming[edit]\nFunctional programming, and in particular purely functional programming, attempts to minimize or eliminate side effects, and is therefore considered declarative. Most[citation needed] functional languages, such as Scheme, Clojure, Erlang, Haskell, OCaml, Standard ML and Unlambda, however, do permit side effects in practice.\n\nWhile functional languages typically do appear to specify \"how\", a compiler for a purely functional programming language is free to extensively rewrite the operational behavior of a function, so long as the same result is returned for the same inputs. This can be used to, for example, make a function compute its result in parallel, or to perform substantial optimizations (such as deforestation) that a compiler may not be able to safely apply to a language with side effects.\n\nHybrid languages[edit]\nSee also: Multi-paradigm programming language\nMakefiles, for example, specify dependencies in a declarative fashion,[5] but include an imperative list of actions to take as well. Similarly, yacc specifies a context free grammar declaratively, but includes code snippets from a host language, which is usually imperative (such as C).\n\nLogic programming[edit]\nLogic programming languages such as Prolog state and query relations. The specifics of how these queries are answered is up to the implementation and its theorem prover, but typically take the form of some sort of unification. Like functional programming, many logic programming languages permit side effects, and as a result are not strictly declarative.\n\nModeling[edit]\nMain article: Mathematical model\nModels, or mathematical representations, of physical systems may be implemented in computer code that is declarative. The code contains a number of equations, not imperative assignments, that describe (\"declare\") the behavioral relationships. When a model is expressed in this formalism, a computer is able to perform algebraic manipulations to best formulate the solution algorithm. The mathematical causality is typically imposed at the boundaries of the physical system, while the behavioral description of the system itself is declarative or acausal. Declarative modeling languages and environments include Analytica, Modelica and Simile.[6]\n\nSee also[edit]\nList of declarative programming languages\nComparison of programming paradigms\nInductive programming",
          "subparadigms": [
            9,
            13,
            16,
            21
          ]
        },
        {
          "pdid": 23,
          "name": "Dynamic programming language",
          "details": "Dynamic programming language, in computer science, is a class of high-level programming languages which, at runtime, execute many common programming behaviors that static programming languages perform during compilation. These behaviors could include extension of the program, by adding new code, by extending objects and definitions, or by modifying the type system. Although similar behaviours can be emulated in nearly any language, with varying degrees of difficulty, complexity and performance costs, dynamic languages provide direct tools to make use of them. Many of these features were first implemented as native features in the Lisp programming language.\n\nMost dynamic languages are also dynamically typed, but not all are. Dynamic languages are frequently (but not always) referred to as \"scripting languages\", although the term \"scripting language\" in its narrowest sense refers to languages specific to a given run-time environment.\n\n\n\nContents  [hide] \n1\tImplementation\n1.1\tEval\n1.2\tObject runtime alteration\n1.3\tFunctional programming\n1.3.1\tClosures\n1.3.2\tContinuations\n1.4\tReflection\n1.5\tMacros\n2\tExamples\n2.1\tComputation of code at runtime and late binding\n2.2\tObject runtime alteration\n2.3\tAssembling of code at runtime based on the class of instances\n3\tExamples\n4\tSee also\n5\tReferences\n6\tFurther reading\n7\tExternal links\nImplementation[edit]\n[icon]\tThis section needs expansion. You can help by adding to it. (October 2009)\nEval[edit]\nSome dynamic languages offer an eval function. This function takes a string parameter containing code in the language, and executes it. If this code stands for an expression, the resulting value is returned. However, Erik Meijer and Peter Drayton suggest that programmers \"use eval as a poor man's substitute for higher-order functions.\"[1]\n\nObject runtime alteration[edit]\nA type or object system can typically be modified during runtime in a dynamic language. This can mean generating new objects from a runtime definition or based on mixins of existing types or objects. This can also refer to changing the inheritance or type tree, and thus altering the way that existing types behave (especially with respect to the invocation of methods).\n\nFunctional programming[edit]\nFunctional programming concepts are a feature of many dynamic languages, and also derive from Lisp.\n\nClosures[edit]\nOne of the most widely used aspects of functional programming in dynamic languages is the closure, which allows creating a new instance of a function which retains access to the context in which it was created. A simple example of this is generating a function for scanning text for a word:\n\nfunction new_scanner (word)\n  temp_function = function (input)\n    scan_for_text (input, word)\n  end function\n  return temp_function\nend function\nNote that the inner function has no name, and is instead stored in the variable temp_function. Each time new_scanner is executed, it will return a new function which remembers the value of the word parameter that was passed in when it was defined.\n\nClosures[2] are one of the core tools of functional programming, and many languages support at least this degree of functional programming.\n\nContinuations[edit]\nAnother feature of some dynamic languages is the continuation. Continuations represent execution states that can be re-invoked. For example, a parser might return an intermediate result and a continuation that, when invoked, will continue to parse the input. Continuations interact in very complex ways with scoping, especially with respect to closures. For this reason, many dynamic languages do not provide continuations.\n\nReflection[edit]\nReflection is common in many dynamic languages, and typically involves analysis of the types and metadata of generic or polymorphic data. It can, however, also include full evaluation and modification of a program's code as data, such as the features that Lisp provides in analyzing S-expressions.\n\nMacros[edit]\nA limited number of dynamic programming languages provide features which combine code introspection (the ability to examine classes, functions and keywords to know what they are, what they do and what they know) and eval in a feature called macros. Most programmers today who are aware of the term macro have encountered them in C or C++, where they are a static feature which are built in a small subset of the language, and are capable only of string substitutions on the text of the program. In dynamic languages, however, they provide access to the inner workings of the compiler, and full access to the interpreter, virtual machine, or runtime, allowing the definition of language-like constructs which can optimize code or modify the syntax or grammar of the language.\n\nAssembly, C, C++, early Java, and FORTRAN do not generally fit into this category.[clarification needed]\n\nExamples[edit]\nThe following examples show dynamic features using the language Common Lisp and its Common Lisp Object System.\n\nComputation of code at runtime and late binding[edit]\nThe example shows how a function can be modified at runtime from computed source code\n\n; the source code is stored as data in a variable\nCL-USER > (defparameter *best-guess-formula* '(lambda (x) (* x x 2.5)))\n*BEST-GUESS-FORMULA*\n\n; a function is created from the code and compiled at runtime, the function is available under the name best-guess\nCL-USER >  (compile 'best-guess *best-guess-formula*)\n#<Function 15 40600152F4>\n\n; the function can be called\nCL-USER > (best-guess 10.3)\n265.225\n\n; the source code might be improved at runtime\nCL-USER > (setf *best-guess-formula* `(lambda (x) ,(list 'sqrt (third *best-guess-formula*))))\n(LAMBDA (X) (SQRT (* X X 2.5)))\n\n; a new version of the function is being compiled\nCL-USER > (compile 'best-guess *best-guess-formula*)\n#<Function 16 406000085C>\n\n; the next call will call the new function, a feature of late binding\nCL-USER > (best-guess 10.3)\n16.28573\nObject runtime alteration[edit]\nThis example shows how an existing instance can be changed to include a new slot when its class changes and that an existing method can be replaced with a new version.\n\n; a person class. The person has a name.\nCL-USER > (defclass person () ((name :initarg :name)))\n#<STANDARD-CLASS PERSON 4020081FB3>\n\n; a custom printing method for the objects of class person\nCL-USER > (defmethod print-object ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (format stream \"~a\" (slot-value p 'name))))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 4020066E5B>\n\n; one example person instance\nCL-USER > (setf *person-1* (make-instance 'person :name \"Eva Luator\"))\n#<PERSON Eva Luator>\n\n; the class person gets a second slot. It then has the slots name and age.\nCL-USER > (defclass person () ((name :initarg :name) (age :initarg :age :initform :unknown)))\n#<STANDARD-CLASS PERSON 4220333E23>\n\n; updating the method to print the object\nCL-USER > (defmethod print-object ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (format stream \"~a age: ~\" (slot-value p 'name) (slot-value p 'age))))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 402022ADE3>\n\n; the existing object has now changed, it has an additional slot and a new print method\nCL-USER > *person-1*\n#<PERSON Eva Luator age: UNKNOWN>\n\n; we can set the new age slot of instance\nCL-USER > (setf (slot-value *person-1* 'age) 25)\n25\n\n; the object has been updated\nCL-USER > *person-1*\n#<PERSON Eva Luator age: 25>\nAssembling of code at runtime based on the class of instances[edit]\nIn the next example the class person gets a new superclass. The print method gets redefined such that it assembles several methods into the effective method. The effective method gets assembled based on the class of the argument and the at runtime available and applicable methods.\n\n; the class person\nCL-USER > (defclass person () ((name :initarg :name)))\n#<STANDARD-CLASS PERSON 4220333E23>\n\n; a person just prints its name\nCL-USER > (defmethod print-object ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (format stream \"~a\" (slot-value p 'name))))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 40200605AB>\n\n; a person instance\nCL-USER > (defparameter *person-1* (make-instance 'person :name \"Eva Luator\"))\n*PERSON-1*\n\n; displaying a person instance\nCL-USER > *person-1*\n#<PERSON Eva Luator>\n\n; now redefining the print method to be extensible\n; the around method creates the context for the print method and it calls the next method\nCL-USER > (defmethod print-object :around ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (call-next-method)))\n#<STANDARD-METHOD PRINT-OBJECT (:AROUND) (PERSON T) 4020263743>\n\n; the primary method prints the name\nCL-USER > (defmethod print-object ((p person) stream)\n            (format stream \"~a\" (slot-value p 'name)))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 40202646BB>\n\n; a new class id-mixin provides an id\nCL-USER > (defclass id-mixin () ((id :initarg :id)))\n#<STANDARD-CLASS ID-MIXIN 422034A7AB>\n\n; the print method just prints the value of the id slot\nCL-USER > (defmethod print-object :after ((object id-mixin) stream)\n          (format stream \" ID: ~a\" (slot-value object 'id)))\n#<STANDARD-METHOD PRINT-OBJECT (:AFTER) (ID-MIXIN T) 4020278E33>\n\n; now we redefine the class person to include the mixin id-mixin\nCL-USER 241 > (defclass person (id-mixin) ((name :initarg :name)))\n#<STANDARD-CLASS PERSON 4220333E23>\n\n; the existing instance *person-1* now has a new slot and we set it to 42\nCL-USER 242 > (setf (slot-value *person-1* 'id) 42)\n42\n\n; displaying the object again. The print-object function now has an effective method, which calls three methods: an around method, the primary method and the after method.\nCL-USER 243 > *person-1*\n#<PERSON Eva Luator ID: 42>\nExamples[edit]\nPopular dynamic programming languages include JavaScript, Python, Ruby, PHP, Lua and Perl. The following are generally considered dynamic languages:\n\nActionScript\nBeanShell[3]\nC# (using Reflection)\nCobolscript\nClojure\nColdFusion Markup Language\nCommon Lisp and most other Lisps\nDylan\nE\nGambas\nGroovy[4]\nJava (using Reflection)\nJavaScript\nJulia\nLua\nMATLAB / Octave\nObjective-C\nPerl\nPHP\nPowershell\nProlog\nPython\nR\nRuby\nScala\nSmalltalk\nSuperCollider\nTcl\nVBScript\nWolfram Language\nGDScript",
          "subparadigms": []
        },
        {
          "pdid": 24,
          "name": "End-user development",
          "details": "End-user development (EUD) or end-user programming (EUP) refers to activities and tools that allow end-users – people who are not professional software developers – to program computers. People who are not professional developers can use EUD tools to create or modify software artifacts (descriptions of automated behavior) and complex data objects without significant knowledge of a programming language. Various approaches exist, and it is an active research topic within the field of computer science and human-computer interaction. Examples include spreadsheets, scripting languages (particularly in an office suite or art application), and programming by example.\n\nThe most popular EUD tool is the spreadsheet.[1] [2] Due to their unrestricted nature, spreadsheets allow relatively un-sophisticated computer users to write programs that represent complex data models, while shielding them from the need to learn lower-level programming languages.[3] Because of their common use in business, spreadsheet skills are among the most beneficial skills for a graduate employee to have, and are therefore the most commonly sought after[4] In the United States of America alone, there are an estimated 13 million end user developers programming with spreadsheets[5]\n\nEarly attempts in End-user development were centered on adding simple scripting programming languages to extend and adapt an existing application, such as an office suite.\n\nMore recent research tries to bring programming closer to the needs of end users. The Programming by example (PbE) approach reduces the need for the user to learn the abstractions of a classic programming language. The user instead introduces some examples of the desired results or operations that should be performed on the data, and the PbE system infers some abstractions corresponding to a program that produces this output, which the user can refine. New data may then be introduced to the automatically created program, and the user can correct any mistakes made by the program in order to improve its definition.\n\nThere are two basic reasons why EUD has become popular.[citation needed] One is because organizations are facing delays on projects and using EUD can effectively cut the time of completion on a project. The second reason is that software tools are more powerful and easier to use.\n\nLessons learned from EUD solutions can significantly influence the software life cycles for commercial software products, in-house intranet/extranet developments and enterprise application deployments.\n\nContents  [hide] \n1\tDefinition\n2\tExamples\n3\tCost-benefit modeling of end-user development\n4\tCriticism\n5\tSee also\n6\tReferences\n7\tFurther reading\n8\tExternal links\nDefinition[edit]\nLieberman et al. propose the following definition:[6]\n\nEnd-User Development can be defined as a set of methods, techniques, and tools that allow users of software systems, who are acting as non-professional software developers, at some point to create, modify or extend a software artifact.\n\nArtifacts defined by end users may be objects describing some automated behavior or control sequence, such as database requests or grammar rules,[7] which can be described with programming paradigms such as programming by demonstration, programming with examples, visual programming, or macro generation.[8] They can also be parameters that choose between alternative predefined behaviors of an application.[9] Other artifacts of end-user development may also refer to the creation of user-generated content such as annotations, which may be or not computationally interpretable (i.e. can be processed by associated automated functions).[10]\n\nExamples[edit]\nExamples of end-user development include the creation and modification of:\n\n3D models created with end-user oriented tools and approaches such as Sketchup\nAnimation scripts used by graphic artists to describe characters, environments and how characters move to produce an intended animation\nConfiguration files that blur the line between programs and data (e.g., email filters are sequenced lists of criteria and actions to take)\nExample-Centric Programming tools[11]\nArguably, contributions to open source projects where users of a software package contribute their own code for inclusion in the main package — in some cases, end-users participate as full-fledged developers\nGame modifications to introduce users' own characters, environments, etc. — many recent games are distributed with modification in mind\nInteraction scripts used in CRM call centres\nProcess models used in workflow applications\nPrototypes and domain-specific programs written by businesspeople, engineers, and scientists to demonstrate or test specific theories\nScientific models used in computer simulation\nScripts and macros added to extend or automate office productivity suites and graphics applications.\nSimulations created using application definition software\nSimultaneous editing of many related items either through a batch process specified by the end user or by direct manipulation, like those available in the Lapis text editor and multi edit.\nSpreadsheet models, e.g., used for budgeting, risk analysis, interactive machine learning,[12] or electronic circuit design[13]\nVisual programming in the form of visual languages such as AgentSheets, LabVIEW, Scratch (programming language) or LEGO Mindstorms.\nEnd-user mobile app development tools such as App Inventor\nWeb pages - plain HTML or HTML and scripting\nWikis - a collaborative end-user development process[citation needed]\nWeb Mashups in the form of visual languages such as Yahoo! Pipes.[14]\nVisual query systems such as OptiqueVQS.[15]\nCost-benefit modeling of end-user development[edit]\nAccording to Sutcliffe,[16] EUD essentially outsources development effort to the end user. Because there is always some effort to learn an EUD tool, the users' motivation depends on their confidence that it will empower their work, save time on the job or raise productivity. In this model, the benefits to users are initially based on marketing, demonstrations and word-of-mouth. Once the technology is put into use, experience of actual benefits becomes the key motivator.\n\nThis study defines costs as the sum of:\n\nTechnical cost: the price of the technology and the effort to install it\nLearning cost: the time taken to understand the technology\nDevelopment cost: the effort to develop applications using the technology\nTest and debugging cost: the time taken to verify the system\nThe first and second costs are incurred once during acquisition, whereas the third and fourth are incurred every time an application is developed. Benefits (which may be perceived or actual) are seen as:\n\nFunctionality delivered by the technology\nFlexibility to respond to new requirements\nUsability of applications produced\nOverall quality of the applications produced\nCriticism[edit]\nCommentators have been concerned that end users do not understand how to test and secure their applications. Warren Harrison, a professor of computer science at Portland State University, wrote:[17]\n\nIt’s simply unfathomable that we could expect security... from the vast majority of software applications out there when they’re written with little, if any, knowledge of generally accepted good practices such as specifying before coding, systematic testing, and so on.... How many X for Complete Idiots (where \"X\" is your favorite programming language) books are out there? I was initially amused by this trend, but recently I’ve become uneasy thinking about where these dabblers are applying their newfound knowledge.\n\nThis viewpoint assumes that all end users are equally naive when it comes to understanding software, although Pliskin and Shoval argue this is not the case, that sophisticated end users are capable of end-user development.[18]\n\nIn response to this, the study of end-user software engineering has emerged. It is concerned with issues beyond end-user development, whereby end users become motivated to consider issues such as reusability, security and verifiability when developing their solutions.[19]\n\nAn alternative scenario is that end users or their consultants employ declarative tools that support rigorous business and security rules at the expense of performance and scalability; tools created using EUD will typically have worse efficiency than those created with professional programming environments. Though separating functionality from efficiency is a valid separation of concerns, it can lead to a situation where end users will complete and document the requirements analysis and prototyping of the tool, without the involvement of business analysts. Thus, users will define the functions required before these experts have a chance to consider the limitations of a specific application or software framework. Senior management support for such end-user initiatives depends on their attitude to existing or potential vendor lock-in.\n\nSee also[edit]\nEnd-user computing\nSituational application\nSoftware engineering\nNatural language programming",
          "subparadigms": []
        },
        {
          "pdid": 25,
          "name": "Service-oriented",
          "details": "A service-oriented architecture (SOA) is a style of software design where services are provided to the other components by application components, through a communication protocol over a network. The basic principles of service oriented architecture are independent of vendors, products and technologies.[1] A service is a discrete unit of functionality that can be accessed remotely and acted upon and updated independently, such as retrieving a credit card statement online.\n\nA service has four properties according to one of many definitions of SOA:[2]\n\nIt logically represents a business activity with a specified outcome.\nIt is self-contained.\nIt is a black box for its consumers.\nIt may consist of other underlying services.[3]\nDifferent services can be used in conjunction to provide the functionality of a large software application.[4] Service-oriented architecture makes it easier for software components to communicate and cooperate over the network, without requiring any human interaction or changes in the underlying program, so that service candidates can be redesigned before their implementation.\n\nContents  [hide] \n1\tOverview\n2\tDefining concepts\n3\tPrinciples\n4\tPatterns\n5\tImplementation approaches\n6\tOrganizational benefits\n7\tCriticisms\n8\tExtensions and variants\n8.1\tEvent-driven architectures\n8.2\tWeb 2.0\n8.3\tMicroservices\n9\tSee also\n10\tReferences\n11\tExternal links\nOverview[edit]\nIn SOA, services use protocols which describe how they pass and parse messages using description metadata, this metadata describes both the functional characteristics of the service and quality-of-service characteristics. Service-oriented architecture aims to allow users to combine large chunks of functionality to form applications which are built purely from existing services and combining them in an ad hoc manner. A service presents a simple interface to the requester that abstracts away the underlying complexity acting as a black box, Further users can also access these independent services without any knowledge of their internal implementation.[5]\n\nDefining concepts[edit]\nThe related buzzword service-orientation promotes loose coupling between services. SOA separates functions into distinct units, or services,[6] which developers make accessible over a network in order to allow users to combine and reuse them in the production of applications. These services and their corresponding consumers communicate with each other by passing data in a well-defined, shared format, or by coordinating an activity between two or more services.[7]\n\nA manifesto was published for service-oriented architecture in October, 2009. This came up with six core values which are listed as follows[8]\n\nBusiness value is given more importance than technical strategy.\nStrategic goals is given more importance than project-specific benefits.\nIntrinsic inter-operability is given more importance than custom integration.\nShared services is given more importance than specific-purpose implementations.\nFlexibility is given more importance than optimization.\nEvolutionary refinement is given more importance than pursuit of initial perfection.\nSOA can be seen as part of the continuum which ranges from the older concept of distributed computing[6][9] and modular programming, through SOA, and on to current practices of mashups, SaaS, and cloud computing (which some see as the offspring of SOA).[10]\n\nPrinciples[edit]\nThere are no industry standards relating to the exact composition of a service-oriented architecture, although many industry sources have published their own principles. Some of these[11][12][13][14] include the following:\n\nStandardized service contract\nServices adhere to a standard communications agreements, as defined collectively by one or more service-description documents within a given set of services.\nService reference autonomy (an aspect of loose coupling)\nThe relationship between services is minimized to the level that they are only aware of their existence.\nService location transparency (an aspect of loose coupling)\nServices can be called from anywhere within the network that it is located no matter where it is present.\nService abstraction\nThe services act as black boxes, that is their inner logic is hidden from the consumers.\nService autonomy\nServices are independent and control the functionality they encapsulate, from a Design-time and a run-time perspective.\nService statelessness\nServices are stateless that is either return the requested value or a give an exception hence minimizing resource use.\nService granularity\nA principle to ensure services have an adequate size and scope. The functionality provided by the service to the user must be relevant.\nService normalization\nServices are decomposed or consolidated (normalized) to minimize redundancy. In some, this may not be done, These are the cases where performance optimization, access, and aggregation are required.[15]\nService composability\nServices can be used to compose other services.\nService discovery\nServices are supplemented with communicative meta data by which they can be effectively discovered and interpreted.\nService re usability\nLogic is divided into various services, to promote re use of code.\nService encapsulation\nMany services which were not initially planned under SOA, may get encapsulated or become a part of SOA.\nPatterns[edit]\nEach SOA building block can play any of the three roles:\n\nService provider\nIt creates a web service and provides its information to the service registry. Each provider debates upon a lot of hows and whys likes which service to expose, whom to give more importance: security or easy availability, what price to offer the service for and many more. The provider also has to decide what category the service should be listed in for a given broker service and what sort of trading partner agreements are required to use the service.\nService broker, service registry or service repository\nIts main functionality is to make the information regarding the web service available to any potential requester. Whoever implements the broker decides the scope of the broker. Public brokers are available anywhere and everywhere but private brokers are only available to a limited amount of public. UDDI was an early, no longer actively supported attempt to provide Web services discovery.\nService requester/consumer\nIt locates entries in the broker registry using various find operations and then binds to the service provider in order to invoke one of its web services. Whichever service the service-consumers need, they have to take it into the brokers, bind it with respective service and then use it. They can access multiple services if the service provides multiple services.\nThe service consumer-provider relationship is governed by a service contract, which has a business part, a functional part and a technical part.\n\nPossible first-class service composition patterns are two sides of the same coin. These patterns are generally event-driven:\n\nOrchestration is usually implemented and executed centrally through a Enterprise service bus\nChoreography is enacted by all participants and could be implemented with workflow management system [16]\nLower level Enterprise Integration Patterns that are not bound to a particular architectural style continue to be relevant and eligible in SOA design.[17][18][19]\n\nImplementation approaches[edit]\nService-oriented architecture can be implemented with Web services.[20] This is done to make the functional building-blocks accessible over standard Internet protocols that are independent of platforms and programming languages. These services can represent either new applications or just wrappers around existing legacy systems to make them network-enabled.[21]\n\nImplementers commonly build SOAs using web services standards (for example, SOAP) that have gained broad industry acceptance after recommendation of Version 1.2 from the W3C[22] (World Wide Web Consortium) in 2003. These standards (also referred to as web service specifications) also provide greater interoperability and some protection from lock-in to proprietary vendor software. One can, however, implement SOA using any service-based technology, such as Jini, CORBA or REST.\n\nArchitectures can operate independently of specific technologies and can therefore be implemented using a wide range of technologies, including:\n\nWeb services based on WSDL and SOAP\nMessaging, e.g., with ActiveMQ, JMS, RabbitMQ\nRESTful HTTP, with Representational state transfer (REST) constituting its own constraints-based architectural style\nOPC-UA\nWCF (Microsoft's implementation of Web services, forming a part of WCF)\nApache Thrift\nSORCER\nImplementations can use one or more of these protocols and, for example, might use a file-system mechanism to communicate data following a defined interface specification between processes conforming to the SOA concept. The key is independent services with defined interfaces that can be called to perform their tasks in a standard way, without a service having foreknowledge of the calling application, and without the application having or needing knowledge of how the service actually performs its tasks. SOA enables the development of applications that are built by combining loosely coupled and interoperable services.\n\nThese services inter-operate based on a formal definition (or contract, e.g., WSDL) that is independent of the underlying platform and programming language. The interface definition hides the implementation of the language-specific service. SOA-based systems can therefore function independently of development technologies and platforms (such as Java, .NET, etc.). Services written in C# running on .NET platforms and services written in Java running on Java EE platforms, for example, can both be consumed by a common composite application (or client). Applications running on either platform can also consume services running on the other as web services that facilitate reuse. Managed environments can also wrap COBOL legacy systems and present them as software services..[23]\n\nHigh-level programming languages such as BPEL and specifications such as WS-CDL and WS-Coordination extend the service concept by providing a method of defining and supporting orchestration of fine-grained services into more coarse-grained business services, which architects can in turn incorporate into workflows and business processes implemented in composite applications or portals[24]\n\nService-oriented modeling is an SOA framework that identifies the various disciplines that guide SOA practitioners to conceptualize, analyze, design, and architect their service-oriented assets. The Service-oriented modeling framework (SOMF) offers a modeling language and a work structure or \"map\" depicting the various components that contribute to a successful service-oriented modeling approach. It illustrates the major elements that identify the \"what to do\" aspects of a service development scheme. The model enables practitioners to craft a project plan and to identify the milestones of a service-oriented initiative. SOMF also provides a common modeling notation to address alignment between business and IT organizations.\n\n\nElements of SOA, by Dirk Krafzig, Karl Banke, and Dirk Slama[25]\n\nSOA meta-model, The Linthicum Group, 2007\nOrganizational benefits[edit]\nSome enterprise architects believe that SOA can help businesses respond more quickly and more cost-effectively to changing market conditions.[26] This style of architecture promotes reuse at the macro (service) level rather than micro (classes) level. It can also simplify interconnection to—and usage of—existing IT (legacy) assets.\n\nWith SOA, the idea is that an organization can look at a problem holistically. A business has more overall control. Theoretically there would not be a mass of developers using whatever tool sets might please them. But rather they would be coding to a standard that is set within the business. They can also develop enterprise-wide SOA that encapsulates a business-oriented infrastructure. SOA has also been illustrated as a highway system providing efficiency for car drivers. The point being that if everyone had a car, but there was no highway anywhere, things would be limited and disorganized, in any attempt to get anywhere quickly or efficiently. IBM Vice President of Web Services Michael Liebow says that SOA \"builds highways\".[27]\n\nIn some respects, SOA could be regarded as an architectural evolution rather than as a revolution. It captures many of the best practices of previous software architectures. In communications systems, for example, little development of solutions that use truly static bindings to talk to other equipment in the network has taken place. By embracing a SOA approach, such systems can position themselves to stress the importance of well-defined, highly inter-operable interfaces. Other predecessors of SOA include Component-based software engineering and Object-Oriented Analysis and Design (OOAD) of remote objects, for instance, in CORBA.\n\nA service comprises a stand-alone unit of functionality available only via a formally defined interface. Services can be some kind of \"nano-enterprises\" that are easy to produce and improve. Also services can be \"mega-corporations\" constructed as the coordinated work of subordinate services. A mature rollout of SOA effectively defines the API of an organization.\n\nReasons for treating the implementation of services as separate projects from larger projects include:\n\nSeparation promotes the concept to the business that services can be delivered quickly and independently from the larger and slower-moving projects common in the organization. The business starts understanding systems and simplified user interfaces calling on services. This advocates agility. That is to say, it fosters business innovations and speeds up time-to-market.[28]\nSeparation promotes the decoupling of services from consuming projects. This encourages good design insofar as the service is designed without knowing who its consumers are.\nDocumentation and test artifacts of the service are not embedded within the detail of the larger project. This is important when the service needs to be reused later.\nSOA promises to simplify testing indirectly. Services are autonomous, stateless, with fully documented interfaces, and separate from the cross-cutting concerns of the implementation. If an organization possesses appropriately defined test data, then a corresponding stub is built that reacts to the test data when a service is being built. A full set of regression tests, scripts, data, and responses is also captured for the service. The service can be tested as a 'black box' using existing stubs corresponding to the services it calls. Test environments can be constructed where the primitive and out-of-scope services are stubs, while the remainder of the mesh is test deployments of full services. As each interface is fully documented with its own full set of regression test documentation, it becomes simple to identify problems in test services. Testing evolves to merely validate that the test service operates according to its documentation, and finds gaps in documentation and test cases of all services within the environment. Managing the data state of idempotent services is the only complexity.\n\nExamples may prove useful to aid in documenting a service to the level where it becomes useful. The documentation of some APIs within the Java Community Process provide good examples. As these are exhaustive, staff would typically use only important subsets. The 'ossjsa.pdf' file within JSR-89 exemplifies such a file.[29]\n\nCriticisms[edit]\nSOA has been conflated with Web services.;[30] however, Web services are only one option to implement the patterns that comprise the SOA style. In the absence of native or binary forms of remote procedure call (RPC), applications could run more slowly and require more processing power, increasing costs. Most implementations do incur these overheads, but SOA can be implemented using technologies (for example, Java Business Integration (JBI), Windows Communication Foundation (WCF) and data distribution service (DDS)) that do not depend on remote procedure calls or translation through XML. At the same time, emerging open-source XML parsing technologies (such as VTD-XML) and various XML-compatible binary formats promise to significantly improve SOA performance. Services implemented using JSON instead of XML do not suffer from this performance concern.[31][32][33]\n\nStateful services require both the consumer and the provider to share the same consumer-specific context, which is either included in or referenced by messages exchanged between the provider and the consumer. This constraint has the drawback that it could reduce the overall scalability of the service provider if the service-provider needs to retain the shared context for each consumer. It also increases the coupling between a service provider and a consumer and makes switching service providers more difficult.[34] Ultimately, some critics feel that SOA services are still too constrained by applications they represent.[35]\n\nA primary challenge faced by service-oriented architecture is managing of metadata. Environments based on SOA include many services which communicate among each other to perform tasks. Due to the fact that the design may involve multiple services working in conjunction, an Application may generate millions of messages. Further services may belong to different organizations or even competing firms creating a huge trust issue. Thus SOA governance comes into the scheme of things.[36]\n\nAnother major problem faced by SOA is the lack of a uniform testing framework. There are no tools that provide the required features for testing these services in a Service Oriented Architecture. The major causes of difficulty are:[37]\n\nHeterogeneity and complexity of solution.\nHuge set of testing combinations due to integration of autonomous services.\nInclusion of services from different and competing vendors.\nPlatform is continuously changing due to availability of new features and services.\nSee [38] for additional challenges, partial solutions and research roadmap ionput regarding software service engineering\n\nExtensions and variants[edit]\nEvent-driven architectures[edit]\nMain article: Event-driven architecture\nWeb 2.0[edit]\nTim O'Reilly coined the term \"Web 2.0\" to describe a perceived, quickly growing set of web-based applications.[39] A topic that has experienced extensive coverage involves the relationship between Web 2.0 and service-oriented architectures.[which?]\n\nSOA is the philosophy of encapsulating application logic in services with a uniformly defined interface and making these publicly available via discovery mechanisms. The notion of complexity-hiding and reuse, but also the concept of loosely coupling services has inspired researchers to elaborate on similarities between the two philosophies, SOA and Web 2.0, and their respective applications. Some argue Web 2.0 and SOA have significantly different elements and thus can not be regarded \"parallel philosophies\", whereas others consider the two concepts as complementary and regard Web 2.0 as the global SOA.[40]\n\nThe philosophies of Web 2.0 and SOA serve different user needs and thus expose differences with respect to the design and also the technologies used in real-world applications. However, as of 2008, use-cases demonstrated the potential of combining technologies and principles of both Web 2.0 and SOA.[40]\n\nMicroservices[edit]\nMicroservices are a modern interpretation of service-oriented architectures used to build distributed software systems. Services in a microservice architecture[41] are processes that communicate with each other over the network in order to fulfill a goal. These services use technology agnostic protocols,[42] which aid in encapsulating choice of language and frameworks, making their choice a concern internal to the service. Microservices is a new realisation and implementation approach to SOA, which has become popular since 2014 (and after the introduction of DevOps), which also emphasizes continuous deployment and other agile practices.[43]\n\nSee also[edit]\nOASIS SOA Reference Model\nService (systems architecture)\nService granularity principle\nSOA governance\nSoftware architecture",
          "subparadigms": []
        },
        {
          "pdid": 26,
          "name": "Time-driven",
          "details": "Time-driven programming is a computer programming paradigm, where the control flow of the computer program is driven by a clock and is often used in Real-time computing. A program is divided into a set of tasks (i.e., processes or threads), each of which has a periodic activation pattern. The activation patterns are stored in a dispatch table ordered by time. The Least-Common-Multiple (LCM) of all period-times determines the length of the dispatch table. The scheduler of the program dispatches tasks by consulting the next entry in the dispatch table. After processing all entries, it continues by looping back to the beginning of the table.\n\nThe programming paradigm is mostly used for safety critical programs, since the behaviour of the program is highly deterministic. No external events are allowed to affect the control-flow of the program, the same pattern (i.e., described by the dispatch table) will be repeated time after time. However, idle time of the processor is also highly deterministic, allowing for the scheduling of other non-critical tasks through slack stealing techniques during these idle periods.\n\nThe drawback with the method is that the program becomes static (in the sense that small changes may recompile into large effects on execution structure), and unsuitable for applications requiring a large amount of flexibility. For example, the execution time of a task may change if its program code is altered. As a consequence, a new dispatch table must be regenerated for the entire task set. Such a change may require expensive retesting as is often required in safety critical systems.",
          "subparadigms": []
        },
        {
          "pdid": 27,
          "name": "Event-driven programming",
          "details": "In computer programming, event-driven programming is a programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs/threads. Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g. JavaScript web applications) that are centered on performing certain actions in response to user input.\n\nIn an event-driven application, there is generally a main loop that listens for events, and then triggers a callback function when one of those events is detected. In embedded systems the same may be achieved using hardware interrupts instead of a constantly running main loop. Event-driven programs can be written in any programming language, although the task is easier in languages that provide high-level abstractions, such as closures. In October 2016 Microsoft open-sourced the P programming language, which was designed specifically for safe event-driven programming.[1]\n\nContents  [hide] \n1\tEvent handlers\n1.1\tA trivial event handler\n1.2\tException handlers\n1.3\tCreating event handlers\n2\tCommon uses\n3\tCriticism\n4\tStackless threading\n5\tSee also\n6\tReferences\n7\tExternal links\nEvent handlers[edit]\nMain article: Event handler\nA trivial event handler[edit]\nBecause the code for checking for events and the main loop do not depend on the application, many programming frameworks take care of their implementation and expect the user to provide only the code for the event handlers. In this simple example there may be a call to an event handler called OnKeyEnter() that includes an argument with a string of characters, corresponding to what the user typed before hitting the ENTER key. To add two numbers, storage outside the event handler must be used. The implementation might look like below.\n\nglobally declare the counter K and the integer T.\nOnKeyEnter(character C)\n{\n   convert C to a number N\n   if K is zero store N in T and increment K\n   otherwise add N to T, print the result and reset K to zero\n}\nWhile keeping track of history is straightforward in a batch program, it requires special attention and planning in an event-driven program.\n\nException handlers[edit]\nIn PL/1, even though a program itself may not be predominantly event-driven, certain abnormal events such as a hardware error, overflow or \"program checks\" may occur that possibly prevent further processing. Exception handlers may be provided by \"ON statements\" in (unseen) callers to provide housekeeping routines to clean up afterwards before termination.\n\nCreating event handlers[edit]\nThe first step in developing an event-driven program is to write a series of subroutines, or methods, called event-handler routines. These routines handle the events to which the main program will respond. For example, a single left-button mouse-click on a command button in a GUI program may trigger a routine that will open another window, save data to a database or exit the application. Many modern-day programming environments provide the programmer with event templates, allowing the programmer to focus on writing the event code.\n\nThe second step is to bind event handlers to events so that the correct function is called when the event takes place. Graphical editors combine the first two steps: double-click on a button, and the editor creates an (empty) event handler associated with the user clicking the button and opens a text window so you can edit the event handler.\n\nThe third step in developing an event-driven program is to write the main loop. This is a function that checks for the occurrence of events, and then calls the matching event handler to process it. Most event-driven programming environments already provide this main loop, so it need not be specifically provided by the application programmer. RPG, an early programming language from IBM, whose 1960s design concept was similar to event-driven programming discussed above, provided a built-in main I/O loop (known as the \"program cycle\") where the calculations responded in accordance to 'indicators' (flags) that were set earlier in the cycle.\n\nCommon uses[edit]\nMost of existing GUI development tools and architectures rely on event-driven programming.[2]\n\nIn addition, systems such as Node.js are also event-driven[3]\n\nCriticism[edit]\nThe design of those programs which rely on event-action model has been criticised, and it has been suggested that event-action model leads programmers to create error prone, difficult to extend and excessively complex application code.[2] Table-driven state machines have been advocated as a viable alternative.[4] On the other hand, table-driven state machines themselves suffer from significant weaknesses including \"state explosion\" phenomenon[5]\n\nStackless threading[edit]\nAn event-driven approach is used in hardware description languages. A thread context only needs a CPU stack while actively processing an event, once done the CPU can move on to process other event-driven threads, which allows an extremely large number of threads to be handled. This is essentially a finite-state machine approach.\n\nSee also[edit]\nTime-triggered system (an alternative architecture for computer systems)\nInterrupt\nComparison of programming paradigms\nDataflow programming (a similar concept)\nDOM events\nEvent-driven architecture\nEvent stream processing (a similar concept)\nHardware description language\nInversion of control\nMessage-oriented middleware\nProgramming paradigm\nPublish–subscribe pattern\nSignal programming (a similar concept)\nStaged event-driven architecture (SEDA)\nVirtual synchrony, a distributed execution model for event-driven programming",
          "subparadigms": [
            25,
            26
          ]
        },
        {
          "pdid": 28,
          "name": "Expression-oriented programming language",
          "details": "An expression-oriented programming language is a programming language where every (or nearly every) construction is an expression and thus yields a value. The typical exceptions are macro definitions, preprocessor commands, and declarations, which expression-oriented languages often treat as statements rather than expressions. Some expression-oriented languages introduce a void return type to be yielded by expressions that merely cause side-effects.\n\nALGOL 68 and Lisp are examples of expression-oriented languages. Pascal is not an expression-oriented language. All functional programming languages are expression-oriented.\n\nContents  [hide] \n1\tCriticism\n2\tSee also\n3\tNotes\n4\tReferences\nCriticism[edit]\nCritics, including language designers,[1] blame expression-orientation for an entire class of programming mistakes wherein a programmer introduces an assignment expression where they meant to test for equality. For example, the designers of Ada and Java were so worried about this type of mistake, they restricted control expressions to those that evaluate strictly to the boolean data type.[2][3] The designers of Python had similar worries but took the alternative strategy of implementing assignment as a statement rather than an expression, thus prohibiting assignment from nesting inside of any other statement or expression.[4]",
          "subparadigms": []
        },
        {
          "pdid": 29,
          "name": "Feature-oriented programming",
          "details": "Feature Oriented Programming (FOP) or Feature Oriented Software Development (FOSD) is a paradigm for program generation in software product lines and for incremental development of programs.\n\nvertical stacking of layers\nConnection Between Layer Stacks and Transformation Compositions\nFOSD arose out of layer-based designs and levels of abstraction in network protocols and extensible database systems in the late-1980s.[1] A program was a stack of layers. Each layer added functionality to previously composed layers and different compositions of layers produced different programs. Not surprisingly, there was a need for a compact language to express such designs. Elementary algebra fit the bill: each layer was function (program transformation) that added new code to an existing program to produce a new program, and a program's design was modeled by an expression, i.e., a composition of transformations (layers). The figure to the left illustrates the stacking of layers i, j, and h (where h is on the bottom and i is on the top). The algebraic notations i(j(h)), i•j•h, and i+j+h have been used to express these designs.\n\n\nOver time, layers were equated to features, where a feature is an increment in program functionality. The paradigm for program design and generation was recognized to be an outgrowth of relational query optimization, where query evaluation programs were defined as relational algebra expressions, and query optimization was expression optimization.[2] A software product line (SPL) is a family of programs where each program is defined by a unique composition of features. FOSD has since evolved into the study of feature modularity, tools, analyses, and design techniques to support feature-based program generation.\n\nThe second generation of FOSD research was on feature interactions, which originated in telecommunications. Later, the term feature-oriented programming was coined;[3] this work exposed interactions between layers. Interactions require features to be adapted when composed with other features.\n\nA third generation of research focussed on the fact that every program has multiple representations (e.g., source, makefiles, documentation, etc.) and adding a feature to a program should elaborate each of its representations so that all are consistent. Additionally, some of representations could be generated (or derived) from others. In the sections below, the mathematics of the three most recent generations of FOSD, namely GenVoca,[1] AHEAD,[4] and FOMDD[5][6] are described, and links to product lines that have been developed using FOSD tools are provided. Also, four additional results that apply to all generations of FOSD are presented elsewhere: MetaModels, Program Cubes, and Feature Interactions.\n\nContents  [hide] \n1\tGenVoca\n2\tAHEAD\n3\tFOMDD\n4\tApplications\n5\tSee also\n6\tReferences\nGenVoca[edit]\nGenVoca (a portmanteau of the names Genesis and Avoca)[1] is a compositional paradigm for defining programs of product lines. Base programs are 0-ary functions or transformations called values:\n\n  f      -- base program with feature f\n  h      -- base program with feature h\nand features are unary functions/transformations that elaborate (modify, extend, refine) a program:\n\n  i + x  -- adds feature i to program x\n  j + x  -- adds feature j to program x\nwhere + denotes function composition. The design of a program is a named expression, e.g.:\n\n  p1 = j + f       -- program p1 has features j and f\n  p2 = j + h       -- program p2 has features j and h\n  p3 = i + j + h   -- program p3 has features i, j, and h\nA GenVoca model of a domain or software product line is a collection of base programs and features (see MetaModels and Program Cubes). The programs (expressions) that can be created defines a product line. Expression optimization is program design optimization, and expression evaluation is program generation.\n\nNote: GenVoca is based on the stepwise development of programs: a process that emphasizes design simplicity and understandability, which are key to program comprehension and automated program construction. Consider program p3 above: it begins with base program h, then feature j is added (read: the functionality of feature j is added to the codebase of h), and finally feature i is added (read: the functionality of feature i is added to the codebase of j•h).\nNote: not all combinations of features are meaningful. Feature models (which can be translated into propositional formulas) are graphical representations that define legal combinations of features.[7]\nNote: A more recent formulation of GenVoca is symmetric: there is only one base program, 0 (the empty program), and all features are unary functions. This suggests the interpretation that GenVoca composes program structures by superposition, the idea that complex structures are composed by superimposing simpler structures.[8][9] Yet another reformulation of GenVoca is as a monoid: a GenVoca model is a set of features with a composition operation (•); composition is associative and there is an identity element (namely 1, the identity function). Although all compositions are possible, not all are meaningful. That's the reason for feature models.\nGenVoca features were originally implemented using C preprocessor (#ifdef feature ... #endif) techniques. A more advanced technique, called mixin layers, showed the connection of features to object-oriented collaboration-based designs.\n\nAHEAD[edit]\nAlgebraic Hierarchical Equations for Application Design (AHEAD) [4] generalized GenVoca in two ways. First it revealed the internal structure of GenVoca values as tuples. Every program has multiple representations, such as source, documentation, bytecode, and makefiles. A GenVoca value is a tuple of program representations. In a product line of parsers, for example, a base parser f is defined by its grammar gf, Java source sf, and documentation df. Parser f is modeled by the tuple f=[gf, sf, df]. Each program representation may have subrepresentations, and they too may have subrepresentations, recursively. In general, a GenVoca value is a tuple of nested tuples that define a hierarchy of representations for a particular program.\n\n\nHierarchical Relationships among Program Artifacts\nExample. Suppose terminal representations are files. In AHEAD, grammar gf corresponds to a single BNF file, source sf corresponds to a tuple of Java files [c1…cn], and documentation df is a tuple of HTML files [h1…hk]. A GenVoca value (nested tuples) can be depicted as a directed graph: the graph for parser f is shown in the figure to the right. Arrows denote projections, i.e., mappings from a tuple to one of its components. AHEAD implements tuples as file directories, so f is a directory containing file gf and subdirectories sf and df. Similarly, directory sf contains files c1…cn, and directory df contains files h1…hk.\nNote: Files can be hierarchically decomposed further. Each Java class can be decomposed into a tuple of members and other class declarations (e.g., initialization blocks, etc.). The important idea here is that the mathematics of AHEAD are recursive.\nSecond, AHEAD expresses features as nested tuples of unary functions called deltas. Deltas can be program refinements (semantics-preserving transformations), extensions (semantics-extending transformations), or interactions (semantics-altering transformations). We use the neutral term “delta” to represent all of these possibilities, as each occurs in FOSD.\n\nTo illustrate, suppose feature j extends a grammar by {\\displaystyle \\Delta } \\Delta gj (new rules and tokens are added), extends source code by {\\displaystyle \\Delta } \\Delta sj (new classes and members are added and existing methods are modified), and extends documentation by {\\displaystyle \\Delta } \\Delta dj. The tuple of deltas for feature j is modeled by j=[ {\\displaystyle \\Delta } \\Delta gj, {\\displaystyle \\Delta } \\Delta sj, {\\displaystyle \\Delta } \\Delta dj], which we call a delta tuple. Elements of delta tuples can themselves be delta tuples. Example: {\\displaystyle \\Delta } \\Delta sj represents the changes that are made to each class in sf by feature j, i.e., {\\displaystyle \\Delta } \\Delta sj=[ {\\displaystyle \\Delta } \\Delta c1… {\\displaystyle \\Delta } \\Delta cn]. The representations of a program are computed recursively by nested vector addition. The representations for parser p2 (whose GenVoca expression is j+f) are:\n\n  p2 = j + f                              -- GenVoca expression\n     = [\n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gj, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta sj, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta dj] + [gf, sf, df]   -- substitution\n     = [\n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gj+gf, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta sj+sf, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta dj+df]         -- compose tuples element-wise\nThat is, the grammar of p2 is the base grammar composed with its extension ( {\\displaystyle \\Delta } \\Delta gj+gf), the source of p2 is the base source composed with its extension ( {\\displaystyle \\Delta } \\Delta sj+sf), and so on. As elements of delta tuples can themselves be delta tuples, composition recurses, e.g., {\\displaystyle \\Delta } \\Delta sj+sf= [ {\\displaystyle \\Delta } \\Delta c1… {\\displaystyle \\Delta } \\Delta cn]+[c1…cn]=[ {\\displaystyle \\Delta } \\Delta c1+c1… {\\displaystyle \\Delta } \\Delta cn+cn]. Summarizing, GenVoca values are nested tuples of program artifacts, and features are nested delta tuples, where + recursively composes them by vector addition. This is the essence of AHEAD.\n\nThe ideas presented above concretely expose two FOSD principles. The Principle of Uniformity states that all program artifacts are treated and modified in the same way. (This is evidenced by deltas for different artifact types above). The Principle of Scalability states all levels of abstractions are treated uniformly. (This gives rise to the hierarchical nesting of tuples above).\n\nThe original implementation of AHEAD is the AHEAD Tool Suite and Jak language, which exhibits both the Principles of Uniformity and Scalability. Next-generation tools include CIDE [10] and FeatureHouse.[11]\n\nFOMDD[edit]\n\nDerivational and Refinement Relationships among Program Artifacts\nFeature Oriented Model Driven Design (FOMDD) [5][6] combines the ideas of AHEAD with Model Driven Design (MDD) (a.k.a. Model-Driven Architecture (MDA)). AHEAD functions capture the lockstep update of program artifacts when a feature is added to a program. But there are other functional relationships among program artifacts that express derivations. For example, the relationship between a grammar gf and its parser source sf is defined by a compiler-compiler tool, e.g., javacc. Similarly, the relationship between Java source sf and its bytecode bf is defined by the javac compiler. A commuting diagram expresses these relationships. Objects are program representations, downward arrows are derivations, and horizontal arrows are deltas. The figure to the right shows the commuting diagram for program p3 = i+j+h = [g3,s3,b3].\n\nA fundamental property of a commuting diagram is that all paths between two objects are equivalent. For example, one way to derive the bytecode b3 of parser p3 (lower right object in the figure to the right) from grammar gh of parser h (upper left object) is to derive the bytecode bh and refine to b3, while another way refines gh to g3, and then derive b3, where + represents delta composition and () is function or tool application:\n\n  b3 = \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta bj + \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta bi + javacc( javac( gh ) ) = javac( javacc( \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gi + \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gj + gh ) )\nThere are {\\displaystyle {\\tbinom {4}{2}}} {\\tbinom  {4}{2}} possible paths to derive the bytecode b3 of parser p3 from the grammar gh of parser h. Each path represents a metaprogram whose execution generates the target object (b3) from the starting object (gf). There is a potential optimization: traversing each arrow of a commuting diagram has a cost. The cheapest (i.e., shortest) path between two objects in a commuting diagram is a geodesic, which represents the most efficient metaprogram that produces the target object from a given object.\n\nNote: A “cost metric” need not be a monetary value; cost may be measured in production time, peak or total memory requirements, power consumption, or some informal metric like “ease of explanation”, or a combination of the above (e.g., multi-objective optimization). The idea of a geodesic is general, and should be understood and appreciated from this more general context.\nNote: It is possible for there to be m starting objects and n ending objects in a geodesic; when m=1 and n>1, this is the Directed Steiner Tree Problem, which is NP-hard.\nCommuting diagrams are important for at least two reasons: (1) there is the possibility of optimizing the generation of artifacts (e.g., geodesics) and (2) they specify different ways of constructing a target object from a starting object.[5][12] A path through a diagram corresponds to a tool chain: for an FOMDD model to be consistent, it should be proven (or demonstrated through testing) that all tool chains that map one object to another in fact yield equivalent results. If this is not the case, then either there is a bug in one or more of the tools or the FOMDD model is wrong.\n\nNote: the above ideas were inspired by category theory.[5][6]\nApplications[edit]\nNetwork Protocols\nExtensible Database Systems\nData Structures\nDistributed Army Fire Support Simulator\nProduction System Compiler\nGraph Product Line\nExtensible Java Preprocessors\nWeb Portlets\nSVG Applications",
          "subparadigms": []
        },
        {
          "pdid": 30,
          "name": "Function-level programming",
          "details": "In computer science, function-level programming refers to one of the two contrasting programming paradigms identified by John Backus in his work on programs as mathematical objects, the other being value-level programming.\n\nIn his 1977 Turing award lecture, Backus set forth what he considered to be the need to switch to a different philosophy in programming language design:[1]\n\nProgramming languages appear to be in trouble. Each successive language incorporates, with a little cleaning up, all the features of its predecessors plus a few more. [...] Each new language claims new and fashionable features... but the plain fact is that few languages make programming sufficiently cheaper or more reliable to justify the cost of producing and learning to use them.\n\nHe designed FP to be the first programming language to specifically support the function-level programming style.\n\nA function-level program is variable-free (cf. point-free programming), since program variables, which are essential in value-level definitions, are not needed in function-level programs.\n\nContents  [hide] \n1\tIntroduction\n2\tContrast to functional programming\n3\tExample languages\n4\tSee also\n5\tReferences\nIntroduction[edit]\nIn the function-level style of programming, a program is built directly from programs that are given at the outset, by combining them with program-forming operations or functionals. Thus, in contrast with the value-level approach that applies the given programs to values to form a succession of values culminating in the desired result value, the function-level approach applies program-forming operations to the given programs to form a succession of programs culminating in the desired result program.\n\nAs a result, the function-level approach to programming invites study of the space of programs under program-forming operations, looking to derive useful algebraic properties of these program-forming operations. The function-level approach offers the possibility of making the set of programs a mathematical space by emphasizing the algebraic properties of the program-forming operations over the space of programs.\n\nAnother potential advantage of the function-level view is the ability to use only strict functions and thereby have bottom-up semantics, which are the simplest kind of all. Yet another is the existence of function level definitions that are not the lifted (that is, lifted from a lower value-level to a higher function-level) image of any existing value-level one: these (often terse) function-level definitions represent a more powerful style of programming not available at the value-level and, arguably, are often easier to understand and reason about.\n\nContrast to functional programming[edit]\nWhen Backus studied and publicized his function-level style of programming, his message was mostly misunderstood,[2] giving boost to the traditional functional programming style languages instead of his own FP and its successor FL.\n\nBackus calls functional programming applicative programming; his function-level programming is a particular, constrained type of applicative programming.\n\nA key distinction from functional languages is that Backus' language has the following hierarchy of types:\n\natoms\nfunctions, which take atoms to atoms\nHigher-order functions (which he calls \"functional forms\"), which take one or two functions to functions\n...and the only way to generate new functions is to use one of the functional forms, which are fixed: you cannot build your own functional form (at least not within FP; you can within FFP (Formal FP)).\n\nThis restriction means that functions in FP are a module (generated by the built-in functions) over the algebra of functional forms, and are thus algebraically tractable. For instance, the general question of equality of two functions is equivalent to the halting problem, and is undecidable, but equality of two functions in FP is just equality in the algebra, and thus (Backus imagines) easier.\n\nEven today, many users of lambda style languages often misinterpret Backus' function-level approach as a restrictive variant of the lambda style, which is a de facto value-level style. In fact, Backus would not have disagreed with the 'restrictive' accusation: he argued that it was precisely due to such restrictions that a well-formed mathematical space could arise, in a manner analogous to the way structured programming limits programming to a restricted version of all the control-flow possibilities available in plain, unrestricted unstructured programs.\n\nThe value-free style of FP is closely related to the equational logic of a cartesian-closed category.\n\nExample languages[edit]\nMain category: Function-level languages\nThe canonical function-level programming language is FP. Others include FL, FPr and J.\n\nSee also[edit]\nValue-level programming, imperative programming (contrast)\nFunctional programming, declarative programming (compare)\nTacit programming\nConcatenative programming language",
          "subparadigms": []
        },
        {
          "pdid": 31,
          "name": "Generic programming",
          "details": "Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. This approach, pioneered by ML in 1973,[1][2] permits writing common functions or types that differ only in the set of types on which they operate when used, thus reducing duplication. Such software entities are known as generics in Ada, Delphi, Eiffel, Java, C#, F#, Objective-C, Swift, and Visual Basic .NET; parametric polymorphism in ML, Scala, Haskell (the Haskell community also uses the term \"generic\" for a related but somewhat different concept) and Julia; templates in C++ and D; and parameterized types in the influential 1994 book Design Patterns.[3] The authors of Design Patterns note that this technique, especially when combined with delegation, is very powerful but also quote the following\n\nDynamic, highly parameterized software is harder to understand than more static software.\n\n— Gang of Four, Design Patterns[3] (Chapter 1)\nThe term generic programming was originally coined by David Musser and Alexander Stepanov[4] in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalised as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above.\n\nContents  [hide] \n1\tStepanov–Musser and other generic programming paradigms\n2\tProgramming language support for genericity\n2.1\tIn object-oriented languages\n2.1.1\tGenerics in Ada\n2.1.1.1\tExample\n2.1.1.2\tAdvantages and limitations\n2.1.2\tTemplates in C++\n2.1.2.1\tTechnical overview\n2.1.2.2\tTemplate specialization\n2.1.2.3\tAdvantages and disadvantages\n2.1.3\tTemplates in D\n2.1.3.1\tCode generation\n2.1.4\tGenericity in Eiffel\n2.1.4.1\tBasic/Unconstrained genericity\n2.1.4.2\tConstrained genericity\n2.1.5\tGenerics in Java\n2.1.6\tGenericity in .NET [C#, VB.NET]\n2.1.7\tGenericity in Delphi\n2.1.8\tGenericity in Free Pascal\n2.2\tFunctional languages\n2.2.1\tGenericity in Haskell\n2.2.1.1\tPolyP\n2.2.1.2\tGeneric Haskell\n2.2.2\tClean\n2.3\tOther languages\n3\tSee also\n4\tReferences\n5\tCitations\n6\tFurther reading\n7\tExternal links\nStepanov–Musser and other generic programming paradigms[edit]\nGeneric programming is defined in Musser & Stepanov (1989) as follows,\n\nGeneric programming centers around the idea of abstracting from concrete, efficient algorithms to obtain generic algorithms that can be combined with different data representations to produce a wide variety of useful software.\n\n— Musser, David R.; Stepanov, Alexander A., Generic Programming[5]\nGeneric programming paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalised as concepts, analogously to the abstraction of algebraic theories in abstract algebra.[6] Early examples of this programming approach were implemented in Scheme and Ada,[7] although the best known example is the Standard Template Library (STL),[8][9] which developed a theory of iterators that is used to decouple sequence data structures and the algorithms operating on them.\n\nFor example, given N sequence data structures, e.g. singly linked list, vector etc., and M algorithms to operate on them, e.g. find, sort etc., a direct approach would implement each algorithm specifically for each data structure, giving N × M combinations to implement. However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type which can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e.g. a pair of iterators pointing to the beginning and end of the subsequence to process. Thus, only N + M data structure-algorithm combinations need be implemented. Several iterator concepts are specified in the STL, each a refinement of more restrictive concepts e.g. forward iterators only provide movement to the next value in a sequence (e.g. suitable for a singly linked list or a stream of input data), whereas a random-access iterator also provides direct constant-time access to any element of the sequence (e.g. suitable for a vector). An important point is that a data structure will return a model of the most general concept that can be implemented efficiently—computational complexity requirements are explicitly part of the concept definition. This limits which data structures a given algorithm can be applied to and such complexity requirements are a major determinant of data structure choice. Generic programming similarly has been applied in other domains, e.g. graph algorithms.[10]\n\nNote that although this approach often utilizes language features of compile-time genericity/templates, it is in fact independent of particular language-technical details. Generic programming pioneer Alexander Stepanov wrote,\n\nGeneric programming is about abstracting and classifying algorithms and data structures. It gets its inspiration from Knuth and not from type theory. Its goal is the incremental construction of systematic catalogs of useful, efficient and abstract algorithms and data structures. Such an undertaking is still a dream.\n\n— Alexander Stepanov, Short History of STL [11][12]\nI believe that iterator theories are as central to Computer Science as theories of rings or Banach spaces are central to Mathematics.\n\n— Alexander Stepanov, An Interview with A. Stepanov[13]\nBjarne Stroustrup noted,\n\nFollowing Stepanov, we can define generic programming without mentioning language features: Lift algorithms and data structures from concrete examples to their most general and abstract form.\n\n— Bjarne Stroustrup, Evolving a language in and for the real world: C++ 1991-2006[12]\nOther programming paradigms that have been described as generic programming include Datatype generic programming as described in “Generic Programming — an Introduction”.[14] The Scrap your boilerplate approach is a lightweight generic programming approach for Haskell.[15]\n\nIn this article we distinguish the high-level programming paradigms of generic programming, above, from the lower-level programming language genericity mechanisms used to implement them (see Programming language support for genericity). For further discussion and comparison of generic programming paradigms, see.[16]\n\nProgramming language support for genericity[edit]\nGenericity facilities have existed in high-level languages since at least the 1970s in languages such as ML, CLU and Ada, and were subsequently adopted by many object-based and object-oriented languages, including BETA, C++, D, Eiffel, Java, and DEC's now defunct Trellis-Owl language.\n\nGenericity is implemented and supported differently in various programming languages; the term \"generic\" has also been used differently in various programming contexts. For example, in Forth the compiler can execute code while compiling and one can create new compiler keywords and new implementations for those words on the fly. It has few words that expose the compiler behaviour and therefore naturally offers genericity capacities which, however, are not referred to as such in most Forth texts. Similarly, dynamically typed languages, especially interpreted ones, usually offer genericity by default as both passing values to functions and value assignment are type-indifferent and such behavior is often utilized for abstraction or code terseness, however this is not typically labeled genericity as it's a direct consequence of dynamic typing system employed by the language[citation needed]. The term has been used in functional programming, specifically in Haskell-like languages, which use a structural type system where types are always parametric and the actual code on those types is generic. These usages still serve a similar purpose of code-saving and the rendering of an abstraction.\n\nArrays and structs can be viewed as predefined generic types. Every usage of an array or struct type instantiates a new concrete type, or reuses a previous instantiated type. Array element types and struct element types are parameterized types, which are used to instantiate the corresponding generic type. All this is usually built-in in the compiler and the syntax differs from other generic constructs. Some extensible programming languages try to unify built-in and user defined generic types.\n\nA broad survey of genericity mechanisms in programming languages follows. For a specific survey comparing suitability of mechanisms for generic programming, see.[17]\n\nIn object-oriented languages[edit]\nWhen creating container classes in statically typed languages, it is inconvenient to write specific implementations for each datatype contained, especially if the code for each datatype is virtually identical. For example, in C++, this duplication of code can be circumvented by defining a class template:\n\ntemplate<typename T> \nclass List \n{ \n   /* class contents */ \n};\n\nList<Animal> list_of_animals;\nList<Car> list_of_cars;\nAbove, T is a placeholder for whatever type is specified when the list is created. These \"containers-of-type-T\", commonly called templates, allow a class to be reused with different datatypes as long as certain contracts such as subtypes and signature are kept. This genericity mechanism should not be confused with inclusion polymorphism, which is the algorithmic usage of exchangeable sub-classes: for instance, a list of objects of type Moving_Object containing objects of type Animal and Car. Templates can also be used for type-independent functions as in the Swap example below:\n\ntemplate<typename T>\nvoid Swap(T & a, T & b) //\"&\" passes parameters by reference\n{\n   T temp = b;\n   b = a;\n   a = temp;\n}\n\nstring hello = \"world!\", world = \"Hello, \";\nSwap( world, hello );\ncout << hello << world << endl; //Output is \"Hello, world!\"\nThe C++ template construct used above is widely cited[citation needed] as the genericity construct that popularized the notion among programmers and language designers and supports many generic programming idioms. The D programming language also offers fully generic-capable templates based on the C++ precedent but with a simplified syntax. The Java programming language has provided genericity facilities syntactically based on C++'s since the introduction of J2SE 5.0.\n\nC# 2.0, Oxygene 1.5 (also known as Chrome) and Visual Basic .NET 2005 have constructs that take advantage of the support for generics present in the Microsoft .NET Framework since version 2.0.\n\nGenerics in Ada[edit]\nAda has had generics since it was first designed in 1977–1980. The standard library uses generics to provide many services. Ada 2005 adds a comprehensive generic container library to the standard library, which was inspired by C++'s standard template library.\n\nA generic unit is a package or a subprogram that takes one or more generic formal parameters.\n\nA generic formal parameter is a value, a variable, a constant, a type, a subprogram, or even an instance of another, designated, generic unit. For generic formal types, the syntax distinguishes between discrete, floating-point, fixed-point, access (pointer) types, etc. Some formal parameters can have default values.\n\nTo instantiate a generic unit, the programmer passes actual parameters for each formal. The generic instance then behaves just like any other unit. It is possible to instantiate generic units at run-time, for example inside a loop.\n\nExample[edit]\nThe specification of a generic package:\n\n generic\n    Max_Size : Natural; -- a generic formal value\n    type Element_Type is private; -- a generic formal type; accepts any nonlimited type\n package Stacks is\n    type Size_Type is range 0 .. Max_Size;\n    type Stack is limited private;\n    procedure Create (S : out Stack;\n                      Initial_Size : in Size_Type := Max_Size);\n    procedure Push (Into : in out Stack; Element : in Element_Type);\n    procedure Pop (From : in out Stack; Element : out Element_Type);\n    Overflow : exception;\n    Underflow : exception;\n private\n    subtype Index_Type is Size_Type range 1 .. Max_Size;\n    type Vector is array (Index_Type range <>) of Element_Type;\n    type Stack (Allocated_Size : Size_Type := 0) is record\n       Top : Index_Type;\n       Storage : Vector (1 .. Allocated_Size);\n    end record;\n end Stacks;\nInstantiating the generic package:\n\n type Bookmark_Type is new Natural;\n -- records a location in the text document we are editing\n \n package Bookmark_Stacks is new Stacks (Max_Size => 20,\n                                        Element_Type => Bookmark_Type);\n -- Allows the user to jump between recorded locations in a document\nUsing an instance of a generic package:\n\n type Document_Type is record\n    Contents : Ada.Strings.Unbounded.Unbounded_String;\n    Bookmarks : Bookmark_Stacks.Stack;\n end record;\n \n procedure Edit (Document_Name : in String) is\n   Document : Document_Type;\n begin\n   -- Initialise the stack of bookmarks:\n   Bookmark_Stacks.Create (S => Document.Bookmarks, Initial_Size => 10);\n   -- Now, open the file Document_Name and read it in...\n end Edit;\nAdvantages and limitations[edit]\nThe language syntax allows precise specification of constraints on generic formal parameters. For example, it is possible to specify that a generic formal type will only accept a modular type as the actual. It is also possible to express constraints between generic formal parameters; for example:\n\n generic\n    type Index_Type is (<>); -- must be a discrete type\n    type Element_Type is private; -- can be any nonlimited type\n    type Array_Type is array (Index_Type range <>) of Element_Type;\nIn this example, Array_Type is constrained by both Index_Type and Element_Type. When instantiating the unit, the programmer must pass an actual array type that satisfies these constraints.\n\nThe disadvantage of this fine-grained control is a complicated syntax, but, because all generic formal parameters are completely defined in the specification, the compiler can instantiate generics without looking at the body of the generic.\n\nUnlike C++, Ada does not allow specialised generic instances, and requires that all generics be instantiated explicitly. These rules have several consequences:\n\nthe compiler can implement shared generics: the object code for a generic unit can be shared between all instances (unless the programmer requests inlining of subprograms, of course). As further consequences:\nthere is no possibility of code bloat (code bloat is common in C++ and requires special care, as explained below).\nit is possible to instantiate generics at run-time, as well as at compile time, since no new object code is required for a new instance.\nactual objects corresponding to a generic formal object are always considered to be nonstatic inside the generic; see Generic formal objects in the Wikibook for details and consequences.\nall instances of a generic being exactly the same, it is easier to review and understand programs written by others; there are no \"special cases\" to take into account.\nall instantiations being explicit, there are no hidden instantiations that might make it difficult to understand the program.\nAda does not permit \"template metaprogramming\", because it does not allow specialisations.\nTemplates in C++[edit]\nMain article: Template (C++)\nC++ uses templates to enable generic programming techniques. The C++ Standard Library includes the Standard Template Library or STL that provides a framework of templates for common data structures and algorithms. Templates in C++ may also be used for template metaprogramming, which is a way of pre-evaluating some of the code at compile-time rather than run-time. Using template specialization, C++ Templates are considered Turing complete.\n\nTechnical overview[edit]\nThere are two kinds of templates: function templates and class templates. A function template is a pattern for creating ordinary functions based upon the parameterizing types supplied when instantiated. For example, the C++ Standard Template Library contains the function template max(x, y) which creates functions that return either x or y, whichever is larger. max() could be defined like this:\n\ntemplate <typename T>\nT max(T x, T y) \n{\n    return x < y ? y : x;\n}\nSpecializations of this function template, instantiations with specific types, can be called just like an ordinary function:\n\ncout << max(3, 7);   // outputs 7\nThe compiler examines the arguments used to call max and determines that this is a call to max(int, int). It then instantiates a version of the function where the parameterizing type T is int, making the equivalent of the following function:\n\nint max(int x, int y) \n{\n    return x < y ? y : x;\n}\nThis works whether the arguments x and y are integers, strings, or any other type for which the expression x < y is sensible, or more specifically, for any type for which operator< is defined. Common inheritance is not needed for the set of types that can be used, and so it is very similar to duck typing. A program defining a custom data type can use operator overloading to define the meaning of < for that type, thus allowing its use with the max() function template. While this may seem a minor benefit in this isolated example, in the context of a comprehensive library like the STL it allows the programmer to get extensive functionality for a new data type, just by defining a few operators for it. Merely defining < allows a type to be used with the standard sort(), stable_sort(), and binary_search() algorithms or to be put inside data structures such as sets, heaps, and associative arrays.\n\nC++ templates are completely type safe at compile time. As a demonstration, the standard type complex does not define the < operator, because there is no strict order on complex numbers. Therefore, max(x, y) will fail with a compile error if x and y are complex values. Likewise, other templates that rely on < cannot be applied to complex data unless a comparison (in the form of a functor or function) is provided. E.g.: A complex cannot be used as key for a map unless a comparison is provided. Unfortunately, compilers historically generate somewhat esoteric, long, and unhelpful error messages for this sort of error. Ensuring that a certain object adheres to a method protocol can alleviate this issue. Languages which use compare instead of < can also use complex values as keys.\n\nThe second kind of template, a class template, extends the same concept to classes. A class template specialization is a class. Class templates are often used to make generic containers. For example, the STL has a linked list container. To make a linked list of integers, one writes list<int>. A list of strings is denoted list<string>. A list has a set of standard functions associated with it, which work for any compatible parameterizing types.\n\nTemplate specialization[edit]\nA powerful feature of C++'s templates is template specialization. This allows alternative implementations to be provided based on certain characteristics of the parameterized type that is being instantiated. Template specialization has two purposes: to allow certain forms of optimization, and to reduce code bloat.\n\nFor example, consider a sort() template function. One of the primary activities that such a function does is to swap or exchange the values in two of the container's positions. If the values are large (in terms of the number of bytes it takes to store each of them), then it is often quicker to first build a separate list of pointers to the objects, sort those pointers, and then build the final sorted sequence. If the values are quite small however it is usually fastest to just swap the values in-place as needed. Furthermore, if the parameterized type is already of some pointer-type, then there is no need to build a separate pointer array. Template specialization allows the template creator to write different implementations and to specify the characteristics that the parameterized type(s) must have for each implementation to be used.\n\nUnlike function templates, class templates can be partially specialized. That means that an alternate version of the class template code can be provided when some of the template parameters are known, while leaving other template parameters generic. This can be used, for example, to create a default implementation (the primary specialization) that assumes that copying a parameterizing type is expensive and then create partial specializations for types that are cheap to copy, thus increasing overall efficiency. Clients of such a class template just use specializations of it without needing to know whether the compiler used the primary specialization or some partial specialization in each case. Class templates can also be fully specialized, which means that an alternate implementation can be provided when all of the parameterizing types are known.\n\nAdvantages and disadvantages[edit]\nSome uses of templates, such as the max() function, were previously filled by function-like preprocessor macros (a legacy of the C programming language). For example, here is a possible max() macro:\n\n#define max(a,b) ((a) < (b) ? (b) : (a))\nMacros are expanded by preprocessor, before compilation proper; templates are expanded at compile time. Macros are always expanded inline; templates can also be expanded as inline functions when the compiler deems it appropriate. Thus both function-like macros and function templates have no run-time overhead.\n\nHowever, templates are generally considered an improvement over macros for these purposes. Templates are type-safe. Templates avoid some of the common errors found in code that makes heavy use of function-like macros, such as evaluating parameters with side effects twice. Perhaps most importantly, templates were designed to be applicable to much larger problems than macros.\n\nThere are three primary drawbacks to the use of templates: compiler support, poor error messages, and code bloat. Many compilers historically have poor support for templates, thus the use of templates can make code somewhat less portable. Support may also be poor when a C++ compiler is being used with a linker which is not C++-aware, or when attempting to use templates across shared library boundaries. Most modern compilers however now have fairly robust and standard template support, and the new C++ standard, C++11, further addresses these issues.\n\nAlmost all compilers produce confusing, long, or sometimes unhelpful error messages when errors are detected in code that uses templates.[18] This can make templates difficult to develop.\n\nFinally, the use of templates requires the compiler to generate a separate instance of the templated class or function for every permutation of type parameters used with it. (This is necessary because types in C++ are not all the same size, and the sizes of data fields are important to how classes work.) So the indiscriminate use of templates can lead to code bloat, resulting in excessively large executables. However, judicious use of template specialization and derivation can dramatically reduce such code bloat in some cases:\n\nSo, can derivation be used to reduce the problem of code replicated because templates are used? This would involve deriving a template from an ordinary class. This technique proved successful in curbing code bloat in real use. People who do not use a technique like this have found that replicated code can cost megabytes of code space even in moderate size programs.\n\n— Bjarne Stroustrup, The Design and Evolution of C++, 1994[19]\nIn simple cases templates can be transformed into generics (not causing code bloat) by creating a class getting a parameter derived from a type in compile time and wrapping a template around this class. It is a nice approach for creating generic heap-based containers.\n\nThe extra instantiations generated by templates can also cause debuggers to have difficulty working gracefully with templates. For example, setting a debug breakpoint within a template from a source file may either miss setting the breakpoint in the actual instantiation desired or may set a breakpoint in every place the template is instantiated.\n\nAlso, because the compiler needs to perform macro-like expansions of templates and generate different instances of them at compile time, the implementation source code for the templated class or function must be available (e.g. included in a header) to the code using it. Templated classes or functions, including much of the Standard Template Library (STL), if not included in header files, cannot be compiled. (This is in contrast to non-templated code, which may be compiled to binary, providing only a declarations header file for code using it.) This may be a disadvantage by exposing the implementing code, which removes some abstractions, and could restrict its use in closed-source projects.[citation needed]\n\nTemplates in D[edit]\nThe D programming language supports templates based in design on C++. Most C++ template idioms will carry over to D without alteration, but D adds some additional functionality:\n\nTemplate parameters in D are not restricted to just types and primitive values, but also allow arbitrary compile-time values (such as strings and struct literals), and aliases to arbitrary identifiers, including other templates or template instantiations.\nTemplate constraints and the static if statement provide an alternative to C++'s substitution failure is not an error (SFINAE) mechanism, similar to C++ concepts.\nThe is(...) expression allows speculative instantiation to verify an object's traits at compile time.\nThe auto keyword and the typeof expression allow type inference for variable declarations and function return values, which in turn allows \"Voldemort types\" (types which do not have a global name).[20]\nTemplates in D use a different syntax as in C++: whereas in C++ template parameters are wrapped in angular brackets (Template<param1, param2>), D uses an exclamation sign and parentheses: Template!(param1, param2). This avoids the C++ parsing difficulties due to ambiguity with comparison operators. If there is only one parameter, the parentheses can be omitted.\n\nConventionally, D combines the above features to provide compile-time polymorphism using trait-based generic programming. For example, an input range is defined as any type which satisfies the checks performed by isInputRange, which is defined as follows:\n\ntemplate isInputRange(R)\n{\n    enum bool isInputRange = is(typeof(\n    (inout int = 0)\n    {\n        R r = R.init;     // can define a range object\n        if (r.empty) {}   // can test for empty\n        r.popFront();     // can invoke popFront()\n        auto h = r.front; // can get the front of the range\n    }));\n}\nA function which accepts only input ranges can then use the above template in a template constraint:\n\nauto fun(Range)(Range range)\n    if (isInputRange!Range)\n{\n    // ...\n}\nCode generation[edit]\nIn addition to template metaprogramming, D also provides several features to enable compile-time code generation:\n\nThe import expression allows reading a file from disk and using its contents as a string expression.\nCompile-time reflection allows enumerating and inspecting declarations and their members during compilation.\nUser-defined attributes allow users to attach arbitrary identifiers to declarations, which can then be enumerated using compile-time reflection.\nCompile-Time Function Execution (CTFE) allows a subset of D (restricted to safe operations) to be interpreted during compilation.\nString mixins allow evaluating and compiling the contents of a string expression as D code which becomes part of the program.\nCombining the above allows generating code based on existing declarations. For example, D serialization frameworks can enumerate a type's members and generate specialized functions for each serialized type to perform serialization and deserialization. User-defined attributes could further indicate serialization rules.\n\nThe import expression and compile-time function execution also allow efficiently implementing domain-specific languages. For example, given a function which takes a string containing an HTML template and returns equivalent D source code, it is possible to use it in the following way:\n\n// Import the contents of example.htt as a string manifest constant.\nenum htmlTemplate = import(\"example.htt\");\n\n// Transpile the HTML template to D code.\nenum htmlDCode = htmlTemplateToD(htmlTemplate);\n\n// Paste the contents of htmlDCode as D code.\nmixin(htmlDCode);\nGenericity in Eiffel[edit]\nGeneric classes have been a part of Eiffel since the original method and language design. The foundation publications of Eiffel,[21][22] use the term genericity to describe the creation and use of generic classes.\n\nBasic/Unconstrained genericity[edit]\nGeneric classes are declared with their class name and a list of one or more formal generic parameters. In the following code, class LIST has one formal generic parameter G\n\nclass\n    LIST [G]\n            ...\nfeature   -- Access\n    item: G\n            -- The item currently pointed to by cursor\n            ...\nfeature   -- Element change\n    put (new_item: G)\n            -- Add `new_item' at the end of the list\n            ...\nThe formal generic parameters are placeholders for arbitrary class names which will be supplied when a declaration of the generic class is made, as shown in the two generic derivations below, where ACCOUNT and DEPOSIT are other class names. ACCOUNT and DEPOSIT are considered actual generic parameters as they provide real class names to substitute for G in actual use.\n\n    list_of_accounts: LIST [ACCOUNT]\n            -- Account list\n\n    list_of_deposits: LIST [DEPOSIT]\n            -- Deposit list\nWithin the Eiffel type system, although class LIST [G] is considered a class, it is not considered a type. However, a generic derivation of LIST [G] such as LIST [ACCOUNT] is considered a type.\n\nConstrained genericity[edit]\nFor the list class shown above, an actual generic parameter substituting for G can be any other available class. To constrain the set of classes from which valid actual generic parameters can be chosen, a generic constraint can be specified. In the declaration of class SORTED_LIST below, the generic constraint dictates that any valid actual generic parameter will be a class which inherits from class COMPARABLE. The generic constraint ensures that elements of a SORTED_LIST can in fact be sorted.\n\nclass\n    SORTED_LIST [G -> COMPARABLE]\nGenerics in Java[edit]\nMain article: Generics in Java\nSupport for the generics, or \"containers-of-type-T\" was added to the Java programming language in 2004 as part of J2SE 5.0. In Java, generics are only checked at compile time for type correctness. The generic type information is then removed via a process called type erasure, to maintain compatibility with old JVM implementations, making it unavailable at runtime. For example, a List<String> is converted to the raw type List. The compiler inserts type casts to convert the elements to the String type when they are retrieved from the list, reducing performance compared to other implementations such as C++ templates.\n\nGenericity in .NET [C#, VB.NET][edit]\nGenerics were added as part of .NET Framework 2.0 in November 2005, based on a research prototype from Microsoft Research started in 1999.[23] Although similar to generics in Java, .NET generics do not apply type erasure, but implement generics as a first class mechanism in the runtime using reification. This design choice provides additional functionality, such as allowing reflection with preservation of generic types, as well as alleviating some of the limitations of erasure (such as being unable to create generic arrays).[24][25] This also means that there is no performance hit from runtime casts and normally expensive boxing conversions. When primitive and value types are used as generic arguments, they get specialized implementations, allowing for efficient generic collections and methods. As in C++ and Java, nested generic types such as Dictionary<string, List<int>> are valid types, however are advised against for member signatures in code analysis design rules.[26]\n\n.NET allows six varieties of generic type constraints using the where keyword including restricting generic types to be value types, to be classes, to have constructors, and to implement interfaces.[27] Below is an example with an interface constraint:\n\nusing System;\n\nclass Sample\n{\n    static void Main()\n    {\n        int[] array = { 0, 1, 2, 3 };\n        MakeAtLeast<int>(array, 2); // Change array to { 2, 2, 2, 3 }\n        foreach (int i in array)\n            Console.WriteLine(i); // Print results.\n        Console.ReadKey(true);\n    }\n\n    static void MakeAtLeast<T>(T[] list, T lowest) where T : IComparable<T>\n    {\n        for (int i = 0; i < list.Length; i++)\n            if (list[i].CompareTo(lowest) < 0)\n                list[i] = lowest;\n    }\n}\nThe MakeAtLeast() method allows operation on arrays, with elements of generic type T. The method's type constraint indicates that the method is applicable to any type T that implements the generic IComparable<T> interface. This ensures a compile time error if the method is called if the type does not support comparison. The interface provides the generic method CompareTo(T).\n\nThe above method could also be written without generic types, simply using the non-generic Array type. However, since arrays are contravariant, the casting would not be type safe, and compiler may miss errors that would otherwise be caught while making use of the generic types. In addition, the method would need to access the array items as objects instead, and would require casting to compare two elements. (For value types like types such as int this requires a boxing conversion, although this can be worked around using the Comparer<T> class, as is done in the standard collection classes.)\n\nA notable behavior of static members in a generic .NET class is static member instantiation per run-time type (see example below).\n\n    //A generic class\n    public class GenTest<T>\n    {\n        //A static variable - will be created for each type on refraction\n        static CountedInstances OnePerType = new CountedInstances();\n\n        //a data member\n        private T mT;\n\n        //simple constructor\n        public GenTest(T pT)\n        {\n            mT = pT;\n        }\n    }\n\n    //a class\n    public class CountedInstances\n    {\n        //Static variable - this will be incremented once per instance\n        public static int Counter;\n\n        //simple constructor\n        public CountedInstances()\n        {\n            //increase counter by one during object instantiation\n            CountedInstances.Counter++;\n        }\n    }\n\n  //main code entry point\n  //at the end of execution, CountedInstances.Counter = 2\n  GenTest<int> g1 = new GenTest<int>(1);\n  GenTest<int> g11 = new GenTest<int>(11);\n  GenTest<int> g111 = new GenTest<int>(111);\n  GenTest<double> g2 = new GenTest<double>(1.0);\nGenericity in Delphi[edit]\nDelphi's Object Pascal dialect acquired generics in the Delphi 2007 release, initially only with the (now discontinued) .NET compiler before being added to the native code one in the Delphi 2009 release. The semantics and capabilities of Delphi generics are largely modelled on those had by generics in .NET 2.0, though the implementation is by necessity quite different. Here's a more or less direct translation of the first C# example shown above:\n\nprogram Sample;\n\n{$APPTYPE CONSOLE}\n\nuses\n  Generics.Defaults; //for IComparer<>\n\ntype\n  TUtils = class\n    class procedure MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T;\n      Comparer: IComparer<T>); overload;\n    class procedure MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T); overload;\n  end;\n\nclass procedure TUtils.MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T;\n  Comparer: IComparer<T>);\nvar\n  I: Integer;\nbegin\n  if Comparer = nil then Comparer := TComparer<T>.Default;\n  for I := Low(Arr) to High(Arr) do\n    if Comparer.Compare(Arr[I], Lowest) < 0 then\n      Arr[I] := Lowest;\nend;\n\nclass procedure TUtils.MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T);\nbegin\n  MakeAtLeast<T>(Arr, Lowest, nil);\nend;\n\nvar\n  Ints: TArray<Integer>;\n  Value: Integer;\nbegin\n  Ints := TArray<Integer>.Create(0, 1, 2, 3);\n  TUtils.MakeAtLeast<Integer>(Ints, 2);\n  for Value in Ints do\n    WriteLn(Value);\n  ReadLn;\nend.\nAs with C#, methods as well as whole types can have one or more type parameters. In the example, TArray is a generic type (defined by the language) and MakeAtLeast a generic method. The available constraints are very similar to the available constraints in C#: any value type, any class, a specific class or interface, and a class with a parameterless constructor. Multiple constraints act as an additive union.\n\nGenericity in Free Pascal[edit]\nFree Pascal implemented generics before Delphi, and with different syntax and semantics. However, work is now underway to implement Delphi generics alongside native FPC ones (see Wiki). This allows Free Pascal programmers to use generics in whatever style they prefer.\n\nDelphi and Free Pascal example:\n\n// Delphi style\nunit A;\n\n{$ifdef fpc}\n  {$mode delphi}\n{$endif}\n\ninterface\n\ntype\n  TGenericClass<T> = class\n    function Foo(const AValue: T): T;\n  end;\n\nimplementation\n\nfunction TGenericClass<T>.Foo(const AValue: T): T;\nbegin\n  Result := AValue + AValue;\nend;\n\nend.\n\n// Free Pascal's ObjFPC style\nunit B;\n\n{$ifdef fpc}\n  {$mode objfpc}\n{$endif}\n\ninterface\n\ntype\n  generic TGenericClass<T> = class\n    function Foo(const AValue: T): T;\n  end;\n\nimplementation\n\nfunction TGenericClass.Foo(const AValue: T): T;\nbegin\n  Result := AValue + AValue;\nend;\n\nend.\n\n// example usage, Delphi style\nprogram TestGenDelphi;\n\n{$ifdef fpc}\n  {$mode delphi}\n{$endif}\n\nuses\n  A,B;\n\nvar\n  GC1: A.TGenericClass<Integer>;\n  GC2: B.TGenericClass<String>;\nbegin\n  GC1 := A.TGenericClass<Integer>.Create;\n  GC2 := B.TGenericClass<String>.Create;\n  WriteLn(GC1.Foo(100)); // 200\n  WriteLn(GC2.Foo('hello')); // hellohello\n  GC1.Free;\n  GC2.Free;\nend.\n\n// example usage, ObjFPC style\nprogram TestGenDelphi;\n\n{$ifdef fpc}\n  {$mode objfpc}\n{$endif}\n\nuses\n  A,B;\n\n// required in ObjFPC\ntype\n  TAGenericClassInt = specialize A.TGenericClass<Integer>;\n  TBGenericClassString = specialize B.TGenericClass<String>;\nvar\n  GC1: TAGenericClassInt;\n  GC2: TBGenericClassString;\nbegin\n  GC1 := TAGenericClassInt.Create;\n  GC2 := TBGenericClassString.Create;\n  WriteLn(GC1.Foo(100)); // 200\n  WriteLn(GC2.Foo('hello')); // hellohello\n  GC1.Free;\n  GC2.Free;\nend.\nFunctional languages[edit]\nGenericity in Haskell[edit]\nThe type class mechanism of Haskell supports generic programming. Six of the predefined type classes in Haskell (including Eq, the types that can be compared for equality, and Show, the types whose values can be rendered as strings) have the special property of supporting derived instances. This means that a programmer defining a new type can state that this type is to be an instance of one of these special type classes, without providing implementations of the class methods as is usually necessary when declaring class instances. All the necessary methods will be \"derived\" – that is, constructed automatically – based on the structure of the type. For instance, the following declaration of a type of binary trees states that it is to be an instance of the classes Eq and Show:\n\ndata BinTree a = Leaf a | Node (BinTree a) a (BinTree a)\n      deriving (Eq, Show)\nThis results in an equality function (==) and a string representation function (show) being automatically defined for any type of the form BinTree T provided that T itself supports those operations.\n\nThe support for derived instances of Eq and Show makes their methods == and show generic in a qualitatively different way from parametrically polymorphic functions: these \"functions\" (more accurately, type-indexed families of functions) can be applied to values of various types, and although they behave differently for every argument type, little work is needed to add support for a new type. Ralf Hinze (2004) has shown that a similar effect can be achieved for user-defined type classes by certain programming techniques. Other researchers have proposed approaches to this and other kinds of genericity in the context of Haskell and extensions to Haskell (discussed below).\n\nPolyP[edit]\nPolyP was the first generic programming language extension to Haskell. In PolyP, generic functions are called polytypic. The language introduces a special construct in which such polytypic functions can be defined via structural induction over the structure of the pattern functor of a regular datatype. Regular datatypes in PolyP are a subset of Haskell datatypes. A regular datatype t must be of kind * → *, and if a is the formal type argument in the definition, then all recursive calls to t must have the form t a. These restrictions rule out higher-kinded datatypes as well as nested datatypes, where the recursive calls are of a different form. The flatten function in PolyP is here provided as an example:\n\n   flatten :: Regular d => d a -> [a]\n   flatten = cata fl\n   \n   polytypic fl :: f a [a] -> [a]\n     case f of\n       g+h -> either fl fl\n       g*h -> \\(x,y) -> fl x ++ fl y\n       () -> \\x -> []\n       Par -> \\x -> [x]\n       Rec -> \\x -> x\n       d@g -> concat . flatten . pmap fl\n       Con t -> \\x -> []\n   \n   cata :: Regular d => (FunctorOf d a b -> b) -> d a -> b\nGeneric Haskell[edit]\nGeneric Haskell is another extension to Haskell, developed at Utrecht University in the Netherlands. The extensions it provides are:\n\nType-indexed values are defined as a value indexed over the various Haskell type constructors (unit, primitive types, sums, products, and user-defined type constructors). In addition, we can also specify the behaviour of a type-indexed values for a specific constructor using constructor cases, and reuse one generic definition in another using default cases.\nThe resulting type-indexed value can be specialised to any type.\n\nKind-indexed types are types indexed over kinds, defined by giving a case for both * and k → k'. Instances are obtained by applying the kind-indexed type to a kind.\nGeneric definitions can be used by applying them to a type or kind. This is called generic application. The result is a type or value, depending on which sort of generic definition is applied.\nGeneric abstraction enables generic definitions be defined by abstracting a type parameter (of a given kind).\nType-indexed types are types which are indexed over the type constructors. These can be used to give types to more involved generic values. The resulting type-indexed types can be specialised to any type.\nAs an example, the equality function in Generic Haskell:[28]\n\n   type Eq {[ * ]} t1 t2 = t1 -> t2 -> Bool\n   type Eq {[ k -> l ]} t1 t2 = forall u1 u2. Eq {[ k ]} u1 u2 -> Eq {[ l ]} (t1 u1) (t2 u2)\n   \n   eq {| t :: k |} :: Eq {[ k ]} t t\n   eq {| Unit |} _ _ = True\n   eq {| :+: |} eqA eqB (Inl a1) (Inl a2) = eqA a1 a2\n   eq {| :+: |} eqA eqB (Inr b1) (Inr b2) = eqB b1 b2\n   eq {| :+: |} eqA eqB _ _ = False\n   eq {| :*: |} eqA eqB (a1 :*: b1) (a2 :*: b2) = eqA a1 a2 && eqB b1 b2\n   eq {| Int |} = (==)\n   eq {| Char |} = (==)\n   eq {| Bool |} = (==)\nClean[edit]\nClean offers generic programming based PolyP and the generic Haskell as supported by the GHC>=6.0. It parametrizes by kind as those but offers overloading.\n\nOther languages[edit]\nThe ML family of programming languages support generic programming through parametric polymorphism and generic modules called functors. Both Standard ML and OCaml provide functors, which are similar to class templates and to Ada's generic packages. Scheme syntactic abstractions also have a connection to genericity – these are in fact a superset of templating à la C++.\n\nA Verilog module may take one or more parameters, to which their actual values are assigned upon the instantiation of the module. One example is a generic register array where the array width is given via a parameter. Such the array, combined with a generic wire vector, can make a generic buffer or memory module with an arbitrary bit width out of a single module implementation.[29]\n\nVHDL, being derived from Ada, also have generic ability.\n\nSee also[edit]\nPartial evaluation\nConcept (generic programming)\nType polymorphism\nTemplate metaprogramming",
          "subparadigms": []
        },
        {
          "pdid": 32,
          "name": "Literate programming",
          "details": "Literate programming is an approach to programming introduced by Donald Knuth in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated.[1]\n\nThe literate programming paradigm, as conceived by Knuth, represents a move away from writing programs in the manner and order imposed by the computer, and instead enables programmers to develop programs in the order demanded by the logic and flow of their thoughts.[2] Literate programs are written as an uninterrupted exposition of logic in an ordinary human language, much like the text of an essay, in which macros are included to hide abstractions and traditional source code.\n\nLiterate programming (LP) tools are used to obtain two representations from a literate source file: one suitable for further compilation or execution by a computer, the \"tangled\" code, and another for viewing as formatted documentation, which is said to be \"woven\" from the literate source.[3] While the first generation of literate programming tools were computer language-specific, the later ones are language-agnostic and exist above the programming languages.\n\nContents  [hide] \n1\tHistory and philosophy\n2\tConcept\n2.1\tAdvantages\n2.2\tContrast with documentation generation\n3\tWorkflow\n4\tExample\n5\tTools\n6\tSee also\n7\tReferences\n8\tFurther reading\nHistory and philosophy[edit]\nLiterate programming was first introduced by Donald E. Knuth. The main intention behind this approach was to treat program as a literature understandable to human beings. This approach was implemented at Stanford university as a part of research on algorithms and digital typography. This implementation was further called as “WEB” by Donald Knuth since he believed that it was one of the few three-letter words of English that hadn’t already been applied to computer. However, it correctly resembles the complicated nature of software delicately pieced together from simple materials.[1]\n\n\n\nConcept[edit]\nLiterate programming is writing out the program logic in a human language with included (separated by a primitive markup) code snippets and macros. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a meta-language on top of the underlying programming language.\n\nA preprocessor is used to substitute arbitrary hierarchies, or rather \"interconnected 'webs' of macros\",[4] to produce the compilable source code with one command (\"tangle\"), and documentation with another (\"weave\"). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.\n\n\n\nAdvantages[edit]\nAccording to Knuth,[5][6] literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one's thoughts during a program's creation.[7] The resulting documentation allows the author to restart his own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher \"bird's eye view\" of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.[5]\n\nContrast with documentation generation[edit]\nLiterate programming is very often misunderstood[8] to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is backwards: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; in literate programming code is embedded in documentation, with the code following the structure of the documentation.\n\nThis misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are \"literate programming tools\". However, because these tools do not implement the \"web of abstract concepts\" hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.[8][9]\n\n\n\nWorkflow[edit]\nImplementing literate programming consists of two steps:\n\n1) Weaving: Generating comprehensive document about program and its maintenance.\n\n2) Tangling: Generating machine executable code\n\nWeaving and Tangling are done on the same source so that they are consistent with each other.\n\nExample[edit]\nA classic example of literate programming is the literate implementation of the standard Unix wc word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his Literate Programming book. The same example was later rewritten for the noweb literate programming tool.[10] This example provides a good illustration of the basic elements of literate programming.\n\nCreation of macros\nThe following snippet of the wc literate program[10] shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new \"operators\" in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets (\"<<...>>\") that indicate macros, the \"@\" symbol which indicates the end of the code section in a noweb file. The \"<<*>>\" symbol stands for the \"root\", topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as \"<<name of the chunk>>=\", with the equal sign), so one literate program file can contain several files with machine source code.\n\nThe purpose of wc is to count lines, words, and/or characters in a list of files. The\nnumber of lines in a file is ......../more explanations/\n\nHere, then, is an overview of the file wc.c that is defined by the noweb program wc.nw:\n    <<*>>=\n    <<Header files to include>>\n    <<Definitions>>\n    <<Global variables>>\n    <<Functions>>\n    <<The main program>>\n    @\n\nWe must include the standard I/O definitions, since we want to send formatted output\nto stdout and stderr.\n    <<Header files to include>>=\n    #include <stdio.h>\n    @\nThe unraveling of the chunks can be done in any place in the literate program text file, not necessarily in the order they are sequenced in the enclosing chunk, but as is demanded by the logic reflected in the explanatory text that envelops the whole program.\n\nProgram as a web—macros are not just section names\nMacros are not the same as \"section names\" in standard documentation. Literate programming macros can hide any chunk of code behind themselves, and be used inside any low-level machine language operators, often inside logical operators such as \"if\", \"while\" or \"case\". This is illustrated by the following snippet of the wc literate program.[10]\n\nThe present chunk, which does the counting, was actually one of\nthe simplest to write. We look at each character and change state if it begins or ends\na word.\n\n    <<Scan file>>=\n    while (1) {\n      <<Fill buffer if it is empty; break at end of file>>\n      c = *ptr++;\n      if (c > ' ' && c < 0177) {\n        /* visible ASCII codes */\n        if (!in_word) {\n          word_count++;\n          in_word = 1;\n        }\n        continue;\n      }\n      if (c == '\\n') line_count++;\n      else if (c != ' ' && c != '\\t') continue;\n      in_word = 0;\n        /* c is newline, space, or tab */\n    }\n    @\nIn fact, macros can stand for any arbitrary chunk of code or other macros, and are thus more general than top-down or bottom-up \"chunking\", or than subsectioning. Knuth says that when he realized this, he began to think of a program as a web of various parts.[1]\n\nOrder of human logic, not that of the compiler\nIn a noweb literate program besides the free order of their exposition, the chunks behind macros, once introduced with \"<<...>>=\", can be grown later in any place in the file by simply writing \"<<name of the chunk>>=\" and adding more content to it, as the following snippet illustrates (\"plus\" is added by the document formatter for readability, and is not in the code).[10]\n\n The grand totals must be initialized to zero at the beginning of the program.\nIf we made these variables local to main, we would have to do this  initialization\nexplicitly; however, C globals are automatically zeroed. (Or rather,``statically\nzeroed.'' (Get it?)\n\n    <<Global variables>>+=\n    long tot_word_count, tot_line_count,\n         tot_char_count;\n      /* total number of words, lines, chars */\n    @\nRecord of the train of thought\nThe documentation for a literate program is produced as part of writing the program. Instead of comments provided as side notes to source code a literate program contains the explanation of concepts on each level, with lower level concepts deferred to their appropriate place, which allows for better communication of thought. The snippets of the literate wc above show how an explanation of the program and its source code are interwoven. Such exposition of ideas creates the flow of thought that is like a literary work. Knuth wrote a \"novel\" which explains the code of the computer strategy game Colossal Cave Adventure.[11]\n\nTools[edit]\nThe first published literate programming environment was WEB, introduced by Donald Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe.[12] The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems and can produce TeX and PDF documentation.\n\nThere are various other implementations of the literate programming concept:\n\nAxiom, which is evolved from scratchpad, a computer algebra system developed by IBM. It is now being developed by Tim Daly, one of the developer of scratchpad, Axiom is totally written as a literate program.\nnoweb is independent of the programming language of the source code. It is well known for its simplicity, given the need of using only two text markup conventions and two tool invocations, and it allows for text formatting in HTML rather than going through the TeX system.\nLiterate is a \"modern literate programming system.\" Like noweb, it works with any programming language, but it produces pretty-printed and syntax-highlighted HTML, and it tries to retain all the advantages of CWEB, including output formatted like CWEB. Other notable advantages compared with older tools include being based on Markdown and generating well-formatted \"tangled\" code. See External Links.\nFunnelWeb is another LP tool that can produce HTML documentation output. It has more complicated markup (with \"@\" escaping any FunnelWeb command), but has many more flexible options. Like noweb, it is independent of the programming language of the source code.\nNuweb can translate a single LP source into any number of code files in any mix of languages together with documentation in LaTeX. It does it in a single invocation; it does not have separate weave and tangle commands. It does not have the extensibility of noweb, but it can use the listings package of LaTeX to provide pretty-printing and the hyperref package to provide hyperlinks in PDF output. It also has extensive indexing and cross-referencing facilities including cross-references from the generated code back to the documentation, both as automatically generated comments and as strings that the code can use to report its behaviour. Vimes is a type-checker for Z notation which shows the use of nuweb in a practical application. Around 15,000 lines of nuweb source are translated into nearly 15,000 lines of C/C++ code and over 460 pages of documentation. See External links.\nMolly is a LP tool written in Perl, which aims to modernize and scale it with \"folding HTML\" and \"virtual views\" on code. It uses \"noweb\" markup for the literate source files. See External links.\nCodnar is an inverse literate programming tool available as a Ruby Gem (see External links). Instead of the machine-readable source code being extracted out of the literate documentation sources, the literate documentation is extracted out of the normal machine-readable source code files. This allows these source code files to be edited and maintained as usual. The approach is similar to that used by popular API documentation tools, such as JavaDoc. Such tools, however, generate API reference documentation, while Codnar generates a linear narrative describing the code, similar to that created by classical LP tools. Codnar can co-exist with API documentation tools, allowing both a reference manual and a linear narrative to be generated from the same set of source code files.\nThe Leo text editor is an outlining editor which supports optional noweb and CWEB markup. The author of Leo mixes two different approaches: first, Leo is an outlining editor, which helps with management of large texts; second, Leo incorporates some of the ideas of literate programming, which in its pure form (i.e., the way it is used by Knuth Web tool or tools like \"noweb\") is possible only with some degree of inventiveness and the use of the editor in a way not exactly envisioned by its author (in modified @root nodes). However, this and other extensions (@file nodes) make outline programming and text management successful and easy and in some ways similar to literate programming.[13]\nThe Haskell programming language has native support for semi-literate programming, generally inspired by CWEB but with a significantly reduced functionality and simpler implementation. When aiming for TeX output, one writes a plain LaTeX file where source code is marked by a given surrounding environment; LaTeX can be set up to handle that environment, while the Haskell compiler looks for the right markers to identify Haskell statements to compile, removing the TeX documentation as if they were comments. However, as described above, this is not literate programming in the sense intended by Knuth. Haskell's functional, modular nature[14] makes literate programming directly in the language somewhat easier, but it is not nearly as powerful as one of the WEB tools where \"tangle\" can reorganize in arbitrary ways.\nThe Web 68 Literate Programming system uses Algol 68 as the underlying programming language, although there is nothing in the pre-processor 'tang' to force the use of that language.[15]\nEmacs org-mode for literate programming through Babel,[16] which allows embedding blocks of source code from multiple programming languages[17] within a single text document. Blocks of code can share data with each other, display images inline, or be parsed into pure source code using the noweb reference syntax.[18]\nCoffeeScript supports a \"literate\" mode, which enables programs to be compiled from a source document written in Markdown with indented blocks of code.[19]\nWolfram Language, formerly known as Mathematica, is written in notebooks which combine text with code.[20]\nSwift (programming language), created by Apple Inc. can be edited in Playgrounds which provide an interactive programming environment that evaluates each statement and displays live results as the code is edited. Playgrounds also allow the user to add Markup language along with the code that provide headers, inline formatting and images.[21]\nJulia (programming language) supports the iJulia mode of development which - inspired by iPython - works in the format of notebooks, which combine text, graphs, etc. with the written code.\nSee also[edit]\nSweave and Knitr – examples of use of the \"noweb\"-like Literate Programming tool inside the R language for creation of dynamic statistical reports\nSelf-documenting code – source code that can be easily understood without documentation",
          "subparadigms": []
        },
        {
          "pdid": 33,
          "name": "Procedural",
          "details": "Procedural programming is a programming paradigm, derived from structured programming, based upon the concept of the procedure call. Procedures, also known as routines, subroutines, or functions (not to be confused with mathematical functions, but similar to those used in functional programming), simply contain a series of computational steps to be carried out. Any given procedure might be called at any point during a program's execution, including by other procedures or itself. Procedural programming languages include C, Go, Fortran, Pascal, Ada, and BASIC.[1]\n\nComputer processors provide hardware support for procedural programming through a stack register and instructions for calling procedures and returning from them. Hardware support for other types of programming is possible, but no attempt was commercially successful (for example Lisp machines or Java processors).\n\nContents  [hide] \n1\tProcedures and modularity\n2\tComparison with imperative programming\n3\tComparison with object-oriented programming\n4\tComparison with functional programming\n5\tComparison with logic programming\n6\tSee also\n7\tReferences\n8\tExternal links\nProcedures and modularity[edit]\nMain article: Modular programming\nModularity is generally desirable, especially in large, complicated programs. Inputs are usually specified syntactically in the form of arguments and the outputs delivered as return values.\n\nScoping is another technique that helps keep procedures modular. It prevents the procedure from accessing the variables of other procedures (and vice versa), including previous instances of itself, without explicit authorization.\n\nLess modular procedures, often used in small or quickly written programs, tend to interact with a large number of variables in the execution environment, which other procedures might also modify.\n\nBecause of the ability to specify a simple interface, to be self-contained, and to be reused, procedures are a convenient vehicle for making pieces of code written by different people or different groups, including through programming libraries.\n\nComparison with imperative programming[edit]\nProcedural programming languages are also imperative languages, because they make explicit references to the state of the execution environment. This could be anything from variables (which may correspond to processor registers) to something like the position of the \"turtle\" in the Logo programming language.\n\nOften, the terms \"procedural programming\" and \"imperative programming\" are used synonymously. However, procedural programming relies heavily on blocks and scope, whereas imperative programming as a whole may or may not have such features. As such, procedural languages generally use reserved words that act on blocks, such as if, while, and for, to implement control flow, whereas non-structured imperative languages use goto statements and branch tables for the same purpose.\n\nComparison with object-oriented programming[edit]\nThe focus of procedural programming is to break down a programming task into a collection of variables, data structures, and subroutines, whereas in object-oriented programming it is to break down a programming task into objects that expose behavior (methods) and data (members or attributes) using interfaces. The most important distinction is that while procedural programming uses procedures to operate on data structures, object-oriented programming bundles the two together, so an \"object\", which is an instance of a class, operates on its \"own\" data structure.[2]\n\nNomenclature varies between the two, although they have similar semantics:\n\nProcedural\tObject-oriented\nprocedure\tmethod\nrecord\tobject\nmodule\tclass\nprocedure call\tmessage\nComparison with functional programming[edit]\nThe principles of modularity and code reuse in practical functional languages are fundamentally the same as in procedural languages, since they both stem from structured programming. So for example:\n\nProcedures correspond to functions. Both allow the reuse of the same code in various parts of the programs, and at various points of its execution.\nBy the same token, procedure calls correspond to function application.\nFunctions and their invocations are modularly separated from each other in the same manner, by the use of function arguments, return values and variable scopes.\nThe main difference between the styles is that functional programming languages remove or at least deemphasize the imperative elements of procedural programming. The feature set of functional languages is therefore designed to support writing programs as much as possible in terms of pure functions:\n\nWhereas procedural languages model execution of the program as a sequence of imperative commands that may implicitly alter shared state, functional programming languages model execution as the evaluation of complex expressions that only depend on each other in terms of arguments and return values. For this reason, functional programs can have a freer order of code execution, and the languages may offer little control over the order in which various parts of the program are executed. (For example, the arguments to a procedure invocation in Scheme are executed in an arbitrary order.)\nFunctional programming languages support (and heavily use) first-class functions, anonymous functions and closures, although these concepts are being included in newer procedural languages.\nFunctional programming languages tend to rely on tail call optimization and higher-order functions instead of imperative looping constructs.\nMany functional languages, however, are in fact impurely functional and offer imperative/procedural constructs that allow the programmer to write programs in procedural style, or in a combination of both styles. It is common for input/output code in functional languages to be written in a procedural style.\n\nThere do exist a few esoteric functional languages (like Unlambda) that eschew structured programming precepts for the sake of being difficult to program in (and therefore challenging). These languages are the exception to the common ground between procedural and functional languages.\n\nComparison with logic programming[edit]\nIn logic programming, a program is a set of premises, and computation is performed by attempting to prove candidate theorems. From this point of view, logic programs are declarative, focusing on what the problem is, rather than on how to solve it.\n\nHowever, the backward reasoning technique, implemented by SLD resolution, used to solve problems in logic programming languages such as Prolog, treats programs as goal-reduction procedures. Thus clauses of the form:\n\nH :- B1, …, Bn.\nhave a dual interpretation, both as procedures\n\nto show/solve H, show/solve B1 and … and Bn\nand as logical implications:\n\nB1 and … and Bn implies H.\nExperienced logic programmers use the procedural interpretation to write programs that are effective and efficient, and they use the declarative interpretation to help ensure that programs are correct.\n\nSee also[edit]\nComparison of programming paradigms\nDeclarative programming\nFunctional programming (contrast)\nImperative programming\nLogic programming\nObject-oriented programming\nProgramming paradigms\nProgramming language\nStructured programming\nSQL procedural extensions",
          "subparadigms": []
        },
        {
          "pdid": 34,
          "name": "Imperative",
          "details": "In computer science, imperative programming is a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\n\nThe term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.\n\nContents  [hide] \n1\tImperative and procedural programming\n2\tRationale and foundations of imperative programming\n3\tHistory of imperative and object-oriented languages\n4\tSee also\n5\tNotes\n6\tReferences\nImperative and procedural programming[edit]\nProcedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavily-procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. From the 1960s onwards, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.[1]\n\nProcedural programming could be considered a step towards declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.\n\nRationale and foundations of imperative programming[edit]\nThe hardware implementation of almost all computers is imperative.[note 1] Nearly all computer hardware is designed to execute machine code, which is native to the computer, written in the imperative style. From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.\n\nAssignment statements, in imperative paradigm, perform an operation on information located in memory and store the results in memory for later use. High-level imperative languages, in addition, permit the evaluation of complex expressions, which may consist of a combination of arithmetic operations and function evaluations, and the assignment of the resulting value to memory. Looping statements (as in while loops, do while loops, and for loops) allow a sequence of statements to be executed multiple times. Loops can either execute the statements they contain a predefined number of times, or they can execute them repeatedly until some condition changes. Conditional branching statements allow a sequence of statements to be executed only if some condition is met. Otherwise, the statements are skipped and the execution sequence continues from the statement following them. Unconditional branching statements allow an execution sequence to be transferred to another part of a program. These include the jump (called goto in many languages), switch, and the subprogram, subroutine, or procedure call (which usually returns to the next statement after the call).\n\nEarly in the development of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by hierarchical decomposition into simpler procedural structures.\n\nMany imperative programming languages (such as Fortran, BASIC, and C) are abstractions of assembly language.[2]\n\nHistory of imperative and object-oriented languages[edit]\nThe earliest imperative languages were the machine languages of the original computers. In these languages, instructions were very simple, which made hardware implementation easier, but hindered the creation of complex programs. FORTRAN, developed by John Backus at International Business Machines (IBM) starting in 1954, was the first major programming language to remove the obstacles presented by machine code in the creation of complex programs. FORTRAN was a compiled language that allowed named variables, complex expressions, subprograms, and many other features now common in imperative languages. The next two decades saw the development of many other major high-level imperative programming languages. In the late 1950s and 1960s, ALGOL was developed in order to allow mathematical algorithms to be more easily expressed, and even served as the operating system's target language for some computers. MUMPS (1966) carried the imperative paradigm to a logical extreme, by not having any statements at all, relying purely on commands, even to the extent of making the IF and ELSE commands independent of each other, connected only by an intrinsic variable named $TEST. COBOL (1960) and BASIC (1964) were both attempts to make programming syntax look more like English. In the 1970s, Pascal was developed by Niklaus Wirth, and C was created by Dennis Ritchie while he was working at Bell Laboratories. Wirth went on to design Modula-2 and Oberon. For the needs of the United States Department of Defense, Jean Ichbiah and a team at Honeywell began designing Ada in 1978, after a 4-year project to define the requirements for the language. The specification was first published in 1983, with revisions in 1995 and 2005–06.\n\nThe 1980s saw a rapid growth in interest in object-oriented programming. These languages were imperative in style, but added features to support objects. The last two decades of the 20th century saw the development of many such languages. Smalltalk-80, originally conceived by Alan Kay in 1969, was released in 1980, by the Xerox Palo Alto Research Center (PARC). Drawing from concepts in another object-oriented language—Simula (which is considered the world's first object-oriented programming language, developed in the 1960s)—Bjarne Stroustrup designed C++, an object-oriented language based on C. C++ was first implemented in 1985. In the late 1980s and 1990s, the notable imperative languages drawing on object-oriented concepts were Perl, released by Larry Wall in 1987; Python, released by Guido van Rossum in 1990; Visual Basic and Visual C++ (which included Microsoft Foundation Class Library (MFC) 2.0), released by Microsoft in 1991 and 1993 respectively; PHP, released by Rasmus Lerdorf in 1994; Java, released by Sun Microsystems in 1994, and Ruby, released by Yukihiro \"Matz\" Matsumoto in 1995. Microsoft's .NET Framework (2002) is imperative at its core, as are its main target languages, VB.NET and C# that run on it; however Microsoft's F#, a functional language, also runs on it.\n\nSee also[edit]\nicon\tComputer science portal\nFunctional programming\nComparison of programming paradigms\nDeclarative programming (contrast)\nProgramming paradigms\nObject-oriented programming\nReactive programming\nHistory of programming languages\nList of imperative programming languages\nCategory:Procedural programming languages lists additional imperative programming languages",
          "subparadigms": [
            32,
            33
          ]
        },
        {
          "pdid": 35,
          "name": "Inductive",
          "details": "Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\nContents  [hide] \n1\tDefinition\n2\tHistory\n3\tApplication areas\n4\tSee also\n5\tExternal links\n6\tFurther reading\n7\tReferences\nDefinition[edit]\nInductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.\n\nOutput of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.\n\nIn many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis,[1][2] usually opposed to 'deductive' program synthesis,[3][4][5] where the specification is usually complete.\n\nIn other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.\n\nThe diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.\n\nHistory[edit]\nResearch on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers[6] and work of Biermann.[7] These approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith.[8] Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.\n\nThe advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro[9] eventually spawning the new field of inductive logic programming (ILP).[10] The early works of Plotkin,[11][12] and his \"relative least general generalization (rlgg)\", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM.[13] But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs[14][15][16] with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.[17]\n\nIn parallel to work in ILP, Koza[18] proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE[19] and the systematic-search-based system MagicHaskeller.[20] Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.\n\nThe early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem.[21] The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold.[22] More recently, the language learning problem was addressed by the inductive programming community.[23][24]\n\nIn the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).\n\nOther ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures;[25][26][27] abstraction has also been explored as a more powerful approach to cumulative learning and function invention.[28][29]\n\nOne powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).[30][31][32][33]\n\nApplication areas[edit]\nThe first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where \"learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations\".\n\nSince then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming,[34] the related areas of programming by example[35] and programming by demonstration,[36] and intelligent tutoring systems.\n\nOther areas where inductive inference has been recently applied are knowledge acquisition,[37] artificial general intelligence,[38] reinforcement learning and theory evaluation,[39][40] and cognitive science in general.[41][42] There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.\n\nSee also[edit]\nAutomatic programming\nDeclarative programming\nEvolutionary programming\nFunctional programming\nGenetic programming\nGrammar induction\nInductive reasoning\nInductive logic programming\nInductive functional programming\nLogic programming\nMachine learning\nProbabilistic programming language\nProgram synthesis\nProgramming by example\nProgramming by demonstration\nStructure mining\nTest-driven development",
          "subparadigms": []
        },
        {
          "pdid": 36,
          "name": "Natural language",
          "details": "Natural Language Programming (NLP) is an ontology-assisted way of programming in terms of natural language sentences, e.g. English. A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural languages and natural language user interfaces include Inform7, a natural programming language for making interactive fiction; Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural language input.\n\nContents  [hide] \n1\tInterpretation\n2\tSoftware paradigm\n3\tPublication value of NLP programs and documents\n4\tContribution of NLP programs to machine knowledge\n5\tSee also\n6\tBibliography\n7\tReferences\n8\tExternal links\nInterpretation[edit]\nThe smallest unit of statement in NLP is a sentence. Each sentence is stated in terms of concepts from the underlying ontology, attributes in that ontology and named objects in capital letters. In an NLP text every sentence unambiguously compiles into a procedure call in the underlying high-level programming language such as MATLAB, Octave, SciLab, Python, etc.\n\nSymbolic languages such as Mathematica are capable of interpreted processing of queries by sentences. This can allow interactive requests such as that implemented in Wolfram Alpha.[1][2] The difference between these and NLP is that the latter builds up a single program or a library of routines that are programmed through natural language sentences using an ontology that defines the available data structures in a high level programming language.\n\nAn example text from an English language NLP program (in sEnglish) is as follows:\n\nIf U_ is 'smc01-control', then do the following. Define surface weights Alpha as \"[0.5, 0.5]\". Initialise matrix Phi as a 'unit matrix'. Define J as the 'inertia matrix' of Spc01. Compute matrix J2 as the inverse of J . Compute position velocity error Ve and angular velocity error Oe from dynamical state X, guidance reference Xnow . Define the joint sliding surface G2 from the position velocity error Ve and angular velocity error Oe using the surface weights Alpha. Compute the smoothed sign function SG2 from the joint sliding surface G2 with sign threshold 0.01. Compute special dynamical force F from dynamical state X and surface weights Alpha. Compute control torque T and control force U from matrix J2, surface weights Alpha, special dynamical force F, smoothed sign function SG2. Finish conditional actions.\n\nthat defines a feedback control scheme using a sliding mode control method.\n\nSoftware paradigm[edit]\nNatural language programming is a top down method of writing software. Its stages are as follows:\n\nDefinition of an ontology - taxonomy - of concepts needed to describe tasks in the topic addressed. Each concept and all their attributes are defined in natural language words. This ontology will define the data structures the NLP can use in sentences.\nDefinition of one or more top level sentences in terms of concepts from the ontology. These sentences are later used to invoke the most important activities in the topic.\nDefining of each of the top level sentences in terms of a sequence of sentences.\nDefining each of the lower level sentences in terms of other sentences or by a simple sentence of the form Execute code \"...\". where ... stands for a code in terms of the associated high level programming language.\nRepeating the previous step until you have no sentences left undefined. During this process each of sentences can be classified to belong to a section of the document to be produced in HTML or Latex format to form the final NLP program.\nTesting the meaning of each sentence by executing its code using testing objects.\nProviding a library of procedure calls (in the underlying high level language) which are needed in the code definitions of some low-level-sentence meanings.\nProviding a title, author data and compiling the sentences into an HTML or LaTex file.\nPublishing the NLP program as a webpage on the Internet or as a PDF file compiled from the LaTex document.\nPublication value of NLP programs and documents[edit]\nAn NLP program is a precise formal description of some procedure that its author created. It is human readable and it can also be read by a suitable software agent. For example, a web page in an NLP format can be read by a software personal assistant agent to a person and she or he can ask the agent to execute some sentences, i.e. carry out some task or answer a question. There is a reader agent available for English interpretation of HTML based NLP documents that a person can run on her personal computer .\n\nContribution of NLP programs to machine knowledge[edit]\nAn ontology class in a natural language program that is not a concept in the sense as humans use concepts. Concepts in an NLP are examples (samples) of generic human concepts. Each sentence in an NLP program is either (1) stating a relationship in a world model or (2) carries out an action in the environment or (3) carries out a computational procedure or (4) invokes an answering mechanism in response to a question.\n\nA set of NLP sentences, with associated ontology defined, can also be used as a pseudo code that does not provide the details in any underlying high level programming language. In such an application the sentences used become high level abstractions (conceptualisations) of computing procedures that are computer language and machine independent.\n\nSee also[edit]\nAttempto Controlled English\nNatural language processing\nKnowledge representation\nEnd-user programming\nProgramming languages with English-like syntax\nAppleScript\nCOBOL\nFLOW-MATIC\nInform 7\nJOSS\nTranscript\nSQL\nxTalk",
          "subparadigms": []
        },
        {
          "pdid": 37,
          "name": "Service-oriented modeling",
          "details": "Service-oriented modeling is the discipline of modeling business and software systems, for the purpose of designing and specifying service-oriented business systems within a variety of architectural styles and paradigms, such as application architecture, service-oriented architecture, microservices, and cloud computing.\n\nAny service-oriented modeling method typically includes a modeling language that can be employed by both the 'problem domain organization' (the Business), and 'solution domain organization' (the Information Technology Department), whose unique perspectives typically influence the service development life-cycle strategy and the projects implemented using that strategy.\n\nService-oriented modeling typically strives to create models that provide a comprehensive view of the analysis, design, and architecture of all 'Software Entities' in an organization, which can be understood by individuals with diverse levels of business and technical understanding. Service-oriented modeling typically encourages viewing software entities as 'assets' (service-oriented assets), and refers to these assets collectively as 'services'. A key service design concern is to find the right service granularity both on the business (domain) level and on a technical (interface contract) level.\n\nContents  [hide] \n1\tPopular approaches\n1.1\tService-oriented design and development methodology\n1.2\tService-oriented modeling and architecture\n1.3\tService-oriented modeling framework (SOMF)\n2\tSee also\n3\tReferences\n4\tFurther reading\n5\tExternal links\nPopular approaches[edit]\nSeveral approaches that have been proposed specifically for designing and modeling services, including SDDM, SOMA and SOMF.\n\nService-oriented design and development methodology[edit]\nService-Oriented Design and Development Methodology (SDDM) is a fusion method created and compiled by M. Papazoglou and W.J. van den Heuvel.[1] The paper argues that SOA designers and service developers cannot be expected to oversee a complex service-oriented development project without relying on a sound design and development methodology. It provides an overview of the methods and techniques used in service-oriented design, approaches the service development methodology from the point of view of both service producers and requesters, and reviews the range of SDDM elements that are available to these roles.\n\nAn update to SDDM was later published in.[2]\n\nService-oriented modeling and architecture[edit]\nIBM announced service-oriented modeling and architecture (SOMA) as its SOA-related methodology in 2004 and published parts of it subsequently.[3] SOMA refers to the more general domain of service modeling necessary to design and create SOA. SOMA covers a broader scope and implements service-oriented analysis and design (SOAD) through the identification, specification and realization of services, components that realize those services (a.k.a. \"service components\"), and flows that can be used to compose services.\n\nSOMA includes an analysis and design method that extends traditional object-oriented and component-based analysis and design methods to include concerns relevant to and supporting SOA. It consists of three major phases of identification, specification and realization of the three main elements of SOA, namely, services, components that realize those services (aka service components) and flows that can be used to compose services.\n\nSOMA is an end-to-end SOA method for the identification, specification, realization and implementation of services (including information services), components, flows (processes/composition). SOMA builds on current techniques in areas such as domain analysis, functional areas grouping, variability-oriented analysis (VOA) process modeling, component-based development, object-oriented analysis and design and use case modeling. SOMA introduces new techniques such as goal-service modeling, service model creation and a service litmus test to help determine the granularity of a service.\n\nSOMA identifies services, component boundaries, flows, compositions, and information through complementary techniques which include domain decomposition, goal-service modeling and existing asset analysis. The service lifecyclein SOMA consists of the phases of identification, specification, realization, implementation, deployment and management in which the fundamental building blocks of SOA are identified then refined and implemented in each phase. The fundamental building blocks of SOA consists of services, components, flows and related to them, information, policy and contracts.[4]\n\nService-oriented modeling framework (SOMF)[edit]\nSOMF has been devised by author Michael Bell as a holistic and anthropomorphic modeling language for software development that employs disciplines and a universal language to provide tactical and strategic solutions to enterprise problems.[5] The term \"holistic language\" pertains to a modeling language that can be employed to design any application, business and technological environment, either local or distributed. This universality may include design of application-level and enterprise-level solutions, including SOA landscapes, cloud computing, or big data environments. The term \"anthropomorphic\", on the other hand, affiliates the SOMF language with intuitiveness of implementation and simplicity of usage.\n\n\nSOMF Version 2.0\nSOMF is a service-oriented development life cycle methodology, a discipline-specific modeling process. It offers a number of modeling practices and disciplines that contribute to a successful service-oriented life cycle development and modeling during a project (see image on left).\n\nIt illustrates the major elements that identify the “what to do” aspects of a service development scheme. These are the modeling pillars that will enable practitioners to craft an effective project plan and to identify the milestones of a service-oriented initiative—either a small or large-scale business or a technological venture.\n\nThe provided image thumb (on the left hand side) depicts the four sections of the modeling framework that identify the general direction and the corresponding units of work that make up a service-oriented modeling strategy: practices, environments, disciplines, and artifacts. These elements uncover the context of a modeling occupation and do not necessarily describe the process or the sequence of activities needed to fulfill modeling goals. These should be ironed out during the project plan – the service-oriented development life cycle strategy – that typically sets initiative boundaries, time frame, responsibilities and accountabilities, and achievable project milestones.\n\n\n\nSee also[edit]\nObject-oriented analysis and design\nDomain-driven design\nService-oriented architecture\nService granularity principle\nUnified Modeling Language",
          "subparadigms": []
        },
        {
          "pdid": 38,
          "name": "Domain-specific",
          "details": "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There is a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as Emacs Lisp for GNU Emacs and XEmacs. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\n\nThe line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.\n\nContents  [hide] \n1\tUse\n2\tOverview\n2.1\tIn design and implementation\n2.2\tProgramming tools\n3\tDomain-specific language topics\n3.1\tUsage patterns\n3.2\tDesign goals\n3.3\tIdioms\n4\tExamples\n4.1\tGame Maker Language\n4.2\tUnix shell scripts\n4.3\tColdFusion Markup Language\n4.4\tErlang OTP\n4.5\tFilterMeister\n4.6\tMediaWiki templates\n4.7\tSoftware engineering uses\n4.8\tMetacompilers\n4.9\tUnreal Engine before version 4 and other games\n4.10\tRules Engines for Policy Automation\n4.11\tStatistical modelling languages\n4.12\tGenerate model and services to multiple programming Languages\n4.13\tOther examples\n5\tAdvantages and disadvantages\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nUse[edit]\nThe design and use of appropriate DSLs is a key part of domain engineering, by using a language suitable to the domain at hand – this may consist of using an existing DSL or GPL, or developing a new DSL. Language-Oriented Programming considers the creation of special-purpose languages for expressing problems a standard part of the problem solving process. Creating a domain-specific language (with software to support it), rather than reusing an existing language, can be worthwhile if the language allows a particular type of problem or solution to be expressed more clearly than an existing language would allow and the type of problem in question reappears sufficiently often. Pragmatically, a DSL may be specialized to a particular problem domain, a particular problem representation technique, a particular solution technique, or other aspect of a domain.\n\nOverview[edit]\nA domain-specific language is created specifically to solve problems in a particular domain and is not intended to be able to solve problems outside it (although that may be technically possible). In contrast, general-purpose languages are created to solve problems in many domains. The domain can also be a business area. Some examples of business areas include:\n\ndomain-specific language for life insurance policies developed internally in large insurance enterprise\ndomain-specific language for combat simulation\ndomain-specific language for salary calculation\ndomain-specific language for billing\nA domain-specific language is somewhere between a tiny programming language and a scripting language, and is often used in a way analogous to a programming library. The boundaries between these concepts are quite blurry, much like the boundary between scripting languages and general-purpose languages.\n\nIn design and implementation[edit]\nDomain-specific languages are languages (or often, declared syntaxes or grammars) with very specific goals in design and implementation. A domain-specific language can be one of a visual diagramming language, such as those created by the Generic Eclipse Modeling System, programmatic abstractions, such as the Eclipse Modeling Framework, or textual languages. For instance, the command line utility grep has a regular expression syntax which matches patterns in lines of text. The sed utility defines a syntax for matching and replacing regular expressions. Often, these tiny languages can be used together inside a shell to perform more complex programming tasks.\n\nThe line between domain-specific languages and scripting languages is somewhat blurred, but domain-specific languages often lack low-level functions for filesystem access, interprocess control, and other functions that characterize full-featured programming languages, scripting or otherwise. Many domain-specific languages do not compile to byte-code or executable code, but to various kinds of media objects: GraphViz exports to PostScript, GIF, JPEG, etc., where Csound compiles to audio files, and a ray-tracing domain- specific language like POV compiles to graphics files. A computer language like SQL presents an interesting case: it can be deemed a domain-specific language because it is specific to a specific domain (in SQL's case, accessing and managing relational databases), and is often called from another application, but SQL has more keywords and functions than many scripting languages, and is often thought of as a language in its own right, perhaps because of the prevalence of database manipulation in programming and the amount of mastery required to be an expert in the language.\n\nFurther blurring this line, many domain-specific languages have exposed APIs, and can be accessed from other programming languages without breaking the flow of execution or calling a separate process, and can thus operate as programming libraries.\n\nProgramming tools[edit]\nSome domain-specific languages expand over time to include full-featured programming tools, which further complicates the question of whether a language is domain-specific or not. A good example is the functional language XSLT, specifically designed for transforming one XML graph into another, which has been extended since its inception to allow (particularly in its 2.0 version) for various forms of filesystem interaction, string and date manipulation, and data typing.\n\nIn model-driven engineering many examples of domain-specific languages may be found like OCL, a language for decorating models with assertions or QVT, a domain-specific transformation language. However languages like UML are typically general purpose modeling languages.\n\nTo summarize, an analogy might be useful: a Very Little Language is like a knife, which can be used in thousands of different ways, from cutting food to cutting down trees. A domain-specific language is like an electric drill: it is a powerful tool with a wide variety of uses, but a specific context, namely, putting holes in things. A General Purpose Language is a complete workbench, with a variety of tools intended for performing a variety of tasks. Domain-specific languages should be used by programmers who, looking at their current workbench, realize they need a better drill, and find that a particular domain-specific language provides exactly that.\n\nDomain-specific language topics[edit]\nUsage patterns[edit]\nThere are several usage patterns for domain-specific languages:[1][2]\n\nprocessing with standalone tools, invoked via direct user operation, often on the command line or from a Makefile (e.g., grep for regular expression matching, sed, lex, yacc, the GraphViz tool set, etc.)\ndomain-specific languages which are implemented using programming language macro systems, and which are converted or expanded into a host general purpose language at compile-time or read-time\nembedded (or internal) domain-specific languages, implemented as libraries which exploit the syntax of their host general purpose language or a subset thereof, while adding domain-specific language elements (data types, routines, methods, macros etc.). (e.g. Embedded SQL, LINQ)\ndomain-specific languages which are called (at runtime) from programs written in general purpose languages like C or Perl, to perform a specific function, often returning the results of operation to the \"host\" programming language for further processing; generally, an interpreter or virtual machine for the domain-specific language is embedded into the host application (e.g. format strings, a regular expression engine)\ndomain-specific languages which are embedded into user applications (e.g., macro languages within spreadsheets) and which are (1) used to execute code that is written by users of the application, (2) dynamically generated by the application, or (3) both.\nMany domain-specific languages can be used in more than one way.[citation needed] DSL code embedded in a host language may have special syntax support, such as regexes in sed, AWK, Perl or JavaScript, or may be passed as strings.\n\nDesign goals[edit]\nAdopting a domain-specific language approach to software engineering involves both risks and opportunities. The well-designed domain-specific language manages to find the proper balance between these.\n\nDomain-specific languages have important design goals that contrast with those of general-purpose languages:\n\ndomain-specific languages are less comprehensive.\ndomain-specific languages are much more expressive in their domain.\ndomain-specific languages should exhibit minimal redundancy.\nIdioms[edit]\nIn programming, idioms are methods imposed by programmers to handle common development tasks, e.g.:\n\nEnsure data is saved before the window is closed.\nEdit code whenever command-line parameters change because they affect program behavior.\nGeneral purpose programming languages rarely support such idioms, but domain-specific languages can describe them, e.g.:\n\nA script can automatically save data.\nA domain-specific language can parameterize command line input.\nExamples[edit]\nExamples of domain-specific languages include HTML, Logo for pencil-like drawing, Verilog and VHDL hardware description languages, MATLAB and GNU Octave for matrix programming, Mathematica, Maple and Maxima for symbolic mathematics, Specification and Description Language for reactive and distributed systems, spreadsheet formulas and macros, SQL for relational database queries, YACC grammars for creating parsers, regular expressions for specifying lexers, the Generic Eclipse Modeling System for creating diagramming languages, Csound for sound and music synthesis, and the input languages of GraphViz and GrGen, software packages used for graph layout and graph rewriting.\n\nGame Maker Language[edit]\nThe GML scripting language used by GameMaker: Studio is a domain-specific language targeted at novice programmers to easily be able to learn programming. While the language serves as a blend of multiple languages including Delphi, C++, and BASIC, there is a lack of structures, data types, and other features of a full-fledged programming language. Many of the built-in functions are sandboxed for the purpose of easy portability. The language primarily serves to make it easy for anyone to pick up the language and develop a game.\n\nUnix shell scripts[edit]\nUnix shell scripts give a good example of a domain-specific language for data[3] organization. They can manipulate data in files or user input in many different ways. Domain abstractions and notations include streams (such as stdin and stdout) and operations on streams (such as redirection and pipe). These abstractions combine to make a robust language to talk about the flow and organization of data.\n\nThe language consists of a simple interface (a script) for running and controlling processes that perform small tasks. These tasks represent the idioms of organizing data into a desired format such as tables, graphs, charts, etc.\n\nThese tasks consist of simple control-flow and string manipulation mechanisms that cover a lot of common usages like searching and replacing string in files, or counting occurrences of strings (frequency counting).\n\nEven though Unix scripting languages are Turing complete, they differ from general purpose languages.[clarification needed]\n\nIn practice, scripting languages are used to weave together small Unix tools such as AWK (e.g., gawk), ls, sort or wc.\n\nColdFusion Markup Language[edit]\nColdFusion's associated scripting language is another example of a domain-specific language for data-driven websites. This scripting language is used to weave together languages and services such as Java, .NET, C++, SMS, email, email servers, http, ftp, exchange, directory services, and file systems for use in websites.\n\nThe ColdFusion Markup Language (CFML) includes a set of tags that can be used in ColdFusion pages to interact with data sources, manipulate data, and display output. CFML tag syntax is similar to HTML element syntax.\n\nErlang OTP[edit]\nThe Erlang Open Telecom Platform was originally designed for use inside Ericsson as a domain-specific language. The language itself offers a platform of libraries to create finite state machines, generic servers and event managers that quickly allow an engineer to deploy applications, or support libraries, that have been shown in industry benchmarks to outperform other languages intended for a mixed set of domains, such as C and C++. The language is now officially open source and can be downloaded from their website.\n\nFilterMeister[edit]\nFilterMeister is a programming environment, with a programming language that is based on C, for the specific purpose of creating Photoshop-compatible image processing filter plug-ins; FilterMeister runs as a Photoshop plug-in itself and it can load and execute scripts or compile and export them as independent plug-ins. Although the FilterMeister language reproduces a significant portion of the C language and function library, it contains only those features which can be used within the context of Photoshop plug-ins and adds a number of specific features only useful in this specific domain.\n\nMediaWiki templates[edit]\nThe Template feature of MediaWiki is an embedded domain-specific language whose fundamental purpose is to support the creation of page templates and the transclusion (inclusion by reference) of MediaWiki pages into other MediaWiki pages.\n\nA detailed description of that domain-specific language can be found at the corresponding article at the Wikimedia Foundation's Meta-Wiki.\n\nSoftware engineering uses[edit]\nThere has been much interest in domain-specific languages to improve the productivity and quality of software engineering. Domain-specific language could possibly provide a robust set of tools for efficient software engineering. Such tools are beginning to make their way into development of critical software systems.\n\nThe Software Cost Reduction Toolkit[4] is an example of this. The toolkit is a suite of utilities including a specification editor to create a requirements specification, a dependency graph browser to display variable dependencies, a consistency checker to catch missing cases in well-formed formulas in the specification, a model checker and a theorem prover to check program properties against the specification, and an invariant generator that automatically constructs invariants based on the requirements.\n\nA newer development is language-oriented programming, an integrated software engineering methodology based mainly on creating, optimizing, and using domain-specific languages.\n\nMetacompilers[edit]\nFor more details on this topic, see Metacompiler.\nComplementing language-oriented programming, as well as all other forms of domain-specific languages, are the class of compiler writing tools called metacompilers. A metacompiler is not only useful for generating parsers and code generators for domain-specific languages, but a metacompiler itself compiles a domain-specific metalanguage specifically designed for the domain of metaprogramming.\n\nBesides parsing domain-specific languages, metacompilers are useful for generating a wide range of software engineering and analysis tools. The meta-compiler methodology is often found in program transformation systems.\n\nMetacompilers that played a significant role in both computer science and the computer industry include Meta-II[5] and its descendent TreeMeta.[6]\n\nUnreal Engine before version 4 and other games[edit]\nUnreal and Unreal Tournament unveiled a language called UnrealScript. This allowed for rapid development of modifications compared to the competitor Quake (using the Id Tech 2 engine). The Id Tech engine used standard C code meaning C had to be learned and properly applied, while UnrealScript was optimized for ease of use and efficiency. Similarly, the development of more recent games introduced their own specific languages, one more common example is Lua for scripting.\n\nRules Engines for Policy Automation[edit]\nVarious Business Rules Engines have been developed for automating policy and business rules used in both government and private industry. ILOG, Oracle Policy Automation, DTRules, Drools and others provide support for DSLs aimed to support various problem domains. DTRules goes so far as to define an interface for the use of multiple DSLs within a Rule Set.\n\nThe purpose of Business Rules Engines is to define a representation of business logic in as human readable fashion as possible. This allows both subject matter experts and developers to work with and understand the same representation of the business logic. Most Rules Engines provide both an approach to simplifying the control structures for business logic (for example, using Declarative Rules or Decision Tables) coupled with alternatives to programming syntax in favor of DSLs.\n\nStatistical modelling languages[edit]\nStatistical modellers have developed domain-specific languages such as Bugs, Jags, and Stan. These languages provide a syntax for describing a Bayesian model, and generate a method for solving it using simulation.\n\nGenerate model and services to multiple programming Languages[edit]\nGenerate object handling and services based on a Interface Description Language for a domain specific language such as JavaScript for web applications, HTML for documentation, C++ for high performance code, etc. This is done by cross language frameworks such as Apache Thrift or Google Protocol Buffers.\n\nOther examples[edit]\nOther prominent examples of domain-specific languages include:\n\nEmacs Lisp\nGame Description Language\nOpenGL Shading Language\nAdvantages and disadvantages[edit]\nSome of the advantages:[1][2]\n\nDomain-specific languages allow solutions to be expressed in the idiom and at the level of abstraction of the problem domain. The idea is domain experts themselves may understand, validate, modify, and often even develop domain-specific language programs. However, this is seldom the case.[7]\nDomain-specific languages allow validation at the domain level. As long as the language constructs are safe any sentence written with them can be considered safe.[citation needed]\nDomain-specific languages can help to shift the development of business information systems from traditional software developers to the typically larger group of domain-experts who (despite having less technical expertise) have deeper knowledge of the domain.[8]\nSome of the disadvantages:\n\nCost of learning a new language vs. its limited applicability\nCost of designing, implementing, and maintaining a domain-specific language as well as the tools required to develop with it (IDE)\nFinding, setting, and maintaining proper scope.\nDifficulty of balancing trade-offs between domain-specificity and general-purpose programming language constructs.\nPotential loss of processor efficiency compared with hand-coded software.\nProliferation of similar non-standard domain-specific languages, for example, a DSL used within one insurance company versus a DSL used within another insurance company.[9]\nNon-technical domain experts can find it hard to write or modify DSL programs by themselves.[7]\nIncreased difficulty of integrating the DSL with other components of the IT system (as compared to integrating with a general-purpose language).\nLow supply of experts in a particular DSL tends to raise labor costs.\nHarder to find code examples.\nSee also[edit]\n\nThis \"see also\" section may contain an excessive number of suggestions. Please ensure that only the most relevant links are given, that they are not red links, and that any links are not already in this article. (December 2013) (Learn how and when to remove this template message)\nArchitecture description language\nCognitive dimensions of notations\nCombinator library\nDomain analysis\nDomain-specific entertainment language\nDomain-specific modeling\nDomain-specific multimodeling\nConfiguration file\nFluent interface\nMetacompiler\nMetalinguistic abstraction\nMetamodeling\nModel-driven engineering\nMulti-paradigm programming language\nProgramming domain\nProgramming paradigm\nSpecification and Description Language\nProbabilistic programming language (PPL)",
          "subparadigms": []
        },
        {
          "pdid": 39,
          "name": "Dialect",
          "details": "A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly, as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC programming language has many dialects.\n\nThe explosion of Forth dialects led to the saying \"If you've seen one Forth... you've seen one Forth.\"\n\nSee also[edit]\nDomain-specific language\nDomain-specific modelling\nExtensible programming\nLanguage oriented programming\nList of BASIC dialects\nModeling language\nScripting language\nReflection\nMetaprogramming\nCategory:Extensible syntax programming languages\nRebol § Dialects\nRuby (programming language) § Metaprogramming\n",
          "subparadigms": []
        },
        {
          "pdid": 40,
          "name": "Grammar-oriented",
          "details": "Grammar-oriented programming (GOP) and Grammar-oriented Object Design (GOOD) are good for designing and creating a domain-specific programming language (DSL) for a specific business domain.\n\nGOOD can be used to drive the execution of the application or it can be used to embed the declarative processing logic of a context-aware component (CAC) or context-aware service (CAS). GOOD is a method for creating and maintaining dynamically reconfigurable software architectures driven by business-process architectures. The business compiler was used to capture business processes within real-time workshops for various lines of business and create an executable simulation of the processes used.\n\nInstead of using one DSL for the entire programming activity, GOOD suggests the combination of defining domain-specific behavioral semantics in conjunction with the use of more traditional, general purpose programming languages.\n\nSee also[edit]\nAdaptive grammar\nExtensible programming\nLanguage-oriented programming\nDialecting\nTransformation language",
          "subparadigms": [
            39
          ]
        },
        {
          "pdid": 41,
          "name": "Intentional",
          "details": "In computer programming, Intentional Programming is a programming paradigm developed by Charles Simonyi that encodes in software source code the precise intention which programmers (or users) have in mind when conceiving their work. By using the appropriate level of abstraction at which the programmer is thinking, creating and maintaining computer programs become easier. By separating the concerns for intentions and how they are being operated upon, the software becomes more modular and allows for more reusable software code.\n\nIntentional Programming was developed by former Microsoft chief architect Charles Simonyi, who led a team in Microsoft Research, which developed the paradigm and built an integrated development environment (IDE) called IP (for Intentional Programming) that demonstrated the paradigm. Microsoft decided not to productize the Intentional Programming paradigm, as in the early 2000s Microsoft was rolling out C# and .NET to counter Java adoption.[1] Charles Simonyi decided, with approval of Microsoft, to take his idea out from Microsoft and commercialize it himself. He founded the company Intentional Software to pursue this. Microsoft licensed the Intentional Programming patents Simonyi had acquired while at Microsoft, but no source code, to Intentional Software.\n\nAn overview of Intentional Programming as it was developed at Microsoft Research is given in Chapter 11 of the book Generative Programming: Methods, Tools, and Applications.[2]\n\nContents  [hide] \n1\tDevelopment cycle\n2\tSeparating source code storage and presentation\n3\tProgramming Example\n3.1\tIdentity\n3.2\tLevels of detail\n4\tSimilar works\n5\tSee also\n6\tReferences\n7\tExternal links\nDevelopment cycle[edit]\nAs envisioned by Simonyi, developing a new application via the Intentional Programming paradigm proceeds as follows. A programmer builds a WYSIWYG-like environment supporting the schema and notation of business knowledge for a given problem domain (such as productivity applications or life insurance). Users then use this environment to capture their intentions, which are recorded at high level of abstraction. The environment can operate on these intentions and assist the user to create semantically richer documents that can be processed and executed, similar to a spreadsheet. The recorded knowledge is executed by an evaluator or is compiled to generate the final program. Successive changes are done at the WYSIWYG level only. As opposed to word processors, spreadsheets or presentation software, an Intentional environment has more support for structure and semantics of the intentions to be expressed, and can create interactive documents that capture more richly what the user is trying to accomplish. A special case is when the content is program code, and the environment becomes an intelligent IDE.[3]\n\nSeparating source code storage and presentation[edit]\nKey to the benefits of Intentional Programming is that domain code which capture the intentions are not stored in source code text files, but in a tree-based storage (could be binary or XML). Tight integration of the environment with the storage format brings some of the nicer features of database normalization to source code. Redundancy is eliminated by giving each definition a unique identity, and storing the name of variables and operators in exactly one place. This makes it easier to intrinsically distinguish declarations from references, and the environment can show them differently.\n\nWhitespace in a program is also not stored as part of the source code, and each programmer working on a project can choose an indentation display of the source. More radical visualizations include showing statement lists as nested boxes, editing conditional expressions as logic gates, or re-rendering names in Chinese.\n\nThe system uses a normalized language for popular languages like C++ and Java, while letting users of the environment mix and match these with ideas from Eiffel and other languages. Often mentioned in the same context as language-oriented programming via domain-specific languages, and aspect-oriented programming, IP purports to provide some breakthroughs in generative programming. These techniques allow developers to extend the language environment to capture domain-specific constructs without investing in writing a full compiler and editor for any new languages.\n\nProgramming Example[edit]\nA Java program that writes out the numbers from 1 to 10, using a curly bracket syntax, might look like this:\n\n for (int i = 1; i <= 10; i++) {\n    System.out.println(\"the number is \" + i);\n }\nThe code above contains a common construct of most programming languages, the bounded loop, in this case represented by the for construct. The code, when compiled, linked and run, will loop 10 times, incrementing the value of i each time after printing it out.\n\nBut this code does not capture the intentions of the programmer, namely to \"print the numbers 1 to 10\". In this simple case, a programmer asked to maintain the code could likely figure out what it is intended to do, but it is not always so easy. Loops that extend across many lines, or pages, can become very difficult to understand, notably if the original programmer uses unclear labels. Traditionally the only way to indicate the intention of the code was to add source code comments, but often comments are not added, or are unclear, or drift out of sync with the source code they originally described.\n\nIn intentional programming systems the above loop could be represented, at some level, as something as obvious as \"print the numbers 1 to 10\". The system would then use the intentions to generate source code, likely something very similar to the code above. The key difference is that the intentional programming systems maintain the semantic level, which the source code lacks, and which can dramatically ease readability in larger programs.\n\nAlthough most languages contain mechanisms for capturing certain kinds of abstraction, IP, like the Lisp family of languages, allows for the addition of entirely new mechanisms. Thus, if a developer started with a language like C, they would be able to extend the language with features such as those in C++ without waiting for the compiler developers to add them. By analogy, many more powerful expression mechanisms could be used by programmers than mere classes and procedures.\n\nIdentity[edit]\nIP focuses on the concept of identity. Since most programming languages represent the source code as plain text, objects are defined by names, and their uniqueness has to be inferred by the compiler. For example, the same symbolic name may be used to name different variables, procedures, or even types. In code that spans several pages – or, for globally visible names, multiple files – it can become very difficult to tell what symbol refers to what actual object. If a name is changed, the code where it is used must carefully be examined.\n\nBy contrast, in an IP system, all definitions not only assign symbolic names, but also unique private identifiers to objects. This means that in the IP development environment, every reference to a variable or procedure is not just a name – it is a link to the original entity.\n\nThe major advantage of this is that if an entity is renamed, all of the references to it in the program remain valid (known as referential integrity). This also means that if the same name is used for unique definitions in different namespaces (such as \".to_string()\"), references with the same name but different identity will not be renamed, as sometimes happens with search/replace in current editors. This feature also makes it easy to have multi-language versions of the program; it can have a set of English-language names for all the definitions as well as a set of Japanese-language names which can be swapped in at will.\n\nHaving a unique identity for every defined object in the program also makes it easy to perform automated refactoring tasks, as well as simplifying code check-ins in versioning systems. For example, in many current code collaboration systems (e.g. CVS), when two programmers commit changes that conflict (i.e. if one programmer renames a function while another changes one of the lines in that function), the versioning system will think that one programmer created a new function while another modified an old function. In an IP versioning system, it will know that one programmer merely changed a name while another changed the code.\n\nLevels of detail[edit]\nIP systems also offer several levels of detail, allowing the programmer to \"zoom in\" or out. In the example above, the programmer could zoom out to get a level that would say something like:\n\n<<print the numbers 1 to 10>>\nThus IP systems are self-documenting to a large degree, allowing the programmer to keep a good high-level picture of the program as a whole.\n\nSimilar works[edit]\nThere are projects that exploit similar ideas to create code with higher level of abstraction. Among them are:\n\nConcept programming\nLanguage-oriented programming (LOP)\nDomain-specific language (DSL)\nProgram transformation\nSemantic-oriented programming (SOP)\nLiterate programming\nModel-driven architecture (MDA)\nSoftware factory\nMetaprogramming\nLisp (programming language)\nSee also[edit]\nProgramming paradigm\nAutomatic programming\nObject database\nProgramming by demonstration\nArtefaktur\nAbstract syntax tree\nSemantic resolution tree\nStructure editor",
          "subparadigms": []
        },
        {
          "pdid": 42,
          "name": "Language-oriented",
          "details": "Language-oriented programming (LOP) is a style of computer programming in which, rather than solving problems in general-purpose programming languages, the programmer creates one or more domain-specific languages for the problem first, and solves the problem in those languages. This concept is described in detail in the paper by Martin Ward entitled \"Language Oriented Programming\",[1] published in Software - Concepts and Tools, Vol.15, No.4, pp 147-161, 1994,[2] and in the article by Sergey Dmitriev entitled \"Language Oriented Programming: The Next Programming Paradigm\".[3]\n\nContents  [hide] \n1\tConcept\n2\tSee also\n3\tReferences\n4\tExternal links\nConcept[edit]\nThe concept of language-oriented programming takes the approach to capture requirements in the user's terms, and then to try to create an implementation language as isomorphic as possible to the user's descriptions, so that the mapping between requirements and implementation is as direct as possible. A measure of the closeness of this isomorphism is the \"redundancy\" of the language, defined as the number of editing operations needed to implement a stand-alone change in requirements. It is not assumed a-priori what is the best language for implementing the new language. Rather, the developer can choose among options created by analysis of the information flows — what information is acquired, what its structure is, when it is acquired, from whom, and what is done with it.[4]\n\nSee also[edit]\nGrammar-oriented programming\nDialecting\nDomain-specific language\nExtensible programming\nHomoiconicity\nReferences[edit]\nJump up ^ M.P. Ward. \"Language Oriented Programing\" (PDF). Cse.dmu.ac.uk. Retrieved 24 February 2015.\nJump up ^ \"dblp: Software - Concepts and Tools, Volume 15\". Informatik.uni-trier.de. Retrieved 2015-02-24.\nJump up ^ \"JetBrains onBoard Online Magazine :: Fabrique - a code generator\". Onboard.jetbrains.com. Retrieved 2015-02-24.\nJump up ^ Dunlavey (1994). Building Better Applications: a Theory of Efficient Software Development. International Thomson Publishing. ISBN 0-442-01740-5.",
          "subparadigms": [
            36,
            38,
            40,
            41
          ]
        },
        {
          "pdid": 43,
          "name": "Automatic",
          "details": "In computer science, the term automatic programming[1] identifies a type of computer programming in which some mechanism generates a computer program to allow human programmers to write the code at a higher abstraction level.\n\nThere has been little agreement on the precise definition of automatic programming, mostly because its meaning has changed over time. David Parnas, tracing the history of \"automatic programming\" in published research, noted that in the 1940s it described automation of the manual process of punching paper tape. Later it referred to translation of high-level programming languages like Fortran and ALGOL. In fact, one of the earliest programs identifiable as a compiler was called Autocode. Parnas concluded that \"automatic programming has always been a euphemism for programming in a higher-level language than was then available to the programmer.\"[2]\n\nProgram synthesis is one type of automatic programming where a procedure is created from scratch, based on mathematical requirements.\n\nContents  [hide] \n1\tOrigin\n2\tGenerative programming\n3\tSource code generation\n3.1\tImplementations\n4\tSee also\n5\tReferences\n6\tExternal links\nOrigin[edit]\nMildred Koss, an early UNIVAC programmer, explains: \"Writing machine code involved several tedious steps—breaking down a process into discrete instructions, assigning specific memory locations to all the commands, and managing the I/O buffers. After following these steps to implement mathematical routines, a sub-routine library, and sorting programs, our task was to look at the larger programming process. We needed to understand how we might reuse tested code and have the machine help in programming. As we programmed, we examined the process and tried to think of ways to abstract these steps to incorporate them into higher-level language. This led to the development of interpreters, assemblers, compilers, and generators—programs designed to operate on or produce other programs, that is, automatic programming.\"[3]\n\nGenerative programming[edit]\nGenerative programming is a style of computer programming that uses automated source code creation through generic frames, classes, prototypes, templates, aspects, and code generators to improve programmer productivity.[4] It is often related to code-reuse topics such as component-based software engineering and product family engineering.\n\nSource code generation[edit]\nSource code generation is the process of generating source code based on an ontological model such as a template and is accomplished with a programming tool such as a template processor or an integrated development environment (IDE). These tools allow the generation of source code through any of various means. A macro processor, such as the C preprocessor, which replaces patterns in source code according to relatively simple rules, is a simple form of source code generator.\n\nImplementations[edit]\n\nThis section contains content that is written like an advertisement. Please help improve it by removing promotional content and inappropriate external links, and by adding encyclopedic content written from a neutral point of view. (October 2010) (Learn how and when to remove this template message)\nSome IDEs for Java and other languages have more advanced forms of source code generation, with which the programmer can interactively select and customize \"snippets\" of source code. Program \"wizards\", which allow the programmer to design graphical user interfaces interactively while the compiler invisibly generates the corresponding source code, are another common form of source code generation. This may be contrasted with, for example, user interface markup languages, which define user interfaces declaratively.\n\nBesides the generation of code from a wizard or template, IDEs can also generate and manipulate code to automate code refactorings that would require multiple (error prone) manual steps, thereby improving developer productivity.[5] Examples of such features in IDEs are the refactoring class browsers for Smalltalk and those found in Java IDEs like Eclipse.\n\nA specialized alternative involves the generation of optimized code for quantities defined mathematically within a Computer algebra system (CAS). Compiler optimization consisting of finding common intermediates of a vector of size {\\displaystyle n} n requires a complexity of {\\displaystyle O(n^{2})} O(n^{2}) or {\\displaystyle O(n^{3})} O(n^{3}) operations whereas the very design of a computer algebra system requires only {\\displaystyle O(n)} O(n) operations.[6][7][8] These facilities can be used as pre-optimizer before processing by the compiler. This option has been used for handling mathematically large expressions in e.g. computational (quantum) chemistry.\n\nExamples:\n\nAcceleo is an open source code generator for Eclipse used to generate any textual language (Java, PHP, Python, etc.) from EMF models defined from any metamodel (UML, SysML, etc.).\nActifsource is a plugin for Eclipse that allows graphical modelling and model-based code generation using custom templates.\nAltova MapForce is a graphical data mapping, conversion, and integration tool capable of generating application code in Java, C#, or C++ for executing recurrent transformations.\nCodeFluent Entities from SoftFluent is a graphical tool integrated into Microsoft Visual Studio that generates .NET source code, in C# or Visual Basic.\nDMS Software Reengineering Toolkit is a system for defining arbitrary domain specific languages and translating them to other languages.\nHPRCARCHITECT (from MNB Technologies, Inc) is an artificial intelligence-based software development tool with a Virtual Whiteboard human interface. Language and technology agnostic, the tool's development was funded by the US Air Force to solve the problem of code generation for systems targeting mixed processor technologies.\nSpring Roo is an open source active code generator for Spring Framework based Java applications. It uses AspectJ mixins to provide separation of concerns during round-trip maintenance.\nRISE is a free information modeling suite for system development using ERD or UML. Database code generation for MySQL, PostgreSQL and Microsoft SQL Server. Persistence code generation for C# (.NET) and PHP including both SOAP and JSON style web services and AJAX proxy code.\nThe Maple computer algebra system offers code generators optimizers with Fortran, MATLAB, C and Java. Wolfram Language (Mathematica), and MuPAD have comparable interfaces.\nScreen Sculptor,[9] SoftCode,[10] UI Programmer,[11] and Genifer[12] are examples of pioneering program generators that arose during the mid-1980s through the early 1990s. They developed and advanced the technology of extendable, template based source code generators on a mass market scale.\nGeneXus is a Cross-Platform, knowledge representation-based, development tool, mainly oriented to enterprise-class applications for Web applications, smart devices and the Microsoft Windows platform. A developer describes an application in a high-level, mostly declarative language, from which native code is generated for multiple environments.\nBidji is an Apache Ant project for code generation and data transformation.\nSee also[edit]\n\nThis \"see also\" section may contain an excessive number of suggestions. Please ensure that only the most relevant links are given, that they are not red links, and that any links are not already in this article. (June 2013) (Learn how and when to remove this template message)\nComparison of code generation tools\nSource-to-source compiler\nModel Driven Engineering (MDE)\nModel Driven Architecture (MDA)\nDomain-Specific Modeling (DSM)\nFeature Oriented Programming\nProgram synthesis\nProgram transformation\nInductive programming\nModeling language\nData transformation\nSemantic translation\nVocabulary-based transformation\nMetaprogramming\nLanguage-oriented programming",
          "subparadigms": []
        },
        {
          "pdid": 44,
          "name": "Attribute-oriented",
          "details": "Attribute-oriented programming (AOP) is a program-level marking technique. Programmers can mark program elements (e.g. classes and methods) to indicate that they maintain application-specific or domain-specific semantics. For example, some programmers may define a \"logging\" attribute and associate it with a method to indicate the method should implement a logging function, while other programmers may define a \"web service\" attribute and associate it with a class to indicate the class should be implemented as a web service. Attributes separate application's core logic (or business logic) from application-specific or domain-specific semantics (e.g. logging and web service functions). By hiding the implementation details of those semantics from program code, attributes increase the level of programming abstraction and reduce programming complexity, resulting in simpler and more readable programs. The program elements associated with attributes are transformed to more detailed programs by a supporting tool (e.g. preprocessor). For example, a preprocessor may insert a logging program into the methods associated with a \"logging\" attribute.\n\nContents  [hide] \n1\tAttribute-oriented programming in various languages\n1.1\tJava\n1.2\tC#\n1.3\tUML\n2\tReferences\n3\tTools\n4\tExternal links\nAttribute-oriented programming in various languages[edit]\nJava[edit]\nWith the inclusion of The Metadata Facility for the Java Programming Language (JSR-175) into the J2SE 5.0 release it is possible to utilize attribute-oriented programming right out of the box. XDoclet library makes it possible to use attribute-oriented programming approach in earlier versions of Java.\n\nC#[edit]\nThe C# language has supported attributes from its very first release. However these attributes are used to give run-time information and are not used by a pre-processor (there isn't one in C#'s reference implementation).\n\nUML[edit]\nThe Unified Modeling Language (UML) supports a kind of attribute named Stereotypes.\n\nReferences[edit]\n\nThis article includes a list of references, related reading or external links, but its sources remain unclear because it lacks inline citations. Please help to improve this article by introducing more precise citations. (August 2009) (Learn how and when to remove this template message)\n\"Attribute-Oriented Programming\". An Introduction to Attribute-Oriented Programming. Archived from the original on May 26, 2005. Retrieved July 22, 2005.\nWada, Hiroshi; Suzuki, Junichi (2005). \"Modeling Turnpike Frontend System: a Model-Driven Development Framework Leveraging UML Metamodeling and Attribute-Oriented Programming\" (PDF). In Proc. of the 8th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MoDELS/UML 2005).\nRouvoy, Romain; Merle, Philippe (2006). \"Leveraging Component-Oriented Programming with Attribute-Oriented Programming\" (PDF). In Proc. of the 11th ECOOP International Workshop on Component-Oriented Programming (WCOP 2006). Archived from the original (PDF) on 2006-12-23.\nTools[edit]\nAnnotation Processing Tool (apt)\nSpoon, an Annotation-Driven Java Program Transformer\nXDoclet, a Javadoc-Driven Program Generator",
          "subparadigms": []
        },
        {
          "pdid": 45,
          "name": "Reflection",
          "details": "In computer science, reflection is the ability of a computer program to examine, introspect, and modify its own structure and behavior at runtime.[1]\n\nContents  [hide] \n1\tHistorical background\n2\tUses\n3\tImplementation\n4\tExamples\n4.1\teC\n4.2\tECMAScript\n4.3\tGo\n4.4\tJava\n4.5\tObjective-C\n4.6\tDelphi\n4.7\tPerl\n4.8\tPHP\n4.9\tPython\n4.10\tR\n4.11\tRuby\n5\tSee also\n6\tReferences\n7\tFurther reading\n8\tExternal links\nHistorical background[edit]\nThe earliest computers were programmed in their native assembly language, which were inherently reflective as these original architectures could be programmed by defining instructions as data and using self-modifying code. As programming moved to compiled higher-level languages such as Fortran, Algol and Cobol (but also Pascal and C and many other languages) this reflective ability largely disappeared until programming languages with reflection built into their type systems appeared.[citation needed]\n\nBrian Cantwell Smith's 1982 doctoral dissertation[2][3] introduced the notion of computational reflection in programming languages, and the notion of the meta-circular interpreter as a component of 3-Lisp.\n\nUses[edit]\nReflection can be used for observing and modifying program execution at runtime. A reflection-oriented program component can monitor the execution of an enclosure of code and can modify itself according to a desired goal related to that enclosure. This is typically accomplished by dynamically assigning program code at runtime.\n\nIn object-oriented programming languages such as Java, reflection allows inspection of classes, interfaces, fields and methods at runtime without knowing the names of the interfaces, fields, methods at compile time. It also allows instantiation of new objects and invocation of methods.\n\nReflection can be used to adapt a given program to different situations dynamically. Reflection-oriented programming almost always requires additional knowledge, framework, relational mapping, and object relevance in order to take advantage of more generic code execution.\n\nReflection is often used as part of software testing, such as for the runtime creation/instantiation of mock objects.\n\nReflection is also a key strategy for metaprogramming.\n\nIn some object-oriented programming languages, such as C# and Java, reflection can be used to override member accessibility rules. For example, reflection makes it possible to change the value of a field marked \"private\" in a third-party library's class.\n\nImplementation[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2008) (Learn how and when to remove this template message)\nA language supporting reflection provides a number of features available at runtime that would otherwise be difficult to accomplish in a lower-level language. Some of these features are the abilities to:\n\nDiscover and modify source code constructions (such as code blocks, classes, methods, protocols, etc.) as a first-class object at runtime.\nConvert a string matching the symbolic name of a class or function into a reference to or invocation of that class or function.\nEvaluate a string as if it were a source code statement at runtime.\nCreate a new interpreter for the language's bytecode to give a new meaning or purpose for a programming construct.\nThese features can be implemented in different ways. In MOO, reflection forms a natural part of everyday programming idiom. When verbs (methods) are called, various variables such as verb (the name of the verb being called) and this (the object on which the verb is called) are populated to give the context of the call. Security is typically managed by accessing the caller stack programmatically: Since callers() is a list of the methods by which the current verb was eventually called, performing tests on callers()[1] (the command invoked by the original user) allows the verb to protect itself against unauthorised use.\n\nCompiled languages rely on their runtime system to provide information about the source code. A compiled Objective-C executable, for example, records the names of all methods in a block of the executable, providing a table to correspond these with the underlying methods (or selectors for these methods) compiled into the program. In a compiled language that supports runtime creation of functions, such as Common Lisp, the runtime environment must include a compiler or an interpreter.\n\nReflection can be implemented for languages not having built-in reflection facilities by using a program transformation system to define automated source code changes.\n\nExamples[edit]\nThe following code snippets create an instance foo of class Foo, and invoke its method hello. For each programming language, normal and reflection-based call sequences are shown.\n\n\nIt has been suggested that this article be split into a new article titled Comparison of programming languages (reflection). (Discuss.) (May 2016)\neC[edit]\nThe following is an example in eC:\n\n// without reflection\nFoo foo { };\nfoo.hello();\n\n// with reflection\nClass fooClass = eSystem_FindClass(__thisModule, \"Foo\");\nInstance foo = eInstance_New(fooClass);\nMethod m = eClass_FindMethod(fooClass, \"hello\", fooClass.module);\n((void (*)())(void *)m.function)(foo);\nECMAScript[edit]\nThe following is an example in ECMAScript, and therefore also applies to JavaScript and ActionScript:\n\n// Without reflection\nnew Foo().hello()\n\n// With reflection\n\n// assuming that Foo resides in this\nnew this['Foo']()['hello']()\n\n// or without assumption\nnew (eval('Foo'))()['hello']()\n\n// or simply\neval('new Foo().hello()')\n\n// Using ECMAScript 2015's new Reflect class:\nReflect.construct(Foo, [])['hello']()\nGo[edit]\nThe following is an example in Go:\n\nimport \"reflect\"\n\n// without reflection\nf := Foo{}\nf.Hello()\n\n// with reflection\nfT := reflect.TypeOf(Foo{})\nfV := reflect.New(fT)\n\nm := fV.MethodByName(\"Hello\")\nif m.IsValid() {\n    m.Call(nil)\n}\nJava[edit]\nThe following is an example in Java:\n\n// without reflection\nFoo foo = new Foo();\nfoo.hello();\n\n// with reflection\nObject foo = Class.forName(\"complete.classpath.and.Foo\").newInstance();\n// Alternatively: Object foo = Foo.class.newInstance();\nMethod m = foo.getClass().getDeclaredMethod(\"hello\", new Class<?>[0]);\nm.invoke(foo);\nObjective-C[edit]\nThe following is an example in Objective-C—implying either the OpenStep or Foundation Kit framework is used:\n\n// Foo class.\n@interface Foo : NSObject\n- (void)hello;\n@end\n\n// Sending \"hello\" to a Foo instance without reflection.\nFoo *obj = [[Foo alloc] init];\n[obj hello];\n\n// Sending \"hello\" to a Foo instance with reflection.\nid obj = [[NSClassFromString(@\"Foo\") alloc] init];\n[obj performSelector: @selector(hello)];\nDelphi[edit]\nThis Delphi example assumes a TFoo class has been declared in a unit called Unit1:\n\nuses RTTI, Unit1;\n\nprocedure WithoutReflection;\nvar\n  Foo: TFoo;\nbegin\n  Foo := TFoo.Create;\n  try\n    Foo.Hello;\n  finally\n    Foo.Free;\n  end;\nend;\n\nprocedure WithReflection;\nvar\n  RttiContext: TRttiContext;\n  RttiType: TRttiInstanceType;\n  Foo: TObject;\nbegin\n  RttiType := RttiContext.FindType('Unit1.TFoo') as TRttiInstanceType;\n  Foo := RttiType.GetMethod('Create').Invoke(RttiType.MetaclassType, []).AsObject;\n  try\n    RttiType.GetMethod('Hello').Invoke(Foo, []);\n  finally\n    Foo.Free;\n  end;\nend;\nThis is a notable example since Delphi is an unmanaged, fully natively compiled language, unlike most other languages that support reflection. Its language architecture inherits from strongly-typed Pascal, but with significant influence from SmallTalk. Compare with the other examples here, many of which are dynamic or script languages like Perl, Python or PHP, or languages with a runtime like Java or C#.\n\nPerl[edit]\nThe following is an example in Perl:\n\n# without reflection\nmy $foo = Foo->new;\n$foo->hello;\n\n# or\nFoo->new->hello;\n\n# with reflection\nmy $class = \"Foo\"\nmy $constructor = \"new\";\nmy $method = \"hello\";\n\nmy $f = $class->$constructor;\n$f->$method;\n\n# or\n$class->$constructor->$method;\n\n# with eval\neval \"new Foo->hello;\";\nPHP[edit]\nThe following is an example in PHP:\n\n// without reflection\n$foo = new Foo();\n$foo->hello();\n\n// with reflection\n$reflector = new ReflectionClass('Foo');\n$foo = $reflector->newInstance();\n$hello = $reflector->getMethod('hello');\n$hello->invoke($foo);\n\n// using callback\n$foo = new Foo();\ncall_user_func(array($foo, 'hello'));\n\n// using variable variables syntax\n$className = 'Foo';\n$foo = new $className();\n$method = 'hello';\n$foo->$method();\nPython[edit]\nThe following is an example in Python:\n\n# without reflection\nobj = Foo()\nobj.hello()\n\n# with reflection\nclass_name = \"Foo\"\nmethod = \"hello\"\nobj = globals()[class_name]()\ngetattr(obj, method)()\n\n# with eval\neval(\"Foo().hello()\")\nR[edit]\nThe following is an example in R:\n\n# Without reflection, assuming foo() returns an S3-type object that has method \"hello\"\nobj <- foo()\nhello(obj)\n\n# With reflection\nthe.class <- \"foo\"\nthe.method <- \"hello\"\nobj <- do.call(the.class, list())\ndo.call(the.method, alist(obj))\nRuby[edit]\nThe following is an example in Ruby:\n\n# without reflection\nobj = Foo.new\nobj.hello\n\n# with reflection\nclass_name = \"Foo\"\nmethod_name = :hello\nobj = Object.const_get(class_name).new\nobj.send method_name\n\n# with eval\neval \"Foo.new.hello\"\nSee also[edit]\nType introspection\nSelf-modifying code\nSelf-hosting\nProgramming paradigms\nList of reflective programming languages and platforms\nMirror (programming)",
          "subparadigms": [
            44
          ]
        },
        {
          "pdid": 46,
          "name": "Homoiconic",
          "details": "In computer programming, homoiconicity (from the Greek words homo meaning the same and icon meaning representation) is a property of some programming languages in which the program structure is similar to its syntax, and therefore the program's internal representation can be inferred by reading the text's layout.[1] If a language is homoiconic, it means that the language text has the same structure as its abstract syntax tree (i.e. the AST and the syntax are isomorphic). This allows all code in the language to be accessed and transformed as data, using the same representation.\n\nIn a homoiconic language the primary representation of programs is also a data structure in a primitive type of the language itself. This makes metaprogramming easier than in a language without this property, since code can be treated as data: reflection in the language (examining the program's entities at runtime) depends on a single, homogeneous structure, and it does not have to handle several different structures that would appear in a complex syntax. To put that another way, homoiconicity is where a program's source code is written as a basic data structure that the programming language knows how to access.\n\nA typical, commonly cited example is the programming language Lisp, which was created to be easy for lists manipulation and where the structure is given by S-expressions that take the form of nested lists. Lisp programs are written in the form of lists; the result is that the program can access its own functions and procedures while running, and programmatically reprogram itself on the fly. Homoiconic languages typically include full support of syntactic macros allowing the programmer to express program transformations in a concise way. Examples are the programming languages Clojure (a contemporary dialect of Lisp), Rebol and Refal.\n\nContents  [hide] \n1\tHistory\n2\tUses, advantages, and disadvantages\n3\tExamples\n3.1\tHomoiconicity in Lisp\n3.2\tHomoiconicity in Prolog\n3.3\tHomoiconicity in Rebol\n4\tSee also\n5\tReferences\n6\tExternal links\nHistory[edit]\nThe original source is the paper Macro Instruction Extensions of Compiler Languages,[2] according to the early and influential paper TRAC, A Text-Handling Language:[3]\n\nOne of the main design goals was that the input script of TRAC (what is typed in by the user) should be identical to the text which guides the internal action of the TRAC processor. In other words, TRAC procedures should be stored in memory as a string of characters exactly as the user typed them at the keyboard. If the TRAC procedures themselves evolve new procedures, these new procedures should also be stated in the same script. The TRAC processor in its action interprets this script as its program. In other words, the TRAC translator program (the processor) effectively converts the computer into a new computer with a new program language -- the TRAC language. At any time, it should be possible to display program or procedural information in the same form as the TRAC processor will act upon it during its execution. It is desirable that the internal character code representation be identical to, or very similar to, the external code representation. In the present TRAC implementation, the internal character representation is based upon ASCII. Because TRAC procedures and text have the same representation inside and outside the processor, the term homoiconic is applicable, from homo meaning the same, and icon meaning representation.\n\n[...]\n\nFollowing suggestion of McCullough, W. S., based upon terminology due to Peirce, C. S. s McIlroy. M. D., \"Macro Instruction Extensions of Compiler Languages,\" Comm. ACM, p. 214-220; April, 1960.\n\nAlan Kay used and possibly popularized the term \"homoiconic\" through his use of the term in his 1969 PhD thesis:[4]\n\nA notable group of exceptions to all the previous systems are Interactive LISP [...] and TRAC. Both are functionally oriented (one list, the other string), both talk to the user with one language, and both are \"homoiconic\" in that their internal and external representations are essentially the same. They both have the ability to dynamically create new functions which may then be elaborated at the users's pleasure.\n\nTheir only great drawback is that programs written in them look like King Burniburiach's letter to the Sumerians done in Babylonian cuniform! [...]\n\nUses, advantages, and disadvantages[edit]\nOne advantage of homoiconicity is that extending the language with new concepts typically becomes simpler, as data representing code can be passed between the meta and base layer of the program. The abstract syntax tree of a function may be composed and manipulated as a data structure in the meta layer, and then evaluated. It can be much easier to understand how to manipulate the code since it can be more easily understood as simple data (since the format of the language itself is as a data format).\n\nThe simplicity that allows this also presents a disadvantage: one blogger argues that, at least in the case of LISP-like list-oriented languages, it can do away with many of the visual cues that help humans visually parse the constructs of the language, and that this can lead to a steep learning curve for the language.[5] See also essay \"The Lisp Curse\"[6] for disadvantages.\n\nA typical demonstration of homoiconicity is the meta-circular evaluator.\n\nExamples[edit]\nLanguages which are considered to be homoiconic include\n\nCurl[7]\nIo[7]\nIoke\nJulia[1][2][7]\nLisp and its dialects,[7] such as Scheme,[5] Clojure[3], Racket[4]\nMathematica[5]\nProlog[7][8]\nRebol[7]\nRed\nSNOBOL[7]\nTcl[5][7]\nXSLT[9]\nREFAL[7]\nWolfram Language[10]\nIn Von Neumann architecture systems (including the vast majority of general purpose computers today), raw machine code also has this property, the data type being bytes in memory.\n\nHomoiconicity in Lisp[edit]\nLisp uses S-expressions as an external representation for data and code. S-expressions can be read with the primitive Lisp function READ. READ returns Lisp data: lists, symbols, numbers, strings. The primitive Lisp function EVAL uses Lisp code represented as Lisp data, computes side-effects and returns a result. The result will be printed by the primitive function PRINT, which creates an external S-expression from Lisp data.\n\nLisp data, a list using different data types: (sub)lists, symbols, strings and integer numbers.\n\n((:name \"john\" :age 20) (:name \"mary\" :age 18) (:name \"alice\" :age 22))\nLisp code. The example uses lists, symbols and numbers.\n\n(* (sin 1.1) (cos 2.03))      ; in infix: sin(1.1)*cos(2.03)\nCreate above expression with the primitive Lisp function LIST and set the variable EXPRESSION to the result\n\n(setf expression  (list '* (list 'sin 1.1) (list 'cos 2.03)) )  \n-> (* (SIN 1.1) (COS 2.03))    ; Lisp returns and prints the result\n\n(third expression)    ; the third element of the expression\n-> (COS 2.03)\nChange the COS term to SIN\n\n(setf (first (third expression)) 'SIN)\n; The expression is now (* (SIN 1.1) (SIN 2.03)).\nEvaluate the expression\n\n(eval expression)\n-> 0.7988834\nPrint the expression to a string\n\n(print-to-string expression)\n->  \"(* (SIN 1.1) (SIN 2.03))\"\nRead the expression from a string\n\n(read-from-string \"(* (SIN 1.1) (SIN 2.03))\")\n->  (* (SIN 1.1) (SIN 2.03))     ; returns a list of lists, numbers and symbols\nHomoiconicity in Prolog[edit]\n1 ?- X is 2*5.\nX = 10.\n\n2 ?- L = (X is 2*5), write_canonical(L).\nis(_, *(2, 5))\nL = (X is 2*5).\n\n3 ?- L = (ten(X):-(X is 2*5)), write_canonical(L).\n:-(ten(A), is(A, *(2, 5)))\nL = (ten(X):-X is 2*5).\n\n4 ?- L = (ten(X):-(X is 2*5)), assert(L).\nL = (ten(X):-X is 2*5).\n\n5 ?- ten(X).\nX = 10.\n\n6 ?-\nOn line 4 we create a new clause. The operator \":-\" separates the head and the body of a clause. With assert/1* we add it to the existing clauses(add it to the \"database\"), so we can call it later. In other languages we would call it \"creating a function during runtime\". We can also remove clauses from the database with abolish/1, or retract/1.\n\n* The number after the clause's name is the number of arguments it can take.(It is also called arity.)\n\nWe can also query the database to get the body of a clause:\n\n7 ?- clause(ten(X),Y).\nY = (X is 2*5).\n\n8 ?- clause(ten(X),Y), Y = (X is Z).\nY = (X is 2*5),\nZ = 2*5.\n\n9 ?- clause(ten(X),Y), call(Y).\nX = 10,\nY = (10 is 2*5).\n\"call\" is analogous to Lisp's \"eval\" function.\n\nHomoiconicity in Rebol[edit]\nThe concept of treating code as data and the manipulation and evaluation thereof can be demonstrated very neatly in Rebol. (Rebol, unlike Lisp, does not require parentheses to separate expressions).\n\nThe following is an example of code in Rebol (Note that '>>' represents the interpreter prompt; spaces between some elements have been added for readability):\n\n>> repeat i 3 [ print [ i \"hello\" ] ]\n\n1 hello\n2 hello\n3 hello\n(repeat is in fact a built-in function in Rebol and is not a language construct or keyword).\n\nBy enclosing the code in square brackets, the interpreter does not evaluate it, but merely treats it as a block containing words:\n\n[ repeat i 3 [ print [ i \"hello\" ] ] ]\nThis block has the type block! and can furthermore be assigned as the value of a word by using what appears to be a syntax for assignment, but is actually understood by the interpreter as a special type (set-word!) and takes the form of a word followed by a colon:\n\n>> block1: [ repeat i 3 [ print [ i \"hello\" ] ] ] ;; Assign the value of the block to the word `block1`\n== [repeat i 3 [print [i \"hello\"]]]\n>> type? block1 ;; Evaluate the type of the word `block1`\n== block!\nThe block can still be interpreted by using the do function provided in Rebol (similar to \"eval\" in Lisp).\n\nIt is possible to interrogate the elements of the block and change their values, thus altering the behavior of the code if it were to be evaluated:\n\n>> block1/3 ;; The third element of the block\n== 3\n>> block1/3: 5 ;; Set the value of the 3rd element to 5\n== 5\n>> probe block1 ;; Show the changed block\n[repeat i 5 [print [i \"hello\"]]]\n== [repeat i 5 [print [i \"hello\"]]]\n>> do block1 ;; Evaluate the block\n1 hello\n2 hello\n3 hello\n4 hello\n5 hello\nSee also[edit]\nConcatenative programming language\nCognitive dimensions of notations, design principles for programming languages' syntax\nLanguage-oriented programming",
          "subparadigms": []
        },
        {
          "pdid": 47,
          "name": "Template",
          "details": "Template metaprogramming (TMP) is a metaprogramming technique in which templates are used by a compiler to generate temporary source code, which is merged by the compiler with the rest of the source code and then compiled. The output of these templates include compile-time constants, data structures, and complete functions. The use of templates can be thought of as compile-time execution. The technique is used by a number of languages, the best-known being C++, but also Curl, D, and XL.\n\nTemplate metaprogramming was, in a sense, discovered accidentally.[1][better source needed]\n\nSome other languages support similar, if not more powerful compile-time facilities (such as Lisp macros), but those are outside the scope of this article.\n\nContents  [hide] \n1\tComponents of template metaprogramming\n1.1\tUsing template metaprogramming\n2\tCompile-time class generation\n3\tCompile-time code optimization\n4\tStatic polymorphism\n5\tBenefits and drawbacks of template metaprogramming\n6\tSee also\n7\tReferences\n8\tExternal links\nComponents of template metaprogramming[edit]\nThe use of templates as a metaprogramming technique requires two distinct operations: a template must be defined, and a defined template must be instantiated. The template definition describes the generic form of the generated source code, and the instantiation causes a specific set of source code to be generated from the generic form in the template.\n\nTemplate metaprogramming is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram.[2]\n\nTemplates are different from macros. A macro, which is also a compile-time language feature, generates code in-line using text manipulation and substitution. Macro systems often have limited compile-time process flow abilities and usually lack awareness of the semantics and type system of their companion language (an exception should be made with Lisp's macros, which are written in Lisp itself and involve manipulation and substitution of Lisp code represented as data structures as opposed to text).\n\nTemplate metaprograms have no mutable variables— that is, no variable can change value once it has been initialized, therefore template metaprogramming can be seen as a form of functional programming. In fact many template implementations implement flow control only through recursion, as seen in the example below.\n\nUsing template metaprogramming[edit]\nThough the syntax of template metaprogramming is usually very different from the programming language it is used with, it has practical uses. Some common reasons to use templates are to implement generic programming (avoiding sections of code which are similar except for some minor variations) or to perform automatic compile-time optimization such as doing something once at compile time rather than every time the program is run — for instance, by having the compiler unroll loops to eliminate jumps and loop count decrements whenever the program is executed.\n\nCompile-time class generation[edit]\nWhat exactly \"programming at compile-time\" means can be illustrated with an example of a factorial function, which in non-template C++ can be written using recursion as follows:\n\nunsigned int factorial(unsigned int n) {\n\treturn n == 0 ? 1 : n * factorial(n - 1); \n}\n\n// Usage examples:\n// factorial(0) would yield 1;\n// factorial(4) would yield 24.\nThe code above will execute at run time to determine the factorial value of the literals 4 and 0. By using template metaprogramming and template specialization to provide the ending condition for the recursion, the factorials used in the program—ignoring any factorial not used—can be calculated at compile time by this code:\n\ntemplate <unsigned int n>\nstruct factorial {\n\tenum { value = n * factorial<n - 1>::value };\n};\n\ntemplate <>\nstruct factorial<0> {\n\tenum { value = 1 };\n};\n\n// Usage examples:\n// factorial<0>::value would yield 1;\n// factorial<4>::value would yield 24.\nThe code above calculates the factorial value of the literals 4 and 0 at compile time and uses the result as if they were precalculated constants. To be able to use templates in this manner, the compiler must know the value of its parameters at compile time, which has the natural precondition that factorial<X>::value can only be used if X is known at compile time. In other words, X must be a constant literal or a constant expression.\n\nIn C++11, constexpr, a way to let the compiler execute simple constant expressions, was added. Using constexpr, one can use the usual recursive factorial definition.[3]\n\nCompile-time code optimization[edit]\nSee also: Compile time function execution\nThe factorial example above is one example of compile-time code optimization in that all factorials used by the program are pre-compiled and injected as numeric constants at compilation, saving both run-time overhead and memory footprint. It is, however, a relatively minor optimization.\n\nAs another, more significant, example of compile-time loop unrolling, template metaprogramming can be used to create length-n vector classes (where n is known at compile time). The benefit over a more traditional length-n vector is that the loops can be unrolled, resulting in very optimized code. As an example, consider the addition operator. A length-n vector addition might be written as\n\ntemplate <int length>\nVector<length>& Vector<length>::operator+=(const Vector<length>& rhs) \n{\n    for (int i = 0; i < length; ++i)\n        value[i] += rhs.value[i];\n    return *this;\n}\nWhen the compiler instantiates the function template defined above, the following code may be produced:[citation needed]\n\ntemplate <>\nVector<2>& Vector<2>::operator+=(const Vector<2>& rhs) \n{\n    value[0] += rhs.value[0];\n    value[1] += rhs.value[1];\n    return *this;\n}\nThe compiler's optimizer should be able to unroll the for loop because the template parameter length is a constant at compile time.\n\nHowever, take caution as this may cause code bloat as separate unrolled code will be generated for each 'N'(vector size) you instantiate with.\n\nStatic polymorphism[edit]\nPolymorphism is a common standard programming facility where derived objects can be used as instances of their base object but where the derived objects' methods will be invoked, as in this code\n\nclass Base\n{\npublic:\n    virtual void method() { std::cout << \"Base\"; }\n    virtual ~Base() {}\n};\n\nclass Derived : public Base\n{\npublic:\n    virtual void method() { std::cout << \"Derived\"; }\n};\n\nint main()\n{\n    Base *pBase = new Derived;\n    pBase->method(); //outputs \"Derived\"\n    delete pBase;\n    return 0;\n}\nwhere all invocations of virtual methods will be those of the most-derived class. This dynamically polymorphic behaviour is (typically) obtained by the creation of virtual look-up tables for classes with virtual methods, tables that are traversed at run time to identify the method to be invoked. Thus, run-time polymorphism necessarily entails execution overhead (though on modern architectures the overhead is small).\n\nHowever, in many cases the polymorphic behaviour needed is invariant and can be determined at compile time. Then the Curiously Recurring Template Pattern (CRTP) can be used to achieve static polymorphism, which is an imitation of polymorphism in programming code but which is resolved at compile time and thus does away with run-time virtual-table lookups. For example:\n\ntemplate <class Derived>\nstruct base\n{\n    void interface()\n    {\n         // ...\n         static_cast<Derived*>(this)->implementation();\n         // ...\n    }\n};\n\nstruct derived : base<derived>\n{\n     void implementation()\n     {\n         // ...\n     }\n};\nHere the base class template will take advantage of the fact that member function bodies are not instantiated until after their declarations, and it will use members of the derived class within its own member functions, via the use of a static_cast, thus at compilation generating an object composition with polymorphic characteristics. As an example of real-world usage, the CRTP is used in the Boost iterator library.[4]\n\nAnother similar use is the \"Barton–Nackman trick\", sometimes referred to as \"restricted template expansion\", where common functionality can be placed in a base class that is used not as a contract but as a necessary component to enforce conformant behaviour while minimising code redundancy.\n\nBenefits and drawbacks of template metaprogramming[edit]\nCompile-time versus execution-time tradeoff \nIf a great deal of template metaprogramming is used, compilation may become slow; section 14.7.1 [temp.inst] of the current standard defines the circumstances under which templates are implicitly instantiated. Defining a template does not imply that it will be instantiated, and instantiating a class template does not cause its member definitions to be instantiated. Depending on the style of use, templates may compile either faster or slower than hand-rolled code.\nGeneric programming \nTemplate metaprogramming allows the programmer to focus on architecture and delegate to the compiler the generation of any implementation required by client code. Thus, template metaprogramming can accomplish truly generic code, facilitating code minimization and better maintainability[citation needed].\nReadability \nWith respect to C++, the syntax and idioms of template metaprogramming are esoteric compared to conventional C++ programming, and template metaprograms can be very difficult to understand. [5][6]\nSee also[edit]\nSubstitution failure is not an error (SFINAE)\nMetaprogramming\nPreprocessor\nParametric polymorphism\nExpression templates\nVariadic Templates\nCompile time function execution",
          "subparadigms": []
        },
        {
          "pdid": 48,
          "name": "Policy-based",
          "details": "Policy-based design, also known as policy-based class design or policy-based programming, is a computer programming paradigm based on an idiom for C++ known as policies. It has been described as a compile-time variant of the strategy pattern, and has connections with C++ template metaprogramming. It was first popularized by Andrei Alexandrescu with his 2001 book Modern C++ Design and his column Generic<Programming> in the C/C++ Users Journal.\n\nAlthough the technique could theoretically be applied to other languages, it is currently closely associated with C++, and depends on the particular feature set of that language. Furthermore, even in C++ it requires a compiler with highly robust support for templates, which wasn't common before about 2003.\n\nContents  [hide] \n1\tOverview\n2\tSimple example\n3\tSee also\n4\tExternal links\nOverview[edit]\nThe central idiom in policy-based design is a class template (called the host class), taking several type parameters as input, which are instantiated with types selected by the user (called policy classes), each implementing a particular implicit interface (called a policy), and encapsulating some orthogonal (or mostly orthogonal) aspect of the behavior of the instantiated host class. By supplying a host class combined with a set of different, canned implementations for each policy, a library or module can support an exponential number of different behavior combinations, resolved at compile time, and selected by mixing and matching the different supplied policy classes in the instantiation of the host class template. Additionally, by writing a custom implementation of a given policy, a policy-based library can be used in situations requiring behaviors unforeseen by the library implementor. Even in cases where no more than one implementation of each policy will ever be used, decomposing a class into policies can aid the design process, by increasing modularity and highlighting exactly where orthogonal design decisions have been made.\n\nWhile assembling software components out of interchangeable modules is a far from new concept, policy-based design represents an innovation in the way it applies that concept at the (relatively low) level of defining the behavior of an individual class. Policy classes have some similarity to callbacks, but differ in that, rather than consisting of a single function, a policy class will typically contain several related functions (methods), often combined with state variables or other facilities such as nested types. A policy-based host class can be thought of as a type of metafunction, taking a set of behaviors represented by types as input, and returning as output a type representing the result of combining those behaviors into a functioning whole. (Unlike MPL metafunctions, however, the output is usually represented by the instantiated host class itself, rather than a nested output type.)\n\nA key feature of the policy idiom is that, usually (though it is not strictly necessary), the host class will derive from (make itself a child class of) each of its policy classes using (public) multiple inheritance. (Alternatives are for the host class to merely contain a member variable of each policy class type, or else to inherit the policy classes privately; however inheriting the policy classes publicly has the major advantage that a policy class can add new methods, inherited by the instantiated host class and accessible to its users, which the host class itself need not even know about.) A notable feature of this aspect of the policy idiom is that, relative to object-oriented programming, policies invert the relationship between base class and derived class - whereas in OOP interfaces are traditionally represented by (abstract) base classes and implementations of interfaces by derived classes, in policy-based design the derived (host) class represents the interfaces and the base (policy) classes implement them. It should also be noted that in the case of policies, the public inheritance does not represent an is-a relationship between the host and the policy classes. While this would traditionally be considered evidence of a design defect in OOP contexts, this doesn't apply in the context of the policy idiom.\n\nA disadvantage of policies in their current incarnation is that the policy interface doesn't have a direct, explicit representation in code, but rather is defined implicitly, via duck typing, and must be documented separately and manually, in comments. The main idea is to use commonality-variability analysis to divide the type into the fixed implementation and interface, the policy-based class, and the different policies. The trick is to know what goes into the main class, and what policies should one create. The article mentioned above gives the following answer: wherever we would need to make a possible limiting design decision, we should postpone that decision, we should delegate it to an appropriately named policy.\n\nPolicy classes can contain implementation, type definitions and so forth. Basically, the designer of the main template class will define what the policy classes should provide, what customization points they need to implement.\n\nIt may be a delicate task to create a good set of policies, just the right number (e.g., the minimum necessary). The different customization points, which belong together, should go into one policy argument, such as storage policy, validation policy and so forth. Graphic designers are able to give a name to their policies, which represent concepts, and not those which represent operations or minor implementation details.\n\nPolicy-based design may incorporate other techniques which will be useful, even if changed. One example is that the template method pattern can be reinterpreted for compile time; so that a main class has a skeleton algorithm, which — at customization points — calls the appropriate functions of some of the policies. Designers can also find themselves in using their policy classes as traits are used, asking type information, delegating type related tasks to it, a storage policy is one example where it can happen.[needs copy edit]\n\nSimple example[edit]\nPresented below is a simple (contrived) example of a C++ hello world program, where the text to be printed and the method of printing it are decomposed using policies. In this example, HelloWorld is a host class where it takes two policies, one for specifying how a message should be shown and the other for the actual message being printed. Note that the generic implementation is in run() and therefore the code is unable to be compiled unless both policies (print and message) are provided.\n\n#include <iostream>\n#include <string>\n \ntemplate <typename OutputPolicy, typename LanguagePolicy>\nclass HelloWorld : private OutputPolicy, private LanguagePolicy\n{\n    using OutputPolicy::print;\n    using LanguagePolicy::message;\n \npublic:\n    // Behaviour method\n    void run() const\n    {\n        // Two policy methods\n        print(message());\n    }\n};\n \nclass OutputPolicyWriteToCout\n{\nprotected:\n    template<typename MessageType>\n    void print(MessageType const &message) const\n    {\n        std::cout << message << std::endl;\n    }\n};\n \nclass LanguagePolicyEnglish\n{\nprotected:\n    std::string message() const\n    {\n        return \"Hello, World!\";\n    }\n};\n \nclass LanguagePolicyGerman\n{\nprotected:\n    std::string message() const\n    {\n        return \"Hallo Welt!\";\n    }\n};\n \nint main()\n{\n    /* Example 1 */\n    typedef HelloWorld<OutputPolicyWriteToCout, LanguagePolicyEnglish> HelloWorldEnglish;\n \n    HelloWorldEnglish hello_world;\n    hello_world.run(); // prints \"Hello, World!\"\n \n    /* Example 2 \n     * Does the same, but uses another language policy */\n    typedef HelloWorld<OutputPolicyWriteToCout, LanguagePolicyGerman> HelloWorldGerman;\n \n    HelloWorldGerman hello_world2;\n    hello_world2.run(); // prints \"Hallo Welt!\"\n}\nDesigners can easily write more OutputPolicies by adding new classes with the member function print() and take those as new OutputPolicies.",
          "subparadigms": [
            47
          ]
        },
        {
          "pdid": 49,
          "name": "Array",
          "details": "In computer science, array programming languages (also known as vector or multidimensional languages) generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.\n\nArray programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon to find array programming language one-liners that require more than a couple of pages of Java code.[1]\n\nModern programming languages that support array programming are commonly used in scientific and engineering settings; these include Fortran 90, Mata, MATLAB, Analytica, TK Solver (as lists), Octave, R, Cilk Plus, Julia, and the NumPy extension to Python. In these languages, an operation that operates on entire arrays can be called a vectorized operation,[2] regardless of whether it is executed on a vector processor or not.\n\nContents  [hide] \n1\tConcepts\n2\tUses\n3\tLanguages\n3.1\tScalar languages\n3.2\tArray languages\n3.2.1\tAda\n3.2.2\tAnalytica\n3.2.3\tBASIC\n3.2.4\tMata\n3.2.5\tMATLAB\n3.2.6\trasql\n3.2.7\tR\n4\tMathematical reasoning and language notation\n5\tThird-party libraries\n6\tSee also\n7\tReferences\n8\tExternal links\nConcepts[edit]\nThe fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\n\nIverson described the rationale behind array programming (actually referring to APL) as follows:[3]\n\nmost programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician. [...]\n\nThe thesis [...] is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. [...] it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\n[...] Users of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted, since a clear statement of an algorithm can usually be used as a basis from which one may easily derive more efficient algorithm.\n\nThe basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\n\nFunction rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.\n\nUses[edit]\nArray programming is very well suited to implicit parallelization; a topic of much research nowadays. Further, Intel and compatible CPUs developed and produced after 1997 contained various instruction set extensions, starting from MMX and continuing through SSSE3 and 3DNow!, which include rudimentary SIMD array capabilities. Array processing is distinct from parallel processing in that one physical processor performs operations on a group of items simultaneously while parallel processing aims to split a larger problem into smaller ones (MIMD) to be solved piecemeal by numerous processors. Processors with two or more cores are increasingly common today.\n\nLanguages[edit]\nThe canonical examples of array programming languages are APL, J, and Fortran. Others include: D, A+, Analytica, Chapel, IDL, Julia, K, Q, Mata, Mathematica, MATLAB, MOLSF, NumPy, GNU Octave, PDL, R, S-Lang, SAC, Nial and ZPL.\n\nScalar languages[edit]\nIn scalar languages such as C and Pascal, operations apply only to single values, so a+b expresses the addition of two numbers. In such languages, adding one array to another requires indexing and looping, the coding of which is tedious and error-prone[citation needed].\n\nfor (i = 0; i < n; i++)\n    for (j = 0; j < n; j++)\n        a[i][j] += b[i][j];\nArray languages[edit]\nIn array languages, operations are generalized to apply to both scalars and arrays. Thus, a+b expresses the sum of two scalars if a and b are scalars, or the sum of two arrays if they are arrays.\n\nAn array language simplifies programming but possibly at a cost known as the abstraction penalty.[4][5][6] Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).\n\nAda[edit]\nThe previous C code would become the following in the Ada language,[7] which supports array-programming syntax.\n\n A := A + B;\nAnalytica[edit]\nAnalytica provides the same economy of expression as Ada.\n\n A := A + B;\nThis operation works whether operands, A or B, are scalar or arrays with one more dimensions. Each dimension is identified by an index variable, which controls the nature of the operation. The result has the union of the dimensions of the operands. If A and B have the same dimensions (indexes), the result has those same dimensions. If A and B are vectors with different dimensions, the resulting A is 2-dimensional, containing both dimensions, with each element the sum of the corresponding values of A and B. Variable A must be a local variable; Analytica, as a declarative language, avoids side effects by disallowing assignment to global variables.\n\nBASIC[edit]\nDartmouth BASIC had MAT statements for matrix and array manipulation in its third edition (1966).\n\n DIM A(4),B(4),C(4)\n MAT A = 1\n MAT B = 2*A\n MAT C = A + B\n MAT PRINT A,B,C\nMata[edit]\nStata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.\n\n. mata:\n\n: A = (1,2,3) \\(4,5,6)\n\n: A\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n: B = (2..4) \\(1..3)\n\n: B\n       1   2   3\n    +-------------+\n  1 |  2   3   4  |\n  2 |  1   2   3  |\n    +-------------+\n\n: C = J(3,2,1)           // A 3 by 2 matrix of ones\n\n: C\n       1   2\n    +---------+\n  1 |  1   1  |\n  2 |  1   1  |\n  3 |  1   1  |\n    +---------+\n\n: D = A + B\n\n: D\n       1   2   3\n    +-------------+\n  1 |  3   5   7  |\n  2 |  5   7   9  |\n    +-------------+\n\n: E = A*C\n\n: E\n        1    2\n    +-----------+\n  1 |   6    6  |\n  2 |  15   15  |\n    +-----------+\n\n: F = A:*B\n\n: F\n        1    2    3\n    +----------------+\n  1 |   2    6   12  |\n  2 |   4   10   18  |\n    +----------------+\n\n: G = E :+ 3\n\n: G\n        1    2\n    +-----------+\n  1 |   9    9  |\n  2 |  18   18  |\n    +-----------+\n\n: H = F[(2\\1), (1, 2)]    // Subscripting to get a submatrix of F and\n\n:                         // switch row 1 and 2\n: H\n        1    2\n    +-----------+\n  1 |   4   10  |\n  2 |   2    6  |\n    +-----------+\n\n: I = invsym(F'*F)        // Generalized inverse (F*F^(-1)F=F) of a\n\n:                         // symmetric positive semi-definite matrix\n: I\n[symmetric]\n                 1             2             3\n    +-------------------------------------------+\n  1 |            0                              |\n  2 |            0          3.25                |\n  3 |            0         -1.75   .9444444444  |\n    +-------------------------------------------+\n\n: end\nMATLAB[edit]\nThe implementation in MATLAB allows the same economy allowed by using the Ada language.\n\nA = A + B;\nA variant of the MATLAB language is the GNU Octave language, which extends the original language with augmented assignments:\n\nA += B;\nBoth MATLAB and GNU Octave natively support linear algebra operations such as matrix multiplication, matrix inversion, and the numerical solution of system of linear equations, even using the Moore–Penrose pseudoinverse.[8][9]\n\nThe Nial example of the inner product of two arrays can be implemented using the native matrix multiplication operator. If a is a row vector of size [1 n] and b is a corresponding column vector of size [n 1].\n\na * b;\nThe inner product between two matrices having the same number of elements can be implemented with the auxiliary operator (:), which reshapes a given matrix into a column vector, and the transpose operator ':\n\nA(:)' * B(:);\nrasql[edit]\nThe rasdaman query language is a database-oriented array-programming language. For example, two arrays could be added with the following query:\n\nSELECT A + B\nFROM   A, B\nR[edit]\nThe R language supports array paradigm by default. The following example illustrates a process of multiplication of two matrices followed by an addition of a scalar (which is, in fact, a one-element vector) and a vector:\n\n> A <- matrix(1:6, nrow=2)                              !!this has nrow=2 ... and A has 2 rows\n> A\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n> B <- t( matrix(6:1, nrow=2) )  # t() is a transpose operator                           !!this has nrow=2 ... and B has 3 rows --- a clear contradiction to the definition of A\n> B\n     [,1] [,2]\n[1,]    6    5\n[2,]    4    3\n[3,]    2    1\n> C <- A %*% B\n> C\n     [,1] [,2]\n[1,]   28   19\n[2,]   40   28\n> D <- C + 1\n> D\n     [,1] [,2]\n[1,]   29   20\n[2,]   41   29\n> D + c(1, 1)  # c() creates a vector\n     [,1] [,2]\n[1,]   30   21\n[2,]   42   30\nMathematical reasoning and language notation[edit]\nThe matrix left-division operator concisely expresses some semantic properties of matrices. As in the scalar equivalent, if the (determinant of the) coefficient (matrix) A is not null then it is possible to solve the (vectorial) equation A * x = b by left-multiplying both sides by the inverse of A: A−1 (in both MATLAB and GNU Octave languages: A^-1). The following mathematical statements hold when A is a full rank square matrix:\n\nA^-1 *(A * x)==A^-1 * (b)\n(A^-1 * A)* x ==A^-1 * b       (matrix-multiplication associativity)\nx = A^-1 * b\nwhere == is the equivalence relational operator. The previous statements are also valid MATLAB expressions if the third one is executed before the others (numerical comparisons may be false because of round-off errors).\n\nIf the system is overdetermined - so that A has more rows than columns - the pseudoinverse A+ (in MATLAB and GNU Octave languages: pinv(A)) can replace the inverse A−1, as follows:\n\npinv(A) *(A * x)==pinv(A) * (b)\n(pinv(A) * A)* x ==pinv(A) * b       (matrix-multiplication associativity)\nx = pinv(A) * b\nHowever, these solutions are neither the most concise ones (e.g. still remains the need to notationally differentiate overdetermined systems) nor the most computationally efficient. The latter point is easy to understand when considering again the scalar equivalent a * x = b, for which the solution x = a^-1 * b would require two operations instead of the more efficient x = b / a. The problem is that generally matrix multiplications are not commutative as the extension of the scalar solution to the matrix case would require:\n\n(a * x)/ a ==b / a\n(x * a)/ a ==b / a       (commutativity does not hold for matrices!)\nx * (a / a)==b / a       (associativity also holds for matrices)\nx = b / a\nThe MATLAB language introduces the left-division operator \\ to maintain the essential part of the analogy with the scalar case, therefore simplifying the mathematical reasoning and preserving the conciseness:\n\nA \\ (A * x)==A \\ b\n(A \\ A)* x ==A \\ b       (associativity also holds for matrices, commutativity is no more required)\nx = A \\ b\nThis is not only an example of terse array programming from the coding point of view but also from the computational efficiency perspective, which in several array programming languages benefits from quite efficient linear algebra libraries such as ATLAS or LAPACK.[10][11]\n\nReturning to the previous quotation of Iverson, the rationale behind it should now be evident:\n\nit is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\nThird-party libraries[edit]\nThe use of specialized and efficient libraries to provide more terse abstractions is also common in other programming languages. In C++ several linear algebra libraries exploit the language ability to overload operators. In some cases a very terse abstraction in those languages is explicitly influenced by the array programming paradigm, as the Armadillo and Blitz++ libraries do.[12][13]\n\nSee also[edit]\nArray slicing\nList of array programming languages",
          "subparadigms": []
        },
        {
          "pdid": 50,
          "name": "Non-structured",
          "details": "Non-structured programming is the historically earliest programming paradigm capable of creating Turing-complete algorithms. It is often contrasted with structured programming paradigms, including procedural, functional, and object-oriented programming.\n\nUnstructured programming has been heavily criticized for producing hardly-readable (\"spaghetti\") code and is sometimes considered a bad approach for creating major projects, but had been praised for the freedom it offers to programmers and has been compared to how Mozart wrote music.[1]\n\nThere are both high- and low-level programming languages that use non-structured programming. Some languages commonly cited as being non-structured include JOSS, FOCAL, TELCOMP, assembly languages, MS-DOS batch files, and early versions of BASIC, Fortran, COBOL, and MUMPS.\n\nContents  [hide] \n1\tFeatures and typical concepts\n1.1\tBasic concepts\n1.2\tData types\n2\tReferences\n3\tFurther reading\n4\tExternal links\nFeatures and typical concepts[edit]\nBasic concepts[edit]\nA program in a non-structured language usually consists of sequentially ordered commands, or statements, usually one in each line. The lines are usually numbered or may have labels: this allows the flow of execution to jump to any line in the program.\n\nNon-structured programming introduces basic control flow concepts such as loops, branches and jumps. Although there is no concept of procedures in the non-structured paradigm[citation needed], subroutines are allowed. Unlike a procedure, a subroutine may have several entry and exit points, and a direct jump into or out of subroutine is (theoretically) allowed. This flexibility allows realization of coroutines.\n\nThere is no concept of locally scoped variables in non-structured programming (although for assembly programs, general purpose registers may serve the same purpose after saving on entry), but labels and variables can have a limited area of effect (for example, a group of lines). This means there is no (automatic) context refresh when calling a subroutine, so all variables might retain their values from the previous call. This makes general recursion difficult, but some cases of recursion—where no subroutine state values are needed after the recursive call—are possible if variables dedicated to the recursive subroutine are explicitly cleared (or re-initialized to their original value) on entry to the subroutine. The depth of nesting also may be limited to one or two levels.\n\nData types[edit]\nNon-structured languages allow only basic data types, such as numbers, strings and arrays[citation needed] (numbered sets of variables of the same type). The introduction of arrays into non-structured languages was a notable step forward, making stream data processing possible despite the lack of structured data types[citation needed].",
          "subparadigms": [
            49
          ]
        },
        {
          "pdid": 51,
          "name": "Nondeterministic",
          "details": "A nondeterministic programming language is a language which can specify, at certain points in the program (called \"choice points\"), various alternatives for program flow. Unlike an if-then statement, the method of choice between these alternatives is not directly specified by the programmer; the program must decide at run time between the alternatives, via some general method applied to all choice points. A programmer specifies a limited number of alternatives, but the program must later choose between them. (\"Choose\" is, in fact, a typical name for the nondeterministic operator.) A hierarchy of choice points may be formed, with higher-level choices leading to branches that contain lower-level choices within them.\n\nOne method of choice is embodied in backtracking systems (such as AMB, or unification in Prolog), in which some alternatives may \"fail,\" causing the program to backtrack and try other alternatives. If all alternatives fail at a particular choice point, then an entire branch fails, and the program will backtrack further, to an older choice point. One complication is that, because any choice is tentative and may be remade, the system must be able to restore old program states by undoing side-effects caused by partially executing a branch that eventually failed.\n\nAnother method of choice is reinforcement learning, embodied in systems such as Alisp. In such systems, rather than backtracking, the system keeps track of some measure of success and learns which choices often lead to success, and in which situations (both internal program state and environmental input may affect the choice). These systems are suitable for applications to robotics and other domains in which backtracking would involve attempting to undo actions performed in a dynamic environment, which may be difficult or impractical.",
          "subparadigms": []
        },
        {
          "pdid": 52,
          "name": "Process-oriented",
          "details": "Process-oriented programming is a programming paradigm that separates the concerns of data structures and the concurrent processes that act upon them. The data structures in this case are typically persistent, complex, and large scale - the subject of general purpose applications, as opposed to specialized processing of specialized data sets seen in high productivity applications (HPC). The model allows the creation of large scale applications that partially share common data sets. Programs are functionally decomposed into parallel processes that create and act upon logically shared data.\n\nThe paradigm was originally invented for parallel computers in the 1980s, especially computers built with transputer microprocessors by INMOS, or similar architectures. Occam was an early process-oriented language developed for the Transputer.\n\nSome derivations have evolved from the message passing paradigm of Occam to enable uniform efficiency when porting applications between distributed memory and shared memory parallel computers[citation needed]. The first such derived example appears in the programming language Ease designed at Yale University[1][2] in 1990. Similar models have appeared since in the loose combination of SQL databases and objected oriented languages such as Java, often referred to as object-relational models and widely used in large scale distributed systems today. The paradigm is likely to appear on desktop computers as microprocessors increase the number of processors (multicore) per chip.\n\nThe Actor model might usefully be described as a specialised kind of process-oriented system in which the message-passing model is restricted to the simple fixed case of one infinite input queue per process (i.e. actor), to which any other process can send messages.\n\nSee also[edit]\nCommunicating process architectures\nMassively parallel processing\nParallel computing\nMulti-core\nActor model",
          "subparadigms": []
        },
        {
          "pdid": 53,
          "name": "Parallel",
          "details": "Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\n\nParallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU).[5][6] In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.\n\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.\n\nIn some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones,[7] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.\n\nA theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.\n\nContents  [hide] \n1\tBackground\n1.1\tAmdahl's law and Gustafson's law\n1.2\tDependencies\n1.3\tRace conditions, mutual exclusion, synchronization, and parallel slowdown\n1.4\tFine-grained, coarse-grained, and embarrassing parallelism\n1.5\tConsistency models\n1.6\tFlynn's taxonomy\n2\tTypes of parallelism\n2.1\tBit-level parallelism\n2.2\tInstruction-level parallelism\n2.3\tTask parallelism\n3\tHardware\n3.1\tMemory and communication\n3.2\tClasses of parallel computers\n3.2.1\tMulti-core computing\n3.2.2\tSymmetric multiprocessing\n3.2.3\tDistributed computing\n3.2.4\tSpecialized parallel computers\n4\tSoftware\n4.1\tParallel programming languages\n4.2\tAutomatic parallelization\n4.3\tApplication checkpointing\n5\tAlgorithmic methods\n6\tFault-tolerance\n7\tHistory\n8\tSee also\n9\tReferences\n10\tFurther reading\n11\tExternal links\nBackground[edit]\nTraditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.[8]\n\nParallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.[8]\n\nFrequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs.[9]\n\nHowever, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second).[10] Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[11]\n\nMoore's law is the empirical observation that the number of transistors in a microprocessor doubles every 18 to 24 months.[12] Despite power consumption issues, and repeated predictions of its end, Moore's law is still in effect. With the end of frequency scaling, these additional transistors (which are no longer used for frequency scaling) can be used to add extra hardware for parallel computing.\n\nAmdahl's law and Gustafson's law[edit]\n\nA graphical representation of Amdahl's law. The speedup of a program from parallelization is limited by how much of the program can be parallelized. For example, if 90% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 10 times no matter how many processors are used.\n\nAssume that a task has two independent parts, A and B. Part B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A be twice as fast. This will make the computation much faster than by optimizing part B, even though part B's speedup is greater by ratio, (5 times versus 2 times).\nOptimally, the speedup from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speedup. Most of them have a near-linear speedup for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements.\n\nThe potential speedup of an algorithm on a parallel computing platform is given by Amdahl's law[13]\n\n{\\displaystyle S_{\\text{latency}}(s)={\\frac {1}{1-p+{\\frac {p}{s}}}},} S_{\\text{latency}}(s)={\\frac {1}{1-p+{\\frac {p}{s}}}},\nwhere\n\nSlatency is the potential speedup in latency of the execution of the whole task;\ns is the speedup in latency of the execution of the parallelizable part of the task;\np is the percentage of the execution time of the whole task concerning the parallelizable part of the task before parallelization.\nSince Slatency < 1/(1 - p), it shows that a small part of the program which cannot be parallelized will limit the overall speedup available from parallelization. A program solving a large mathematical or engineering problem will typically consist of several parallelizable parts and several non-parallelizable (serial) parts. If the non-parallelizable part of a program accounts for 10% of the runtime (p = 0.9), we can get no more than a 10 times speedup, regardless of how many processors are added. This puts an upper limit on the usefulness of adding more parallel execution units. \"When a task cannot be partitioned because of sequential constraints, the application of more effort has no effect on the schedule. The bearing of a child takes nine months, no matter how many women are assigned.\"[14]\n\n\nA graphical representation of Gustafson's law.\nAmdahl's law only applies to cases where the problem size is fixed. In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently serial work.[15] In this case, Gustafson's law gives a less pessimistic and more realistic assessment of parallel performance:[16]\n\n{\\displaystyle S_{\\text{latency}}(s)=1-p+sp.} S_{\\text{latency}}(s)=1-p+sp.\nBoth Amdahl's law and Gustafson's law assume that the running time of the serial part of the program is independent of the number of processors. Amdahl's law assumes that the entire problem is of fixed size so that the total amount of work to be done in parallel is also independent of the number of processors, whereas Gustafson's law assumes that the total amount of work to be done in parallel varies linearly with the number of processors.\n\nDependencies[edit]\nUnderstanding data dependencies is fundamental in implementing parallel algorithms. No program can run more quickly than the longest chain of dependent calculations (known as the critical path), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel.\n\nLet Pi and Pj be two program segments. Bernstein's conditions[17] describe when the two are independent and can be executed in parallel. For Pi, let Ii be all of the input variables and Oi the output variables, and likewise for Pj. Pi and Pj are independent if they satisfy\n\n{\\displaystyle I_{j}\\cap O_{i}=\\varnothing ,} I_{j}\\cap O_{i}=\\varnothing ,\n{\\displaystyle I_{i}\\cap O_{j}=\\varnothing ,} I_{i}\\cap O_{j}=\\varnothing ,\n{\\displaystyle O_{i}\\cap O_{j}=\\varnothing .} O_{i}\\cap O_{j}=\\varnothing .\nViolation of the first condition introduces a flow dependency, corresponding to the first segment producing a result used by the second segment. The second condition represents an anti-dependency, when the second segment produces a variable needed by the first segment. The third and final condition represents an output dependency: when two segments write to the same location, the result comes from the logically last executed segment.[18]\n\nConsider the following functions, which demonstrate several kinds of dependencies:\n\n1: function Dep(a, b)\n2: c := a * b\n3: d := 3 * c\n4: end function\nIn this example, instruction 3 cannot be executed before (or even in parallel with) instruction 2, because instruction 3 uses a result from instruction 2. It violates condition 1, and thus introduces a flow dependency.\n\n1: function NoDep(a, b)\n2: c := a * b\n3: d := 3 * b\n4: e := a + b\n5: end function\nIn this example, there are no dependencies between the instructions, so they can all be run in parallel.\n\nBernstein's conditions do not allow memory to be shared between different processes. For that, some means of enforcing an ordering between accesses is necessary, such as semaphores, barriers or some other synchronization method.\n\nRace conditions, mutual exclusion, synchronization, and parallel slowdown[edit]\nSubtasks in a parallel program are often called threads. Some parallel computer architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, \"threads\" is generally accepted as a generic term for subtasks. Threads will often need to update some variable that is shared between them. The instructions between the two programs may be interleaved in any order. For example, consider the following program:\n\nThread A\tThread B\n1A: Read variable V\t1B: Read variable V\n2A: Add 1 to variable V\t2B: Add 1 to variable V\n3A: Write back to variable V\t3B: Write back to variable V\nIf instruction 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, the program will produce incorrect data. This is known as a race condition. The programmer must use a lock to provide mutual exclusion. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its critical section (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks:\n\nThread A\tThread B\n1A: Lock variable V\t1B: Lock variable V\n2A: Read variable V\t2B: Read variable V\n3A: Add 1 to variable V\t3B: Add 1 to variable V\n4A: Write back to variable V\t4B: Write back to variable V\n5A: Unlock variable V\t5B: Unlock variable V\nOne thread will successfully lock variable V, while the other thread will be locked out—unable to proceed until V is unlocked again. This guarantees correct execution of the program. Locks, while necessary to ensure correct program execution, can greatly slow a program.\n\nLocking multiple variables using non-atomic locks introduces the possibility of program deadlock. An atomic lock locks multiple variables all at once. If it cannot lock all of them, it does not lock any of them. If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. In such a case, neither thread can complete, and deadlock results.\n\nMany parallel programs require that their subtasks act in synchrony. This requires the use of a barrier. Barriers are typically implemented using a software lock. One class of algorithms, known as lock-free and wait-free algorithms, altogether avoids the use of locks and barriers. However, this approach is generally difficult to implement and requires correctly designed data structures.\n\nNot all parallelization results in speed-up. Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other. Eventually, the overhead from communication dominates the time spent solving the problem, and further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. This is known as parallel slowdown.\n\nFine-grained, coarse-grained, and embarrassing parallelism[edit]\nApplications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.\n\nConsistency models[edit]\nMain article: Consistency model\nParallel programming languages and parallel computers must have a consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced.\n\nOne of the first consistency models was Leslie Lamport's sequential consistency model. Sequential consistency is the property of a parallel program that its parallel execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if \"… the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program\".[19]\n\nSoftware transactional memory is a common type of consistency model. Software transactional memory borrows from database theory the concept of atomic transactions and applies them to memory accesses.\n\nMathematically, these models can be represented in several ways. Petri nets, which were introduced in Carl Adam Petri's 1962 doctoral thesis, were an early attempt to codify the rules of consistency models. Dataflow theory later built upon these, and Dataflow architectures were created to physically implement the ideas of dataflow theory. Beginning in the late 1970s, process calculi such as Calculus of Communicating Systems and Communicating Sequential Processes were developed to permit algebraic reasoning about systems composed of interacting components. More recent additions to the process calculus family, such as the π-calculus, have added the capability for reasoning about dynamic topologies. Logics such as Lamport's TLA+, and mathematical models such as traces and Actor event diagrams, have also been developed to describe the behavior of concurrent systems.\n\nSee also: Relaxed sequential\nFlynn's taxonomy[edit]\nMichael J. Flynn created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.\n\nFlynn's taxonomy\nSingle data stream\nSISD MISD\nMultiple data streams\nSIMD MIMD SPMD MPMD\nThe single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as systolic arrays), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.\n\nAccording to David A. Patterson and John L. Hennessy, \"Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme.\"[20]\n\nTypes of parallelism[edit]\nBit-level parallelism[edit]\nMain article: Bit-level parallelism\nFrom the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information the processor can manipulate per cycle.[21] Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.\n\nHistorically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until the early twothousands, with the advent of x86-64 architectures, did 64-bit processors become commonplace.\n\nInstruction-level parallelism[edit]\nMain article: Instruction-level parallelism\n\nA canonical processor without pipeline. It takes five clock cycles to complete one instruction and thus the processor can issue subscalar performance (IPC = 0.2 < 1).\n\nA canonical five-stage pipelined processor. In the best case scenario, it takes one clock cycle to complete one instruction and thus the processor can issue scalar performance (IPC = 1).\nA computer program, is in essence, a stream of instructions executed by a processor. Without instruction-level parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1). These processors are known as subscalar processors. These instructions can be re-ordered and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.[22]\n\nAll modern processors have multi-stage instruction pipelines. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC = 1). These processors are known as scalar processors. The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and register write back (WB). The Pentium 4 processor had a 35-stage pipeline.[23]\n\n\nA canonical five-stage pipelined superscalar processor. In the best case scenario, it takes one clock cycle to complete two instructions and thus the processor can issue superscalar performance (IPC = 2 > 1).\nMost modern processors also have multiple execution units. They usually combine this feature with pipelining and thus can issue more than one instruction per clock cycle (IPC > 1). These processors are known as superscalar processors. Instructions can be grouped together only if there is no data dependency between them. Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.\n\nTask parallelism[edit]\nMain article: Task parallelism\nTask parallelisms is the characteristic of a parallel program that \"entirely different calculations can be performed on either the same or different sets of data\".[24] This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks simultaneously and often cooperatively. Task parallelism does not usually scale with the size of a problem.[25]\n\nHardware[edit]\nMemory and communication[edit]\nMain memory in a parallel computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space).[26] Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory.\n\n\nA logical view of a non-uniform memory access (NUMA) architecture. Processors in one directory can access that directory's memory with less latency than they can access memory in the other directory's memory.\nComputer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, that can be achieved only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.\n\nComputer systems make use of caches—small and fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping is one of the most common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.[26]\n\nProcessor–processor and processor–memory communication can be implemented in hardware in several ways, including via shared (either multiported or multiplexed) memory, a crossbar switch, a shared bus or an interconnect network of a myriad of topologies including star, ring, tree, hypercube, fat hypercube (a hypercube with more than one processor at a node), or n-dimensional mesh.\n\nParallel computers based on interconnected networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.\n\nClasses of parallel computers[edit]\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.\n\nMulti-core computing[edit]\nMain article: Multi-core processor\nA multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip. This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one instruction stream (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. IBM's Cell microprocessor, designed for use in the Sony PlayStation 3, is a prominent multi-core processor. Each core in a multi-core processor can potentially be superscalar as well—that is, on every clock cycle, each core can issue multiple instructions from one thread.\n\nSimultaneous multithreading (of which Intel's Hyper-Threading is the best known) was an early form of pseudo-multi-coreism. A processor capable of simultaneous multithreading includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. Temporal multithreading on the other hand includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.\n\nSymmetric multiprocessing[edit]\nMain article: Symmetric multiprocessing\nA symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.[27] Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32 processors.[28] Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.[27]\n\nDistributed computing[edit]\nMain article: Distributed computing\nA distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable.\n\nCluster computing[edit]\nMain article: Computer cluster\n\nA Beowulf cluster.\nA cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer.[29] Clusters are composed of multiple standalone machines connected by a network. While machines in a cluster do not have to be symmetric, load balancing is more difficult if they are not. The most common type of cluster is the Beowulf cluster, which is a cluster implemented on multiple identical commercial off-the-shelf computers connected with a TCP/IP Ethernet local area network.[30] Beowulf technology was originally developed by Thomas Sterling and Donald Becker. The vast majority of the TOP500 supercomputers are clusters.[31]\n\nBecause grid computing systems (described below) can easily handle embarrassingly parallel problems, modern clusters are typically designed to handle more difficult problems—problems that require nodes to share intermediate results with each other more often. This requires a high bandwidth and, more importantly, a low-latency interconnection network. Many historic and current supercomputers use customized high-performance network hardware specifically designed for cluster computing, such as the Cray Gemini network.[32] As of 2014, most current supercomputers use some off-the-shelf standard network hardware, often Myrinet, InfiniBand, or Gigabit Ethernet.\n\nMassively parallel computing[edit]\nMain article: Massively parallel (computing)\n\nA cabinet from IBM's Blue Gene/L massively parallel supercomputer.\nA massively parallel processor (MPP) is a single computer with many networked processors. MPPs have many of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having \"far more\" than 100 processors.[33] In an MPP, \"each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect.\"[34]\n\nIBM's Blue Gene/L, the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP.\n\nGrid computing[edit]\nMain article: Grid computing\nGrid computing is the most distributed form of parallel computing. It makes use of computers communicating over the Internet to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, distributed computing typically deals only with embarrassingly parallel problems. Many distributed computing applications have been created, of which SETI@home and Folding@home are the best-known examples.[35]\n\nMost grid computing applications use middleware, software that sits between the operating system and the application to manage network resources and standardize the software interface. The most common distributed computing middleware is the Berkeley Open Infrastructure for Network Computing (BOINC). Often, distributed computing software makes use of \"spare cycles\", performing computations at times when a computer is idling.\n\nSpecialized parallel computers[edit]\nWithin parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific, they tend to be applicable to only a few classes of parallel problems.\n\nReconfigurable computing with field-programmable gate arrays[edit]\nReconfigurable computing is the use of a field-programmable gate array (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task.\n\nFPGAs can be programmed with hardware description languages such as VHDL or Verilog. However, programming in these languages can be tedious. Several vendors have created C to HDL languages that attempt to emulate the syntax and semantics of the C programming language, with which most programmers are familiar. The best known C to HDL languages are Mitrion-C, Impulse C, DIME-C, and Handel-C. Specific subsets of SystemC based on C++ can also be used for this purpose.\n\nAMD's decision to open its HyperTransport technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.[36] According to Michael R. D'Amour, Chief Operating Officer of DRC Computer Corporation, \"when we first walked into AMD, they called us 'the socket stealers.' Now they call us their partners.\"[36]\n\nGeneral-purpose computing on graphics processing units (GPGPU)[edit]\nMain article: GPGPU\n\nNvidia's Tesla GPGPU card\nGeneral-purpose computing on graphics processing units (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for computer graphics processing.[37] Computer graphics processing is a field dominated by data parallel operations—particularly linear algebra matrix operations.\n\nIn the early days, GPGPU programs used the normal graphics APIs for executing programs. However, several new programming languages and platforms have been built to do general purpose computation on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively. Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for computation in their Tesla series. The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs. AMD, Apple, Intel, Nvidia and others are supporting OpenCL.\n\nApplication-specific integrated circuits[edit]\nMain article: Application-specific integrated circuit\nSeveral application-specific integrated circuit (ASIC) approaches have been devised for dealing with parallel applications.[38][39][40]\n\nBecause an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by UV photolithography. This process requires a mask set, which can be extremely expensive. A mask set can cost over a million US dollars.[41] (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore's law) tend to wipe out these gains in only one or two chip generations.[36] High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the PFLOPS RIKEN MDGRAPE-3 machine which uses custom ASICs for molecular dynamics simulation.\n\nVector processors[edit]\nMain article: Vector processor\n\nThe Cray-1 is a vector processor.\nA vector processor is a CPU or computer system that can execute the same instruction on large sets of data. Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers.[42] They are closely related to Flynn's SIMD classification.[42]\n\nCray computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern processor instruction sets do include some vector processing instructions, such as with Freescale Semiconductor's AltiVec and Intel's Streaming SIMD Extensions (SSE).\n\nSoftware[edit]\nParallel programming languages[edit]\nMain article: List of concurrent and parallel programming languages\nConcurrent programming languages, libraries, APIs, and parallel programming models (such as algorithmic skeletons) have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP are two of the most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API.[43] One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.\n\nCAPS entreprise and Pathscale are also coordinating their effort to make hybrid multi-core parallel programming (HMPP) directives an open standard called OpenHMPP. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory. OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU main memory and the accelerator memory.\n\nThe rise of consumer GPUs has led to support for compute kernels, either in graphics APIs (referred to as compute shaders), in dedicated APIs (such as OpenCL), or in other language extensions.\n\nAutomatic parallelization[edit]\nMain article: Automatic parallelization\nAutomatic parallelization of a sequential program by a compiler is the holy grail of parallel computing. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.[44]\n\nMainstream parallel programming languages remain either explicitly parallel or (at best) partially implicit, in which a programmer gives the compiler directives for parallelization. A few fully implicit parallel programming languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.\n\nApplication checkpointing[edit]\nMain article: Application checkpointing\nAs a computer system grows in complexity, the mean time between failures usually decreases. Application checkpointing is a technique whereby the computer system takes a \"snapshot\" of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high performance computing.[45]\n\nAlgorithmic methods[edit]\nAs parallel computers become larger and faster, it becomes feasible to solve problems that previously took too long to run. Parallel computing is used in a wide range of fields, from bioinformatics (protein folding and sequence analysis) to economics (mathematical finance). Common types of problems found in parallel computing applications are:[46]\n\ndense linear algebra;\nsparse linear algebra;\nspectral methods (such as Cooley–Tukey fast Fourier transform)\nN-body problems (such as Barnes–Hut simulation);\nstructured grid problems (such as Lattice Boltzmann methods);\nunstructured grid problems (such as found in finite element analysis);\nMonte Carlo method;\ncombinational logic (such as brute-force cryptographic techniques);\ngraph traversal (such as sorting algorithms);\ndynamic programming;\nbranch and bound methods;\ngraphical models (such as detecting hidden Markov models and constructing Bayesian networks);\nfinite-state machine simulation.\nFault-tolerance[edit]\nFurther information: Fault-tolerant computer system\nParallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component should fail, and also allows automatic error detection and error correction if the results differ. These methods can be used to help prevent single event upsets caused by transient errors.[47] Although additional measures may be required in embedded or specialized systems, this method can provide a cost effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.\n\nHistory[edit]\nMain article: History of computing\n\nILLIAC IV, \"the most infamous of supercomputers\".[48]\nThe origins of true (MIMD) parallelism go back to Luigi Federico Menabrea and his Sketch of the Analytic Engine Invented by Charles Babbage.[49][50][51]\n\nIn April 1958, S. Gill (Ferranti) discussed parallel programming and the need for branching and waiting.[52] Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.[53] Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch.[54] In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference.[53] It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.\n\nIn 1969, company Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel.[53] C.mmp, a 1970s multi-processor project at Carnegie Mellon University, was among the first multiprocessors with more than a few processors.[50] The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984.\"[50]\n\nSIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions.[55] In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory.[53] His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV.[53] The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called \"the most infamous of supercomputers\", because the project was only one fourth completed, but took 11 years and cost almost four times the original estimate.[48] When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.\n\nSee also[edit]\nList of important publications in concurrent, parallel, and distributed computing\nList of distributed computing conferences\nConcurrency (computer science)\nSynchronous programming\nContent Addressable Parallel Processor\nManycore\nSerializability\nTransputer\nParallel programming model\nvector processing\nMulti tasking\nFeng's Classification",
          "subparadigms": [
            52
          ]
        },
        {
          "pdid": 54,
          "name": "Concatenative",
          "details": "A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition.[1] Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.\n\nContents  [hide] \n1\tExample\n2\tProperties\n3\tImplementations\n4\tSee also\n5\tReferences\n6\tExternal links\nExample[edit]\nFor example, a sequence of operations in an applicative language like the following:\n\ny = foo(x)\nz = bar(y)\nw = baz(z)\n...is written in a concatenative language as a sequence of functions, without parameters:[2]\n\nfoo bar baz\nFunctions and procedures written in concatenative style are not value level, i.e. they typically don't represent the data structures they operate on with explicit names or identifiers; instead they are function level - a function is defined as a pipeline, a sequence of operations that take parameters from an implicit data structure on which all functions operate, and return the function results to that shared structure so that it will be used by the next operator.[3]\n\nThe combination of a compositional semantics with a syntax that mirrors such a semantics makes concatenative languages highly amenable to algebraic manipulation of programs;[4] although it may be difficult to write mathematical expressions directly in them.[5] Concatenative languages can be implemented in an efficient way with a stack machine, and are a common strategy to program virtual machines.[5]\n\nMuch of the original work on concatenative language theory was carried out by Manfred von Thun.[citation needed]\n\nProperties[edit]\nThe properties of concatenative languages are the result of their compositional syntax and semantics:\n\nThe reduction of any expression is the simplification of one function to another function; it is never necessary to deal with the application of functions to objects.[6]\nAny subexpression can be replaced with a name that represents the same subexpression. This is referred to in the concatenative community as factoring and is used extensively to simplify programs into smaller parts.\nThe syntax and semantics of concatenative languages form the algebraic structure of a monoid.[7]\nConcatenative languages can be made well-suited to an implementation inspired by linear logic where no garbage is ever generated.[8]\nImplementations[edit]\nThe first concatenative programming language was Forth, although Joy was the first language to call itself concatenative. Other concatenative languages are Cat, Enchilada, Factor, Onyx, PostScript, RPL, Staapl, Trith, XY, Kitten, and Om.\n\nMost existing concatenative languages are stack-based; this is not a requirement and other models have been proposed.[9][10][11] Concatenative languages are currently used for embedded, desktop, and web programming, as target languages, and for research purposes.\n\nMost concatenative languages are dynamically typed. One exception is the statically typed Cat language.[12]\n\nSee also[edit]\nFunction-level programming\nStack-oriented programming language\nTacit programming\nHomoiconicity\nReferences[edit]\nJump up ^ \"Christopher Diggins: What is a concatenative language\". Drdobbs.com. 2008-12-31. Retrieved 2013-07-01.\nJump up ^ \"Name code not values\". Concatenative.org. Retrieved 13 September 2013.\nJump up ^ \"Concatenative language\". Concatenative.org. Retrieved 13 September 2013.\nJump up ^ \"Rationale for Joy, a functional language\". Archived from the original on 2011-01-15.\n^ Jump up to: a b \"Why Concatenative Programming Matters\". Retrieved 13 September 2013.\nJump up ^ \"von Thun, Manfred: Joy compared with other functional languages\". Archived from the original on 2011-10-06.\nJump up ^ \"von Thun, Manfred: Mathematical foundations of Joy\". Archived from the original on 2010-07-31.\nJump up ^ \"Henry Baker: Linear Logic and Permutation Stacks — The Forth Shall Be First\". Home.pipeline.com. Retrieved 2013-07-01.\nJump up ^ \"The Concatenative Language XY\". Nsl.com. Retrieved 2013-07-01.\nJump up ^ \"The Enchilada Programming Language\". Enchiladacode.nl. Retrieved 2013-07-01.\nJump up ^ \"The Om Programming Language\". Om-language.org. Retrieved 2013-07-01.\nJump up ^ \"Cat Specification\". Cat-language.com. Archived from the original on 2015-02-05. Retrieved 2013-07-01.",
          "subparadigms": []
        },
        {
          "pdid": 55,
          "name": "Tacit",
          "details": "Tacit programming, also called point-free style, is a programming paradigm in which function definitions do not identify the arguments (or \"points\") on which they operate. Instead the definitions merely compose other functions, among which are combinators that manipulate the arguments. Tacit programming is of theoretical interest, because the strict use of composition results in programs that are well adapted for equational reasoning.[1] It is also the natural style of certain programming languages, including APL and its derivatives,[2] and concatenative languages such as Forth. Despite this base, the lack of argument naming gives point-free style a reputation of being unnecessarily obscure, hence the epithet \"pointless style.\"[1]\n\nUNIX scripting uses the paradigm with pipes.\n\nFor example, a sequence of operations in an applicative language like the following:\n\ndef example(x):\n  y = foo(x)\n  z = bar(y)\n  w = baz(z)\n  return w\n...is written in point-free style as the composition of a sequence of functions, without parameters:[3]\n\ndef example: baz bar foo\nThe key idea in tacit programming is to assist in operating at the appropriate level of abstraction. That is, to translate the natural transformation given by currying\n\n{\\displaystyle \\hom(A\\times B,C)\\cong \\hom(B,C^{A})} \\hom(A\\times B,C)\\cong \\hom(B,C^{A})\ninto computer functions, where the left represents the uncurried form of a function and the right the curried. CA denotes the functionals from A to C, while A × B denotes the Cartesian product of A and B.\n\nContents  [hide] \n1\tExamples\n1.1\tFunctional programming\n1.2\tAPL family\n1.3\tStack-based\n1.4\tUNIX pipeline\n2\tSee also\n3\tReferences\n4\tExternal links\nExamples[edit]\nFunctional programming[edit]\nA simple example (in Haskell) is a program which takes a sum of a list. A programmer might define a sum recursively using a pointed (cf. value-level programming) method as:\n\nsum (x:xs) = x + sum xs\nsum [] = 0\nHowever, by noting this as a fold the programmer could replace this with:\n\nsum xs = foldr (+) 0 xs\nAnd then the argument is not needed, so this can be replaced with\n\nsum = foldr (+) 0\nwhich is point-free.\n\nAnother example uses the dot operator:\n\np x y z = f (g x y) z\nThe following Haskell-like pseudo-code exposes how to reduce a function definition to its point-free equivalent:\n\np = \\x -> \\y -> \\z -> f (g x y) z\n  = \\x -> \\y -> f (g x y)\n  = \\x -> \\y -> (f . (g x)) y\n  = \\x -> f . (g x)\n  = \\x -> ((.) f) (g x)\n  = ((.) f) . g\nso\n\np = ((.) f) . g\nFinally, to see a complex example imagine a map filter program which takes a list, applies a function to it, and then filters the elements based on a criterion\n\nmf criteria operator list = filter criteria (map operator list)\nIt can be expressed point-free[4] as\n\nmf = (. map) . (.) . filter\nNote that, as stated previously, the points in 'point-free' refer to the arguments, not to the use of dots; a common misconception.[5]\n\nAPL family[edit]\nIn J, the same sort of point-free code occurs in a function made to compute the average of a list (array) of numbers:\n\navg=: +/ % #\n+/ sums the items of the array by mapping (/) summation (+) to the array. % divides the sum by the number of elements (#) in the array.\n\nStack-based[edit]\nIn stack-oriented programming languages (and concatenative ones, most of which are stack based), point-free methods are commonly used. For example, a procedure to compute the Fibonacci numbers might look like:\n\n /fib\n {\n    dup dup 1 eq exch 0 eq or not\n    {\n       dup 1 sub fib\n       exch 2 sub fib\n       add\n    } if\n } def\nUNIX pipeline[edit]\nMain article: Pipeline (Unix)\nIn UNIX scripting the functions are computer programs which receive data from standard input and send the results to standard output. For example,\n\nsort | uniq -c | sort -rn\nis a tacit or point-free composition which returns the counts of its arguments and the arguments, in the order of decreasing counts. The 'sort' and 'uniq' are the functions, the '-c' and '-rn' control the functions, but the arguments are not mentioned. The '|' is the composition operator.\n\nSee also[edit]\nCombinatory logic\nConcatenative programming language\nFunction-level programming\nJoy (programming language), modern highly tacit language\nPointless topology",
          "subparadigms": [
            54
          ]
        },
        {
          "pdid": 56,
          "name": "Semantic",
          "details": "Semantic-oriented programming (SOP) is a programming paradigm in which the programmer formulizes the logic of a domain by means of semantic structures. Similar to Concept programming and Concept-oriented programming.\n\nContents  [hide] \n1\tCommon features\n2\tGoals\n3\tSOPlets\n4\tSymADE\n5\tSee also\n6\tExternal links\nCommon features[edit]\nThe way of how these semantic information are represented in the system vary according to the approach chosen (see below), common to these approaches are the following features:\n\nThe semantics represent static facts, that is: facts that describe the domain in question at a given moment, and which do not change during runtime (as opposed to Semantic Web for instance)\nThe system has native access to these semantic structures during compile time and runtime, and can interpret them in order to fulfill the requested features\nClear separation from logic and implementation (where possible)\nIn many cases, SOP supports the notion of Single Source of Truth (SSoT), such that every semantic concept is stored exactly once, Any possible linkages to this concept are by reference only\nA programmer can freely and quickly add new semantic meanings without breaking compatibility with the system environment\nGoals[edit]\nThe goals of SOP are:\n\nImproving the maintainability of a software\nImproving the transparency of a software\nFlexibility by allowing exchangeability of logic or implementation\nOptimal support for agile development processes (refactoring)\nSOPlets[edit]\nSoplets is a method of describing semantic concepts as blocks of code, using existing features of the (Java) language, namely annotations and enumerations. Each block of code (called Soplet) represents all properties and features of a given concept (as far as reasonable and feasible), including features outside of the traditional modelling scope, such as translations, documentation, requirement tracking and so on.\n\nSoplets can be referenced and used from anywhere inside the code. Given the strong-typed nature of the references they can be safely refactored as seen fit.\n\nA Soplet may be enhanced by one or more (stateless) functions, which are directly attached to the code block. That way also related features related to a given concept (such as calculations, validation, transformations etc.) beyond pure key-value pairs may be associated with a given Soplet.\n\nThe structure of a Soplet is formally defined by the stereotype it implements. This stereotype may be individually composed of several aspects (such as Translatable, Beanable, Bindable, Testable etc.), which in turn may be freely defined by the developer (or which are part of a framework which he uses).\n\nAn open-source plugin (based on the Project Lombok plugin) allows the creation of byte-code during compile-time, based on the information contained in the Soplets. For instance, a data bean may have all of its attributes, getters and setters generated.\n\nSymADE[edit]\nSymADE (Symbolic Adaptable Development Environment) is an open-source IDE and implementation of SOP (Semantic-oriented programming) paradigm.\n\nIn SymADE a program is edited and stored as a tree of semantic nodes (meanings). The tree is edited by structural editor, and programmers can edit either the semantic tree directly or a projection of the semantic tree onto syntax tree. There may be multiple projections of the same tree, and they can be rendered on the screen as reach text, as UML diagrams and so on.\n\nSemantic meanings are completely user-defined. This allows to use SymADE for creating and editing new domain-specific languages, modify existing languages, use in the same piece of code a mix of multiple languages.\n\nSymADE is common in spirit with IP (Intentional Programming)and JetBrains MPS. The main difference is that they define and edit syntax trees, but in SymADE you create and edit semantic trees. This gives an unbound possibility for automating code writing, i.e. the actual code can be written by computer based on dialog interaction with programmers. And of cause, the SymADE project is open-source, unlike proprietary IP and MPS development environments.\n\nThe higher automation of code writing will allow to create more complex programs without increasing the amount of abstraction layers - because the computer, not programmers, will take care of the code complexity. This will allow to write more complex programs without increasing resource requirements (CPU speed and memory size).\n\nSee also[edit]\nModel-driven engineering\nDomain-specific languages\nService-oriented programming\nLanguage-oriented programming\nAspect-oriented programming\nGenerative programming\nIntentional programming\nAutomatic programming\nResource-oriented architecture\nTransaction-level modeling\nConcept programming",
          "subparadigms": []
        },
        {
          "pdid": 57,
          "name": "Aspect-oriented",
          "details": "In computing, aspect-oriented programming (AOP) is a programming paradigm that aims to increase modularity by allowing the separation of cross-cutting concerns. It does so by adding additional behavior to existing code (an advice) without modifying the code itself, instead separately specifying which code is modified via a \"pointcut\" specification, such as \"log all function calls when the function's name begins with 'set'\". This allows behaviors that are not central to the business logic (such as logging) to be added to a program without cluttering the code core to the functionality. AOP forms a basis for aspect-oriented software development.\n\nAOP includes programming methods and tools that support the modularization of concerns at the level of the source code, while \"aspect-oriented software development\" refers to a whole engineering discipline.\n\nAspect-oriented programming entails breaking down program logic into distinct parts (so-called concerns, cohesive areas of functionality). Nearly all programming paradigms support some level of grouping and encapsulation of concerns into separate, independent entities by providing abstractions (e.g., functions, procedures, modules, classes, methods) that can be used for implementing, abstracting and composing these concerns. Some concerns \"cut across\" multiple abstractions in a program, and defy these forms of implementation. These concerns are called cross-cutting concerns or horizontal concerns.\n\nLogging exemplifies a crosscutting concern because a logging strategy necessarily affects every logged part of the system. Logging thereby crosscuts all logged classes and methods.\n\nAll AOP implementations have some crosscutting expressions that encapsulate each concern in one place. The difference between implementations lies in the power, safety, and usability of the constructs provided. For example, interceptors that specify the methods to intercept express a limited form of crosscutting, without much support for type-safety or debugging. AspectJ has a number of such expressions and encapsulates them in a special class, an aspect. For example, an aspect can alter the behavior of the base code (the non-aspect part of a program) by applying advice (additional behavior) at various join points (points in a program) specified in a quantification or query called a pointcut (that detects whether a given join point matches). An aspect can also make binary-compatible structural changes to other classes, like adding members or parents.\n\nContents  [hide] \n1\tHistory\n2\tMotivation and basic concepts\n3\tJoin point models\n3.1\tAspectJ's join-point model\n3.2\tOther potential join point models\n3.3\tInter-type declarations\n4\tImplementation\n4.1\tTerminology\n5\tComparison to other programming paradigms\n6\tAdoption issues\n7\tCriticism\n8\tImplementations\n9\tSee also\n10\tNotes and references\n11\tFurther reading\n12\tExternal links\nHistory[edit]\nAOP has several direct antecedents A1 and A2:[1] reflection and metaobject protocols, subject-oriented programming, Composition Filters and Adaptive Programming.[2]\n\nGregor Kiczales and colleagues at Xerox PARC developed the explicit concept of AOP, and followed this with the AspectJ AOP extension to Java. IBM's research team pursued a tool approach over a language design approach and in 2001 proposed Hyper/J and the Concern Manipulation Environment, which have not seen wide usage. The examples in this article use AspectJ as it is the most widely known AOP language.[citation needed]\n\nThe Microsoft Transaction Server is considered to be the first major application of AOP followed by Enterprise JavaBeans.[3][4]\n\nMotivation and basic concepts[edit]\nTypically, an aspect is scattered or tangled as code, making it harder to understand and maintain. It is scattered by virtue of the function (such as logging) being spread over a number of unrelated functions that might use its function, possibly in entirely unrelated systems, different source languages, etc. That means to change logging can require modifying all affected modules. Aspects become tangled not only with the mainline function of the systems in which they are expressed but also with each other. That means changing one concern entails understanding all the tangled concerns or having some means by which the effect of changes can be inferred.\n\nFor example, consider a banking application with a conceptually very simple method for transferring an amount from one account to another:[5]\n\nvoid transfer(Account fromAcc, Account toAcc, int amount) throws Exception {\n  if (fromAcc.getBalance() < amount)\n      throw new InsufficientFundsException();\n\n  fromAcc.withdraw(amount);\n  toAcc.deposit(amount);\n}\nHowever, this transfer method overlooks certain considerations that a deployed application would require: it lacks security checks to verify that the current user has the authorization to perform this operation; a database transaction should encapsulate the operation in order to prevent accidental data loss; for diagnostics, the operation should be logged to the system log, etc.\n\nA version with all those new concerns, for the sake of example, could look somewhat like this:\n\nvoid transfer(Account fromAcc, Account toAcc, int amount, User user,\n    Logger logger, Database database) throws Exception {\n  logger.info(\"Transferring money...\");\n  \n  if (!isUserAuthorised(user, fromAcc)) {\n    logger.info(\"User has no permission.\");\n    throw new UnauthorisedUserException();\n  }\n  \n  if (fromAcc.getBalance() < amount) {\n    logger.info(\"Insufficient funds.\");\n    throw new InsufficientFundsException();\n  }\n\n  fromAcc.withdraw(amount);\n  toAcc.deposit(amount);\n\n  database.commitChanges();  // Atomic operation.\n\n  logger.info(\"Transaction successful.\");\n}\nIn this example other interests have become tangled with the basic functionality (sometimes called the business logic concern). Transactions, security, and logging all exemplify cross-cutting concerns.\n\nNow consider what happens if we suddenly need to change (for example) the security considerations for the application. In the program's current version, security-related operations appear scattered across numerous methods, and such a change would require a major effort.\n\nAOP attempts to solve this problem by allowing the programmer to express cross-cutting concerns in stand-alone modules called aspects. Aspects can contain advice (code joined to specified points in the program) and inter-type declarations (structural members added to other classes). For example, a security module can include advice that performs a security check before accessing a bank account. The pointcut defines the times (join points) when one can access a bank account, and the code in the advice body defines how the security check is implemented. That way, both the check and the places can be maintained in one place. Further, a good pointcut can anticipate later program changes, so if another developer creates a new method to access the bank account, the advice will apply to the new method when it executes.\n\nSo for the above example implementing logging in an aspect:\n\naspect Logger {\n  void Bank.transfer(Account fromAcc, Account toAcc, int amount, User user, Logger logger)  {\n    logger.info(\"Transferring money...\");\n  }\n\n  void Bank.getMoneyBack(User user, int transactionId, Logger logger)  {\n    logger.info(\"User requested money back.\");\n  }\n\n  // Other crosscutting code.\n}\nOne can think of AOP as a debugging tool or as a user-level tool. Advice should be reserved for the cases where you cannot get the function changed (user level)[6] or do not want to change the function in production code (debugging).\n\nJoin point models[edit]\nThe advice-related component of an aspect-oriented language defines a join point model (JPM). A JPM defines three things:\n\nWhen the advice can run. These are called join points because they are points in a running program where additional behavior can be usefully joined. A join point needs to be addressable and understandable by an ordinary programmer to be useful. It should also be stable across inconsequential program changes in order for an aspect to be stable across such changes. Many AOP implementations support method executions and field references as join points.\nA way to specify (or quantify) join points, called pointcuts. Pointcuts determine whether a given join point matches. Most useful pointcut languages use a syntax like the base language (for example, AspectJ uses Java signatures) and allow reuse through naming and combination.\nA means of specifying code to run at a join point. AspectJ calls this advice, and can run it before, after, and around join points. Some implementations also support things like defining a method in an aspect on another class.\nJoin-point models can be compared based on the join points exposed, how join points are specified, the operations permitted at the join points, and the structural enhancements that can be expressed.\n\nAspectJ's join-point model[edit]\nMain article: AspectJ\nThe join points in AspectJ include method or constructor call or execution, the initialization of a class or object, field read and write access, exception handlers, etc. They do not include loops, super calls, throws clauses, multiple statements, etc.\nPointcuts are specified by combinations of primitive pointcut designators (PCDs).\n\"Kinded\" PCDs match a particular kind of join point (e.g., method execution) and tend to take as input a Java-like signature. One such pointcut looks like this:\n execution(* set*(*))\nThis pointcut matches a method-execution join point, if the method name starts with \"set\" and there is exactly one argument of any type.\n\"Dynamic\" PCDs check runtime types and bind variables. For example,\n\n  this(Point)\nThis pointcut matches when the currently executing object is an instance of class Point. Note that the unqualified name of a class can be used via Java's normal type lookup.\n\"Scope\" PCDs limit the lexical scope of the join point. For example:\n\n within(com.company.*)\nThis pointcut matches any join point in any type in the com.company package. The * is one form of the wildcards that can be used to match many things with one signature.\nPointcuts can be composed and named for reuse. For example:\n\n pointcut set() : execution(* set*(*) ) && this(Point) && within(com.company.*);\nThis pointcut matches a method-execution join point, if the method name starts with \"set\" and this is an instance of type Point in the com.company package. It can be referred to using the name \"set()\".\nAdvice specifies to run at (before, after, or around) a join point (specified with a pointcut) certain code (specified like code in a method). The AOP runtime invokes Advice automatically when the pointcut matches the join point. For example:\nafter() : set() {\n   Display.update();\n}\nThis effectively specifies: \"if the set() pointcut matches the join point, run the code Display.update() after the join point completes.\"\nOther potential join point models[edit]\nThere are other kinds of JPMs. All advice languages can be defined in terms of their JPM. For example, a hypothetical aspect language for UML may have the following JPM:\n\nJoin points are all model elements.\nPointcuts are some boolean expression combining the model elements.\nThe means of affect at these points are a visualization of all the matched join points.\nInter-type declarations[edit]\nInter-type declarations provide a way to express crosscutting concerns affecting the structure of modules. Also known as open classes and extension methods, this enables programmers to declare in one place members or parents of another class, typically in order to combine all the code related to a concern in one aspect. For example, if a programmer implemented the crosscutting display-update concern using visitors instead, an inter-type declaration using the visitor pattern might look like this in AspectJ:\n\n  aspect DisplayUpdate {\n    void Point.acceptVisitor(Visitor v) {\n      v.visit(this);\n    }\n    // other crosscutting code...\n  }\nThis code snippet adds the acceptVisitor method to the Point class.\n\nIt is a requirement that any structural additions be compatible with the original class, so that clients of the existing class continue to operate, unless the AOP implementation can expect to control all clients at all times.\n\nImplementation[edit]\nAOP programs can affect other programs in two different ways, depending on the underlying languages and environments:\n\na combined program is produced, valid in the original language and indistinguishable from an ordinary program to the ultimate interpreter\nthe ultimate interpreter or environment is updated to understand and implement AOP features.\nThe difficulty of changing environments means most implementations produce compatible combination programs through a process known as weaving - a special case of program transformation. An aspect weaver reads the aspect-oriented code and generates appropriate object-oriented code with the aspects integrated. The same AOP language can be implemented through a variety of weaving methods, so the semantics of a language should never be understood in terms of the weaving implementation. Only the speed of an implementation and its ease of deployment are affected by which method of combination is used.\n\nSystems can implement source-level weaving using preprocessors (as C++ was implemented originally in CFront) that require access to program source files. However, Java's well-defined binary form enables bytecode weavers to work with any Java program in .class-file form. Bytecode weavers can be deployed during the build process or, if the weave model is per-class, during class loading. AspectJ started with source-level weaving in 2001, delivered a per-class bytecode weaver in 2002, and offered advanced load-time support after the integration of AspectWerkz in 2005.\n\nAny solution that combines programs at runtime has to provide views that segregate them properly to maintain the programmer's segregated model. Java's bytecode support for multiple source files enables any debugger to step through a properly woven .class file in a source editor. However, some third-party decompilers cannot process woven code because they expect code produced by Javac rather than all supported bytecode forms (see also \"Criticism\", below).\n\nDeploy-time weaving offers another approach.[7] This basically implies post-processing, but rather than patching the generated code, this weaving approach subclasses existing classes so that the modifications are introduced by method-overriding. The existing classes remain untouched, even at runtime, and all existing tools (debuggers, profilers, etc.) can be used during development. A similar approach has already proven itself in the implementation of many Java EE application servers, such as IBM's WebSphere.\n\nTerminology[edit]\nStandard terminology used in Aspect-oriented programming may include:\n\nCross-cutting concerns\nMain article: Cross-cutting concern\nEven though most classes in an OO model will perform a single, specific function, they often share common, secondary requirements with other classes. For example, we may want to add logging to classes within the data-access layer and also to classes in the UI layer whenever a thread enters or exits a method. Further concerns can be related to security such as access control [8] or information flow control.[9] Even though each class has a very different primary functionality, the code needed to perform the secondary functionality is often identical.\nAdvice\nMain article: Advice (programming)\nThis is the additional code that you want to apply to your existing model. In our example, this is the logging code that we want to apply whenever the thread enters or exits a method.\nPointcut\nMain article: Pointcut\nThis is the term given to the point of execution in the application at which cross-cutting concern needs to be applied. In our example, a pointcut is reached when the thread enters a method, and another pointcut is reached when the thread exits the method.\nAspect\nMain article: Aspect (computer science)\nThe combination of the pointcut and the advice is termed an aspect. In the example above, we add a logging aspect to our application by defining a pointcut and giving the correct advice.\nComparison to other programming paradigms[edit]\nAspects emerged from object-oriented programming and computational reflection. AOP languages have functionality similar to, but more restricted than metaobject protocols. Aspects relate closely to programming concepts like subjects, mixins, and delegation. Other ways to use aspect-oriented programming paradigms include Composition Filters and the hyperslices approach. Since at least the 1970s, developers have been using forms of interception and dispatch-patching that resemble some of the implementation methods for AOP, but these never had the semantics that the crosscutting specifications provide written in one place.\n\nDesigners have considered alternative ways to achieve separation of code, such as C#'s partial types, but such approaches lack a quantification mechanism that allows reaching several join points of the code with one declarative statement.\n\nThough it may seem unrelated, in testing, the use of mocks or stubs requires the use of AOP techniques, like around advice, and so forth. Here the collaborating objects are for the purpose of the test, a cross cutting concern. Thus the various Mock Object frameworks provide these features. For example, a process invokes a service to get a balance amount. In the test of the process, where the amount comes from is unimportant, only that the process uses the balance according to the requirements.\n\nAdoption issues[edit]\nProgrammers need to be able to read code and understand what is happening in order to prevent errors.[10] Even with proper education, understanding crosscutting concerns can be difficult without proper support for visualizing both static structure and the dynamic flow of a program.[11] Beginning in 2002, AspectJ began to provide IDE plug-ins to support the visualizing of crosscutting concerns. Those features, as well as aspect code assist and refactoring are now common.\n\nGiven the power of AOP, if a programmer makes a logical mistake in expressing crosscutting, it can lead to widespread program failure. Conversely, another programmer may change the join points in a program – e.g., by renaming or moving methods – in ways that the aspect writer did not anticipate, with unforeseen consequences. One advantage of modularizing crosscutting concerns is enabling one programmer to affect the entire system easily; as a result, such problems present as a conflict over responsibility between two or more developers for a given failure. However, the solution for these problems can be much easier in the presence of AOP, since only the aspect needs to be changed, whereas the corresponding problems without AOP can be much more spread out.\n\nCriticism[edit]\nThe most basic criticism of the effect of AOP is that control flow is obscured, and is not only worse than the much-maligned GOTO, but is in fact closely analogous to the joke COME FROM statement. The obliviousness of application, which is fundamental to many definitions of AOP (the code in question has no indication that an advice will be applied, which is specified instead in the pointcut), means that the advice is not visible, in contrast to an explicit method call.[12][13] For example, compare the COME FROM program:[12]\n\n 5 input x\n10 print 'result is :'\n15 print x\n\n20 come from 10\n25      x = x * x\n30 return\nwith an AOP fragment with analogous semantics:\n\nmain() {\n    input x\n    print(result(x))\n}\ninput result(int x) { return x }\naround(int x): call(result(int)) && args(x) {\n    int temp = proceed(x)\n//    return temp * temp\n}\nIndeed, the pointcut may depend on runtime condition and thus not be statically deterministic. This can be mitigated but not solved by static analysis and IDE support showing which advices potentially match.\n\nGeneral criticisms are that AOP purports to improve \"both modularity and the structure of code\", but some counter that it instead undermines these goals and impedes \"independent development and understandability of programs\".[14] Specifically, quantification by pointcuts breaks modularity: \"one must, in general, have whole-program knowledge to reason about the dynamic execution of an aspect-oriented program.\"[15] Further, while its goals (modularizing cross-cutting concerns) are well-understood, its actual definition is unclear and not clearly distinguished from other well-established techniques.[14] Cross-cutting concerns potentially cross-cut each other, requiring some resolution mechanism, such as ordering.[14] Indeed, aspects can apply to themselves, leading to problems such as the liar paradox.[16]\n\nTechnical criticisms include that the quantification of pointcuts (defining where advices are executed) is \"extremely sensitive to changes in the program\", which is known as the fragile pointcut problem.[14] The problems with pointcuts are deemed intractable: if one replaces the quantification of pointcuts with explicit annotations, one obtains attribute-oriented programming instead, which is simply an explicit subroutine call and suffers the identical problem of scattering that AOP was designed to solve.[14]\n\nImplementations[edit]\nThe following programming languages have implemented AOP, within the language, or as an external library:\n\n.NET Framework languages (C# / VB.NET)[17]\nUnity, It provides an API to facilitate proven practices in core areas of programming including data access, security, logging, exception handling and others.\nActionScript[18]\nAda[19]\nAutoHotkey[20]\nC / C++[21]\nCOBOL[22]\nThe Cocoa Objective-C frameworks[23]\nColdFusion[24]\nCommon Lisp[25]\nDelphi[26][27][28]\nDelphi Prism[29]\ne (IEEE 1647)\nEmacs Lisp[30]\nGroovy\nHaskell[31]\nJava[32]\nAspectJ\nJavaScript[33]\nLogtalk[34]\nLua[35]\nmake[36]\nMatlab[37]\nML[38]\nPerl[39]\nPHP[40]\nProlog[41]\nPython[42]\nRacket[43]\nRuby[44][45][46]\nSqueak Smalltalk[47][48]\nUML 2.0[49]\nXML[50]\nSee also[edit]\nAspect-oriented software development\nDistributed AOP\nAttribute grammar, a formalism that can be used for aspect-oriented programming on top of functional programming languages\nProgramming paradigms\nSubject-oriented programming, an alternative to Aspect-oriented programming\nRole-oriented programming, an alternative to Aspect-oriented programming\nPredicate dispatch, an older alternative to Aspect-oriented programming\nExecutable UML\nDecorator pattern\nDomain-driven design",
          "subparadigms": []
        },
        {
          "pdid": 58,
          "name": "Role-oriented",
          "details": "Role-oriented programming is a form of computer programming aimed at expressing things in terms that are analogous to human conceptual understanding of the World. This should make programs easier to understand and maintain.[citation needed]\n\nThe main idea of role-oriented programming is that humans think in terms of roles. This claim is often backed up by examples of social relations. For example, a student attending a class and the same student at a party are the same person, yet he plays two different roles. In particular, the interactions of this person with the outside world depend on his current role. The roles typically share features, e.g., the intrinsic properties of being a person. This sharing of properties is often handled by the delegation mechanism.\n\nIn the older literature and in the field of databases, it seems that there has been little consideration for the context in which roles interplay with each other. Such a context is being established in newer role- and aspect-oriented programming languages such as Object Teams.\n\nMany researchers have argued the advantages of roles in modeling and implementation. Roles allow objects to evolve over time, they enable independent and concurrently existing views (interfaces) of the object, explicating the different contexts of the object, and separating concerns. Generally roles are a natural element of our daily concept forming. Roles in programming languages enable objects to have changing interfaces, as we see it in real life - things change over time, are used differently in different contexts, etc.\n\nContents  [hide] \n1\tAuthors of role literature\n2\tProgramming languages with explicit support for roles\n3\tSee also\n4\tReferences\n5\tExternal links\nAuthors of role literature[edit]\nBarbara Pernici\nBent Bruun Kristensen[1]\nBruce Wallace\nCharles Bachman[2]\nFriedrich Steimann\nGeorg Gottlob\nKasper B. Graversen\nKasper Østerbye\nStephan Herrmann\nTrygve Reenskaug[3]\nThomas Kühn\nProgramming languages with explicit support for roles[edit]\nChameleon\nEpsilonJ\nJavaScript Delegation - Functions as Roles (Traits and Mixins)\nObject Teams\nPerl 5 (Moose)\nPerl 6\npowerJava\nSCala ROLes Language\nSee also[edit]\nAspect-oriented programming\nData, context and interaction\nObject Oriented Role Analysis Method\nObject-role modeling\nSubject (programming)\nSubject-oriented programming\nTraits (computer science)",
          "subparadigms": []
        },
        {
          "pdid": 59,
          "name": "Subject-oriented",
          "details": "In computing, Subject-Oriented Programming is an object-oriented software paradigm in which the state (fields) and behavior (methods) of objects are not seen as intrinsic to the objects themselves, but are provided by various subjective perceptions (“subjects”) of the objects. The term and concepts were first published in September 1993 in a conference paper[1] which was later recognized as being one of the three most influential papers to be presented at the conference between 1986 and 1996.[2] As illustrated in that paper, an analogy is made with the contrast between the philosophical views of Plato and Kant with respect to the characteristics of “real” objects, but applied to software ones. For example, while we may all perceive a tree as having a measurable height, weight, leaf-mass, etc., from the point of view of a bird, a tree may also have measures of relative value for food or nesting purposes, or from the point of view of a tax-assessor, it may have a certain taxable value in a given year. Neither the bird’s nor the tax-assessor’s additional state information need be seen as intrinsic to the tree, but are added by the perceptions of the bird and tax-assessor, and from Kant’s analysis, the same may be true even of characteristics we think of as intrinsic.\n\nSubject-oriented programming advocates the organization of the classes that describe objects into “subjects”, which may be composed to form larger subjects. At points of access to fields or methods, several subjects’ contributions may be composed. These points were characterized as the join-points[3] of the subjects. For example, if a tree is cut-down, the methods involved may need to join behavior in the bird and tax-assessor’s subjects with that of the tree’s own. It is therefore fundamentally a view of the compositional nature of software development, as opposed to the algorithmic (procedural) or representation-hiding (object) nature.\n\nContents  [hide] \n1\tExamples\n2\tRelationship to aspect-oriented programming\n3\tRelationship to aspect-oriented software development\n4\tMulti-dimensional separation of concerns, Hyper/J, and the Concern Manipulation Environment\n5\tSubject-oriented programming as a \"third dimension\"\n6\tSee also\n7\tReferences\n8\tExternal links\nExamples[edit]\n[icon]\tThis section is empty. You can help by adding to it. (March 2012)\nRelationship to aspect-oriented programming[edit]\nThe introduction of aspect-oriented programming in 1997[4] raised questions about its relationship to subject-oriented programming, and about the difference between subjects and aspects. These questions were unanswered for some time, but were addressed in the patent on Aspect-oriented programming filed in 1999[5] in which two points emerge as characteristic differences from earlier art:\n\nthe aspect program comprises both a) a cross-cut that comprises a point in the execution where cross-cutting behavior is to be included; and b) a cross-cut action comprising a piece of implementation associated with the cross-cut, the piece of implementation comprising computer readable program code that implements the cross-cutting behavior.\nthe aspect transparently forces the cross-cutting behavior on object classes and other software entities\nIn the subject-oriented view, the cross-cut may be placed separately from the aspect (subject) and the behavior is not forced by the aspect, but governed by rules of composition. Hindsight[6] makes it also possible to distinguish aspect-oriented programming by its introduction and exploitation of the concept of a query-like pointcut to externally impose the join-points used by aspects in general ways.\n\nIn the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension. The use of externally imposed pointcuts is an important linguistic capability, but remains one of the most controversial features of aspect-oriented programming.[7]\n\nRelationship to aspect-oriented software development[edit]\nBy the turn of the millennium, it was clear that a number of research groups were pursuing different technologies that employed the composition or attachment of separately packaged state and function to form objects.[8] To distinguish the common field of interest from Aspect-Oriented Programming with its particular patent definitions and to emphasize that the compositional technology deals with more than just the coding phase of software development, these technologies were organized together under the term Aspect-Oriented Software Development,[9] and an organization and series on international conferences begun on the subject. Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches.\n\nMulti-dimensional separation of concerns, Hyper/J, and the Concern Manipulation Environment[edit]\nThe original formulation of subject-oriented programming deliberately envisioned it as a packaging technology – allowing the space of functions and data types to be extended in either dimension. The first implementations had been for C++,[10] and Smalltalk.[11] These implementations exploited the concepts of software labels and composition rules to describe the joining of subjects.\n\nTo address the concern that a better foundation should be provided for the analysis and composition of software not just in terms of its packaging but in terms of the various concerns these packages addressed, an explicit organization of the material was developed in terms of a multi-dimensional “matrix” in which concerns are related to the software units that implement them. This organization is called Multi-Dimensional Separation of Concerns, and the paper describing it[12] has been recognized as the most influential paper of the ICSE 1999 Conference[13]\n\nThis new concept was implemented for composing Java software, using the name Hyper/J for the tool.[14]\n\nComposition and the concept of subject can be applied to software artifacts that have no executable semantics, like requirement specifications or documentation. A research vehicle for Eclipse, called the Concern Manipulation Environment (CME), has been described[15] in which tools for query, analysis, modelling,[16] and composition are applied to artifacts in any language or representation, through the use of appropriate plug-in adapters to manipulate the representation.\n\nA successor to the Hyper/J composition engine[17] was developed as part of CME which uses a general approach for the several elements of a composition engine:\n\na query language with unification to identify join points,\na flexible structural-attachment model,\na nested-graph specification for ordering identified elements,\nand a priority ordering specification to resolve conflicts among conflicting rules.\nBoth Hyper/J and CME are available, from alphaWorks[18] or sourceforge,[19] respectively, but neither is actively supported.\n\nSubject-oriented programming as a \"third dimension\"[edit]\nMethod dispatch in object oriented programming can be thought of as \"two dimensional\" in the sense that the code executed depends on both the method name and the object in question. This can be contrasted[20] with procedural programming, where a procedure name resolves directly, or one dimensionally, onto a subroutine, and also to subject oriented programming, where the sender or subject is also relevant to dispatch, constituting a third dimension.\n\nSee also[edit]\nSeparation of concerns\nData, context and interaction",
          "subparadigms": []
        },
        {
          "pdid": 60,
          "name": "Prototype-based",
          "details": "Prototype-based programming is a style of object-oriented programming in which behaviour reuse (known as inheritance) is performed via a process of reusing existing objects via delegation that serve as prototypes. This model can also be known as prototypal, prototype-oriented, classless, or instance-based programming. Delegation is the language feature that supports prototype-based programming.\n\nA fruit bowl serves as one example. A \"fruit\" object would represent the properties and functionality of fruit in general. A \"banana\" object would be cloned from the \"fruit\" object, and would also be extended to include general properties specific to bananas. Each individual \"banana\" object would be cloned from the generic \"banana\" object.\n\nThe first prototype-oriented programming language was Self, developed by David Ungar and Randall Smith in the mid-1980s to research topics in object-oriented language design. Since the late 1990s, the classless paradigm has grown increasingly popular.[citation needed] Some current prototype-oriented languages are JavaScript (and other ECMAScript implementations, JScript and Flash's ActionScript 1.0), Lua, Cecil, NewtonScript, Io, Ioke, MOO, REBOL, Lisaac and AHk.\n\nContents  [hide] \n1\tDesign and implementation\n2\tObject construction\n3\tDelegation\n4\tConcatenation\n5\tCriticism\n6\tLanguages supporting prototype-based programming\n7\tSee also\n8\tReferences\n9\tFurther reading\nDesign and implementation[edit]\nPrototypal inheritance in JavaScript is described by Douglas Crockford as: you make prototype objects, and then … make new instances. Objects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects. We don't need classes to make lots of similar objects… Objects inherit from objects. What could be more object oriented than that?.[1]\n\nAdvocates of prototype-based programming argue that it encourages the programmer to focus on the behavior of some set of examples and only later worry about classifying these objects into archetypal objects that are later used in a fashion similar to classes.[2] Many prototype-based systems encourage the alteration of prototypes during run-time, whereas only very few class-based object-oriented systems (such as the dynamic object-oriented system, Common Lisp, Dylan, Objective-C, Perl, Python, Ruby, or Smalltalk) allow classes to be altered during the execution of a program.\n\nAlmost all prototype-based systems are based on interpreted and dynamically typed languages. Systems based on statically typed languages are technically feasible, however. The Omega language discussed in Prototype-Based Programming[3] is an example of such a system, though according to Omega's website even Omega is not exclusively static, but rather its \"compiler may choose to use static binding where this is possible and may improve the efficiency of a program.\"\n\nObject construction[edit]\nIn prototype-based languages there are no explicit classes. Objects inherit directly from other objects through a prototype property. The prototype property is called prototype in Self, proto in Io and __proto__ in JavaScript. There are two methods of constructing new objects: ex nihilo (\"from nothing\") object creation or through cloning an existing object. The former is supported through some form of object literal, declarations where objects can be defined at runtime through special syntax such as {...} and passed directly to a variable. While most systems support a variety of cloning, ex nihilo object creation is not as prominent.[4]\n\nIn class-based languages, a new instance is constructed through a class's constructor function, a special function that reserves a block of memory for the object's members (properties and methods) and returns a reference to that block. An optional set of constructor arguments can be passed to the function and are usually held in properties. The resulting instance will inherit all the methods and properties that were defined in the class, which acts as a kind of template from which similar typed objects can be constructed.\n\nSystems that support ex nihilo object creation allow new objects to be created from scratch without cloning from an existing prototype. Such systems provide a special syntax for specifying the properties and behaviors of new objects without referencing existing objects. In many prototype languages there exists a root object, often called Object, which is set as the default prototype for all other objects created in run-time and which carries commonly needed methods such as a toString() function to return a description of the object as a string. One useful aspect of ex nihilo object creation is to ensure that a new object's slot (properties and methods) names do not have namespace conflicts with the top-level Object object. (In the JavaScript language, one can do this by using a null prototype, i.e. Object.create(null).)\n\nCloning refers to a process whereby a new object is constructed by copying the behavior of an existing object (its prototype). The new object then carries all the qualities of the original. From this point on, the new object can be modified. In some systems the resulting child object maintains an explicit link (via delegation or resemblance) to its prototype, and changes in the prototype cause corresponding changes to be apparent in its clone. Other systems, such as the Forth-like programming language Kevo, do not propagate change from the prototype in this fashion, and instead follow a more concatenative model where changes in cloned objects do not automatically propagate across descendants.[2]\n\n// Example of true prototypal inheritance style \n// in JavaScript.\n\n// \"ex nihilo\" object creation using the literal \n// object notation {}.\nvar foo = {name: \"foo\", one: 1, two: 2};\n\n// Another \"ex nihilo\" object.\nvar bar = {two: \"two\", three: 3};\n\n// Object.setPrototypeOf() is a method introduced in ECMAScript 2015.\n// For the sake of simplicity, let us pretend \n// that the following line works regardless of the \n// engine used:\nObject.setPrototypeOf(bar, foo); // foo is now the prototype of bar.\n\n// If we try to access foo's properties from bar \n// from now on, we'll succeed. \nbar.one // Resolves to 1.\n\n// The child object's properties are also accessible.\nbar.three // Resolves to 3.\n\n// Own properties shadow prototype properties\nbar.two; // Resolves to \"two\"\nbar.name; // unaffected, resolves to \"foo\"\nfoo.name; // Resolves to \"foo\"\nThis example in JS 1.8.5 + ( see http://kangax.github.com/es5-compat-table/ )\n\nvar foo = {one: 1, two: 2};\n\n// bar.[[prototype]] = foo\nvar bar = Object.create( foo );\n\nbar.three = 3;\n\nbar.one; // 1\nbar.two; // 2\nbar.three; // 3\nDelegation[edit]\nIn prototype-based languages that use delegation, the language runtime is capable of dispatching the correct method or finding the right piece of data simply by following a series of delegation pointers (from object to its prototype) until a match is found. All that is required to establish this behavior-sharing between objects is the delegation pointer. Unlike the relationship between class and instance in class-based object-oriented languages, the relationship between the prototype and its offshoots does not require that the child object have a memory or structural similarity to the prototype beyond this link. As such, the child object can continue to be modified and amended over time without rearranging the structure of its associated prototype as in class-based systems. It is also important to note that not only data, but also methods can be added or changed. For this reason, some prototype-based languages refer to both data and methods as \"slots\" or \"members\".[citation needed]\n\nConcatenation[edit]\nUnder pure prototyping, which is also referred to as concatenative prototyping, and is exemplified in the Kevo language, there are no visible pointers or links to the original prototype from which an object is cloned. The prototype (parent) object is copied rather than linked to. As a result, changes to the prototype will not be reflected in cloned objects.[5]\n\nThe main conceptual difference under this arrangement is that changes made to a prototype object are not automatically propagated to clones. This may be seen as an advantage or disadvantage. (However, Kevo does provide additional primitives for publishing changes across sets of objects based on their similarity — so-called family resemblances or clone family mechanism[5] — rather than through taxonomic origin, as is typical in the delegation model.) It is also sometimes claimed that delegation-based prototyping has an additional disadvantage in that changes to a child object may affect the later operation of the parent. However, this problem is not inherent to the delegation-based model and does not exist in delegation-based languages such as JavaScript, which ensure that changes to a child object are always recorded in the child object itself and never in parents (i.e. the child's value shadows the parent's value rather than changing the parent's value).\n\nIn simplistic implementations, concatenative prototyping will have faster member lookup than delegation-based prototyping (because there is no need to follow the chain of parent objects), but will conversely use more memory (because all slots are copied, rather than there being a single slot pointing to the parent object). More sophisticated implementations can avoid these problems, however, although trade-offs between speed and memory are required. For example, systems with concatenative prototyping can use a copy-on-write implementation to allow for behind-the-scenes data sharing — and such an approach is indeed followed by Kevo.[6] Conversely, systems with delegation-based prototyping can use caching to speed up data lookup..\n\nCriticism[edit]\nAdvocates of class-based object models who criticize prototype-based systems often have concerns similar to the concerns that proponents of static type systems for programming languages have of dynamic type systems (see datatype). Usually, such concerns involve: correctness, safety, predictability, efficiency and programmer unfamiliarity.\n\nOn the first three points, classes are often seen as analogous to types (in most statically typed object-oriented languages they serve that role) and are proposed to provide contractual guarantees to their instances, and to users of their instances, that they will behave in some given fashion.\n\nRegarding efficiency, declaring classes simplifies many compiler optimizations that allow developing efficient method and instance-variable lookup. For the Self language, much development time was spent on developing, compiling, and interpreting techniques to improve the performance of prototype-based systems versus class-based systems.\n\nA common criticism made against prototype-based languages is that the community of software developers is unfamiliar with them, despite the popularity and market permeation of JavaScript. This knowledge level of prototype-based systems seems to be increasing with the proliferation of JavaScript frameworks and the complex use of JavaScript as the Web matures.[citation needed] ECMAScript 6 introduced classes as syntactic sugar over JavaScript's existing prototype-based inheritance, providing an alternative way to create objects and deal with inheritance.[7]\n\nLanguages supporting prototype-based programming[edit]\nActor-Based Concurrent Language (ABCL): ABCL/1, ABCL/R, ABCL/R2, ABCL/c+\nAgora\nAutoHotkey\nCecil and Diesel of Craig Chambers\nColdC\nCOLA\nCommon Lisp\nECMAScript\nActionScript 1.0, used by Adobe Flash and Adobe Flex\nE4X\nJavaScript\nJScript\nFalcon\nIo\nIoke\nLisaac\nLogtalk\nLPC\nLua\nMaple\nMOO\nNeko\nNewtonScript\nObject Lisp\nObliq\nOmega\nOpenLaszlo\nPerl, with the Class::Prototyped module\nPython with prototype.py.\nR, with the proto package\nREBOL\nSelf\nSeph\nSmartFrog\nTADS\nTcl with snit extension\nUmajin[8]",
          "subparadigms": []
        },
        {
          "pdid": 61,
          "name": "Concurrent object-oriented",
          "details": "Concurrent object-oriented programming is a programming paradigm which combines object-oriented programming (OOP) together with concurrency. While numerous programming languages, such as Java, combine OOP with concurrency mechanisms like threads, the phrase \"concurrent object-oriented programming\" primarily refers to systems where objects themselves are a concurrency primitive, such as when objects are combined with the actor model.",
          "subparadigms": []
        },
        {
          "pdid": 62,
          "name": "Class-based",
          "details": "Class-based programming, or more commonly class-orientation, is a style of object-oriented programming (OOP) in which inheritance is achieved by defining classes of objects, as opposed to the objects themselves (compare prototype-based programming).\n\nThe most popular and developed model of OOP is a class-based model, as opposed to an object-based model. In this model, objects are entities that combine state (i.e. data), behavior (i.e. procedures, or methods) and identity (unique existence among all other objects). The structure and behavior of an object are defined by a class, which is a definition, or blueprint, of all objects of a specific type. An object must be explicitly created based on a class and an object thus created is considered to be an instance of that class. An object is similar to a structure, with the addition of method pointers, member access control, and an implicit data member which locates instances of the class (i.e. actual objects of that class) in the class hierarchy (essential for runtime inheritance features).\n\nContents  [hide] \n1\tEncapsulation\n2\tInheritance\n3\tCritique of class-based models\n4\tExample languages\n5\tSee also\n6\tReferences\nEncapsulation[edit]\nEncapsulation prevents users from breaking the invariants of the class, which is useful because it allows the implementation of a class of objects to be changed for aspects not exposed in the interface without impact to user code. The definitions of encapsulation focus on the grouping and packaging of related information (cohesion) rather than security issues. OOP languages do not normally offer formal security restrictions to the internal object state. Using a method of access is a matter of convention for the interface design.\n\nInheritance[edit]\nMain article: Inheritance\nIn class-based programming, inheritance is done by defining new classes as extensions of existing classes: the existing class is the parent class and the new class is the child class. If a child class has only one parent class, this is known as single inheritance, while if a child class can have more than one parent class, this is known as multiple inheritance. This organizes classes into a hierarchy, either a tree (if single inheritance) or lattice (if multiple inheritance).\n\nThe defining feature of inheritance is that both interface and implementation are inherited; if only interface is inherited, this is known as interface inheritance or subtyping. Inheritance can also be done without classes, as in prototype-based programming.\n\nCritique of class-based models[edit]\nClass-based languages, or, to be more precise, typed languages, where subclassing is the only way of subtyping, have been criticized for mixing up implementations and interfaces—the essential principle in object-oriented programming. The critics say one might create a bag class that stores a collection of objects, then extend it to make a new class called a set class where the duplication of objects is eliminated.[1][2] Now, a function that takes an object of the bag class may expect that adding two objects increases the size of a bag by two, yet if one passes an object of a set class, then adding two objects may or may not increase the size of a bag by two. The problem arises precisely because subclassing implies subtyping even in the instances where the principle of subtyping, known as the Liskov substitution principle, does not hold. Barbara Liskov and Jeannette Wing formulated the principle succinctly in a 1994 paper as follows:\n\nSubtype Requirement: Let {\\displaystyle \\phi (x)} \\phi (x) be a property provable about objects x of type T. Then {\\displaystyle \\phi (y)} {\\displaystyle \\phi (y)} should be true for objects y of type S where S is a subtype of T.\n\nTherefore normally one must distinguish subtyping and subclassing. Most current object-oriented languages distinguish subtyping and subclassing, however some approaches to design do not.\n\nAlso, another common example is that a person object created from a child class cannot become an object of parent class because a child class and a parent class inherit a person class but class-based languages mostly do not allow to change the kind of class of the object at runtime. For class-based languages, this restriction is essential in order to preserve unified view of the class to its users. The users should not need to care whether one of the implementations of a method happens to cause changes that break the invariants of the class. Such changes can be made by destroying the object and constructing another in its place. Polymorphism can be used to preserve the relevant interfaces even when such changes are done, because the objects are viewed as black box abstractions and accessed via object identity. However, usually the value of object references referring to the object is changed, which causes effects to client code.\n\nExample languages[edit]\nSee also: Category:Class-based programming languages\nAlthough Simula introduced the class abstraction, the canonical example of a class-based language is Smalltalk. Others include PHP, C++, Java, C#, and Objective-C.\n\nSee also[edit]\nPrototype-based programming (contrast)\nProgramming paradigms\nClass (computer programming)",
          "subparadigms": []
        },
        {
          "pdid": 63,
          "name": "Actor-based",
          "details": "The actor model in computer science is a mathematical model of concurrent computation that treats \"actors\" as the universal primitives of concurrent computation. In response to a message that it receives, an actor can: make local decisions, create more actors, send more messages, and determine how to respond to the next message received. Actors may modify private state, but can only affect each other through messages (avoiding the need for any locks).\n\nThe actor model originated in 1973.[1] It has been used both as a framework for a theoretical understanding of computation and as the theoretical basis for several practical implementations of concurrent systems. The relationship of the model to other work is discussed in Indeterminacy in concurrent computation and Actor model and process calculi.\n\nContents  [hide] \n1\tHistory\n2\tFundamental concepts\n3\tFormal systems\n4\tApplications\n5\tMessage-passing semantics\n5.1\tUnbounded nondeterminism controversy\n5.2\tDirect communication and asynchrony\n5.3\tActor creation plus addresses in messages means variable topology\n5.4\tInherently concurrent\n5.5\tNo requirement on order of message arrival\n5.6\tLocality\n5.7\tComposing Actor Systems\n5.8\tBehaviors\n5.9\tModeling other concurrency systems\n5.10\tComputational Representation Theorem\n5.11\tRelationship to logic programming\n5.12\tMigration\n5.13\tSecurity\n5.14\tSynthesizing addresses of actors\n5.15\tContrast with other models of message-passing concurrency\n6\tInfluence\n6.1\tTheory\n6.2\tPractice\n7\tCurrent issues\n8\tEarly Actor researchers\n9\tProgramming with Actors\n9.1\tEarly Actor programming languages\n9.2\tLater Actor programming languages\n9.3\tActor libraries and frameworks\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\n13.1\tVideos\n13.2\tArticles\n13.3\tProcedural Libraries\nHistory[edit]\nMain article: History of the Actor model\nAccording to Carl Hewitt, unlike previous models of computation, the Actor model was inspired by physics, including general relativity and quantum mechanics. It was also influenced by the programming languages Lisp, Simula and early versions of Smalltalk, as well as capability-based systems and packet switching. Its development was \"motivated by the prospect of highly parallel computing machines consisting of dozens, hundreds, or even thousands of independent microprocessors, each with its own local memory and communications processor, communicating via a high-performance communications network.\"[2] Since that time, the advent of massive concurrency through multi-core and manycore computer architectures has revived interest in the Actor model.\n\nFollowing Hewitt, Bishop, and Steiger's 1973 publication, Irene Greif developed an operational semantics for the Actor model as part of her doctoral research.[3] Two years later, Henry Baker and Hewitt published a set of axiomatic laws for Actor systems.[4][5] Other major milestones include William Clinger's 1981 dissertation introducing a denotational semantics based on power domains[2] and Gul Agha's 1985 dissertation which further developed a transition-based semantic model complementary to Clinger's.[6] This resulted in the full development of actor model theory.\n\nMajor software implementation work was done by Russ Atkinson, Giuseppe Attardi, Henry Baker, Gerry Barber, Peter Bishop, Peter de Jong, Ken Kahn, Henry Lieberman, Carl Manning, Tom Reinhardt, Richard Steiger and Dan Theriault in the Message Passing Semantics Group at Massachusetts Institute of Technology (MIT). Research groups led by Chuck Seitz at California Institute of Technology (Caltech) and Bill Dally at MIT constructed computer architectures that further developed the message passing in the model. See Actor model implementation.\n\nResearch on the Actor model has been carried out at California Institute of Technology, Kyoto University Tokoro Laboratory, MCC, MIT Artificial Intelligence Laboratory, SRI, Stanford University, University of Illinois at Urbana-Champaign,[7] Pierre and Marie Curie University (University of Paris 6), University of Pisa, University of Tokyo Yonezawa Laboratory, Centrum Wiskunde & Informatica (CWI) and elsewhere.\n\nFundamental concepts[edit]\nThe Actor model adopts the philosophy that everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages.\n\nAn actor is a computational entity that, in response to a message it receives, can concurrently:\n\nsend a finite number of messages to other actors;\ncreate a finite number of new actors;\ndesignate the behavior to be used for the next message it receives.\nThere is no assumed sequence to the above actions and they could be carried out in parallel.\n\nDecoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages.[8]\n\nRecipients of messages are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created.\n\nThe Actor model is characterized by inherent concurrency of computation within and among actors, dynamic creation of actors, inclusion of actor addresses in messages, and interaction only through direct asynchronous message passing with no restriction on message arrival order.\n\nFormal systems[edit]\nOver the years, several different formal systems have been developed which permit reasoning about systems in the Actor model. These include:\n\nOperational semantics[3][9]\nLaws for Actor systems[4]\nDenotational semantics[2][10]\nTransition semantics[6]\nThere are also formalisms that are not fully faithful to the Actor model in that they do not formalize the guaranteed delivery of messages including the following (See Attempts to relate Actor semantics to algebra and linear logic):\n\nSeveral different Actor algebras[11][12][13]\nLinear logic[14]\nApplications[edit]\n\nThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)\nThe Actor model can be used as a framework for modeling, understanding, and reasoning about a wide range of concurrent systems. For example:\n\nElectronic mail (e-mail) can be modeled as an Actor system. Accounts are modeled as Actors and email addresses as Actor addresses.\nWeb Services can be modeled with SOAP endpoints modeled as Actor addresses.\nObjects with locks (e.g., as in Java and C#) can be modeled as a Serializer, provided that their implementations are such that messages can continually arrive (perhaps by being stored in an internal queue). A serializer is an important kind of Actor defined by the property that it is continually available to the arrival of new messages; every message sent to a serializer is guaranteed to arrive.\nTesting and Test Control Notation (TTCN), both TTCN-2 and TTCN-3, follows Actor model rather closely. In TTCN, Actor is a test component: either parallel test component (PTC) or main test component (MTC). Test components can send and receive messages to and from remote partners (peer test components or test system interface), the latter being identified by its address. Each test component has a behaviour tree bound to it; test components run in parallel and can be dynamically created by parent test components. Built-in language constructs allow the definition of actions to be taken when an expected message is received from the internal message queue, like sending a message to another peer entity or creating new test components.\nMessage-passing semantics[edit]\nThe Actor model is about the semantics of message passing.\n\nUnbounded nondeterminism controversy[edit]\nArguably, the first concurrent programs were interrupt handlers. During the course of its normal operation a computer needed to be able to receive information from outside (characters from a keyboard, packets from a network, etc). So when the information arrived the execution of the computer was \"interrupted\" and special code called an interrupt handler was called to put the information in a buffer where it could be subsequently retrieved.\n\nIn the early 1960s, interrupts began to be used to simulate the concurrent execution of several programs on a single processor.[15] Having concurrency with shared memory gave rise to the problem of concurrency control. Originally, this problem was conceived as being one of mutual exclusion on a single computer. Edsger Dijkstra developed semaphores and later, between 1971 and 1973,[16] Tony Hoare[17] and Per Brinch Hansen[18] developed monitors to solve the mutual exclusion problem. However, neither of these solutions provided a programming-language construct that encapsulated access to shared resources. This encapsulation was later accomplished by the serializer construct ([Hewitt and Atkinson 1977, 1979] and [Atkinson 1980]).\n\nThe first models of computation (e.g., Turing machines, Post productions, the lambda calculus, etc.) were based on mathematics and made use of a global state to represent a computational step (later generalized in [McCarthy and Hayes 1969] and [Dijkstra 1976] see Event orderings versus global state). Each computational step was from one global state of the computation to the next global state. The global state approach was continued in automata theory for finite state machines and push down stack machines, including their nondeterministic versions. Such nondeterministic automata have the property of bounded nondeterminism; that is, if a machine always halts when started in its initial state, then there is a bound on the number of states in which it halts.\n\nEdsger Dijkstra further developed the nondeterministic global state approach. Dijkstra's model gave rise to a controversy concerning unbounded nondeterminism (also called unbounded indeterminacy), a property of concurrency by which the amount of delay in servicing a request can become unbounded as a result of arbitration of contention for shared resources while still guaranteeing that the request will eventually be serviced. Hewitt argued that the Actor model should provide the guarantee of service. In Dijkstra's model, although there could be an unbounded amount of time between the execution of sequential instructions on a computer, a (parallel) program that started out in a well defined state could terminate in only a bounded number of states [Dijkstra 1976]. Consequently, his model could not provide the guarantee of service. Dijkstra argued that it was impossible to implement unbounded nondeterminism.\n\nHewitt argued otherwise: there is no bound that can be placed on how long it takes a computational circuit called an arbiter to settle (see metastability in electronics).[19] Arbiters are used in computers to deal with the circumstance that computer clocks operate asynchronously with respect to input from outside, e.g., keyboard input, disk access, network input, etc. So it could take an unbounded time for a message sent to a computer to be received and in the meantime the computer could traverse an unbounded number of states.\n\nThe Actor Model features unbounded nondeterminism which was captured in a mathematical model by Will Clinger using domain theory.[2] There is no global state in the Actor model.[dubious – discuss]\n\nDirect communication and asynchrony[edit]\nMessages in the Actor model are not necessarily buffered. This was a sharp break with previous approaches to models of concurrent computation. The lack of buffering caused a great deal of misunderstanding at the time of the development of the Actor model and is still a controversial issue. Some researchers argued that the messages are buffered in the \"ether\" or the \"environment\". Also, messages in the Actor model are simply sent (like packets in IP); there is no requirement for a synchronous handshake with the recipient.\n\nActor creation plus addresses in messages means variable topology[edit]\nA natural development of the Actor model was to allow addresses in messages. Influenced by packet switched networks [1961 and 1964], Hewitt proposed the development of a new model of concurrent computation in which communications would not have any required fields at all: they could be empty. Of course, if the sender of a communication desired a recipient to have access to addresses which the recipient did not already have, the address would have to be sent in the communication.\n\nFor example, an Actor might need to send a message to a recipient Actor from which it later expects to receive a response, but the response will actually be handled by a third Actor component that has been configured to receive and handle the response (for example, a different Actor implementing the Observer pattern). The original Actor could accomplish this by sending a communication that includes the message it wishes to send, along with the address of the third Actor that will handle the response. This third Actor that will handle the response is called the resumption (sometimes also called a continuation or stack frame). When the recipient Actor is ready to send a response, it sends the response message to the resumption Actor address that was included in the original communication.\n\nSo, the ability of Actors to create new Actors with which they can exchange communications, along with the ability to include the addresses of other Actors in messages, gives Actors the ability to create and participate in arbitrarily variable topological relationships with one another, much as the objects in Simula and other object-oriented languages may also be relationally composed into variable topologies of message-exchanging objects.\n\nInherently concurrent[edit]\nAs opposed to the previous approach based on composing sequential processes, the Actor model was developed as an inherently concurrent model. In the Actor model sequentiality was a special case that derived from concurrent computation as explained in Actor model theory.\n\nNo requirement on order of message arrival[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nHewitt argued against adding the requirement that messages must arrive in the order in which they are sent to the Actor. If output message ordering is desired, then it can be modeled by a queue Actor that provides this functionality. Such a queue Actor would queue the messages that arrived so that they could be retrieved in FIFO order. So if an Actor X sent a message M1 to an Actor Y, and later X sent another message M2 to Y, there is no requirement that M1 arrives at Y before M2.\n\nIn this respect the Actor model mirrors packet switching systems which do not guarantee that packets must be received in the order sent. Not providing the order of delivery guarantee allows packet switching to buffer packets, use multiple paths to send packets, resend damaged packets, and to provide other optimizations.\n\nFor example, Actors are allowed to pipeline the processing of messages. What this means is that in the course of processing a message M1, an Actor can designate the behavior to be used to process the next message, and then in fact begin processing another message M2 before it has finished processing M1. Just because an Actor is allowed to pipeline the processing of messages does not mean that it must pipeline the processing. Whether a message is pipelined is an engineering tradeoff. How would an external observer know whether the processing of a message by an Actor has been pipelined? There is no ambiguity in the definition of an Actor created by the possibility of pipelining. Of course, it is possible to perform the pipeline optimization incorrectly in some implementations, in which case unexpected behavior may occur.\n\nLocality[edit]\nAnother important characteristic of the Actor model is locality.\n\nLocality means that in processing a message, an Actor can send messages only to addresses that it receives in the message, addresses that it already had before it received the message, and addresses for Actors that it creates while processing the message. (But see Synthesizing Addresses of Actors.)\n\nAlso locality means that there is no simultaneous change in multiple locations. In this way it differs from some other models of concurrency, e.g., the Petri net model in which tokens are simultaneously removed from multiple locations and placed in other locations.\n\nComposing Actor Systems[edit]\nThe idea of composing Actor systems into larger ones is an important aspect of modularity that was developed in Gul Agha's doctoral dissertation,[6] developed later by Gul Agha, Ian Mason, Scott Smith, and Carolyn Talcott.[9]\n\nBehaviors[edit]\nA key innovation was the introduction of behavior specified as a mathematical function to express what an Actor does when it processes a message, including specifying a new behavior to process the next message that arrives. Behaviors provided a mechanism to mathematically model the sharing in concurrency.\n\nBehaviors also freed the Actor model from implementation details, e.g., the Smalltalk-72 token stream interpreter. However, it is critical to understand that the efficient implementation of systems described by the Actor model require extensive optimization. See Actor model implementation for details.\n\nModeling other concurrency systems[edit]\nOther concurrency systems (e.g., process calculi) can be modeled in the Actor model using a two-phase commit protocol.[20]\n\nComputational Representation Theorem[edit]\nSee also: Denotational semantics of the Actor model\nThere is a Computational Representation Theorem in the Actor model for systems which are closed in the sense that they do not receive communications from outside. The mathematical denotation denoted by a closed system S is constructed from an initial behavior ⊥S and a behavior-approximating function progressionS. These obtain increasingly better approximations and construct a denotation (meaning) for S as follows [Hewitt 2008; Clinger 1981]:\n\n{\\displaystyle \\mathbf {Denote} _{\\mathtt {S}}\\equiv \\lim _{i\\to \\infty }\\mathbf {progression} _{{\\mathtt {S}}^{i}}(\\bot _{\\mathtt {S}})} {\\displaystyle \\mathbf {Denote} _{\\mathtt {S}}\\equiv \\lim _{i\\to \\infty }\\mathbf {progression} _{{\\mathtt {S}}^{i}}(\\bot _{\\mathtt {S}})}\nIn this way, S can be mathematically characterized in terms of all its possible behaviors (including those involving unbounded nondeterminism). Although DenoteS is not an implementation of S, it can be used to prove a generalization of the Church-Turing-Rosser-Kleene thesis [Kleene 1943]:\n\nA consequence of the above theorem is that a finite Actor can nondeterministically respond with an uncountable[clarify] number of different outputs.\n\nRelationship to logic programming[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nOne of the key motivations for the development of the actor model was to understand and deal with the control structure issues that arose in development of the Planner programming language.[citation needed] Once the actor model was initially defined, an important challenge was to understand the power of the model relative to Robert Kowalski's thesis that \"computation can be subsumed by deduction\". Hewitt argued that Kowalski's thesis turned out to be false for the concurrent computation in the actor model (see Indeterminacy in concurrent computation).\n\nNevertheless, attempts were made to extend logic programming to concurrent computation. However, Hewitt and Agha [1991] claimed that the resulting systems were not deductive in the following sense: computational steps of the concurrent logic programming systems do not follow deductively from previous steps (see Indeterminacy in concurrent computation). Recently, logic programming has been integrated into the actor model in a way that maintains logical semantics.[19]\n\nMigration[edit]\nMigration in the Actor model is the ability of Actors to change locations. E.g., in his dissertation, Aki Yonezawa modeled a post office that customer Actors could enter, change locations within while operating, and exit. An Actor that can migrate can be modeled by having a location Actor that changes when the Actor migrates. However the faithfulness of this modeling is controversial and the subject of research.[citation needed]\n\nSecurity[edit]\nThe security of Actors can be protected in the following ways:\n\nhardwiring in which Actors are physically connected\ncomputer hardware as in Burroughs B5000, Lisp machine, etc.\nvirtual machines as in Java virtual machine, Common Language Runtime, etc.\noperating systems as in capability-based systems\nsigning and/or encryption of Actors and their addresses\nSynthesizing addresses of actors[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nA delicate point in the Actor model is the ability to synthesize the address of an Actor. In some cases security can be used to prevent the synthesis of addresses (see Security). However, if an Actor address is simply a bit string then clearly it can be synthesized although it may be difficult or even infeasible to guess the address of an Actor if the bit strings are long enough. SOAP uses a URL for the address of an endpoint where an Actor can be reached. Since a URL is a character string, it can clearly be synthesized although encryption can make it virtually impossible to guess.\n\nSynthesizing the addresses of Actors is usually modeled using mapping. The idea is to use an Actor system to perform the mapping to the actual Actor addresses. For example, on a computer the memory structure of the computer can be modeled as an Actor system that does the mapping. In the case of SOAP addresses, it's modeling the DNS and the rest of the URL mapping.\n\nContrast with other models of message-passing concurrency[edit]\nRobin Milner's initial published work on concurrency[21] was also notable in that it was not based on composing sequential processes. His work differed from the Actor model because it was based on a fixed number of processes of fixed topology communicating numbers and strings using synchronous communication. The original Communicating Sequential Processes model[22] published by Tony Hoare differed from the Actor model because it was based on the parallel composition of a fixed number of sequential processes connected in a fixed topology, and communicating using synchronous message-passing based on process names (see Actor model and process calculi history). Later versions of CSP abandoned communication based on process names in favor of anonymous communication via channels, an approach also used in Milner's work on the Calculus of Communicating Systems and the π-calculus.\n\nThese early models by Milner and Hoare both had the property of bounded nondeterminism. Modern, theoretical CSP ([Hoare 1985] and [Roscoe 2005]) explicitly provides unbounded nondeterminism.\n\nPetri nets and their extensions (e.g., coloured Petri nets) are like actors in that they are based on asynchronous message passing and unbounded nondeterminism, while they are like early CSP in that they define fixed topologies of elementary processing steps (transitions) and message repositories (places).\n\nInfluence[edit]\nThe Actor Model has been influential on both theory development and practical software development.\n\nTheory[edit]\nThe Actor Model has influenced the development of the Pi-calculus and subsequent Process calculi. In his Turing lecture, Robin Milner wrote:[23]\n\nNow, the pure lambda-calculus is built with just two kinds of thing: terms and variables. Can we achieve the same economy for a process calculus? Carl Hewitt, with his Actors model, responded to this challenge long ago; he declared that a value, an operator on values, and a process should all be the same kind of thing: an Actor.\nThis goal impressed me, because it implies the homogeneity and completeness of expression ... But it was long before I could see how to attain the goal in terms of an algebraic calculus...\nSo, in the spirit of Hewitt, our first step is to demand that all things denoted by terms or accessed by names--values, registers, operators, processes, objects--are all of the same kind of thing; they should all be processes.\nPractice[edit]\nThe Actor Model has had extensive influence on commercial practice. For example, Twitter has used actors for scalability.[24] Also, Microsoft has used the Actor Model in the development of its Asynchronous Agents Library.[25] There are numerous other Actor libraries listed in the Actor Libraries and Frameworks section below.\n\nCurrent issues[edit]\nAccording to Hewitt [2006], the Actor model addresses issues in computer and communications architecture, concurrent programming languages, and Web Services including the following:\n\nscalability: the challenge of scaling up concurrency both locally and nonlocally.\ntransparency: bridging the chasm between local and nonlocal concurrency. Transparency is currently a controversial issue. Some researchers[who?] have advocated a strict separation between local concurrency using concurrent programming languages (e.g., Java and C#) from nonlocal concurrency using SOAP for Web services. Strict separation produces a lack of transparency that causes problems when it is desirable/necessary to change between local and nonlocal access to Web Services (see distributed computing).\ninconsistency: Inconsistency is the norm because all very large knowledge systems about human information system interactions are inconsistent. This inconsistency extends to the documentation and specifications of very large systems (e.g., Microsoft Windows software, etc.), which are internally inconsistent.\nMany of the ideas introduced in the Actor model are now also finding application in multi-agent systems for these same reasons [Hewitt 2006b 2007b]. The key difference is that agent systems (in most definitions) impose extra constraints upon the Actors, typically requiring that they make use of commitments and goals.\n\nThe Actor model is also being applied to client cloud computing.[26]\n\nEarly Actor researchers[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nGnome-searchtool.svg\nThis article's factual accuracy is disputed. Please help to ensure that disputed statements are reliably sourced. See the relevant discussion on the talk page. (March 2012) (Learn how and when to remove this template message)\nThere is a growing community of researchers working on the Actor Model as it is becoming commercially more important. Early Actor researchers included:\n\nImportant contributions to the semantics of Actors have been made by: Gul Agha, Beppe Attardi, Henry Baker, Will Clinger, Irene Greif, Carl Hewitt, Carl Manning, Ian Mason, Ugo Montanari, Maria Simi, Scott Smith, Carolyn Talcott, Prasanna Thati, and Akinori Yonezawa.\nImportant contributions to the implementation of Actors have been made by: Bill Athas, Russ Atkinson, Beppe Attardi, Henry Baker, Gerry Barber, Peter Bishop, Nanette Boden, Jean-Pierre Briot, Bill Dally, Peter de Jong, Jessie Dedecker, Travis Desell, Ken Kahn, Carl Hewitt, Henry Lieberman, Carl Manning, Tom Reinhardt, Chuck Seitz, Richard Steiger, Dan Theriault, Mario Tokoro, Carlos Varela, Darrell Woelk.\nProgramming with Actors[edit]\nA number of different programming languages employ the Actor model or some variation of it. These languages include:\n\nEarly Actor programming languages[edit]\nAct 1, 2 and 3[27][28]\nActtalk[29]\nAni[30]\nCantor[31]\nRosette[32]\nLater Actor programming languages[edit]\nABCL\nAmbientTalk[33]\nAxum[34]\nCAL Actor Language\nD\nE\nElixir\nErlang\nFantom\nHumus[35]\nIo\nLFE\nEncore[36]\nPony[37]\nPtolemy Project\nRebeca Modeling Language\nReia\nSALSA[38]\nScala[39][40]\nScratch\nTNSDL\nActor libraries and frameworks[edit]\nActor libraries or frameworks have also been implemented to permit actor-style programming in languages that don't have actors built-in. Among these frameworks are:\n\nName\tStatus\tLatest release\tLicense\tLanguages\nAojet\tActive\t2016-10-17\tMIT\tSwift\nActor\tActive\t2013-05-31\tMIT\tJava\nVert.x\tActive\t2016-09-12\tApache 2.0\tJava, Groovy, Javascript, Ruby, Scala\nActor Framework\tActive\t2013-11-13\tApache 2.0\t.NET\nAkka (toolkit)\tActive\t2016-08-19\tApache 2.0\tJava and Scala\nAkka.NET\tActive\t2016-01-18\tApache 2.0\t.NET\nRemact.Net\tActive\t2016-06-26\tMIT\t.NET, Javascript\nAteji PX\tActive\t ?\t ?\tJava\nF# MailboxProcessor\tActive\tsame as F# (built-in core library)\tApache License\tF#\nKorus\tActive\t2010-02-04\tGPL 3\tJava\nKilim[41]\tActive\t2011-10-13[42]\tMIT\tJava\nActorFoundry (based on Kilim)\tActive\t2008-12-28\t ?\tJava\nActorKit\tActive\t2011-09-13[43]\tBSD\tObjective-C\nCloud Haskell\tActive\t2015-06-17[44]\tBSD\tHaskell\nCloudI\tActive\t2015-12-24[45]\tBSD\tC/C++, Elixir/Erlang/LFE, Java, Javascript, Perl, PHP, Python, Ruby\nNAct\tActive\t2012-02-28\tLGPL 3.0\t.NET\nRetlang\tActive\t2011-05-18[46]\tNew BSD\t.NET\nJActor\tActive\t2013-01-22\tLGPL\tJava\nJetlang\tActive\t2013-05-30[47]\tNew BSD\tJava\nHaskell-Actor\tActive?\t2008\tNew BSD\tHaskell\nGPars\tActive\t2014-05-09[48]\tApache 2.0\tGroovy\nOOSMOS\tActive\t2016-02-17[49]\tGPL 2.0 and commercial (dual licensing)\tC. C++ friendly\nPanini\tActive\t2014-05-22\tMPL 1.1\tProgramming Language by itself\nPARLEY\tActive?\t2007-22-07\tGPL 2.1\tPython\nPeernetic\tActive\t2007-06-29\tLGPL 3.0\tJava\nPostSharp\tActive\t2014-09-24\tCommercial / Freemium\t.NET\nPulsar\tActive\t2016-07-09[50]\tNew BSD\tPython\nPulsar\tActive\t2016-02-18[51]\tLGPL/Eclipse\tClojure\nPykka\tActive\t2015-07-20[52]\tApache 2.0\tPython\nTermite Scheme\tActive?\t2009-05-21\tLGPL\tScheme (Gambit implementation)\nTheron\tActive\t2014-01-18[53]\tMIT[54]\tC++\nThespian\tActive\t2016-02-11\tGoDaddy Public Release[55]\tPython\nQuasar\tActive\t2016-01-18[56]\tLGPL/Eclipse\tJava\nLibactor\tActive?\t2009\tGPL 2.0\tC\nActor-CPP\tActive\t2012-03-10[57]\tGPL 2.0\tC++\nS4\tActive\t2012-07-31[58]\tApache 2.0\tJava\nC++ Actor Framework (CAF)\tActive\t2016-03-14[59]\tBoost Software License 1.0 and BSD 3-Clause\tC++11\nCelluloid\tActive\t2016-01-19[60]\tMIT\tRuby\nLabVIEW Actor Framework\tActive\t2012-03-01[61]\tNational Instruments SLA\tLabVIEW\nLabVIEW Messenger Library\tActive\t2016-06-01\tBSD\tLabVIEW\nOrbit\tActive\t2016-04-22[62]\tNew BSD\tJava\nQP frameworks for real-time embedded systems\tActive\t2015-09-29[63]\tGPL 2.0 and commercial (dual licensing)\tC and C++\nlibprocess\tActive\t2013-06-19\tApache 2.0\tC++\nSObjectizer\tActive\t2016-09-29\tNew BSD\tC++11\nOrleans\tActive\t2016-05-18[64]\tMIT License\tC#/.NET\nSkynet\tActive\t2016-07-11\tMIT License\tC/Lua\nReactors.IO\tActive\t2016-06-14\tBSD License\tJava/Scala\nPlease note that not all frameworks and libraries are listed here.\n\nSee also[edit]\nActor model theory\nActor model early history\nActor model and process calculi\nActor model implementation\nData flow\nMulti-agent system\nGordon Pask\nScientific Community Metaphor\nCommunicating sequential processes\nInput/output automaton",
          "subparadigms": []
        },
        {
          "pdid": 64,
          "name": "Object-oriented",
          "details": "Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated (objects have a notion of \"this\" or \"self\"). In OOP, computer programs are designed by making them out of objects that interact with one another.[1][2] There is significant diversity of OOP languages, but the most popular ones are class-based, meaning that objects are instances of classes, which typically also determine their type.\n\nMany of the most widely used programming languages are multi-paradigm programming languages that support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include Java, C++, C#, Python, PHP, Ruby, Perl, Delphi, Objective-C, Swift, Common Lisp, and Smalltalk.\n\nContents  [hide] \n1\tFeatures\n1.1\tShared with non-OOP predecessor languages\n1.2\tObjects and classes\n1.3\tDynamic dispatch/message passing\n1.4\tEncapsulation\n1.5\tComposition, inheritance, and delegation\n1.6\tPolymorphism\n1.7\tOpen recursion\n2\tHistory\n3\tOOP languages\n3.1\tOOP in dynamic languages\n3.2\tOOP in a network protocol\n4\tDesign patterns\n4.1\tInheritance and behavioral subtyping\n4.2\tGang of Four design patterns\n4.3\tObject-orientation and databases\n4.4\tReal-world modeling and relationships\n4.5\tOOP and control flow\n4.6\tResponsibility- vs. data-driven design\n4.7\tSOLID and GRASP guidelines\n5\tCriticism\n6\tFormal semantics\n7\tSee also\n7.1\tSystems\n7.2\tModeling languages\n8\tReferences\n9\tFurther reading\n10\tExternal links\nFeatures[edit]\nObject-oriented Programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP. The features listed below are, however, common among languages considered strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.[3][4][5][6]\n\nSee also: Comparison of programming languages (object-oriented programming) and List of object-oriented programming terms\nShared with non-OOP predecessor languages[edit]\nObject-oriented programming languages typically share low-level features with high-level procedural programming languages (which were invented first). The fundamental tools that can be used to construct a program include:\n\nVariables that can store information formatted in a small number of built-in data types like integers and alphanumeric characters. This may include data structures like strings, lists, and hash tables that are either built-in or result from combining variables using memory pointers\nProcedures – also known as functions, methods, routines, or subroutines – that take input, generate output, and manipulate data. Modern languages include structured programming constructs like loops and conditionals.\nModular programming support provides the ability to group procedures into files and modules for organizational purposes. Modules are namespaced so code in one module will not be accidentally confused with the same procedure or variable name in another file or module.\n\nObjects and classes[edit]\nLanguages that support object-oriented programming typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. Those that use classes support two main concepts:\n\nClasses – the definitions for the data format and available procedures for a given type or class of object; may also contain data and procedures (known as class methods) themselves, i.e. classes contains the data members and member functions\nObjects – instances of classes\nObjects sometimes correspond to things found in the real world. For example, a graphics program may have objects such as \"circle\", \"square\", \"menu\". An online shopping system might have objects such as \"shopping cart\", \"customer\", and \"product\".[7] Sometimes objects represent more abstract entities, like an object that represents an open file, or an object that provides the service of translating measurements from U.S. customary to metric.\n\nEach object is said to be an instance of a particular class (for example, an object with its name field set to \"Mary\" might be an instance of class Employee). Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties. This leads to the following terms:\n\nClass variables – belong to the class as a whole; there is only one copy of each one\nInstance variables or attributes – data that belongs to individual objects; every object has its own copy of each one\nMember variables – refers to both the class and instance variables that are defined by a particular class\nClass methods – belong to the class as a whole and have access only to class variables and inputs from the procedure call\nInstance methods – belong to individual objects, and have access to instance variables for the specific object they are called on, inputs, and class variables\nObjects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack. They provide a layer of abstraction which can be used to separate internal from external code. External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. Objects are created by calling a special type of method in the class known as a constructor. A program may create many instances of the same class as it runs, which operate independently. This is an easy way for the same procedures to be used on different sets of data.\n\nObject-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. As a result, a significantly different yet analogous terminology is used to define the concepts of object and instance.\n\nIn some languages classes and objects can be composed using other concepts like traits and mixins.\n\nDynamic dispatch/message passing[edit]\nIt is the responsibility of the object, not any external code, to select the procedural code to execute in response to a method call, typically by looking up the method at run time in a table associated with the object. This feature is known as dynamic dispatch, and distinguishes an object from an abstract data type (or module), which has a fixed (static) implementation of the operations for all instances. If there are multiple methods that might be run for a given name, it is known as multiple dispatch.\n\nA method call is also known as message passing. It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.\n\nEncapsulation[edit]\nEncapsulation is an Object Oriented Programming concept that binds together the data and functions that manipulate the data, and that keeps both safe from outside interference and misuse. Data encapsulation led to the important OOP concept of data hiding.\n\nIf a class does not allow calling code to access internal object data and permits access through methods only, this is a strong form of abstraction or information hiding known as encapsulation. Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the private keyword and designating methods intended for use by code outside the class with the public keyword. Methods may also be designed public, private, or intermediate levels such as protected (which allows access from the same class and its subclasses, but not objects of a different class). In other languages (like Python) this is enforced only by convention (for example, private methods may have names that start with an underscore). Encapsulation prevents external code from being concerned with the internal workings of an object. This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as \"public\" method calls work the same way). It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers. Encapsulation is a technique that encourages decoupling.\n\nComposition, inheritance, and delegation[edit]\nObjects can contain other objects in their instance variables; this is known as object composition. For example, an object in the Employee class might contain (point to) an object in the Address class, in addition to its own instance variables like \"first_name\" and \"position\". Object composition is used to represent \"has-a\" relationships: every employee has an address, so every Employee object has a place to store an Address object.\n\nLanguages that support classes almost always support inheritance. This allows classes to be arranged in a hierarchy that represents \"is-a-type-of\" relationships. For example, class Employee might inherit from class Person. All the data and methods available to the parent class also appear in the child class with the same names. For example, class Person might define variables \"first_name\" and \"last_name\" with method \"make_full_name()\". These will also be available in class Employee, which might add the variables \"position\" and \"salary\". This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.[8]\n\nSubclasses can override the methods defined by superclasses. Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated. Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes. For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don't share a common parent.\n\nAbstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other \"concrete\" classes which can be instantiated. In Java, the final keyword can be used to prevent a class from being subclassed.\n\nThe doctrine of composition over inheritance advocates implementing has-a relationships using composition instead of inheritance. For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods. Some languages, like Go do not support inheritance at all.\n\nThe \"open/closed principle\" advocates that classes and functions \"should be open for extension, but closed for modification\".\n\nDelegation is another language feature that can be used as an alternative to inheritance.\n\nPolymorphism[edit]\nSubtyping, a form of polymorphism, is when calling code can be agnostic as to whether an object belongs to a parent class or one of its descendants. For example, a function might call \"make_full_name()\" on an object, which will work whether the object is of class Person or class Employee. This is another type of abstraction which simplifies code external to the class hierarchy and enables strong separation of concerns.\n\nOpen recursion[edit]\nIn languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called this or self. This variable is late-bound; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.\n\nHistory[edit]\nTerminology invoking \"objects\" and \"oriented\" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. In the environment of the artificial intelligence group, as early as 1960, \"object\" could refer to identified items (LISP atoms) with properties (attributes);[9][10] Alan Kay was later to cite a detailed understanding of LISP internals as a strong influence on his thinking in 1966.[11] Another early MIT example was Sketchpad created by Ivan Sutherland in 1960–61; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.[12] Also, an MIT ALGOL version, AED-0, established a direct link between data structures (\"plexes\", in that dialect) and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".[13][14]\n\nThe formal programming concept of objects was introduced in the mid-1960s with Simula 67, a major revision of Simula I, a programming language designed for discrete event simulation, created by Ole-Johan Dahl and Kristen Nygaard of the Norwegian Computing Center in Oslo.[15][not in citation given][citation needed] [16][not in citation given][citation needed] [17] [18] [19]\n\nSimula 67 was influenced by SIMSCRIPT and C.A.R. \"Tony\" Hoare's proposed \"record classes\".[13][20] Simula introduced the notion of classes and instances or objects (as well as subclasses, virtual procedures, coroutines, and discrete event simulation) as part of an explicit programming paradigm. The language also used automatic garbage collection that had been invented earlier for the functional programming language Lisp. Simula was used for physical modeling, such as models to study and improve the movement of ships and their content through cargo ports. The ideas of Simula 67 influenced many later languages, including Smalltalk, derivatives of LISP (CLOS), Object Pascal, and C++.\n\nThe Smalltalk language, which was developed at Xerox PARC (by Alan Kay and others) in the 1970s, introduced the term object-oriented programming to represent the pervasive use of objects and messages as the basis for computation. Smalltalk creators were influenced by the ideas introduced in Simula 67, but Smalltalk was designed to be a fully dynamic system in which classes could be created and modified dynamically rather than statically as in Simula 67.[21] Smalltalk and with it OOP were introduced to a wider audience by the August 1981 issue of Byte Magazine.\n\nIn the 1970s, Kay's Smalltalk work had influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine. Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.\n\nIn 1985, Bertrand Meyer produced the first design of the Eiffel language. Focused on software quality, Eiffel is among the purely object-oriented languages, but differs in the sense that the language itself is not only a programming language, but a notation supporting the entire software lifecycle. Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. Essential to the quality focus of Eiffel is Meyer's reliability mechanism, Design by Contract, which is an integral part of both the method and language.\n\nObject-oriented programming developed as the dominant programming methodology in the early and mid 1990s when programming languages supporting the techniques became widely available. These included Visual FoxPro 3.0,[22][23][24] C++,[25] and Delphi[citation needed]. Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).\n\nAt ETH Zürich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.\n\nObject-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.\n\nMore recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. Two such languages are Python and Ruby. Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft's .NET platform. Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.\n\nOOP languages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2009) (Learn how and when to remove this template message)\nSee also: List of object-oriented programming languages\nSimula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. It was created for making simulation programs, in which what came to be called objects were the most important information representation. Smalltalk (1972 to 1980) is another early example, and the one with which much of the theory of OOP was developed. Concerning the degree of object orientation, the following distinctions can be made:\n\nLanguages called \"pure\" OO languages, because everything in them is treated consistently as an object, from primitives such as characters and punctuation, all the way up to whole classes, prototypes, blocks, modules, etc. They were designed specifically to facilitate, even enforce, OO methods. Examples: Python, Ruby, Scala, Smalltalk, Eiffel, Emerald,[26] JADE, Self.\nLanguages designed mainly for OO programming, but with some procedural elements. Examples: Java, C++, C#, Delphi/Object Pascal, VB.NET.\nLanguages that are historically procedural languages, but have been extended with some OO features. Examples: PHP, Perl, Visual Basic (derived from BASIC), MATLAB, COBOL 2002, Fortran 2003, ABAP, Ada 95, Pascal.\nLanguages with most of the features of objects (classes, methods, inheritance), but in a distinctly original form. Examples: Oberon (Oberon-1 or Oberon-2).\nLanguages with abstract data type support which may be used to resemble OO programming, but without all features of object-orientation. This includes object-based and prototype-based languages. Examples: JavaScript, Lua, Modula-2, CLU.\nChameleon languages that support multiple paradigms, including OO. Tcl stands out among these for TclOO, a hybrid object system that supports both prototype-based programming and class-based OO.\nOOP in dynamic languages[edit]\nIn recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\n\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.\n\nOOP in a network protocol[edit]\nThe messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server. For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value. A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command's parameters. Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:\n\nFields defining the data values that form messages, such as their length, codepoint and data values.\nObjects and collections of objects similar to what would be found in a Smalltalk program for messages and parameters.\nManagers similar to AS/400 objects, such as a directory to files and files consisting of metadata and records. Managers conceptually provide memory and processing resources for their contained objects.\nA client or server consisting of all the managers necessary to implement a full processing environment, supporting such aspects as directory services, security and concurrency control.\nThe initial version of DDM defined distributed file services. It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).\n\nDesign patterns[edit]\nChallenges of object-oriented design are addressed by several methodologies. Most common is known as the design patterns codified by Gamma et al.. More broadly, the term \"design patterns\" can be used to refer to any general, repeatable solution to a commonly occurring problem in software design. Some of these commonly occurring problems have implications and solutions particular to object-oriented development.\n\nInheritance and behavioral subtyping[edit]\nSee also: Object-oriented design\nIt is intuitive to assume that inheritance creates a semantic \"is a\" relationship, and thus to infer that objects instantiated from subclasses can always be safely used instead of those instantiated from the superclass. This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. This issue is known as the Liskov substitution principle.\n\nGang of Four design patterns[edit]\nMain article: Design pattern (computer science)\nDesign Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1995 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them. As of April 2007, the book was in its 36th printing.\n\nThe book describes the following patterns:\n\nCreational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern\nStructural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern\nBehavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern\nObject-orientation and databases[edit]\nMain articles: Object-relational impedance mismatch, Object-relational mapping, and Object database\nBoth object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. Since relational databases don't store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides.[27] One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails' ActiveRecord.\n\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.\n\nReal-world modeling and relationships[edit]\nOOP can be used to associate real-world objects and processes with digital counterparts. However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in Object-Oriented Software Construction[28] that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\". At the same time, some principal limitations of OOP had been noted.[29] For example, the circle-ellipse problem is difficult to handle using OOP's concept of inheritance.\n\nHowever, Niklaus Wirth (who popularized the adage now known as Wirth's law: \"Software is getting slower more rapidly than hardware becomes faster\") said of OOP in his paper, \"Good Ideas through the Looking Glass\", \"This paradigm closely reflects the structure of systems 'in the real world', and it is therefore well suited to model complex systems with complex behaviours\"[30] (contrast KISS principle).\n\nSteve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs).[31] This problem may cause OOP to suffer more convoluted solutions than procedural programming.[32]\n\nOOP and control flow[edit]\nOOP was developed to increase the reusability and maintainability of source code.[33] Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.[34][35][36][37]\n\nResponsibility- vs. data-driven design[edit]\nResponsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. The authors hold that responsibility-driven design is preferable.\n\nSOLID and GRASP guidelines[edit]\nSOLID is a mnemonic invented by Michael Feathers that stands for and advocates five programming practices:\n\nSingle responsibility principle\nOpen/closed principle\nLiskov substitution principle\nInterface segregation principle\nDependency inversion principle\nGRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.\n\nCriticism[edit]\nThe OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity,[38][39] and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).[40][41]\n\nLuca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex.[38] The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:[39]\n\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\n\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.[42]\n\nChristopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP;[43] however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.[44]\n\nIn an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.[45]\n\nAlexander Stepanov compares object orientation unfavourably to generic programming:[40]\n\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.\n\nPaul Graham has suggested that OOP's popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".[46]\n\nSteve Yegge noted that, as opposed to functional programming:[47]\n\nObject Oriented Programming puts the Nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It's not as if OOP has suddenly made verbs less important in the way we actually think. It's a strangely skewed perspective.\n\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.[41]\n\nEric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency.[48] Raymond compares this unfavourably to the approach taken with Unix and the C programming language.[48]\n\nRob Pike, a programmer involved in the creation of UTF-8 and Go, has called the design of object-oriented programming \"the Roman numerals of computing\"[49] and has said that OOP languages frequently shift the focus from data structures and algorithms to types.[50] Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.[51]\n\nFormal semantics[edit]\nSee also: Formal semantics of programming languages\nObjects are the run-time entities in an object-oriented system. They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.\n\nThere have been several attempts at formalizing the concepts used in object-oriented programming. The following concepts and constructs have been used as interpretations of OOP concepts:\n\nco algebraic data types[52]\nabstract data types (which have existential types) allow the definition of modules but these do not support dynamic dispatch\nrecursive types\nencapsulated state\ninheritance\nrecords are basis for understanding objects if function literals can be stored in fields (like in functional programming languages), but the actual calculi need be considerably more complex to incorporate essential features of OOP. Several extensions of System F<: that deal with mutable objects have been studied;[53] these allow both subtype polymorphism and parametric polymorphism (generics)\nAttempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, A Theory of Objects[53] for formal definitions of many OOP concepts and constructs), and often diverge widely. For example, some definitions focus on mental activities, and some on program structuring. One of the simpler definitions is that OOP is the act of using \"map\" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. Inheritance can be performed by cloning the maps (sometimes called \"prototyping\").\n\nSee also[edit]\nicon\tComputer programming portal\nComparison of programming languages (object-oriented programming)\nComparison of programming paradigms\nComponent-based software engineering\nDesign by contract\nObject association\nObject database\nObject modeling language\nObject-oriented analysis and design\nObject-relational impedance mismatch (and The Third Manifesto)\nObject-relational mapping\nSystems[edit]\nCADES\nCommon Object Request Broker Architecture (CORBA)\nDistributed Component Object Model\nDistributed Data Management Architecture\nJeroo\nModeling languages[edit]\nIDEF4\nInterface description language\nLepus3\nUML",
          "subparadigms": [
            63,
            62,
            60,
            57,
            59
          ]
        },
        {
          "pdid": 65,
          "name": "Block",
          "details": "In computer programming, a block or code block is a section of code which is grouped together. Blocks consist of one or more declarations and statements. A programming language that permits the creation of blocks, including blocks nested within other blocks, is called a block-structured programming language. Blocks are fundamental to structured programming, where control structures are formed from blocks.\n\nThe function of blocks in programming is to enable groups of statements to be treated as if they were one statement, and to narrow the lexical scope of variables, procedures and functions declared in a block so that they do not conflict with variables having the same name used elsewhere in a program for different purposes. In a block-structured programming language, the names of variables and other objects such as procedures which are declared in outer blocks are visible inside other inner blocks, unless they are shadowed by an object of the same name.\n\nContents  [hide] \n1\tHistory\n2\tSyntax\n3\tLimitations\n4\tBasic semantics\n5\tHoisting\n6\tSee also\n7\tReferences\nHistory[edit]\nIdeas of block structure were developed in the 1950s during the development of the first autocodes, and were formalized in the Algol 58 and Algol 60 reports. Algol 58 introduced the notion of the \"compound statement\", which was related solely to control flow.[1] The subsequent Revised Report which described the syntax and semantics of Algol 60 introduced the notion of a block and block scope, with a block consisting of \" A sequence of declarations followed by a sequence of statements and enclosed between begin and end...\" in which \"[e]very declaration appears in a block in this way and is valid only for that block.\"[2]\n\nSyntax[edit]\nBlocks use different syntax in different languages. Two broad families are:\n\nthe ALGOL family in which blocks are delimited by the keywords \"begin\" and \"end\"\nthe C family in which blocks are delimited by curly braces - \"{\" and \"}\"\nSome other techniques used are as follows :\n\nparentheses - \"(\" and \")\", as in batch language and ALGOL 68.\nindentation, as in Python\ns-expressions with a syntactic keyword such as lambda or let (as in the Lisp family)\nIn 1968 (with ALGOL 68), then in Edsger W. Dijkstra's 1974 Guarded Command Language the conditional and iterative code block are alternatively terminated with the block reserved word reversed: e.g. if ~ then ~ elif ~ else ~ fi, case ~ in ~ out ~ esac and for ~ while ~ do ~ od\nLimitations[edit]\nSome languages which support blocks with variable declarations do not fully support all declarations; for instance many C-derived languages do not permit a function definition within a block (nested functions). And unlike its ancestor Algol, Pascal does not support the use of blocks with their own declarations inside the begin and end of an existing block, only compound statements enabling sequences of statements to be grouped together in if, while, repeat and other control statements.\n\nBasic semantics[edit]\nThe semantic meaning of a block is twofold. Firstly, it provides the programmer with a way for creating arbitrarily large and complex structures that can be treated as units. Secondly, it enables the programmer to limit the scope of variables and sometimes other objects that have been declared.\n\nIn primitive languages such as early Fortran and BASIC, there were a few built-in statement types, and little or no means of extending them in a structured manner. For instance, until 1978 standard Fortran had no \"block if\" statement, so to write a standard-complying code to implement simple decisions the programmer had to resort to gotos:\n\nC     LANGUAGE: ANSI STANDARD FORTRAN 66\nC     INITIALIZE VALUES TO BE CALCULATED\n      PAYSTX = .FALSE.\n      PAYSST = .FALSE.\n      TAX = 0.0\n      SUPTAX = 0.0\nC     SKIP TAX DEDUCTION IF EMPLOYEE EARNS LESS THAN TAX THRESHOLD\n      IF (WAGES .LE. TAXTHR) GOTO 100\n      PAYSTX = .TRUE.\n      TAX = (WAGES - TAXTHR) * BASCRT\nC     SKIP SUPERTAX DEDUCTION IF EMPLOYEE EARNS LESS THAN SUPERTAX THRESHOLD\n      IF (WAGES .LE. SUPTHR) GOTO 100\n      PAYSST = .TRUE.\n      SUPTAX = (WAGES - SUPTHR) * SUPRAT\n  100 TAXED = WAGES - TAX - SUPTAX\nEven in this very brief Fortran fragment, written to the Fortran 66 standard, it is not easy to see the structure of the program, because that structure is not reflected in the language. Without careful study it is not easy to see the circumstances in which a given statement is executed.\n\nBlocks allow the programmer to treat a group of statements as a unit, and the default values which had to appear in initialization in this style of programming can, with a block structure, be placed closer to the decision:\n\n    { Language: Jensen and Wirth Pascal }\n    if wages > tax_threshold then\n        begin\n        paystax := true;\n        tax := (wages - tax_threshold) * tax_rate\n        { The block structure makes it easier to see how the code could\n          be refactored for clarity, and also makes it easier to do,\n          because the structure of the inner conditional can easily be moved\n          out of the outer conditional altogether and the effects of doing\n          so are easily predicted. }\n        if wages > supertax_threshold then\n            begin\n            pays_supertax := true;\n            supertax := (wages - supertax_threshold) * supertax_rate\n            end\n        else begin\n            pays_supertax := false;\n            supertax := 0\n            end\n        end\n    else begin\n        paystax := false; pays_supertax := false;\n        tax := 0; supertax := 0\n        end;\n    taxed := wages - tax - supertax;\nUse of blocks in the above fragment of Pascal enables the programmer to be clearer about what he or she intends, and to combine the resulting blocks into a nested hierarchy of conditional statements. The structure of the code reflects the programmer's thinking more closely, making it easier to understand and modify.\n\nFrom looking at the above code the programmer can easily see that he or she can make the source code even clearer by taking the inner if statement out of the outer one altogether, placing the two blocks one after the other to be executed consecutively. Semantically there is little difference in this case, and the use of block structure, supported by indenting for readability, makes it easy for the programmer to refactor the code.\n\nIn primitive languages, variables had broad scope. For instance, an integer variable called IEMPNO might be used in one part of a Fortran subroutine to denote an employee social security number (ssn), but during maintenance work on the same subroutine, a programmer might accidentally use the same variable, IEMPNO, for a different purpose, and this could result in a bug that was difficult to trace. Block structure makes it easier for programmers to control scope to a minute level.\n\n;; Language: R5RS Standard Scheme\n(let ((empno (ssn-of employee-name)))\n  (while (is-manager empno)\n    (let ((employees (length (underlings-of empno))))\n      (printf \"~a has ~a employees working under him:~%\" employee-name employees)\n      (for-each\n        (lambda(empno)\n          ;; Within this lambda expression the variable empno refers to the ssn\n          ;; of an underling. The variable empno in the outer expression,\n          ;; referring to the manager's ssn, is shadowed.\n          (printf \"Name: ~a, role: ~a~%\"\n                  (name-of empno)\n                  (role-of empno)))\n        (underlings-of empno)))))\nIn the above Scheme fragment, empno is used to identify both the manager and his or her underlings each by their respective ssn, but because the underling ssn is declared within an inner block it does not interact with the variable of the same name that contains the manager's ssn. In practice, considerations of clarity would probably lead the programmer to choose distinct variable names, but he or she has the choice and it is more difficult to introduce a bug inadvertently.\n\nHoisting[edit]\nIn a few circumstances, code in a block is evaluated as if the code were actually at the top of the block or outside the block. This is often colloquially known as hoisting, and includes:\n\nLoop-invariant code motion, a compiler optimization where code in the loop that is invariant is evaluated before the loop;\nVariable hoisting, scope rule in JavaScript, where variables have function scope, and behave as if they were declared (but not defined) at the top of a function.\nSee also[edit]\nicon\tComputer programming portal\nBasic block\nBlock scope\nClosure (computer programming)\nControl flow",
          "subparadigms": []
        },
        {
          "pdid": 66,
          "name": "Modular",
          "details": "Modular programming is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules, such that each contains everything necessary to execute only one aspect of the desired functionality.\n\nA module interface expresses the elements that are provided and required by the module. The elements defined in the interface are detectable by other modules. The implementation contains the working code that corresponds to the elements declared in the interface. Modular programming is closely related to structured programming and object-oriented programming, all having the same goal of facilitating construction of large software programs and systems by decomposition into smaller pieces, and all originating around the 1960s. While historically usage of these terms has been inconsistent, today \"modular programming\" refers to high-level decomposition of the code of an entire program into pieces, structured programming to the low-level code use of structured control flow, and object-oriented programming to the data use of objects, a kind of data structure.\n\nIn object-oriented programming, the use of interfaces as an architectural pattern to construct modules is known as interface-based programming[citation needed].\n\nContents  [hide] \n1\tTerminology\n2\tLanguage support\n3\tKey aspects\n4\tHistory\n5\tSee also\n6\tNotes\n7\tReferences\nTerminology[edit]\nThe term assembly (as in .NET languages like C#, F# or Visual Basic .NET) or package (as in Dart, Go or Java) is sometimes used instead of module . In other implementations, this is a distinct concept; in Python a package is a collection of modules, while in the upcoming Java 9 the introduction of the new module concept (a collection of packages with enhanced access control) is planned.\n\nFurthermore, the term \"package\" has other uses in software (for example .NET NuGet packages). A component is a similar concept, but typically refers to a higher level; a component is a piece of a whole system, while a module is a piece of an individual program. The scale of the term \"module\" varies significantly between languages; in Python it is very small-scale and each file is a module, while in Java 9 it is planned to be large-scale, where a module is a collection of packages, which are in turn collections of files.\n\nOther terms for modules include unit, used in Pascal dialects.\n\nLanguage support[edit]\nLanguages that formally support the module concept include Ada, Algol, BlitzMax, C#, Clojure, COBOL, D, Dart, eC, Erlang, Elixir, F, F#, Fortran, Go, Haskell, IBM/360 Assembler, IBM i Control Language (CL), IBM RPG, Java,[a] MATLAB, ML, Modula, Modula-2, Modula-3, Morpho, NEWP, Oberon, Oberon-2, Objective-C, OCaml, several derivatives of Pascal (Component Pascal, Object Pascal, Turbo Pascal, UCSD Pascal), Perl, PL/I, PureBasic, Python, Ruby,[2] Rust, JavaScript,[3] Visual Basic .NET and WebDNA.\n\nConspicuous examples of languages that lack support for modules are C, C++,[4] and Pascal (in its original form). As of 2014, modules have been proposed for C++;[5] modules were added to Objective-C in iOS 7 (2013); and Pascal was superseded by Modula and Oberon, which included modules from the start, and various derivatives that included modules. JavaScript has got native modules since ECMAScript 2015.\n\nModular programming can be performed even where the programming language lacks explicit syntactic features to support named modules. For example, the IBM System i also uses modules when programming in the Integrated Language Environment (ILE).\n\nKey aspects[edit]\nWith modular programming, concerns are separated such that modules perform logically discrete functions, interacting through well-defined interfaces. Often modules form a directed acyclic graph (DAG); in this case a cyclic dependency between modules is seen as indicating that these should be a single module. In the case where modules do form a DAG they can be arranged as a hierarchy, where the lowest-level modules are independent, depending on no other modules, and higher-level modules depend on lower-level ones. A particular program or library is a top-level module of its own hierarchy, but can in turn be seen as a lower-level module of a higher-level program, library, or system.\n\nWhen creating a modular system, instead of creating a monolithic application (where the smallest component is the whole), several smaller modules are written separately so that, when composed together, they construct the executable application program. Typically these are also compiled separately, via separate compilation, and then linked by a linker. A just-in-time compiler may perform some of this construction \"on-the-fly\" at run time.\n\nThis makes modular designed systems, if built correctly, far more reusable than a traditional monolithic design, since all (or many) of these modules may then be reused (without change) in other projects. This also facilitates the \"breaking down\" of projects into several smaller projects. Theoretically, a modularized software project will be more easily assembled by large teams, since no team members are creating the whole system, or even need to know about the system as a whole. They can focus just on the assigned smaller task (this, it is claimed, counters the key assumption of The Mythical Man Month—making it actually possible to add more developers to a late software project—without making it later still).\n\nHistory[edit]\nModular programming, in the form of subsystems (particularly for I/O) and software libraries, dates to early software systems, where it was used for code reuse. Modular programming per se, with a goal of modularity, developed in the late 1960s and 1970s, as a programming in the large analog of the programming in the small concept of structured programming (1960s). The term \"modular programming\" dates at least to the National Symposium on Modular Programming, organized at the Information and Systems Institute in July 1968 by Larry Constantine; other key concepts were information hiding (1972) and separation of concerns (SoC, 1974).\n\nModules were not included in the original specification for ALGOL 68 (1968), but were included as extensions in early implementations, ALGOL 68-R (1970) and ALGOL 68C (1970), and later formalized.[6] One of the first languages designed from the start for modular programming was the short-lived Modula (1975), by Niklaus Wirth. Another early modular language was Mesa (1970s), by Xerox PARC, and Wirth drew on Mesa as well as the original Modula in its successor, Modula-2 (1978), which influenced later languages, particularly through its successor, Modula-3 (1980s). Modula's use of dot-qualified names, like M.a to refer to object a from module M, coincides with notation to access a field of a record (and similarly for attributes or methods of objects), and is now widespread, seen in C#, Dart, Go, Java, and Python, among others. Modular programming became widespread from the 1980s: the original Pascal language (1970) did not include modules, but later versions, notably UCSD Pascal (1978) and Turbo Pascal (1983) included them in the form of \"units\", as did the Pascal-influenced Ada (1980). The Extended Pascal ISO 10206:1990 standard kept closer to Modula2 in its modular support. Standard ML (1984)[7] has one of the most complete module systems, including functors (parameterized modules) to map between modules.\n\nIn the 1980s and 1990s modular programming was overshadowed by and often conflated with object-oriented programming, particularly due to the popularity of C++ and Java; this was also seen in the failure of Modula-3, which included modules but not objects. For example, the C family of languages had support for objects and classes in C++ (originally C with Classes, 1980) and Objective-C (1983), only supporting modules 30 years or more later. Java (1995) supports modules in the form of packages, though the primary unit of code organization is a class. However, Python (1991) prominently used both modules and objects from the start, using modules as the primary unit of code organization and \"packages\" as a larger-scale unit; and Perl 5 (1994) includes support for both modules and objects, with a vast array of modules being available from CPAN (1993).\n\nModular programming is now widespread, and found in virtually all major languages developed since the 1990s. The relative importance of modules varies between languages, and in class-based object-oriented languages there is still overlap and confusion with classes as a unit of organization and encapsulation, but these are both well-established as distinct concepts.\n\nSee also[edit]\nArchitecture description language\nCohesion (computer science)\nComponent-based software engineering\nConstructionist design methodology, a methodology for creating modular, broad Artificial Intelligence systems\nConway's law\nCoupling (computer science)\nDavid Parnas\nInformation hiding (encapsulation)\nLibrary (computing)\nList of system quality attributes\nPlug-in (computing)\nSnippet (programming)\nStructured Design\nStructured programming",
          "subparadigms": []
        },
        {
          "pdid": 67,
          "name": "Structured",
          "details": "Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of subroutines, block structures, for and while loops—in contrast to using simple tests and jumps such as the go to statement which could lead to \"spaghetti code\" causing difficulty to both follow and maintain.\n\nIt emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages,[1] with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966,[2] and the publication of the influential \"Go To Statement Considered Harmful\" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term \"structured programming\".[3]\n\nStructured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.\n\nContents  [hide] \n1\tElements\n1.1\tControl structures\n1.2\tSubroutines\n1.3\tBlocks\n2\tStructured programming languages\n3\tHistory\n3.1\tTheoretical foundation\n3.2\tDebate\n3.3\tOutcome\n4\tCommon deviations\n4.1\tEarly exit\n4.2\tException handling\n4.3\tMultiple entry\n4.4\tState machines\n5\tSee also\n6\tReferences\n7\tExternal links\nElements[edit]\nControl structures[edit]\nFollowing the structured program theorem, all programs are seen as composed of control structures:\n\n\"Sequence\"; ordered statements or subroutines executed in sequence.\n\"Selection\"; one or a number of statements is executed depending on the state of the program. This is usually expressed with keywords such as if..then..else..endif.\n\"Iteration\"; a statement or block is executed until the program reaches a certain state, or operations have been applied to every element of a collection. This is usually expressed with keywords such as while, repeat, for or do..until. Often it is recommended that each loop should only have one entry point (and in the original structural programming, also only one exit point, and a few languages enforce this).\n\"Recursion\"; a statement is executed by repeatedly calling itself until termination conditions are met. While similar in practice to iterative loops, recursive loops may be more computationally efficient, and are implemented differently as a cascading stack.\n\nGraphical representations of the three basic patterns using NS diagrams (blue) and flow charts (green).\nSubroutines[edit]\nSubroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.\n\nBlocks[edit]\nBlocks are used to enable groups of statements to be treated as if they were one statement. Block-structured languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by if..fi as in ALGOL 68, or a code section bracketed by BEGIN..END, as in PL/I, whitespace indentation as in Python - or the curly braces {...} of C and many later languages.\n\nStructured programming languages[edit]\nIt is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada – but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult. Structured programming (sometimes known as modular programming) is a subset of imperative programming that enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.\n\nHistory[edit]\nTheoretical foundation[edit]\nThe structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a \"structured program\" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself.[4] The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.\n\nDebate[edit]\nP. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:\n\nUs converts waved this interesting bit of news under the noses of the unreconstructed assembly-language programmers who kept trotting forth twisty bits of logic and saying, 'I betcha can't structure this.' Neither the proof by Böhm and Jacopini nor our repeated successes at writing structured code brought them around one day sooner than they were ready to convince themselves.[5]\nDonald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees[citation needed]) with abolishing the GOTO statement. In his 1974 paper, \"Structured Programming with Goto Statements\",[6] he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs[when defined as?].[who?]\n\nStructured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for the New York Times research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.[citation needed]\n\nAs late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled \"\"GOTO considered harmful\" considered harmful\".[7] Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.\n\nOutcome[edit]\nBy the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.\n\nCommon deviations[edit]\nWhile goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.\n\nEarly exit[edit]\nThe most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a return statement. At the level of loops, this is a break statement (terminate the loop) or continue statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have \"labeled breaks\", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.\n\nMultiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered \"exceptional\" circumstances that prevent it from continuing, hence needing exception handling.\n\nThe most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not deallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal don't have this problem.\n\nMost modern languages provide language-level support to prevent such leaks;[8] see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a goto. This is most often known as try...finally, and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.\n\nKent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that \"one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t\". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors).[9] Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.[10]\n\nIn his 2004 textbook, David Watt writes that \"single-entry multi-exit control flows are often desirable\". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as escape sequencers, defined as a \"sequencer that terminates execution of a textually enclosing command or procedure\", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce \"spaghetti code\". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.[11]\n\nIn contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like break and continue \"are just the old goto in sheep's clothing\" and strongly advised against their use.[12]\n\nException handling[edit]\nBased on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and propose that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as throw()[13] Bonang proposes that all single-exit conforming C++ should be written along the lines of:\n\nbool myCheck1() throw()\n{\n  bool success = false;\n  try {\n    // do something that may throw exceptions\n    if(myCheck2() == false) {\n      throw SomeInternalException();\n    }\n    // other code similar to the above\n    success = true;\n  }\n  catch(...) { // all exceptions caught and logged\n  }\n  return success;\n}\nPeter Ritchie also notes that, in principle, even a single throw right before the return in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.[14]\n\nDavid Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic overflows or input/output failures like file not found) is a kind of error that \"is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit\". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where \"the application code tends to get cluttered by tests of status flags\" and that \"the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!\" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.[15]\n\nThe textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like while loops because the transfer of control \"is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred.\"[16] Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like for, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if init() throws an exception in for (init(); check(); increm()), then the usual exit point after check() is not reached.[17] Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they \"create hidden control-flow paths that are difficult for programmers to reason about\".[18]:8:27\n\nThe necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like parallel do, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from break to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.[19]\n\nMultiple entry[edit]\nFurther information: Coroutine\nMore rarely, subprograms allow multiple entry. This is most commonly only re-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.\n\nIt is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.\n\nState machines[edit]\nSome programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers (including Knuth[citation needed]) implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.\n\nHowever, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.\n\nSee also[edit]\nDRAKON\nMinimal evaluation\nNassi–Shneiderman diagram\nStructure chart\nSwitch statement",
          "subparadigms": [
            65,
            66,
            64,
            68
          ]
        },
        {
          "pdid": 68,
          "name": "Recursive",
          "details": "Recursion in computer science is a method where the solution to a problem depends on solutions to smaller instances of the same problem (as opposed to iteration).[1] The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[2]\n\n\"The power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions.\"[3]\n\nMost computer programming languages support recursion by allowing a function to call itself within the program text. Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. Computability theory proves that these recursive-only languages are Turing complete; they are as computationally powerful as Turing complete imperative languages, meaning they can solve the same kinds of problems as imperative languages even without iterative control structures such as “while” and “for”.\n\nContents  [hide] \n1\tRecursive functions and algorithms\n2\tRecursive data types\n2.1\tInductively defined data\n2.2\tCoinductively defined data and corecursion\n3\tTypes of recursion\n3.1\tSingle recursion and multiple recursion\n3.2\tIndirect recursion\n3.3\tAnonymous recursion\n3.4\tStructural versus generative recursion\n4\tRecursive programs\n4.1\tRecursive procedures\n4.2\tRecursive data structures (structural recursion)\n5\tImplementation issues\n5.1\tWrapper function\n5.2\tShort-circuiting the base case\n5.3\tHybrid algorithm\n6\tRecursion versus iteration\n6.1\tExpressive power\n6.2\tPerformance issues\n6.3\tStack space\n6.4\tMultiply recursive problems\n7\tTail-recursive functions\n8\tOrder of execution\n8.1\tFunction 1\n8.2\tFunction 2 with swapped lines\n9\tTime-efficiency of recursive algorithms\n9.1\tShortcut rule (master theorem)\n10\tSee also\n10.1\tRecursive functions\n10.2\tBooks\n11\tNotes and references\n12\tFurther reading\n13\tExternal links\nRecursive functions and algorithms[edit]\nA common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.\n\nA recursive function definition has one or more base cases, meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases, meaning input(s) for which the program recurs (calls itself). For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n! = n(n − 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\".\n\nThe job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances—for example, some system and server processes—are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop.\n\nFor some functions (such as one that computes the series for e = 1/0! + 1/1! + 1/2! + 1/3! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case. Such an example is more naturally treated by co-recursion, where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the nth term (nth partial sum)\".\n\nRecursive data types[edit]\nMany computer programs must process or generate an arbitrarily large quantity of data. Recursion is one technique for representing data whose exact size the programmer does not know: the programmer can specify this data with a self-referential definition. There are two types of self-referential definitions: inductive and coinductive definitions.\n\nFurther information: Algebraic data type\nInductively defined data[edit]\nMain article: Recursive data type\nAn inductively defined recursive data definition is one that specifies how to construct instances of the data. For example, linked lists can be defined inductively (here, using Haskell syntax):\n\ndata ListOfStrings = EmptyList | Cons String ListOfStrings\nThe code above specifies a list of strings to be either empty, or a structure that contains a string and a list of strings. The self-reference in the definition permits the construction of lists of any (finite) number of strings.\n\nAnother example of inductive definition is the natural numbers (or positive integers):\n\nA natural number is either 1 or n+1, where n is a natural number.\nSimilarly recursive definitions are often used to model the structure of expressions and statements in programming languages. Language designers often express grammars in a syntax such as Backus-Naur form; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition:\n\n <expr> ::= <number>\n          | (<expr> * <expr>)\n          | (<expr> + <expr>)\nThis says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complex arithmetic expressions such as (5 * ((3 * 6) + 8)), with more than one product or sum operation in a single expression.\n\nCoinductively defined data and corecursion[edit]\nMain articles: Coinduction and Corecursion\nA coinductive data definition is one that specifies the operations that may be performed on a piece of data; typically, self-referential coinductive definitions are used for data structures of infinite size.\n\nA coinductive definition of infinite streams of strings, given informally, might look like this:\n\nA stream of strings is an object s such that:\n head(s) is a string, and\n tail(s) is a stream of strings.\nThis is very similar to an inductive definition of lists of strings; the difference is that this definition specifies how to access the contents of the data structure—namely, via the accessor functions head and tail—and what those contents may be, whereas the inductive definition specifies how to create the structure and what it may be created from.\n\nCorecursion is related to coinduction, and can be used to compute particular instances of (possibly) infinite objects. As a programming technique, it is used most often in the context of lazy programming languages, and can be preferable to recursion when the desired size or precision of a program's output is unknown. In such cases the program requires both a definition for an infinitely large (or infinitely precise) result, and a mechanism for taking a finite portion of that result. The problem of computing the first n prime numbers is one that can be solved with a corecursive program (e.g. here).\n\nTypes of recursion[edit]\nSingle recursion and multiple recursion[edit]\nRecursion that only contains a single self-reference is known as single recursion, while recursion that contains multiple self-references is known as multiple recursion. Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal, such as in a depth-first search.\n\nSingle recursion is often much more efficient than multiple recursion, and can generally be replaced by an iterative computation, running in linear time and requiring constant space. Multiple recursion, by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack.\n\nMultiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively is multiple iteration, as each value requires two previous values, it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion, building up from the initial values, tracking at each step two successive values – see corecursion: examples. A more sophisticated example is using a threaded binary tree, which allows iterative tree traversal, rather than multiple recursion.\n\nIndirect recursion[edit]\nMain article: Mutual recursion\nMost basic examples of recursion, and most of the examples presented here, demonstrate direct recursion, in which a function calls itself. Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again.\n\nIndirect recursion is also called mutual recursion, which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.\n\nAnonymous recursion[edit]\nMain article: Anonymous recursion\nRecursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions, and is known as anonymous recursion.\n\nStructural versus generative recursion[edit]\nSee also: Structural recursion\nSome authors classify recursion as either \"structural\" or \"generative\". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data:\n\n[Functions that consume structured data] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS.[4]\n\nThus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals, including XML processing, binary tree creation and search, etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion.\n\nGenerative recursion is the alternative:\n\nMany well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it. HtDP (How To Design Programs) refers to this kind as generative recursion. Examples of generative recursion include: gcd, quicksort, binary search, mergesort, Newton's method, fractals, and adaptive integration.[5]\n\nThis distinction is important in proving termination of a function.\n\nAll structurally recursive functions on finite (inductively defined) data structures can easily be shown to terminate, via structural induction: intuitively, each recursive call receives a smaller piece of input data, until a base case is reached.\nGeneratively recursive functions, in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions – each step generates the new data, such as successive approximation in Newton's method – and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed.\nIn terms of loop variants, structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step.\nBy contrast, generative recursion is when there is not such an obvious loop variant, and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis.\nRecursive programs[edit]\nRecursive procedures[edit]\nFactorial[edit]\nA classic example of a recursive procedure is the function used to calculate the factorial of a natural number:\n\n{\\displaystyle \\operatorname {fact} (n)={\\begin{cases}1&{\\mbox{if }}n=0\\\\n\\cdot \\operatorname {fact} (n-1)&{\\mbox{if }}n>0\\\\\\end{cases}}} \\operatorname {fact} (n)={\\begin{cases}1&{\\mbox{if }}n=0\\\\n\\cdot \\operatorname {fact} (n-1)&{\\mbox{if }}n>0\\\\\\end{cases}}\nPseudocode (recursive):\nfunction factorial is:\ninput: integer n such that n >= 0\noutput: [n × (n-1) × (n-2) × … × 1]\n\n    1. if n is 0, return 1\n    2. otherwise, return [ n × factorial(n-1) ]\n\nend factorial\nThe function can also be written as a recurrence relation:\n\n{\\displaystyle b_{n}=nb_{n-1}} b_{n}=nb_{n-1}\n{\\displaystyle b_{0}=1} b_{0}=1\nThis evaluation of the recurrence relation demonstrates the computation that would be performed in evaluating the pseudocode above:\n\nComputing the recurrence relation for n = 4:\nb4           = 4 * b3\n             = 4 * (3 * b2)\n             = 4 * (3 * (2 * b1))\n             = 4 * (3 * (2 * (1 * b0)))\n             = 4 * (3 * (2 * (1 * 1)))\n             = 4 * (3 * (2 * 1))\n             = 4 * (3 * 2)\n             = 4 * 6\n             = 24\nThis factorial function can also be described without using recursion by making use of the typical looping constructs found in imperative programming languages:\n\nPseudocode (iterative):\nfunction factorial is:\ninput: integer n such that n >= 0\noutput: [n × (n-1) × (n-2) × … × 1]\n\n    1. create new variable called running_total with a value of 1\n\n    2. begin loop\n          1. if n is 0, exit loop\n          2. set running_total to (running_total × n)\n          3. decrement n\n          4. repeat loop\n\n    3. return running_total\n\nend factorial\nThe imperative code above is equivalent to this mathematical definition using an accumulator variable t:\n\n{\\displaystyle {\\begin{array}{rcl}\\operatorname {fact} (n)&=&\\operatorname {fact_{acc}} (n,1)\\\\\\operatorname {fact_{acc}} (n,t)&=&{\\begin{cases}t&{\\mbox{if }}n=0\\\\\\operatorname {fact_{acc}} (n-1,nt)&{\\mbox{if }}n>0\\\\\\end{cases}}\\end{array}}} {\\begin{array}{rcl}\\operatorname {fact} (n)&=&\\operatorname {fact_{acc}} (n,1)\\\\\\operatorname {fact_{acc}} (n,t)&=&{\\begin{cases}t&{\\mbox{if }}n=0\\\\\\operatorname {fact_{acc}} (n-1,nt)&{\\mbox{if }}n>0\\\\\\end{cases}}\\end{array}}\nThe definition above translates straightforwardly to functional programming languages such as Scheme; this is an example of iteration implemented recursively.\n\nGreatest common divisor[edit]\nThe Euclidean algorithm, which computes the greatest common divisor of two integers, can be written recursively.\n\nFunction definition:\n\n{\\displaystyle \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\\\end{cases}}} \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\\\end{cases}}\nPseudocode (recursive):\nfunction gcd is:\ninput: integer x, integer y such that x > 0 and y >= 0\n\n    1. if y is 0, return x\n    2. otherwise, return [ gcd( y, (remainder of x/y) ) ]\n\nend gcd\nRecurrence relation for greatest common divisor, where {\\displaystyle x\\%y} x\\%y expresses the remainder of {\\displaystyle x/y} x/y:\n\n{\\displaystyle \\gcd(x,y)=\\gcd(y,x\\%y)} \\gcd(x,y)=\\gcd(y,x\\%y) if {\\displaystyle y\\neq 0} y\\neq 0\n{\\displaystyle \\gcd(x,0)=x} \\gcd(x,0)=x\nComputing the recurrence relation for x = 27 and y = 9:\ngcd(27, 9)   = gcd(9, 27% 9)\n             = gcd(9, 0)\n             = 9\nComputing the recurrence relation for x = 111 and y = 259:\ngcd(111, 259)   = gcd(259, 111% 259)\n                = gcd(259, 111)\n                = gcd(111, 259% 111)\n                = gcd(111, 37)\n                = gcd(37, 111% 37)\n                = gcd(37, 0)\n                = 37\nThe recursive program above is tail-recursive; it is equivalent to an iterative algorithm, and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls. Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack.\n\nPseudocode (iterative):\nfunction gcd is:\ninput: integer x, integer y such that x >= y and y >= 0\n\n    1. create new variable called remainder\n\n    2. begin loop\n          1. if y is zero, exit loop\n          2. set remainder to the remainder of x/y\n          3. set x to y\n          4. set y to remainder\n          5. repeat loop\n\n    3. return x\n\nend gcd\nThe iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.\n\nTowers of Hanoi[edit]\n\nTowers of Hanoi\nMain article: Towers of Hanoi\nThe Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.[6][7] There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack?\n\nFunction definition:\n\n{\\displaystyle \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\\\end{cases}}} \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\\\end{cases}}\nRecurrence relation for hanoi:\n\n{\\displaystyle h_{n}=2h_{n-1}+1} h_{n}=2h_{n-1}+1\n{\\displaystyle h_{1}=1} h_{1}=1\nComputing the recurrence relation for n = 4:\nhanoi(4)     = 2*hanoi(3) + 1\n             = 2*(2*hanoi(2) + 1) + 1\n             = 2*(2*(2*hanoi(1) + 1) + 1) + 1\n             = 2*(2*(2*1 + 1) + 1) + 1\n             = 2*(2*(3) + 1) + 1\n             = 2*(7) + 1\n             = 15\n\n\nExample implementations:\n\nPseudocode (recursive):\nfunction hanoi is:\ninput: integer n, such that n >= 1\n\n    1. if n is 1 then return 1\n\n    2. return [2 * [call hanoi(n-1)] + 1]\n\nend hanoi\nAlthough not all recursive functions have an explicit solution, the Tower of Hanoi sequence can be reduced to an explicit formula.[8]\n\nAn explicit formula for Towers of Hanoi:\nh1 = 1   = 21 - 1\nh2 = 3   = 22 - 1\nh3 = 7   = 23 - 1\nh4 = 15  = 24 - 1\nh5 = 31  = 25 - 1\nh6 = 63  = 26 - 1\nh7 = 127 = 27 - 1\nIn general:\nhn = 2n - 1, for all n >= 1\nBinary search[edit]\nThe binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for.\n\nRecursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass.\n\nExample implementation of binary search in C:\n\n /*\n  Call binary_search with proper initial conditions.\n\n  INPUT:\n    data is an array of integers SORTED in ASCENDING order,\n    toFind is the integer to search for,\n    count is the total number of elements in the array\n\n  OUTPUT:\n    result of binary_search\n\n */\n int search(int *data, int toFind, int count)\n {\n    //  Start = 0 (beginning index)\n    //  End = count - 1 (top index)\n    return binary_search(data, toFind, 0, count-1);\n }\n\n /*\n   Binary Search Algorithm.\n\n   INPUT:\n        data is a array of integers SORTED in ASCENDING order,\n        toFind is the integer to search for,\n        start is the minimum array index,\n        end is the maximum array index\n   OUTPUT:\n        position of the integer toFind within array data,\n        -1 if not found\n */\n int binary_search(int *data, int toFind, int start, int end)\n {\n    //Get the midpoint.\n    int mid = start + (end - start)/2;   //Integer division\n\n    //Stop condition.\n    if (start > end)\n       return -1;\n    else if (data[mid] == toFind)        //Found?\n       return mid;\n    else if (data[mid] > toFind)         //Data is greater than toFind, search lower half\n       return binary_search(data, toFind, start, mid-1);\n    else                                 //Data is less than toFind, search upper half\n       return binary_search(data, toFind, mid+1, end);\n }\nRecursive data structures (structural recursion)[edit]\nMain article: Recursive data type\nAn important application of recursion in computer science is in defining dynamic data structures such as lists and trees. Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.\n\n\"Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"[9]\n\nThe examples in this section illustrate what is known as \"structural recursion\". This term refers to the fact that the recursive procedures are acting on data that is defined recursively.\n\nAs long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.[5]\n\nLinked lists[edit]\nMain article: Linked list\nBelow is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of struct node is a pointer to another struct node, effectively creating a list type.\n\nstruct node\n{\n  int data;           // some integer data\n  struct node *next;  // pointer to another struct node\n};\nBecause the struct node data structure is defined recursively, procedures that operate on it can be implemented naturally as recursive procedures. The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the list_print procedure.\n\nvoid list_print(struct node *list)\n{\n    if (list != NULL)               // base case\n    {\n       printf (\"%d \", list->data);  // print integer data followed by a space\n       list_print (list->next);     // recursive call on the next node\n    }\n}\nBinary trees[edit]\nMain article: Binary tree\nBelow is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree).\n\nstruct node\n{\n  int data;            // some integer data\n  struct node *left;   // pointer to the left subtree\n  struct node *right;  // point to the right subtree\n};\nOperations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls:\n\n// Test if tree_node contains i; return 1 if so, 0 if not.\nint tree_contains(struct node *tree_node, int i) {\n    if (tree_node == NULL)\n        return 0;  // base case\n    else if (tree_node->data == i)\n        return 1;\n    else\n        return tree_contains(tree_node->left, i) || tree_contains(tree_node->right, i);\n}\nAt most two recursive calls will be made for any given call to tree_contains as defined above.\n\n// Inorder traversal:\nvoid tree_print(struct node *tree_node) {\n        if (tree_node != NULL) {                  // base case\n                tree_print(tree_node->left);      // go left\n                printf(\"%d \", tree_node->data);   // print the integer followed by a space\n                tree_print(tree_node->right);     // go right\n        }\n}\nThe above example illustrates an in-order traversal of the binary tree. A Binary search tree is a special case of the binary tree where the data elements of each node are in order.\n\nFilesystem traversal[edit]\nSince the number of files in a filesystem may vary, recursion is the only practical way to traverse and thus enumerate its contents. Traversing a filesystem is very similar to that of tree traversal, therefore the concepts behind tree traversal are applicable to traversing a filesystem. More specifically, the code below would be an example of a preorder traversal of a filesystem.\n\nimport java.io.*;\n\npublic class FileSystem {\n\n\tpublic static void main (String [] args) {\n\t\ttraverse ();\n\t}\n\n\t/**\n\t * Obtains the filesystem roots\n\t * Proceeds with the recursive filesystem traversal\n\t */\n\tprivate static void traverse () {\n\t\tFile [] fs = File.listRoots ();\n\t\tfor (int i = 0; i < fs.length; i++) {\n\t\t\tif (fs[i].isDirectory () && fs[i].canRead ()) {\n\t\t\t\trtraverse (fs[i]);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Recursively traverse a given directory\n\t *\n\t * @param fd indicates the starting point of traversal\n\t */\n\tprivate static void rtraverse (File fd) {\n\t\tFile [] fss = fd.listFiles ();\n\n\t\tfor (int i = 0; i < fss.length; i++) {\n\t\t\tSystem.out.println (fss[i]);\n\t\t\tif (fss[i].isDirectory () && fss[i].canRead ()) {\n\t\t\t\trtraverse (fss[i]);\n\t\t\t}\n\t\t}\n\t}\n\n}\nThis code blends the lines, at least somewhat, between recursion and iteration. It is, essentially, a recursive implementation, which is the best way to traverse a filesystem. It is also an example of direct and indirect recursion. The method \"rtraverse\" is purely a direct example; the method \"traverse\" is the indirect, which calls \"rtraverse.\" This example needs no \"base case\" scenario due to the fact that there will always be some fixed number of files or directories in a given filesystem.\n\nImplementation issues[edit]\nIn actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include:\n\nWrapper function (at top)\nShort-circuiting the base case, aka \"Arm's-length recursion\" (at bottom)\nHybrid algorithm (at bottom) – switching to a different algorithm once data is small enough\nOn the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.\n\nWrapper function[edit]\nA wrapper function is a function that is directly called but does not recurse itself, instead calling a separate auxiliary function which actually does the recursion.\n\nWrapper functions can be used to validate parameters (so the recursive function can skip these), perform initialization (allocate memory, initialize variables), particularly for auxiliary variables such as \"level of recursion\" or partial computations for memoization, and handle exceptions and errors. In languages that support nested functions, the auxiliary function can be nested inside the wrapper function and use a shared scope. In the absence of nested functions, auxiliary functions are instead a separate function, if possible private (as they are not called directly), and information is shared with the wrapper function by using pass-by-reference.\n\nShort-circuiting the base case[edit]\nShort-circuiting the base case, also known as arm's-length recursion, consists of checking the base case before making a recursive call – i.e., checking if the next call will be the base case, instead of calling and then checking for the base case. Short-circuiting is particularly done for efficiency reasons, to avoid the overhead of a function call that immediately returns. Note that since the base case has already been checked for (immediately before the recursive step), it does not need to be checked for separately, but one does need to use a wrapper function for the case when the overall recursion starts with the base case itself. For example, in the factorial function, properly the base case is 0! = 1, while immediately returning 1 for 1! is a short-circuit, and may miss 0; this can be mitigated by a wrapper function.\n\nShort-circuiting is primarily a concern when many base cases are encountered, such as Null pointers in a tree, which can be linear in the number of function calls, hence significant savings for O(n) algorithms; this is illustrated below for a depth-first search. Short-circuiting on a tree corresponds to considering a leaf (non-empty node with no children) as the base case, rather than considering an empty node as the base case. If there is only a single base case, such as in computing the factorial, short-circuiting provides only O(1) savings.\n\nConceptually, short-circuiting can be considered to either have the same base case and recursive step, only checking the base case before the recursion, or it can be considered to have a different base case (one step removed from standard base case) and a more complex recursive step, namely \"check valid then recurse\", as in considering leaf nodes rather than Null nodes as base cases in a tree. Because short-circuiting has a more complicated flow, compared with the clear separation of base case and recursive step in standard recursion, it is often considered poor style, particularly in academia.\n\nDepth-first search[edit]\nA basic example of short-circuiting is given in depth-first search (DFS) of a binary tree; see binary trees section for standard recursive discussion.\n\nThe standard recursive algorithm for a DFS is:\n\nbase case: If current node is Null, return false\nrecursive step: otherwise, check value of current node, return true if match, otherwise recurse on children\nIn short-circuiting, this is instead:\n\ncheck value of current node, return true if match,\notherwise, on children, if not Null, then recurse.\nIn terms of the standard steps, this moves the base case check before the recursive step. Alternatively, these can be considered a different form of base case and recursive step, respectively. Note that this requires a wrapper function to handle the case when the tree itself is empty (root node is Null).\n\nIn the case of a perfect binary tree of height h, there are 2h+1−1 nodes and 2h+1 Null pointers as children (2 for each of the 2h leaves), so short-circuiting cuts the number of function calls in half in the worst case.\n\nIn C, the standard recursive algorithm may be implemented as:\n\nbool tree_contains(struct node *tree_node, int i) {\n    if (tree_node == NULL)\n        return false;  // base case\n    else if (tree_node->data == i)\n        return true;\n    else\n        return tree_contains(tree_node->left, i) ||\n               tree_contains(tree_node->right, i);\n}\nThe short-circuited algorithm may be implemented as:\n\n// Wrapper function to handle empty tree\nbool tree_contains(struct node *tree_node, int i) {\n    if (tree_node == NULL)\n        return false;  // empty tree\n    else\n        return tree_contains_do(tree_node, i);  // call auxiliary function\n}\n\n// Assumes tree_node != NULL\nbool tree_contains_do(struct node *tree_node, int i) {\n    if (tree_node->data == i)\n        return true;  // found\n    else  // recurse\n        return (tree_node->left  && tree_contains_do(tree_node->left,  i)) ||\n               (tree_node->right && tree_contains_do(tree_node->right, i));\n}\nNote the use of short-circuit evaluation of the Boolean && (AND) operators, so that the recursive call is only made if the node is valid (non-Null). Note that while the first term in the AND is a pointer to a node, the second term is a bool, so the overall expression evaluates to a bool. This is a common idiom in recursive short-circuiting. This is in addition to the short-circuit evaluation of the Boolean || (OR) operator, to only check the right child if the left child fails. In fact, the entire control flow of these functions can be replaced with a single Boolean expression in a return statement, but legibility suffers at no benefit to efficiency.\n\nHybrid algorithm[edit]\nRecursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. An important example is merge sort, which is often implemented by switching to the non-recursive insertion sort when the data is sufficiently small, as in the tiled merge sort. Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort.\n\nRecursion versus iteration[edit]\nRecursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit stack, while iteration can be replaced with tail recursion. Which approach is preferable depends on the problem under consideration and the language used. In imperative programming, iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion. By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead, and sometimes explicit iteration is not available.\n\nCompare the templates to compute xn defined by xn = f(n, xn-1) from xbase:\n\nfunction recursive(n)\n    if n==base\n        return xbase\n    else\n        return f(n, recursive(n-1)) \nfunction iterative(n)\n    x = xbase\n    for i = n downto base\n        x = f(i, x)\n    return x\nFor imperative language the overhead is to define the function, for functional language the overhead is to define the accumulator variable x.\n\nFor example, the factorial function may be implemented iteratively in C by assigning to an loop index variable and accumulator variable, rather than passing arguments and returning values by recursion:\n\nunsigned int factorial(unsigned int n) {\n  unsigned int product = 1; // empty product is 1\n  while (n) {\n    product *= n;\n    --n;\n  }\n  return product;\n}\nExpressive power[edit]\nMost programming languages in use today allow the direct specification of recursive functions and procedures. When such a function is called, the program's runtime environment keeps track of the various instances of the function (often using a call stack, although other methods may be used). Every recursive function can be transformed into an iterative function by replacing recursive calls with iterative control constructs and simulating the call stack with a stack explicitly managed by the program.[10][11]\n\nConversely, all iterative functions and procedures that can be evaluated by a computer (see Turing completeness) can be expressed in terms of recursive functions; iterative control constructs such as while loops and do loops are routinely rewritten in recursive form in functional languages.[12][13] However, in practice this rewriting depends on tail call elimination, which is not a feature of all languages. C, Java, and Python are notable mainstream languages in which all function calls, including tail calls, may cause stack allocation that would not occur with the use of looping constructs; in these languages, a working iterative program rewritten in recursive form may overflow the call stack, although tail call elimination may be a feature that is not covered by a language's specification, and different implementations of the same language may differ in tail call elimination capabilities.\n\nPerformance issues[edit]\nIn languages (such as C and Java) that favor iterative looping constructs, there is usually significant time and space cost associated with recursive programs, due to the overhead required to manage the stack and the relative slowness of function calls; in functional languages, a function call (particularly a tail call) is typically a very fast operation, and the difference is usually less noticeable.\n\nAs a concrete example, the difference in performance between recursive and iterative implementations of the \"factorial\" example above depends highly on the compiler used. In languages where looping constructs are preferred, the iterative version may be as much as several orders of magnitude faster than the recursive one. In functional languages, the overall time difference of the two implementations may be negligible; in fact, the cost of multiplying the larger numbers first rather than the smaller numbers (which the iterative version given here happens to do) may overwhelm any time saved by choosing iteration.\n\nStack space[edit]\nIn some programming languages, the stack space available to a thread is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. Consequently, these languages sometimes place a limit on the depth of recursion to avoid stack overflows; Python is one such language.[14] Note the caveat below regarding the special case of tail recursion.\n\nMultiply recursive problems[edit]\nMultiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search; contrast with list traversal and linear search in a list, which is singly recursive and thus naturally iterative. Other examples include divide-and-conquer algorithms such as Quicksort, and functions such as the Ackermann function. All of these algorithms can be implemented iteratively with the help of an explicit stack, but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.\n\nTail-recursive functions[edit]\nTail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls, a tail-recursive function such as gcd will execute using constant space. Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops.\n\nTail recursion:\tAugmenting recursion:\n//INPUT: Integers x, y such that x >= y and y > 0\nint gcd(int x, int y)\n{\n  if (y == 0)\n     return x;\n  else\n     return gcd(y, x % y);\n}\n//INPUT: n is an Integer such that n >= 0\nint fact(int n)\n{\n   if (n == 0)\n      return 1;\n   else\n      return n * fact(n - 1);\n}\nThe significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.\n\nOrder of execution[edit]\nIn the simple case of a function calling itself only once, instructions placed before the recursive call are executed once per recursion before any of the instructions placed after the recursive call. The latter are executed repeatedly after the maximum recursion has been reached. Consider this example:\n\nFunction 1[edit]\nvoid recursiveFunction(int num) {\n   printf(\"%d\\n\", num);\n   if (num < 4)\n      recursiveFunction(num + 1);\n}\nRecursive1.svg\n\nFunction 2 with swapped lines[edit]\nvoid recursiveFunction(int num) {\n   if (num < 4)\n      recursiveFunction(num + 1);\n   printf(\"%d\\n\", num);\n}\nRecursive2.svg\n\nTime-efficiency of recursive algorithms[edit]\nThe time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation. They can (usually) then be simplified into a single Big-Oh term.\n\nShortcut rule (master theorem)[edit]\nMain article: Master theorem\nIf the time-complexity of the function is in the form\n\n{\\displaystyle T(n)=a\\cdot T(n/b)+f(n)} {\\displaystyle T(n)=a\\cdot T(n/b)+f(n)}\n\nThen the Big-Oh of the time-complexity is thus:\n\nIf {\\displaystyle f(n)=O(n^{\\log _{b}a-\\epsilon })} {\\displaystyle f(n)=O(n^{\\log _{b}a-\\epsilon })} for some constant {\\displaystyle \\epsilon >0} \\epsilon >0, then {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a})} {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a})}\nIf {\\displaystyle f(n)=\\Theta (n^{\\log _{b}a})} {\\displaystyle f(n)=\\Theta (n^{\\log _{b}a})}, then {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a}\\log n)} {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a}\\log n)}\nIf {\\displaystyle f(n)=\\Omega (n^{\\log _{b}a+\\epsilon })} {\\displaystyle f(n)=\\Omega (n^{\\log _{b}a+\\epsilon })} for some constant {\\displaystyle \\epsilon >0} \\epsilon >0, and if {\\displaystyle a\\cdot f(n/b)\\leq c\\cdot f(n)} {\\displaystyle a\\cdot f(n/b)\\leq c\\cdot f(n)} for some constant c < 1 and all sufficiently large n, then {\\displaystyle T(n)=\\Theta (f(n))} {\\displaystyle T(n)=\\Theta (f(n))}\nwhere a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f (n) represents the work the function does independent of any recursion (e.g. partitioning, recombining) at each level of recursion.\n\nSee also[edit]\nFunctional programming\nHierarchical and recursive queries in SQL\nKleene–Rosser paradox\nOpen recursion\nRecursion\nSierpiński curve\nRecursive functions[edit]\nMcCarthy 91 function\nμ-recursive functions\nPrimitive recursive functions\nTak (function)\nBooks[edit]\nStructure and Interpretation of Computer Programs\nWalls and Mirrors",
          "subparadigms": []
        },
        {
          "pdid": 69,
          "name": "Value-level",
          "details": "Value-level programming refers to one of the two contrasting programming paradigms identified by John Backus in his work on programs as mathematical objects, the other being function-level programming. Backus originally used the term object-level programming but that term is now prone to confusion with object-oriented programming.\n\nValue-level programs are those that describe how to combine various values (i.e., numbers, symbols, strings, etc.) to form other values until the final result values are obtained. New values are constructed from existing ones by the application of various value-to-value functions, such as addition, concatenation, matrix inversion, and so on.\n\nConventional, von Neumann programs are value-level: expressions on the right side of assignment statements are exclusively concerned with building a value that is then to be stored.\n\nConnection with Data Types[edit]\nThe value-level approach to programming invites the study of the space of values under the value-forming operations, and of the algebraic properties of those operations. This is what is called the study of data types, and it has advanced from focusing on the values themselves and their structure, to a primary concern with the value-forming operations and their structure, as given by certain axioms and algebraic laws, that is, to the algebraic study of data types.\n\nConnection with Lambda Calculus languages[edit]\nLambda calculus-based languages (such as Lisp, ISWIM, and Scheme) are in actual practice value-level languages, although they are not thus restricted by design.\n\nTo see why typical lambda style programs are primarily value-level, consider the usual definition of a value-to-value function, say\n\nf = λx.E\nhere, x must be a value variable (since the argument of f is a value by definition) and E must denote a value too (since f's result is a value by definition). Typically, E is an expression involving the application of value-forming functions to value variables and constants; nevertheless, a few value-forming functions having both function and value arguments do exist and are used for limited purposes.\n\nIf the term values is defined to include the value variables themselves, then the value-level view of programming is one of building values by the application of existing programs (value-forming operations/functions) to other values. Lambda-style programming builds a new program from the result-value by lambda-abstracting the value variables.",
          "subparadigms": []
        },
        {
          "pdid": 70,
          "name": "Probabilistic",
          "details": "A probabilistic programming language (PPL) is a programming language designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks, but are more expressive and flexible.[1] Probabilistic programming represents an attempt to \"[unify] general purpose programming with probabilistic modeling.\"[2]\n\nProbabilistic reasoning is a foundational technology of machine learning. It is used by companies such as Google, Amazon.com and Microsoft. Probabilistic reasoning has been used for predicting stock prices, recommending movies, diagnosing computers, detecting cyber intrusions and image detection.[3]\n\nPPLs often extend from a basic language. The choice of underlying basic language depends on the similarity of the model to the basic language's ontology, as well as commercial considerations and personal preference. For instance, Dimple[4] and Chimple[5] are based on Java, Infer.NET is based on .NET framework,[6] while PRISM extends from Prolog.[7] However, some PPLs such as WinBUGS and Stan offer a self-contained language, with no obvious origin in another language.[8][9]\n\nSeveral PPLs are in active development, including some in beta test.\n\nContents  [hide] \n1\tRelational\n2\tProbabilistic programming\n3\tApplications\n4\tList of probabilistic programming languages\n5\tSee also\n6\tNotes\n7\tExternal links\nRelational[edit]\nA probabilistic relational programming language (PRPL) is a PPL specially designed to describe and infer with probabilistic relational models (PRMs).\n\nA PRM is usually developed with a set of algorithms for reducing, inference about and discovery of concerned distributions, which are embedded into the corresponding PRPL.\n\nProbabilistic programming[edit]\nProbabilistic programming creates systems that help make decisions in the face of uncertainty. Probabilistic reasoning combines knowledge of a situation with the laws of probability. Until recently, probabilistic reasoning systems have been limited in scope, and have not successfully addressed real world situations. Probabilistic programming is a new approach that makes probabilistic reasoning systems easier to build and more widely applicable.[10] Reasoning about variables as probability distributions causes difficulties for novice programmers, but these difficulties can be addressed through use of Bayesian network visualisations and graphs of variable distributions embedded within the source code editor.[11]\n\nApplications[edit]\nIn 2015, a 50-line PPL computer vision program was used to generate 3D models of human faces based on 2D images of those faces. The program used inverse graphics as the basis of its inferencing,[3] done by a PPL language they call Picture, (and the Julia host language) that made possible \"in 50 lines of code what used to take thousands [whereas their experiments used their] probabilistic programming language they call Picture, which is an extension of Julia language, another language developed at MIT\".[12][13] A paper on the Picture language, shown at the 2015 Computer Vision and Pattern Recognition conference, was awarded \"Best Paper Honorable Mention\".[14]\n\nList of probabilistic programming languages[edit]\nThis list is incomplete; you can help by expanding it.\nName\tExtends from\tHost language\nVenture[15]\tScheme\tC++\nProbabilistic-C[16]\tC\tC\nAnglican[17]\tScheme\tClojure\nIBAL[18]\tOCaml\t\nPRISM[7]\tB-Prolog\t\nInfer.NET[6]\t.NET Framework\t.NET Framework\ndimple[4]\tMATLAB, Java\t\nchimple[5]\tMATLAB, Java\t\nBLOG[19]\tJava\t\nPSQL[20]\tSQL\t\nBUGS[8]\t\t\nFACTORIE[21]\tScala\t\nPMTK[22]\tMATLAB\tMATLAB\nAlchemy[23]\tC++\t\nDyna[24]\tProlog\t\nFigaro[25]\tScala\t\nChurch[26]\tScheme\tVarious: JavaScript, Scheme\nProbLog[27]\tProlog\tPython, Jython\nProBT[28]\tC++, Python\t\nStan (software)[9]\t\tC++\nHakaru[29]\tHaskell\tHaskell\nBAli-Phy (software)[30]\tHaskell\tC++\nProbCog[31]\t\tJava, Python\nGamble[32]\t\tRacket\nTuffy[33]\t\tJava\nPyMC[34]\tPython\tPython\nLea[35]\tPython\tPython\nWebPPL[36]\tJavaScript\tJavaScript\nPicture[3]\tJulia\tJulia\nTuring.jl[37]\tJulia\tJulia\nSee also[edit]\nStatistical relational learning\nInductive programming\nBayesian programming",
          "subparadigms": []
        },
        {
          "pdid": 71,
          "name": "Concept",
          "details": "Concept programming is a programming paradigm focusing on how concepts, that live in the programmer's mind, translate into representations that are found in the code space. This approach was introduced in 2001 by Christophe de Dinechin with the XL Programming Language.\n\nContents  [hide] \n1\tPseudo-metrics\n2\tRule of equivalence, equivalence breakdown\n3\tMethodology\n4\tLanguages\n5\tSimilar works\n6\tSee also\n7\tExternal links\nPseudo-metrics[edit]\nConcept programming uses pseudo-metrics to evaluate the quality of code. They are called pseudo-metrics because they relate the concept space and the code space, with a clear understanding that the concept space cannot be formalized strictly enough for a real metric to be defined. Concept programming pseudo-metrics include:\n\nSyntactic noise measures discrepancies between the concept and the syntax used to represent it. For instance, the semi-colon at the end of statements in C can be considered as syntactic noise, because it has no equivalent in the concept space.\nSemantic noise measures discrepancies between the expected meaning or behavior of the concept and its actual meaning or behavior in the code. For instance, the fact that integer data types overflow (when mathematical integers do not) is a form of semantic noise.\nBandwidth measures how much of the concept space a given code construct can represent. For instance, the overloaded addition operator in C has higher bandwidth than the Add instruction in assembly language, because the C operator can represent addition on floating-point numbers and not just integer numbers.\nSignal/noise ratio measures what fraction of the code space is used for representing actual concepts, as opposed to implementation information.\nRule of equivalence, equivalence breakdown[edit]\nThe rule of equivalence is verified when the code behavior matches the original concept. This equivalence may break down in many cases. Integer overflow breaks the equivalence between the mathematical integer concept and the computerized approximation of the concept.\n\nMany ways to break the equivalence have been given specific names, because they are very common:\n\nA domain error is a condition where code executes outside of the domain of equivalence, which is the domain where the concept and the implementation match. An integer overflow is an example of domain error.\nA concept cast (also concept recast or concept recasting) is a rewrite of a concept as a different concept because the original concept cannot be represented by the tools. In C, using pointers for output arguments because C doesn't support output arguments explicitly is an example of concept cast.\nA priority inversion is a form of syntactic or semantic noise introduced by some language-enforced general rule. It is called a priority inversion because the language takes precedence over the concept. In Smalltalk, everything is an object, and that rule leads to the undesirable consequence that an expression like 2+3*5 doesn't obey the usual order of operations (Smalltalk interprets this as sending the message * to the number resulting from 2+3, which yields result 25 instead of 17).\nMethodology[edit]\nTo write code, concept programming recommends the following steps:\n\nIdentify and define the relevant concepts in the concept space.\nIdentify traditional notations for the concepts, or invent usable notations.\nIdentify a combination of programming constructs that allows the concepts to be represented comfortably in code - That includes finding a code notation that matches the notation identified in the previous step as closely as possible.\nWrite code that preserves, as much as possible, the expected behavior and semantics of the relevant aspects of the original concept.\nMany programming tools often lack in notational abilities, thus concept programming sometimes requires the use of preprocessors, domain-specific languages, or metaprogramming techniques.\n\nLanguages[edit]\nXL is the only programming language known to date to be explicitly created for concept programming, but concept programming can be done in nearly any language, with varying degrees of success. Lisp and Forth (and their derivatives) are examples of pre-existing languages which lend themselves well to concept programming.[citation needed]\n\nSimilar works[edit]\nThere are projects that exploit similar ideas to create code with higher level of abstraction. Among them are:\n\nIntentional Programming\nLanguage-oriented programming\nLiterate programming\nModel-driven architecture\nSee also[edit]\nProgramming paradigms\nAutomatic programming\nArtefaktur (AAL)\nAbstract syntax tree (AST)\nLanguage syntax tree (LST)\nSemantic resolution tree (RST)\nInterpretation syntax tree (IST)\nCode generation syntax tree (CST)\nDomain-specific programming language\nExternal links[edit]\nThe XL Programming Language on SourceForge\nA presentation of Concept Programming\nAn interview about Concept Programming on The Register",
          "subparadigms": []
        }
      ],
      "programminglanguages": [
        {
          "name": "A+",
          "details": "A+ is an array programming language descendent from the programming language A, which in turn was created to replace APL in 1988.[1] Arthur Whitney developed the \"A\" portion of A+, while other developers at Morgan Stanley extended it, adding a graphical user interface and other language features. A+ was designed for numerically intensive applications, especially those found in financial applications. A+ runs on many Unix variants, including Linux. A+ is a high-level, interactive, interpreted language.\n\nA+ provides an extended set of functions and operators, a graphical user interface with automatic synchronization of widgets and variables, asynchronous execution of functions associated with variables and events, dynamic loading of user compiled subroutines, and other features. A newer graphical user interface has not yet been ported to all supported platforms\n\n\nThe A+ language implements the following changes to the APL language:\n\nan A+ function may have up to nine formal parameters\nA+ code statements are separated by semicolons, so a single statement may be divided into two or more physical lines\nThe explicit result of a function or operator is the result of the last statement executed\nA+ implements an object called a dependency, which is a global variable (the dependent variable) and an associated definition that is like a function with no arguments. Values can be explicitly set and referenced in exactly the same ways as for a global variable, but they can also be set through the associated definition.\nInteractive A+ development is primarily done in the Xemacs editor, through extensions to the editor. Because A+ code uses the original APL symbols, displaying A+ requires a font with those special characters; a font called \"kapl\" is provided on the web site for that purpose.\n\nArthur Whitney went on to create the K language, a proprietary array language. Like J, K omits the APL character set. It does not have some of the perceived complexities of A+, such as the existence of statements and two different modes of syntax.",
          "type": "compiled",
          "plid": 1
        },
        {
          "plid": 2,
          "name": "Ada",
          "details": "Ada is a structured, statically typed, imperative, wide-spectrum, and object-oriented high-level computer programming language, extended from Pascal and other languages. It has built-in language support for design-by-contract, extremely strong typing, explicit concurrency, offering tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international standard; the current version (known as Ada 2012[7]) is defined by ISO/IEC 8652:2012.[8]\n\nAda was originally designed by a team led by Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time.[9] Ada was named after Ada Lovelace (1815–1852), who is credited with being the first computer programmer.[10]\n\nContents  [hide] \n1\tFeatures\n2\tHistory\n3\tStandardization\n4\tLanguage constructs\n4.1\t\"Hello, world!\" in Ada\n4.2\tData types\n4.3\tControl structures\n4.4\tPackages, procedures and functions\n4.5\tConcurrency\n4.6\tPragmas\n5\tSee also\n6\tReferences\n6.1\tInternational standards\n6.2\tRationale\n6.3\tBooks\n6.4\tArchives\n7\tExternal links\nFeatures[edit]\nAda was originally targeted at embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP).\n\nFeatures of Ada include: strong typing, modularity mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada 95 added support for object-oriented programming, including dynamic dispatch.\n\nThe syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as \"or else\" and \"and then\") to symbols (such as \"||\" and \"&&\"). Ada uses the basic arithmetical operators \"+\", \"-\", \"*\", and \"/\", but avoids using other symbols. Code blocks are delimited by words such as \"declare\", \"begin\", and \"end\", where the \"end\" (in most cases) is followed by the identifier of the block it closes (e.g., if … end if, loop … end loop). In the case of conditional blocks this avoids a dangling else that could pair with the wrong nested if-expression in other languages like C or Java.\n\nAda is designed for development of very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts.\n\nA large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code. For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detection of many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks.[citation needed] Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.\n\nAda also supports run-time checks to protect against access to unallocated memory, buffer overflow errors, range violations, off-by-one errors, array access errors, and other detectable bugs. These checks can be disabled in the interest of runtime efficiency, but can often be compiled efficiently. It also includes facilities to help program verification. For these reasons, Ada is widely used in critical systems, where any anomaly might lead to very serious consequences, e.g., accidental death, injury or severe financial loss. Examples of systems where Ada is used include avionics, ATC, railways, banking, military and space technology.[11][12]\n\nAda's dynamic memory management is high-level and type-safe. Ada does not have generic or untyped pointers; nor does it implicitly declare any pointer type. Instead, all dynamic memory allocation and deallocation must take place through explicitly declared access types. Each access type has an associated storage pool that handles the low-level details of memory management; the programmer can either use the default storage pool or define new ones (this is particularly relevant for Non-Uniform Memory Access). It is even possible to declare several different access types that all designate the same type but use different storage pools. Also, the language provides for accessibility checks, both at compile time and at run time, that ensures that an access value cannot outlive the type of the object it points to.\n\nThough the semantics of the language allow automatic garbage collection of inaccessible objects, most implementations do not support it by default, as it would cause unpredictable behaviour in real-time systems. Ada does support a limited form of region-based memory management; also, creative use of storage pools can provide for a limited form of automatic garbage collection, since destroying a storage pool also destroys all the objects in the pool.\n\nAda was designed to resemble the English language in its syntax for comments: a double-dash (\"--\"), resembling an em dash, denotes comment text. Comments stop at end of line, so there is no danger of unclosed comments accidentally voiding whole sections of source code. Prefixing each line (or column) with \"--\" will skip all that code, while being clearly denoted as a column of repeated \"--\" down the page. There is no limit to the nesting of comments, thereby allowing prior code, with commented-out sections, to be commented-out as even larger sections. All Unicode characters are allowed in comments, such as for symbolic formulas (E[0]=m×c²). To the compiler, the double-dash is treated as end-of-line, allowing continued parsing of the language as a context-free grammar.\n\nThe semicolon (\";\") is a statement terminator, and the null or no-operation statement is null;. A single ; without a statement to terminate is not allowed.\n\nUnlike most ISO standards, the Ada language definition (known as the Ada Reference Manual or ARM, or sometimes the Language Reference Manual or LRM) is free content. Thus, it is a common reference for Ada programmers and not just programmers implementing Ada compilers. Apart from the reference manual, there is also an extensive rationale document which explains the language design and the use of various language constructs. This document is also widely used by programmers. When the language was revised, a new rationale document was written.\n\nOne notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio.\n\nHistory[edit]\nIn the 1970s, the US Department of Defense (DoD) was concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and UK Ministry of Defence requirements. After many iterations beginning with an original Straw man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.\n\n\tWikisource has original text related to this article:\nSteelman language requirements\nThe HOLWG working group crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications.\n\nRequests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (CII Honeywell Bull, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough)[13] and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at CII Honeywell Bull, was chosen and given the name Ada—after Augusta Ada, Countess of Lovelace. This proposal was influenced by the programming language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable,[14] but subsequently seemed to recant in the foreword he wrote for an Ada textbook.[15]\n\nAda attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not just defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain, Ada and Lisp.[16] Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive. Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required \"ACVC\" validation suite that was required in another novel feature of the Ada language effort.[16]\n\nThe first validated Ada implementation was the NYU Ada/Ed translator,[17] certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL.[18] A number of commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, TLD Systems, Verdix, and others.[19]\n\n\nAugusta Ada King, Countess of Lovelace.\nIn 1991, the US Department of Defense began to require the use of Ada (the Ada mandate) for all software,[20] though exceptions to this rule were often granted. The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace COTS technology.[citation needed] Similar requirements existed in other NATO countries.[citation needed]\n\nBy the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to full exploitation of Ada's abilities, including a tasking model that was different from what most real-time programmers were used to.[16]\n\nBecause of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets (e.g., Ariane 4 and 5), satellites and other space systems, railway transport and banking.[12] For example, the Airplane Information Management System, the fly-by-wire system software in the Boeing 777, was written in Ada; developed by Honeywell Air Transport Systems in collaboration with consultants from DDC-I, it became arguably the best-known of any Ada project, civilian or military.[21][22] The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK’s next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.[23] It is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.[12][24]\n\nStandardization[edit]\nThe language became an ANSI standard in 1983 (ANSI/MIL-STD 1815A), and without any further changes became an ISO standard in 1987 (ISO-8652:1987). This version of the language is commonly known as Ada 83, from the date of its adoption by ANSI, but is sometimes referred to also as Ada 87, from the date of its adoption by ISO.\n\nAda 95, the joint ISO/ANSI standard (ISO-8652:1995) was published in February 1995, making Ada 95 the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection.\n\nWork has continued on improving and updating the technical content of the Ada programming language. A Technical Corrigendum to Ada 95 was published in October 2001, and a major Amendment, ISO/IEC 8652:1995/Amd 1:2007 was published on March 9, 2007. At the Ada-Europe 2012 conference in Stockholm, the Ada Resource Association (ARA) and Ada-Europe announced the completion of the design of the latest version of the Ada programming language and the submission of the reference manual to the International Organization for Standardization (ISO) for approval. ISO/IEC 8652:2012 was published in December 2012.[8]\n\nOther related standards include ISO 8651-3:1988 Information processing systems—Computer graphics—Graphical Kernel System (GKS) language bindings—Part 3: Ada.\n\nLanguage constructs[edit]\nAda is an ALGOL-like programming language featuring control structures with reserved words such as if, then, else, while, for, and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited or inspired from Pascal.\n\n\"Hello, world!\" in Ada[edit]\nA common example of a language's syntax is the Hello world program: (hello.adb)\n\nwith Ada.Text_IO; use Ada.Text_IO;\nprocedure Hello is\nbegin\n  Put_Line (\"Hello, world!\");\nend Hello;\nThis program can be compiled by using the freely available open source compiler GNAT, by executing\n\ngnatmake hello.adb\nData types[edit]\nAda's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted. Special types provided by the language are task types and protected types.\n\nFor example, a date might be represented as:\n\ntype Day_type   is range    1 ..   31;\ntype Month_type is range    1 ..   12;\ntype Year_type  is range 1800 .. 2100;\ntype Hours is mod 24;\ntype Weekday is (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday);\n\ntype Date is\n   record\n     Day   : Day_type;\n     Month : Month_type;\n     Year  : Year_type;\n   end record;\nTypes can be refined by declaring subtypes:\n\nsubtype Working_Hours is Hours range 0 .. 12;            -- at most 12 Hours to work a day\nsubtype Working_Day is Weekday range Monday .. Friday;   -- Days to work\n\nWork_Load: constant array(Working_Day) of Working_Hours  -- implicit type declaration\n   := (Friday => 6, Monday => 4, others => 10);           -- lookup table for working hours with initialization\nTypes can have modifiers such as limited, abstract, private etc. Private types can only be accessed and limited types can only be modified or copied within the scope of the package that defines them.[25] Ada 95 adds additional features for object-oriented extension of types.\n\nControl structures[edit]\nAda is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep level early exit are supported so the use of the also supported 'go to' commands is seldom needed.\n\n-- while a is not equal to b, loop.\nwhile a /= b loop\n  Ada.Text_IO.Put_Line (\"Waiting\");\nend loop;\n\nif a > b then\n  Ada.Text_IO.Put_Line (\"Condition met\");\nelse\n  Ada.Text_IO.Put_Line (\"Condition not met\");\nend if;\n\nfor i in 1 .. 10 loop\n  Ada.Text_IO.Put (\"Iteration: \");\n  Ada.Text_IO.Put (i);\n  Ada.Text_IO.Put_Line;\nend loop;\n\nloop\n  a := a + 1;\n  exit when a = 10;\nend loop;\n\ncase i is\n  when 0 => Ada.Text_IO.Put (\"zero\");\n  when 1 => Ada.Text_IO.Put (\"one\");\n  when 2 => Ada.Text_IO.Put (\"two\");\n  -- case statements have to cover all possible cases:\n  when others => Ada.Text_IO.Put (\"none of the above\");\nend case;\n\nfor aWeekday in Weekday'Range loop               -- loop over an enumeration\n   Put_Line ( Weekday'Image(aWeekday) );         -- output string representation of an enumeration\n   if aWeekday in Working_Day then               -- check of a subtype of an enumeration\n      Put_Line ( \" to work for \" &\n               Working_Hours'Image (Work_Load(aWeekday)) ); -- access into a lookup table\n   end if;\nend loop;\nPackages, procedures and functions[edit]\nAmong the parts of an Ada program are packages, procedures and functions.\n\nExample: Package specification (example.ads)\n\npackage Example is\n     type Number is range 1 .. 11;\n     procedure Print_and_Increment (j: in out Number);\nend Example;\nPackage body (example.adb)\n\nwith Ada.Text_IO;\npackage body Example is\n\n  i : Number := Number'First;\n\n  procedure Print_and_Increment (j: in out Number) is\n\n    function Next (k: in Number) return Number is\n    begin\n      return k + 1;\n    end Next;\n\n  begin\n    Ada.Text_IO.Put_Line ( \"The total is: \" & Number'Image(j) );\n    j := Next (j);\n  end Print_and_Increment;\n\n-- package initialization executed when the package is elaborated\nbegin\n  while i < Number'Last loop\n    Print_and_Increment (i);\n  end loop;\nend Example;\nThis program can be compiled, e.g., by using the freely available open source compiler GNAT, by executing\n\ngnatmake -z example.adb\nPackages, procedures and functions can nest to any depth and each can also be the logical outermost block.\n\nEach package, procedure or function can have its own declarations of constants, types, variables, and other procedures, functions and packages, which can be declared in any order.\n\nConcurrency[edit]\nAda has language support for task-based concurrency. The fundamental concurrent unit in Ada is a task which is a built-in limited type. Tasks are specified in two parts – the task declaration defines the task interface (similar to a type declaration), the task body specifies the implementation of the task. Depending on the implementation, Ada tasks are either mapped to operating system threads or processes, or are scheduled internally by the Ada runtime.\n\nTasks can have entries for synchronisation (a form of synchronous message passing). Task entries are declared in the task specification. Each task entry can have one or more accept statements within the task body. If the control flow of the task reaches an accept statement, the task is blocked until the corresponding entry is called by another task (similarly, a calling task is blocked until the called task reaches the corresponding accept statement). Task entries can have parameters similar to procedures, allowing tasks to synchronously exchange data. In conjunction with select statements it is possible to define guards on accept statements (similar to Dijkstra's guarded commands).\n\nAda also offers protected objects for mutual exclusion. Protected objects are a monitor-like construct, but use guards instead of conditional variables for signaling (similar to conditional critical regions). Protected objects combine the data encapsulation and safe mutual exclusion from monitors, and entry guards from conditional critical regions. The main advantage over classical monitors is that conditional variables are not required for signaling, avoiding potential deadlocks due to incorrect locking semantics. Like tasks, the protected object is a built-in limited type, and it also has a declaration part and a body.\n\nA protected object consists of encapsulated private data (which can only be accessed from within the protected object), and procedures, functions and entries which are guaranteed to be mutually exclusive (with the only exception of functions, which are required to be side effect free and can therefore run concurrently with other functions). A task calling a protected object is blocked if another task is currently executing inside the same protected object, and released when this other task leaves the protected object. Blocked tasks are queued on the protected object ordered by time of arrival.\n\nProtected object entries are similar to procedures, but additionally have guards. If a guard evaluates to false, a calling task is blocked and added to the queue of that entry; now another task can be admitted to the protected object, as no task is currently executing inside the protected object. Guards are re-evaluated whenever a task leaves the protected object, as this is the only time when the evaluation of guards can have changed.\n\nCalls to entries can be requeued to other entries with the same signature. A task that is requeued is blocked and added to the queue of the target entry; this means that the protected object is released and allows admission of another task.\n\nThe select statement in Ada can be used to implement non-blocking entry calls and accepts, non-deterministic selection of entries (also with guards), time-outs and aborts.\n\nThe following example illustrates some concepts of concurrent programming in Ada.\n\nwith Ada.Text_IO; use Ada.Text_IO;\n\nprocedure Traffic is\n\n   type Airplane_ID is range 1..10;             -- 10 airplanes\n\n   task type Airplane (ID: Airplane_ID);        -- task representing airplanes, with ID as initialisation parameter\n   type Airplane_Access is access Airplane;     -- reference type to Airplane\n\n   protected type Runway is                     -- the shared runway (protected to allow concurrent access)\n      entry Assign_Aircraft (ID: Airplane_ID);  -- all entries are guaranteed mutually exclusive\n      entry Cleared_Runway (ID: Airplane_ID);\n      entry Wait_For_Clear;\n   private\n      Clear: Boolean := True;                   -- protected private data - generally more than just a flag...\n   end Runway;\n   type Runway_Access is access all Runway;\n\n   -- the air traffic controller task takes requests for takeoff and landing\n   task type Controller (My_Runway: Runway_Access) is\n      -- task entries for synchronous message passing\n      entry Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access);\n      entry Request_Approach(ID: in Airplane_ID; Approach: out Runway_Access);\n   end Controller;\n\n   --  allocation of instances\n   Runway1    : aliased Runway;              -- instantiate a runway\n   Controller1: Controller (Runway1'Access); -- and a controller to manage it\n\n   ------ the implementations of the above types ------\n   protected body Runway is\n      entry Assign_Aircraft (ID: Airplane_ID)\n when Clear is   -- the entry guard - calling tasks are blocked until the condition is true\n      begin\n       Clear := False;\n       Put_Line (Airplane_ID'Image (ID) & \" on runway \");\n      end;\n\n      entry Cleared_Runway (ID: Airplane_ID)\n when not Clear is\n      begin\n         Clear := True;\n         Put_Line (Airplane_ID'Image (ID) & \" cleared runway \");\n      end;\n\n      entry Wait_For_Clear\n when Clear is\n      begin\n         null;      -- no need to do anything here - a task can only enter if \"Clear\" is true\n      end;\n   end Runway;\n\n   task body Controller is\n   begin\n      loop\n         My_Runway.Wait_For_Clear;   -- wait until runway is available (blocking call)\n         select                      -- wait for two types of requests (whichever is runnable first)\n            when Request_Approach'count = 0 =>  -- guard statement - only accept if there are no tasks queuing on Request_Approach\n             accept Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access)\n             do                                 -- start of synchronized part\n               My_Runway.Assign_Aircraft (ID);  -- reserve runway (potentially blocking call if protected object busy or entry guard false)\n               Takeoff := My_Runway;            -- assign \"out\" parameter value to tell airplane which runway\n             end Request_Takeoff;               -- end of the synchronised part\n         or\n            accept Request_Approach (ID: in Airplane_ID; Approach: out Runway_Access) do\n               My_Runway.Assign_Aircraft (ID);\n               Approach := My_Runway;\n            end Request_Approach;\n         or                          -- terminate if no tasks left who could call\n            terminate;\n         end select;\n      end loop;\n   end;\n\n   task body Airplane is\n      Rwy : Runway_Access;\n   begin\n      Controller1.Request_Takeoff (ID, Rwy); -- This call blocks until Controller task accepts and completes the accept block\n      Put_Line (Airplane_ID'Image (ID) & \"  taking off...\");\n      delay 2.0;\n      Rwy.Cleared_Runway (ID);               -- call will not block as \"Clear\" in Rwy is now false and no other tasks should be inside protected object\n      delay 5.0; -- fly around a bit...\n      loop\n         select   -- try to request a runway\n            Controller1.Request_Approach (ID, Rwy); -- this is a blocking call - will run on controller reaching accept block and return on completion\n            exit; -- if call returned we're clear for landing - leave select block and proceed...\n         or\n            delay 3.0;  -- timeout - if no answer in 3 seconds, do something else (everything in following block)\n            Put_Line (Airplane_ID'Image (ID) & \"   in holding pattern\");  -- simply print a message\n         end select;\n      end loop;\n      delay 4.0;  -- do landing approach...\n      Put_Line (Airplane_ID'Image (ID) & \"            touched down!\");\n      Rwy.Cleared_Runway (ID);  -- notify runway that we're done here.\n   end;\n\n   New_Airplane: Airplane_Access;\n\nbegin\n   for I in Airplane_ID'Range loop  -- create a few airplane tasks\n      New_Airplane := new Airplane (I); -- will start running directly after creation\n      delay 4.0;\n   end loop;\nend Traffic;\nPragmas[edit]\nA pragma is a compiler directive that conveys information to the compiler to allow specific manipulation of compiled output.[26] Certain pragmas are built into the language[27] while other are implementation-specific.\n\nExamples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code in lieu of a function call (as C/C++ does with inline functions).\n\nSee also[edit]\nAPSE – a specification for a programming environment to support software development in Ada\nRavenscar profile – a subset of the Ada tasking features designed for safety-critical hard real-time computing\nSPARK (programming language) – a programming language consisting of a highly restricted subset of Ada, annotated with meta information describing desired component behavior and individual runtime requirements",
          "type": "compiled"
        },
        {
          "name": "GAUSS",
          "details": "GAUSS is a matrix programming language for mathematics and statistics, developed and marketed by Aptech Systems. Its primary purpose is the solution of numerical problems in statistics, econometrics, time-series, optimization and 2D- and 3D-visualization. It was first published in 1984 for MS-DOS and is currently also available for Linux, Mac OS X and Windows.\n\nContents  [hide] \n1\tExamples of Functions Included in Run-Time Library\n2\tGAUSS Applications\n3\tSee also\n4\tExternal links\nExamples of Functions Included in Run-Time Library[edit]\nGAUSS has several Application Modules as well as functions in its Run-Time Library (i.e., functions that come with GAUSS without extra cost)\nQprog — Quadratic programming\nSqpSolvemt - Sequential quadratic programming\nQNewton - Quasi-Newton unconstrained optimization\nEQsolve - Nonlinear equations solver\nGAUSS Applications[edit]\nA range of toolboxes are available for GAUSS at additional cost. See https://www.aptech.com/products/gauss-applications/ for complete listing of products.\n\nAlgorithmic Derivatives\tA program for generating GAUSS procedures for computing algorithmic derivatives.\nConstrained Maximum Likelihood MT\tSolves the general maximum likelihood problem subject to general constraints on the parameters.\nConstrained Optimization\tSolves the nonlinear programming problem subject to general constraints on the parameters.\nCurveFit\tNonlinear curve fitting.\nDescriptive Statistics\tBasic sample statistics including means, frequencies and crosstabs. This application is backwards compatible with programs written with Descriptive Statistics 3.1\nDescriptive Statistics MT\tBasic sample statistics including means, frequencies and crosstabs. This application is thread-safe and takes advantage of structures.\nDiscrete Choice\tA statistical package for estimating discrete choice and other models in which the dependent variable is qualitative in some way.\nFANPAC MT\tComprehensive suite of GARCH (Generalized AutoRegressive Conditional Heteroskedastic) models for estimating volatility.\nLinear Programming MT\tSolves small and large scale linear programming problems\nLinear Regression MT\tLeast squares estimation.\nLoglinear Analysis MT\tAnalysis of categorical data using loglinear analysis.\nMaximum Likelihood MT\tMaximum likelihood estimation of the parameters of statistical models.\nNonlinear Equations MT\tSolves systems of nonlinear equations having as many equations as unknowns.\nOptimization\tUnconstrained optimization.\nTime Series MT\tExact ML estimation of VARMAX, VARMA, ARIMAX, ARIMA, and ECM models subject to general constraints on the parameters. Panel data estimation. Unit root and cointegration tests.\nSee also[edit]\nList of numerical analysis software\nComparison of numerical analysis software\nExternal links[edit]\nInternational homepage\nGAUSS Mailing List\nReview of version 7.0\nSome more links\nSoftware for Economists",
          "type": "unknown",
          "plid": 3
        },
        {
          "name": "Analytica",
          "details": "Analytica is a visual software package developed by Lumina Decision Systems for creating, analyzing and communicating quantitative decision models.[1] As a modeling environment, it is interesting in the way it combines hierarchical influence diagrams for visual creation and view of models, intelligent arrays for working with multidimensional data, Monte Carlo simulation for analyzing risk and uncertainty, and optimization, including linear and nonlinear programming. Its design, especially its influence diagrams and treatment of uncertainty, is based on ideas from the field of decision analysis. As a computer language, it is notable in combining a declarative (non-procedural) structure for referential transparency, array abstraction, and automatic dependency maintenance for efficient sequencing of computation.\n\nContents  [hide] \n1\tHierarchical influence diagrams\n2\tIntelligent multidimensional arrays\n3\tUncertainty analysis\n4\tSystems dynamics modeling\n5\tAs a programming language\n6\tApplications of Analytica\n7\tEditions\n8\tHistory\n9\tReferences\n10\tExternal links\nHierarchical influence diagrams[edit]\nAnalytica models are organized as influence diagrams. Variables (and other objects) appear as nodes of various shapes on a diagram, connected by arrows that provide a visual representation of dependencies. Analytica influence diagrams may be hierarchical, in which a single module node on a diagram represents an entire submodel.\n\nHierarchical influence diagrams in Analytica serve as a key organizational tool. Because the visual layout of an influence diagram matches these natural human abilities both spatially and in the level of abstraction, people are able to take in far more information about a model's structure and organization at a glance than is possible with less visual paradigms, such as spreadsheets and mathematical expressions. Managing the structure and organization of a large model can be a significant part of the modeling process, but is substantially aided by the visualization of influence diagrams.\n\nInfluence diagrams also serve as a tool for communication. Once a quantitative model has been created and its final results computed, it is often the case that an understanding of how the results are obtained, and how various assumptions impact the results, is far more important than the specific numbers computed. The ability of a target audience to understand these aspects is critical to the modeling enterprise. The visual representation of an influence diagram quickly communicates an understanding at a level of abstraction that is normally more appropriate than detailed representations such as mathematical expressions or cell formulae. When more detail is desired, users can drill down to increasing levels of detail, speeded by the visual depiction of the model's structure.\n\nThe existence of an easily understandable and transparent model supports communication and debate within an organization, and this effect is one of the primary benefits of investing in quantitative model building. When all interested parties are able to understand a common model structure, debates and discussions will often focus more directly on specific assumptions, can cut down on \"cross-talk\", and therefore lead to more productive interactions within the organization. The influence diagram serves as a graphical representation that can help to make models accessible to people at different levels.\n\nIntelligent multidimensional arrays[edit]\nAnalytica uses index objects to track the dimensions of multidimensional arrays. An index object has a name and a list of elements. When two multidimensional values are combined, for example in an expression such as\n\nProfit = Revenue − Expenses\nwhere Revenue and Expenses are each multidimensional, Analytica repeats the profit calculation over each dimension, but recognizes when same dimension occurs in both values and treats it as the same dimension during the calculation, in a process called intelligent array abstraction. Unlike most programming languages, there is no inherent ordering to the dimensions in a multidimensional array. This avoids duplicated formulas and explicit FOR loops, both common sources of modeling errors. The simplified expressions made possible by intelligent array abstraction allow the model to be more accessible, interpretable, and transparent.\n\nAnother consequence of intelligent array abstraction is that new dimensions can be introduced or removed from an existing model, without requiring changes to the model structure or changes to variable definitions. For example, while creating a model, the model builder might assume a particular variable, for example discount_rate, contains a single number. Later, after constructing a model, a user might replace the single number with a table of numbers, perhaps discount_rate broken down by Country and by Economic_scenario. These new divisions may reflect the fact that the effective discount rate is not the same for international divisions of a company, and that different rates are applicable to different hypothetical scenarios. Analytica automatically propagates these new dimensions to any results that depend upon discount_rate, so for example, the result for Net present value will become multidimensional and contain these new dimensions. In essence, Analytica repeats the same calculation using the discount rate for each possible combination of Country and Economic_scenario.\n\nThis flexibility is important when exploring computation tradeoffs between the level of detail, computation time, available data, and overall size or dimensionality of parametric spaces. Such adjustments are common after models have been fully constructed as a way of exploring what-if scenarios and overall relationships between variables.\n\nUncertainty analysis[edit]\nIncorporating uncertainty into model outputs helps to provide more realistic and informative projections. Uncertain quantities in Analytica can be specified using a distribution function. When evaluated, distributions are sampled using either Latin hypercube or Monte Carlo sampling, and the samples are propagated through the computations to the results. The sampled result distribution and summary statistics can then be viewed directly (mean, fractile bands, probability density function (PDF), cumulative distribution function (CDF)), Analytica supports collaborative Decision Analysis and Probability Management through the use of the DIST standard.[2][3]\n\nSystems dynamics modeling[edit]\nSystem dynamics is an approach to simulating the behaviour of complex systems over time. It deals with feedback loops and time delays on the behaviour of the entire system. The Dynamic() function in Analytica allows definition of variables with cyclic dependencies, such as feedback loops. It expands the influence diagram notation, which does not normally allow cycles. At least one link in each cycle includes a time lag, depicted as a gray influence arrow to distinguish it from standard black arrows without time lags.\n\nAs a programming language[edit]\nAnalytica includes a general language of operators and functions for expressing mathematical relationships among variables. Users can define functions and libraries to extend the language.\n\nAnalytica has several features as a programming language designed to make it easy to use for quantitative modeling: It is a visual programming language, where users view programs (or \"models\") as influence diagrams, which they create and edit visually by adding and linking nodes. It is a declarative language, meaning that a model declares a definition for each variable without specifying an execution sequence as required by conventional imperative languages. Analytica determines a correct and efficient execution sequence using the dependency graph. It is a referentially transparent functional language, in that execution of functions and variables have no side effects i.e. changing other variables. Analytica is an array programming language, where operations and functions generalize to work on multidimensional arrays.\n\nApplications of Analytica[edit]\nAnalytica has been used for policy analysis, business modeling, and risk analysis.[4] Areas in which Analytica has been applied include energy,[5][6][7][8][9][10] health and pharmaceuticals,[11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26] environmental risk and emissions policy analysis,[27][28][29][30][31][32][33][34][35] wildlife management,[36][37][38][39] ecology,[40][41][42][43][44][45][46] climate change,[47][48][49][50][51][52][53][54][55][56] technology and defense,[57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72][73][74] strategic financial planning,[75][76] R&D planning and portfolio management,[77][78][79] financial services, aerospace,[80] manufacturing[81] and environmental health impact assessment.[82]\n\nEditions[edit]\nThe Analytica software runs on Microsoft Windows operating systems. Three editions (Professional, Enterprise, Optimizer) each with more functions and cost, are purchased by users interested in building models. A free edition is available, called Analytica Free 101, which allows you to build medium to moderate sized models of up to 101 user objects.. Free 101 also allows you to view models with more than 101 objects, change inputs, and compute results, which enables free sharing of models for review. A more capable but non-free Power Player enables users to save inputs and utilize database connections. The Analytica Cloud Player allows you to share models over the web and lets users access and run via a web browser.\n\nThe most recent release of Analytica is version 4.6, released in May 2015.\n\nHistory[edit]\nAnalytica's predecessor, called Demos,[83] grew from the research on tools for policy analysis by Max Henrion as a PhD student and later professor at Carnegie Mellon University between 1979 and 1990. Henrion founded Lumina Decision Systems in 1991 with Brian Arnold. Lumina continued to develop the software and apply it to environmental and public policy analysis applications. Lumina first released Analytica as a product in 1996.",
          "type": "compiled",
          "plid": 4
        },
        {
          "name": "APL",
          "details": "APL (named after the book A Programming Language)[8] is a programming language developed in the 1960s by Kenneth E. Iverson. Its central datatype is the multidimensional array. It uses a large range of special graphic symbols[9] to represent most functions and operators, leading to very concise code. It has been an important influence on the development of concept modeling, spreadsheets, functional programming,[10] and computer math packages.[6] It has also inspired several other programming languages.[4][5][7] It is still used today for certain applications.[11][12]\n\nContents  [hide] \n1\tHistory\n1.1\tAPL2\n1.2\tMicrocomputers\n1.3\tExtensions\n2\tDesign\n3\tExecution\n3.1\tInterpreters\n3.2\tCompilers\n3.3\tMatrix optimizations\n4\tTerminology\n5\tSyntax\n6\tExamples\n6.1\tHello, World\n6.2\tExponentiation\n6.3\t\"Pick 6\" lottery numbers\n6.4\tPrime numbers\n6.5\tSorting\n6.6\tGame of Life\n6.7\tHTML tags removal\n7\tCharacter set\n8\tUse\n9\tStandardization\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory[edit]\nThe mathematical notation for manipulating arrays which developed into the APL programming language was developed by Iverson at Harvard University starting in 1957, and published in his A Programming Language in 1962.[8] The preface states its premise:\n\nApplied mathematics is largely concerned with the design and analysis of explicit procedures for calculating the exact or approximate values of various functions. Such explicit procedures are called algorithms or programs. Because an effective notation for the description of programs exhibits considerable syntactic structure, it is called a programming language.\n\nIn 1960, he began work for IBM and, working with Adin Falkoff, created APL based on the notation he had developed. This notation was used inside IBM for short research reports on computer systems, such as the Burroughs B5000 and its stack mechanism when stack machines versus register machines were being evaluated by IBM for upcoming computers.\n\nAlso in 1960, Iverson used his notation in a draft of the chapter \"A Programming Language\", written for a book he was writing with Fred Brooks, Automatic Data Processing, which would be published in 1963.[13][14]\n\nAs early as 1962, the first attempt to use the notation to describe a complete computer system happened after Falkoff discussed with Dr. William C. Carter his work in the standardization of the instruction set for the machines that later became the IBM System/360 family.\n\nIn 1963, Herbert Hellerman, working at the IBM Systems Research Institute, implemented a part of the notation on an IBM 1620 computer, and it was used by students in a special high school course on calculating transcendental functions by series summation. Students tested their code in Hellerman's lab. This implementation of a portion of the notation was called PAT (Personalized Array Translator).[15]\n\nIn 1963, Falkoff, Iverson, and Edward H. Sussenguth Jr., all working at IBM, used the notation for a formal description of the IBM System/360 series machine architecture and functionality, which resulted in a paper published in IBM Systems Journal in 1964. After this was published, the team turned their attention to an implementation of the notation on a computer system. One of the motivations for this focus of implementation was the interest of John L. Lawrence who had new duties with Science Research Associates, an educational company bought by IBM in 1964. Lawrence asked Iverson and his group to help utilize the language as a tool for the development and use of computers in education.[16]\n\nAfter Lawrence M. Breed and Philip S. Abrams of Stanford University joined the team at IBM Research, they continued their prior work on an implementation programmed in FORTRAN IV for a portion of the notation which had been done for the IBM 7090 computer running under the IBSYS operating system. This work was finished in late 1965 and later known as IVSYS (Iverson System). The basis of this implementation was described in detail by Abrams in a Stanford University Technical Report, \"An Interpreter for Iverson Notation\" in 1966.[17] this was formally supervised by Niklaus Wirth. Like Hellerman's PAT system earlier, this implementation did not include the APL character set but used special English reserved words for functions and operators. The system was later adapted for a time-sharing system and, by November 1966, it had been reprogrammed for the IBM System/360 Model 50 computer running in a time sharing mode and was used internally at IBM.[18]\n\n\nIBM typeballs (one OCR) with clip, €2 coin for scale\nA key development in the ability to use APL effectively, before the widespread use of CRT terminals, was the development of a special IBM Selectric typewriter interchangeable typeball with all the special APL characters on it. This was used on paper printing terminal workstations using the Selectric typewriter and typeball mechanism, such as the IBM 1050 and IBM 2741 terminal. Keycaps could be placed over the normal keys to show which APL characters would be entered and typed when that key was struck. For the first time, a programmer could actually type in and see real APL characters as used in Iverson's notation and not be forced to use awkward English keyword representations of them. Falkoff and Iverson had the special APL Selectric typeballs, 987 and 988, designed in late 1964, although no APL computer system was available to use them.[19] Iverson cited Falkoff as the inspiration for the idea of using an IBM Selectric typeball for the APL character set.[20]\n\n\nA programmer's view of the IBM 2741 keyboard layout with the APL typeball print head inserted\nSome APL symbols, even with the APL characters on the typeball, still had to be typed in by over-striking two existing typeball characters. An example would be the \"grade up\" character, which had to be made from a \"delta\" (shift-H) and a \"Sheffer stroke\" (shift-M). This was necessary because the APL character set was larger than the 88 characters allowed on the Selectric typeball.\n\nThe first APL interactive login and creation of an APL workspace was in 1966 by Larry Breed using an IBM 1050 terminal at the IBM Mohansic Labs near Thomas J. Watson Research Center, the home of APL, in Yorktown Heights, New York.[19]\n\nIBM was chiefly responsible for the introduction of APL to the marketplace. APL was first available in 1967 for the IBM 1130 as APL\\1130.[21][22] It would run in as little as 8k 16-bit words of memory, and used a dedicated 1 megabyte hard disk.\n\nAPL gained its foothold on mainframe timesharing systems from the late 1960s through the early 1980s, in part because it would run on lower-specification systems that were not equipped with Dynamic Address Translation hardware.[23] Additional improvements in performance for selected IBM System/370 mainframe systems included the \"APL Assist Microcode\" in which some support for APL execution was included in the actual firmware as opposed to APL being exclusively a software product. Somewhat later, as suitably performing hardware was finally becoming available in the mid- to late-1980s, many users migrated their applications to the personal computer environment.\n\nEarly IBM APL interpreters for IBM 360 and IBM 370 hardware implemented their own multi-user management instead of relying on the host services, thus they were timesharing systems in their own right. First introduced in 1966, the APL\\360[24][25][26] system was a multi-user interpreter. The ability to programmatically communicate with the operating system for information and setting interpreter system variables was done through special privileged \"I-beam\" functions, using both monadic and dyadic operations.[27]\n\nIn 1973, IBM released APL.SV, which was a continuation of the same product, but which offered shared variables as a means to access facilities outside of the APL system, such as operating system files. In the mid-1970s, the IBM mainframe interpreter was even adapted for use on the IBM 5100 desktop computer, which had a small CRT and an APL keyboard, when most other small computers of the time only offered BASIC. In the 1980s, the VSAPL program product enjoyed widespread usage with CMS, TSO, VSPC, MUSIC/SP and CICS users.\n\nIn 1973-1974, Dr. Patrick E. Hagerty directed the implementation of the University of Maryland APL interpreter for the Sperry Univac 1100 Series mainframe computers.[28] At the time, Sperry had nothing. In 1974, student Alan Stebbens was assigned the task of implementing an internal function.[29]\n\nSeveral timesharing firms sprang up in the 1960s and 1970s that sold APL services using modified versions of the IBM APL\\360[26] interpreter. In North America, the better-known ones were I. P. Sharp Associates, STSC, Time Sharing Resources (TSR) and The Computer Company (TCC). CompuServe also entered the fray in 1978 with an APL Interpreter based on a modified version of Digital Equipment Corp and Carnegie Mellon's which ran on DEC's KI and KL 36 bit machines. CompuServe's APL was available both to its commercial market and the consumer information service. With the advent first of less expensive mainframes such as the IBM 4300 and later the personal computer, the timesharing industry had all but disappeared by the mid-1980s.\n\nSharp APL was available from I. P. Sharp Associates, first on a timesharing basis in the 1960s, and later as a program product starting around 1979. Sharp APL was an advanced APL implementation with many language extensions, such as packages (the ability to put one or more objects into a single variable), file system, nested arrays, and shared variables.\n\nAPL interpreters were available from other mainframe and mini-computer manufacturers as well, notably Burroughs, CDC, Data General, DEC, Harris, Hewlett-Packard, Siemens AG, Xerox, and others.\n\nGarth Foster of Syracuse University sponsored regular meetings of the APL implementers' community at Syracuse's Minnowbrook Conference Center in Blue Mountain Lake, New York. In later years, Eugene McDonnell organized similar meetings at the Asilomar Conference Grounds near Monterey, California, and at Pajaro Dunes near Watsonville, California. The SIGAPL special interest group of the Association for Computing Machinery continues to support the APL community.[30]\n\nIn 1979, Iverson received the Turing Award for his work on APL.[31]\n\nFilmography, Videos: Over the years APL has been the subject of more than a few films and videos. Some of these include:\n\n\"Chasing Men Who Stare at Arrays\" Catherine Lathwell's Film Diaries; 2014, film synopsis - \"people who accept significantly different ways of thinking, challenge the status quo and as a result, created an invention that subtly changes the world. And no one knows about it. And a Canadian started it all… I want everyone to know about it.\"[32]\n\"The Origins of APL - 1974 - YouTube\", YouTube video, 2012, uploaded by Catherine Lathwell; a talk show style interview with the original developers of APL.[33]\n\"50 Years of APL\", YouTube, 2009, by Graeme Robertson, uploaded by MindofZiggi, history of APL, quick introduction to APL, a powerful programming language currently finding new life due to its ability to create and implement systems, web-based or otherwise.[34]\n\"APL demonstration 1975\", YouTube, 2013, uploaded by Imperial College London; 1975 live demonstration of the computer language APL (A Programming Language) by Professor Bob Spence, Imperial College London.[35]\nAPL2[edit]\nStarting in the early 1980s, IBM APL development, under the leadership of Dr Jim Brown, implemented a new version of the APL language that contained as its primary enhancement the concept of nested arrays, where an array can contain other arrays, as well as new language features which facilitated the integration of nested arrays into program workflow. Ken Iverson, no longer in control of the development of the APL language, left IBM and joined I. P. Sharp Associates, where one of his major contributions was directing the evolution of Sharp APL to be more in accordance with his vision.[36][37][38]\n\nAs other vendors were busy developing APL interpreters for new hardware, notably Unix-based microcomputers, APL2 was almost always the standard chosen for new APL interpreter developments. Even today, most APL vendors or their users cite APL2 compatibility, as a selling point for those products.[39][40]\n\nAPL2 for IBM mainframe computers is still available. IBM cites its use for problem solving, system design, prototyping, engineering and scientific computations, expert systems,[41] for teaching mathematics and other subjects, visualization and database access[42] and was first available for CMS and TSO in 1984.[43] The APL2 Workstation edition (Windows, OS/2, AIX, Linux, and Solaris) followed much later in the early 1990s.[citation needed]\n\nMicrocomputers[edit]\nThe first microcomputer implementation of APL was on the Intel 8008-based MCM/70, the first general purpose personal computer, in 1973. Size of arrays along any dimension could not be larger than 255 and the machine was quite slow, but very convenient for education purposes.\n\nIBM's own IBM 5100 microcomputer (1975) offered APL as one of two built-in ROM-based interpreted languages for the computer, complete with a keyboard and display that supported all the special symbols used in the language. While the 5100 was very slow and operated its screen only like a typewriter, its follower the 5110 had more acceptable performances and a read-write addressable text screen. Graphics could be printed on an external matrix printer.\n\nIn 1976 DNA Systems introduced an APL interpreter for their TSO Operating System, which ran timesharing on the IBM 1130, Digital Scientific Meta-4, General Automation GA 18/30 and Computer Hardware CHI 21/30.\n\nThe VideoBrain Family Computer, released in 1977, only had one programming language available for it, and that was a dialect of APL called APL/S.[44]\n\nA Small APL for the Intel 8080 called EMPL was released in 1977, and Softronics APL, with most of the functions of full APL, for 8080-based CP/M systems was released in 1979.\n\nIn 1977, the Canadian firm Telecompute Integrated Systems, Inc. released a business-oriented APL interpreter known as TIS APL, for Z80-based systems. It featured the full set of file functions for APL, plus a full screen input and switching of right and left arguments for most dyadic operators by introducing the ~. prefix to all single character dyadic functions such as - or /.\n\nVanguard APL was available for Z80 CP/M-based processors in the late 1970s. TCC released APL.68000 in the early 1980s for Motorola 68000-based processors, this system being the basis for MicroAPL Limited's APLX product. I. P. Sharp Associates released a version of their APL interpreter for the IBM PC and PC-XT/370.[45] For the IBM PC, an emulator was written that facilitated reusing much of the IBM 370 mainframe code. Arguably, the best known APL interpreter for the IBM Personal Computer was STSC's APL*Plus/PC.[citation needed]\n\nThe Commodore SuperPET, introduced in 1981, included an APL interpreter developed by the University of Waterloo.\n\nIn the early 1980s, the Analogic Corporation developed The APL Machine, which was an array processing computer designed to be programmed only in APL. There were actually three processing units, the user's workstation, an IBM PC, where programs were entered and edited, a Motorola 68000 processor that ran the APL interpreter, and the Analogic array processor that executed the primitives.[46] At the time of its introduction, The APL Machine was likely the fastest APL system available. Although a technological success, The APL Machine was a marketing failure. The initial version supported a single process at a time. At the time the project was discontinued, the design had been completed to allow multiple users. As an aside, an unusual aspect of The APL Machine was that the library of workspaces was organized such that a single function or variable that was shared by many workspaces existed only once in the library. Several of the members of The APL Machine project had previously spent a number of years with Burroughs implementing APL\\700.\n\nAt one stage, it was claimed by Bill Gates in his Open Letter to Hobbyists, Microsoft Corporation planned to release a version of APL, but these plans never materialized.\n\nAn early 1978 publication of Rodnay Zaks from Sybex was A microprogrammed APL implementation ISBN 0-89588-005-9, which is the complete source listing for the microcode for a Digital Scientific Corporation Meta 4 microprogrammable processor implementing APL. This topic was also the subject of his PhD thesis.[47][48]\n\nIn 1979, William Yerazunis wrote a partial version of APL in Prime Computer FORTRAN, extended it with graphics primitives, and released it. This was also the subject of his Masters thesis.[49]\n\nExtensions[edit]\nVarious implementations of APL by APLX, Dyalog, et al., include extensions for object-oriented programming, support for .NET, XML-array conversion primitives, graphing, operating system interfaces, and lambda expressions.\n\nDesign[edit]\nUnlike traditionally structured programming languages, APL code is typically structured as chains of monadic or dyadic functions, and operators[50] acting on arrays.[51] APL has many nonstandard primitives (functions and operators) that are indicated by a single symbol or a combination of a few symbols. All primitives are defined to have the same precedence, and always associate to the right; hence APL is read or best understood from right-to-left.\n\nEarly APL implementations (circa 1970 or so) did not have programming loop-flow control structures, such as \"do\" or \"while\" loops, and \"if-then-else\" constructions. Instead, they used array operations, and use of structured programming constructs was often not necessary, since an operation could be carried out on an entire array in a single statement. For example, the iota function (ι) can replace for-loop iteration: ιN when applied to a scalar positive integer yields a one-dimensional array (vector), 1 2 3 ... N. More recent implementations of APL generally include comprehensive control structures, so that data structure and program control flow can be clearly and cleanly separated.\n\nThe APL environment is called a workspace. In a workspace the user can define programs and data, i.e. the data values exist also outside the programs, and the user can also manipulate the data without having to define a program.[52] In the examples below, the APL interpreter first types six spaces before awaiting the user's input. Its own output starts in column one.\n\n      n ← 4 5 6 7\nAssigns vector of values, {4 5 6 7}, to variable n, an array create operation. An equivalent yet more concise APL expression would be n ← 3 + ⍳4. Multiple values are stored in array n, the operation performed without formal loops or control flow language.\n      n \n4 5 6 7\nDisplay the contents of n, currently an array or vector.\n      n+4\n8 9 10 11\n4 is now added to all elements of vector n, creating a 4-element vector {8 9 10 11}.\nAs above, APL's interpreter displays the result because the expression's value was not assigned to a variable (with a ←).\n      +/n\n22\nAPL displays the sum of components of the vector n, i.e. 22 (= 4 + 5 + 6 + 7) using a very compact notation: read +/ as \"plus, over...\" and a slight change would be \"multiply, over...\"\n      m ← +/(3+⍳4)\n      m\n22\nThese operations can be combined into a single statement, remembering that APL evaluates expressions right to left: first ⍳4 creates an array, [1,2,3,4], then 3 is added to each component, which are summed together and the result stored in variable m, finally displayed.\nIn conventional mathematical notation, it is equivalent to: {\\displaystyle \\displaystyle m=\\sum \\limits _{i=1}^{4}(i+3)} {\\displaystyle \\displaystyle m=\\sum \\limits _{i=1}^{4}(i+3)}. - remember that mathematical expressions are not read right-to-left...\n\nThe user can save the workspace with all values, programs, and execution status.\n\nAPL is well known for its use of a set of non-ASCII symbols, which are an extension of traditional arithmetic and algebraic notation. Having single character names for SIMD vector functions is one way that APL enables compact formulation of algorithms for data transformation such as computing Conway's Game of Life in one line of code.[53] In nearly all versions of APL, it is theoretically possible to express any computable function in one expression, that is, in one line of code.\n\nBecause of the unusual character set, many programmers use special keyboards with APL keytops to write APL code.[54] Although there are various ways to write APL code using only ASCII characters,[55] in practice it is almost never done. (This may be thought to support Iverson's thesis about notation as a tool of thought.[56]) Most if not all modern implementations use standard keyboard layouts, with special mappings or input method editors to access non-ASCII characters. Historically, the APL font has been distinctive, with uppercase italic alphabetic characters and upright numerals and symbols. Most vendors continue to display the APL character set in a custom font.\n\nAdvocates of APL[who?] claim that the examples of so-called \"write-only code\" (badly written and almost incomprehensible code) are almost invariably examples of poor programming practice or novice mistakes, which can occur in any language. Advocates of APL also claim that they are far more productive with APL than with more conventional computer languages, and that working software can be implemented in far less time and with far fewer programmers than using other technology.\n\nThey also may claim that because it is compact and terse, APL lends itself well to larger-scale software development and complexity, because the number of lines of code can be dramatically reduced. Many APL advocates and practitioners also view standard programming languages such as COBOL and Java as being comparatively tedious. APL is often found where time-to-market is important, such as with trading systems.[57][58][59][60]\n\nIverson later designed the J programming language, which uses ASCII with digraphs instead of special symbols.\n\nExecution[edit]\nBecause APL's core objects are arrays,[61] it lends itself well to parallelism,[62] parallel computing,[63][64] massively parallel applications,[65][66] and very-large-scale integration or VLSI.[67][68]\n\nInterpreters[edit]\nAPLNext (formerly APL2000) offers an advanced APL interpreter that operates under Linux, Unix, and Windows. It supports Windows automation, supports calls to operating system and user defined DLLs, has an advanced APL File System, and represents the current level of APL language development. APL2000's product is an advanced continuation of STSC's successful APL*Plus/PC and APL*Plus/386 product line.\n\nDyalog APL is an advanced APL interpreter that operates under AIX, Linux (including on the Raspberry Pi), macOS and Microsoft Windows.[69] Dyalog has extensions to the APL language, which include new object-oriented features, numerous language enhancements, plus a consistent namespace model used for both its Microsoft Automation interface, as well as native namespaces. For the Windows platform, Dyalog APL offers tight integration with .NET, plus limited integration with the Microsoft Visual Studio development platform.\n\nIBM offers a version of IBM APL2 for IBM AIX, Linux, Sun Solaris and Windows systems. This product is a continuation of APL2 offered for IBM mainframes. IBM APL2 was arguably the most influential APL system, which provided a solid implementation standard for the next set of extensions to the language, focusing on nested arrays.\n\nNARS2000 is an open-source APL interpreter written by Bob Smith, a well-known APL developer and implementor from STSC in the 1970s and 1980s. NARS2000 contains advanced features and new datatypes, runs natively under Windows (32- and 64-bit versions), and runs under Linux and Apple macOS with Wine.\n\nMicroAPL Limited offers APLX, a full-featured 64 bit interpreter for Linux, Microsoft Windows, and macOS systems. The core language is closely modelled on IBM's APL2 with various enhancements. APLX includes close integration with .NET, Java, Ruby and R. Effective July 11, 2016,[70] MicroAPL withdrew APLX from commercial sale. Dyalog began hosting the APLX website including the download area and documentation.\n\nSoliton Incorporated offers the SAX interpreter, which stands for Sharp APL for UniX, for Unix and Linux systems. This is a further development of I. P. Sharp Associates' Sharp APL product. Unlike most other APL interpreters, Kenneth E. Iverson had some influence in the way nested arrays were implemented in Sharp APL and SAX. Nearly all other APL implementations followed the course set by IBM with APL2, thus some important details in Sharp APL differ from other implementations.\n\nOpenAPL is an open source implementation of APL published by Branko Bratkovic. It is based on code by Ken Thompson of Bell Laboratories, together with contributions by others. It is licensed under the GNU General Public License, and runs on Unix systems including Linux on x86, SPARC and other CPUs.\n\nGNU APL is a free implementation of ISO Standard 13751 and hence similar to APL2. It runs on GNU/Linux and on Windows using Cygwin. It uses Unicode internally. GNU APL was written by Jürgen Sauermann.\n\nCompilers[edit]\nAPL programs are normally interpreted and less often compiled. In reality, most APL compilers translated source APL to a lower level language such as C, leaving the machine-specific details to the lower level compiler. Compilation of APL programs was a frequently discussed topic in conferences. Although some of the newer enhancements to the APL language such as nested arrays have rendered the language increasingly difficult to compile, the idea of APL compilation is still under development today.\n\nIn the past, APL compilation was regarded as a means to achieve execution speed comparable to other mainstream languages, especially on mainframe computers. Several APL compilers achieved some levels of success, though comparatively little of the development effort spent on APL over the years went to perfecting compilation into machine code.\n\nAs is the case when moving APL programs from one vendor's APL interpreter to another, APL programs invariably will require changes to their content. Depending on the compiler, variable declarations might be needed, certain language features would need to be removed or avoided, or the APL programs would need to be cleaned up in some way. Some features of the language, such as the execute function (an expression evaluator) and the various reflection and introspection functions from APL, such as the ability to return a function's text or to materialize a new function from text, are simply not practical to implement in machine code compilation.\n\nA commercial compiler was brought to market by STSC in the mid-1980s as an add-on to IBM's VSAPL Program Product.[71][72] Unlike more modern APL compilers, this product produced machine code that would execute only in the interpreter environment, it was not possible to eliminate the interpreter component. The compiler could compile many scalar and vector operations to machine code, but it would rely on the APL interpreter's services to perform some more advanced functions, rather than attempt to compile them. However, dramatic speedups did occur, especially for heavily iterative APL code.\n\nAround the same time, the book An APL Compiler by Timothy Budd appeared in print.[73] This book detailed the construction of an APL translator (aplc),[74] written in C, which performed certain optimizations such as loop fusion specific to the needs of an array language. The source language was APL-like in that a few rules of the APL language were changed or relaxed to permit more efficient compilation. The translator would emit C code which could then be compiled and run outside of the APL workspace. Another compiler, also named aplc, was later created by Samuel W. Sirlin, based on Budd's work.[75]\n\nThe Burroughs/Unisys APLB interpreter (1982) was the first to use dynamic incremental compilation to produce code for an APL-specific virtual machine. It recompiled on-the-fly as identifiers changed their functional meanings. In addition to removing parsing and some error checking from the main execution path, such compilation also streamlines the repeated entry and exit of user-defined functional operands. This avoids the stack setup and take-down for function calls made by APL's built-in operators such as Reduce and Each.\n\nArray Contraction\nDefinition:\nA program transformation which reduces array size while preserving the correct output. This technique has been used in the past to reduce memory requirement of the program, which can be important to out-of-core computing and embedded systems.\n\n'Improving Data Locality by Array Contraction'[76]\nReducing Virtual Memory Requirements\nAPEX, a research APL compiler, is available under the GNU Public License, per Snake Island Research Inc.[77] APEX compiles flat APL (a subset of ISO N8485) into SAC, a functional array language with parallel semantics, and currently runs under Linux. APEX-generated code uses loop fusion and 'array contraction', special-case algorithms not generally available to interpreters (e.g., upgrade of permutation matrix/vector), to achieve a level of performance comparable to that of Fortran.\n\nThe APLNext VisualAPL system is a departure from a conventional APL system in that VisualAPL is a true .NET language which is fully interoperable with other .NET languages such as VB.NET and C#. VisualAPL is inherently object-oriented and Unicode-based. While VisualAPL incorporates most of the features of standard APL implementations, the VisualAPL language extends standard APL to be .NET-compliant. VisualAPL is hosted in the standard Microsoft Visual Studio IDE and as such, invokes compilation in a manner identical to that of other .NET languages. By producing Common Intermediate Language (CIL) code, it utilizes the Microsoft just-in-time compiler (JIT) to support 32-bit or 64-bit hardware. Substantial performance speed-ups over standard APL have been reported,[citation needed] especially when (optional) strong typing of function arguments is used.\n\nAn APL-to-C# translator is available from Causeway Graphical Systems. This product was designed to allow the APL code, translated to equivalent C#, to run completely outside of the APL environment. The Causeway compiler requires a run-time library of array functions. Some speedup, sometimes dramatic, is visible, but happens on account of the optimisations inherent in Microsoft's .NET Framework.\n\nMatrix optimizations[edit]\nAPL was unique in the speed with which it could perform complicated matrix operations. For example, a very large matrix multiplication would take only a few seconds on a machine that was much less powerful than those today, ref. history of supercomputing and \"because it operates on arrays and performs operations like matrix inversion internally, well written APL can be surprisingly fast.\"[78][79] There were both technical and economic reasons for this advantage:\n\nCommercial interpreters delivered highly tuned linear algebra library routines.\nVery low interpretive overhead was incurred per-array—not per-element.\nAPL response time compared favorably to the runtimes of early optimizing compilers.\nIBM provided microcode assist for APL on a number of IBM370 mainframes.\nPhil Abrams' much-cited paper \"An APL Machine\" illustrated how APL could make effective use of lazy evaluation where calculations would not actually be performed until the results were needed and then only those calculations strictly required. An obvious (and easy to implement) lazy evaluation is the J-vector: when a monadic iota is encountered in the code, it is kept as a representation instead of being expanded in memory; in future operations, a J-vectors contents are the loop's induction register, not reads from memory.\n\nAlthough such techniques were not widely used by commercial interpreters, they exemplify the language's best survival mechanism: not specifying the order of scalar operations or the exact contents of memory. As standardized, in 1983 by ANSI working group X3J10, APL remains highly data-parallel. This gives language implementers immense freedom to schedule operations as efficiently as possible. As computer innovations such as cache memory, and SIMD execution became commercially available, APL programs are ported with almost no extra effort spent re-optimizing low-level details.\n\nTerminology[edit]\nAPL makes a clear distinction between functions and operators.[50][80] Functions take arrays (variables or constants or expressions) as arguments, and return arrays as results. Operators (similar to higher-order functions) take functions or arrays as arguments, and derive related functions. For example, the \"sum\" function is derived by applying the \"reduction\" operator to the \"addition\" function. Applying the same reduction operator to the \"maximum\" function (which returns the larger of two numbers) derives a function which returns the largest of a group (vector) of numbers. In the J language, Iverson substituted the terms \"verb\" for \"function\" and \"adverb\" or \"conjunction\" for \"operator\".\n\nAPL also identifies those features built into the language, and represented by a symbol, or a fixed combination of symbols, as primitives. Most primitives are either functions or operators. Coding APL is largely a process of writing non-primitive functions and (in some versions of APL) operators. However a few primitives are considered to be neither functions nor operators, most noticeably assignment.\n\nSome words used in APL literature have meanings that differ from those in both mathematics and the generality of computer science.\n\nTerm\tDescription\nfunction\toperation or mapping that takes zero, one (right) or two (left & right) arguments which may be scalars, arrays, or more complicated structures, and may return a similarly complex result. A function may be:\nPrimitive: built-in and represented by a single glyph;[81]\nDefined: as a named and ordered collection of program statements;[81]\nDerived: as a combination of an operator with its arguments.[81]\narray\tdata valued object of zero or more orthogonal dimensions in row-major order in which each item is a primitive scalar datum or another array.[82]\nniladic\tnot taking or requiring any arguments,[83]\nmonadic\trequiring only one argument; on the right for a function, on the left for an operator, unary[83]\ndyadic\trequiring both a left and a right argument, binary[83]\nambivalent or monadic\tcapable of use in a monadic or dyadic context, permitting its left argument to be elided[81]\noperator\toperation or mapping that takes one (left) or two (left & right) function or array valued arguments (operands) and derives a function. An operator may be:\nPrimitive: built-in and represented by a single glyph;[81]\nDefined: as a named and ordered collection of program statements.[81]\nSyntax[edit]\nMain article: APL syntax and symbols\nAPL has explicit representations of functions, operators, and syntax, thus providing a basis for the clear and explicit statement of extended facilities in the language, as well as tools for experimentation upon them.[84]\n\nExamples[edit]\nHello, World[edit]\nThis displays \"Hello, world\":\n\n'Hello, world'\n'Hello World,' sample user session on YouTube[85]\n\nA design theme in APL is to define default actions in some cases that would produce syntax errors in most other programming languages.\n\nThe 'Hello, world' string constant above displays, because display is the default action on any expression for which no action is specified explicitly (e.g. assignment, function parameter).\n\nExponentiation[edit]\nAnother example of this theme is that exponentiation in APL is written as \"2⋆3\", which indicates raising 2 to the power 3 (this would be written as \"2^3\" in some other languages and \"2**3\" in FORTRAN and Python). However, if no base is specified (as with the statement \"⋆3\" in APL, or \"^3\" in other languages), most other programming languages one would have a syntax error. APL however assumes the missing base to be the natural logarithm constant e (2.71828....), and so interpreting \"⋆3\" as \"2.71828⋆3\".\n\n\"Pick 6\" lottery numbers[edit]\nThis following immediate-mode expression generates a typical set of \"Pick 6\" lottery numbers: six pseudo-random integers ranging from 1 to 40, guaranteed non-repeating, and displays them sorted in ascending order:\n\nx[⍋x←6?40]\nThe above does a lot, concisely; although it seems complex to a beginning APLer. It combines the following APL functions (also called primitives[86] and glyphs[87]):\n\nThe first to be executed (APL executes from rightmost to leftmost) is dyadic function \"?\" (named deal when dyadic) that returns a vector consisting of a select number (left argument: 6 in this case) of random integers ranging from 1 to a specified maximum (right argument: 40 in this case), which, if said maximum ≥ vector length, is guaranteed to be non-repeating; thus, generate/create 6 random integers ranging from 1-40.[88]\nThis vector is then assigned (←) to the variable x, because it is needed later.\nThis vector is then sorted in ascending order by a monadic \"⍋\" function, which has as its right argument everything to the right of it up to the next unbalanced close-bracket or close-parenthesis. The result of ⍋ is the indices that will put its argument into ascending order.\nThen the output of ⍋ is applied to the variable x, which we saved earlier, and it puts the items of x into ascending sequence.\nSince there is no function to the left of the left-most x to tell APL what to do with the result, it simply outputs it to the display (on a single line, separated by spaces) without needing any explicit instruction to do that.\n\n\"?\" also has a monadic equivalent called roll, which simply returns a single random integer between 1 and its sole operand [to the right of it], inclusive. Thus, a role-playing game program might use the expression \"?20\" to roll a twenty-sided die.\n\nPrime numbers[edit]\nThe following expression finds all prime numbers from 1 to R. In both time and space, the calculation complexity is {\\displaystyle O(R^{2})\\,\\!} O(R^{2})\\,\\! (in Big O notation).\n\n(~R∊R∘.×R)/R←1↓ιR\nExecuted from right to left, this means:\n\nIota ι creates a vector containing integers from 1 to R (if R = 6 at the beginning of the program, ιR is 1 2 3 4 5 6)\nDrop first element of this vector (↓ function), i.e. 1. So 1↓ιR is 2 3 4 5 6\nSet R to the new vector (←, assignment primitive), i.e. 2 3 4 5 6\nThe / reduction operator is dyadic (binary) and the interpreter first evaluates its left argument (entirely in parentheses):\nGenerate outer product of R multiplied by R, i.e. a matrix that is the multiplication table of R by R (°.× operator), i.e.\n4\t6\t8\t10\t12\n6\t9\t12\t15\t18\n8\t12\t16\t20\t24\n10\t15\t20\t25\t30\n12\t18\t24\t30\t36\nBuild a vector the same length as R with 1 in each place where the corresponding number in R is in the outer product matrix (∈, set inclusion or element of or Epsilon operator), i.e. 0 0 1 0 1\nLogically negate (not) values in the vector (change zeros to ones and ones to zeros) (∼, logical not or Tilde operator), i.e. 1 1 0 1 0\nSelect the items in R for which the corresponding element is 1 (/ reduction operator), i.e. 2 3 5\n(Note, this assumes the APL origin is 1, i.e. indices start with 1. APL can be set to use 0 as the origin, so that ι6 is 0 1 2 3 4 5, which is convenient for some calculations).\n\nSorting[edit]\nThe following expression sorts a word list stored in matrix X according to word length:\n\nX[⍋X+.≠' ';]\nGame of Life[edit]\nThe following function \"life\", written in Dyalog APL, takes a boolean matrix and calculates the new generation according to Conway's Game of Life. It demonstrates the power of APL to implement a complex algorithm in very little code, but it is also very hard to follow unless one has advanced knowledge of APL.\n\nlife←{↑1 ⍵∨.∧3 4=+/,¯1 0 1∘.⊖¯1 0 1∘.⌽⊂⍵}\nHTML tags removal[edit]\nIn the following example, also Dyalog, the first line assigns some HTML code to a variable txt and then uses an APL expression to remove all the HTML tags:\n\ntxt←'<nowiki><html><body><p>This is <em>emphasized</em> text.</p></body></html></nowiki>'\n⎕←{⍵/⍨~{⍵∨≠\\⍵}⍵∊'<>'}txt\nThis returns the text This is emphasized text.\n\nCharacter set[edit]\nMain articles: APL (codepage) and syntax and symbols.\nAPL has been both criticized and praised for its choice of a unique, non-standard character set. Some who learn it become ardent adherents, suggesting that there is some weight behind Iverson's idea that the notation used does make a difference. In the beginning, there were few terminal devices and even display monitors that could reproduce the APL character set—the most popular ones employing the IBM Selectric print mechanism used with a special APL type element. One of the early APL line terminals (line-mode operation only, not full screen) was the Texas Instruments TI Model 745 (circa 1977) with the full APL character set[89] which featured half and full duplex telecommunications modes, for interacting with an APL time-sharing service or remote mainframe to run a remote computer job, called an RJE.\n\nOver time, with the universal use of high-quality graphic displays, printing devices and Unicode support, the APL character font problem has largely been eliminated. However, entering APL characters requires the use of input method editors, keyboard mappings, virtual/on-screen APL symbol sets,[90][91] or easy-reference printed keyboard cards which can frustrate beginners accustomed to other programming languages.[92][93][94] With beginners who have no prior experience with other programming languages, a study involving high school students found that typing and using APL characters did not hinder the students in any measurable way.[95]\n\nIn defense of the APL community, APL requires less coding to type in, and keyboard mappings become memorized over time. Also, special APL keyboards are manufactured and in use today, as are freely available downloadable fonts for operating system platforms such as Microsoft Windows.[96] The reported productivity gains assume that one will spend enough time working in APL to make memorization of the symbols, their semantics, and keyboard mappings worthwhile.[citation needed]\n\nUse[edit]\nAPL has long had a select, mathematically inclined and curiosity-driven user base, who reference its powerful and symbolic nature: one symbol/character can perform an entire sort, another can perform regression, for example. It was and still is popular in financial, pre-modeling applications, and insurance applications, in simulations, and in mathematical applications. APL has been used in a wide variety of contexts and for many and varied purposes, including artificial intelligence[97][98] and robotics.[99][100] A newsletter titled \"Quote-Quad\" dedicated to APL was published from the 1970s until 2007 by the SIGAPL section of the Association for Computing Machinery (Quote-Quad is the name of the APL character used for text input and output).[101]\n\nBefore the advent of full-screen systems and until as late as the mid-1980s, systems were written such that the user entered instructions in his own business specific vocabulary. APL time-sharing vendors delivered applications in this form. On the I. P. Sharp timesharing system, a workspace called 39 MAGIC offered access to financial and airline data plus sophisticated (for the time) graphing and reporting. Another example is the GRAPHPAK workspace supplied with IBM's APL, then APL2.\n\nBecause of its matrix operations, APL was for some time quite popular for computer graphics programming, where graphic transformations could be encoded as matrix multiplications. One of the first commercial computer graphics houses, Digital Effects, based in New York City, produced an APL graphics product known as \"Visions\", which was used to create television commercials and, reportedly, animation for the 1982 film Tron. Digital Effects' use of APL was informally described at a number of SIGAPL conferences in the late 1980s; examples discussed included the early UK Channel 4 TV logo/ident.\n\nInterest in APL has declined from a peak in the mid-1980s. This appears partly due to lack of smooth migration pathways from higher performing memory-intensive mainframe implementations to low-cost personal computer alternatives - APL implementations for computers before the Intel 80386 released in the late 1980s were only suitable for small applications. Another important reason for the decline is the lack of low cost, standardized and robust, compiled APL executables - usable across multiple computer hardware and OS platforms. There are several APL version permutations across various APL implementations, particularly differences between IBM's APL2 and APL2000's APL+ versions. Another practical limitation is that APL has fallen behind modern integrated computing environments with respect to debugging capabilities or test-driven development. Consequently, while APL remains eminently suitable for small-to-medium-sized programs, productivity gains for larger projects involving teams of developers would be questionable.[citation needed]\n\nThe growth of end-user computing tools such as Microsoft Excel and Microsoft Access has indirectly eroded potential APL usage. These are frequently appropriate platforms for what may have been APL applications in the 1970s and 1980s. Some APL users migrated to the J programming language, which offers some advanced features. Lastly, the decline was also due in part to the growth of MATLAB, GNU Octave, and Scilab. These scientific computing array-oriented platforms provide an interactive computing experience similar to APL, but more closely resemble conventional programming languages such as Fortran, and use standard ASCII. Other APL users continue to wait for a very low-cost, standardized, broad-hardware-usable APL implementation.[102][103]\n\nNotwithstanding this decline, APL finds continued use in certain fields, such as accounting research, pre-hardcoded modeling, DNA identification technology,[104][105] symbolic mathematical expression and learning. It remains an inspiration to its current user base as well as for other languages.[106]\n\nStandardization[edit]\nAPL has been standardized by the ANSI working group X3J10 and ISO/IEC Joint Technical Committee 1 Subcommittee 22 Working Group 3. The Core APL language is specified in ISO 8485:1989, and the Extended APL language is specified in ISO/IEC 13751:2001.\n\nSee also[edit]\nA+ (programming language)\nAPL EBCDIC code page\nAPL Shared Variables\nI. P. Sharp Associates\nTK Solver\nIBM Type-III Library\nIBM 1130\nIverson Award\nJ (programming language)\nK (programming language)\nQ (programming language from Kx Systems)\nLYaPAS\nScientific Time Sharing Corporation\nSoliton Incorporated\nELI (programming language)\nRPL (programming language)",
          "type": "intepreted",
          "plid": 5
        },
        {
          "name": "Chapel ",
          "details": "Chapel, the Cascade High Productivity Language, is a parallel programming language developed by Cray.[2] It is being developed as part of the Cray Cascade project, a participant in DARPA's High Productivity Computing Systems (HPCS) program, which had the goal of increasing supercomputer productivity by the year 2010. It is being developed as an open source project, under version 2 of the Apache license.[3]\n\nContents  [hide] \n1\tGoals\n2\tFeatures\n3\tSee also\n4\tNotes\n5\tReferences\n6\tFurther reading\n7\tExternal links\nGoals[edit]\nChapel aims to improve the programmability of parallel computers in general and the Cascade system in particular, by providing a higher level of expression than current programming languages do and by improving the separation between algorithmic expression and data structure implementation details.\n\nThe language designers aspire for Chapel to bridge the gap between current HPC programming practitioners, who they describe as Fortran, C or C++ users writing procedural code using technologies like OpenMP and MPI on one side, and newly graduating computer programmers who tend to prefer Java, Python or Matlab with only some of them having experience with C++ or C. Chapel should offer the productivity advances offered by the latter suite of languages while not alienating the users of the first.[1]\n\nFeatures[edit]\nChapel supports a multithreaded parallel programming model at a high level by supporting abstractions for data parallelism, task parallelism, and nested parallelism. It enables optimizations for the locality of data and computation in the program via abstractions for data distribution and data-driven placement of subcomputations. It allows for code reuse and generality through object-oriented concepts and generic programming features. For instance, Chapel allows for the declaration of locales.[4]\n\nWhile Chapel borrows concepts from many preceding languages, its parallel concepts are most closely based on ideas from High Performance Fortran (HPF), ZPL, and the Cray MTA's extensions to Fortran and C.\n\nSee also[edit]\n\tFree software portal\nCoarray Fortran\nFortress\nUnified Parallel C\nX10\nRaftLib\nNotes[edit]\n^ Jump up to: a b Chamberlain, Bradford L. \"A Brief Overview of Chapel\" (PDF). Cray Inc. Retrieved 22 April 2015.\nJump up ^ Lightfoot, David E. (2006). Modular programming languages: 7th Joint Modular Languages Conference. p. 20. ISBN 3-540-40927-0.\nJump up ^ \"Chapel license\". Chapel.Cray.com. Retrieved November 15, 2015.\nJump up ^ Bongen Gu; Wikuan Yu; Yoonsik Kwak (June 28–30, 2011). \"Communication and Computation Overlap through Task Synchronization in Multi-locale Chapel Environment\". In James J. Park, Laurence T. Yang and Changhoon Lee. Future Information Technology, Part I: 6th International Conference. Loutraki, Greece: Springer-Verlag. pp. 285–292. ISBN 978-3-642-22332-7. Retrieved August 17, 2011.\nReferences[edit]\nChamberlain, Bradford L. (2011). \"Chapel (Cray Inc. HPCS Language)\". In Padua, David. Encyclopedia of Parallel Computing, Volume 4. Springer. ISBN 9780387097657.\nFurther reading[edit]\nBrueckner, Rich (August 6, 2014). \"Why Chapel for Parallel Programming?\". InsideHPC. Retrieved 2015-03-23.\nDun, Nan; Taura, K. (2012). \"An Empirical Performance Study of Chapel Programming Language\". Parallel and Distributed Processing Symposium Workshops & PhD Forum (IPDPSW), 2012 IEEE 26th International. IEEE: 497–506. doi:10.1109/IPDPSW.2012.64. ISBN 978-1-4673-0974-5. Retrieved 2015-03-23.\nPadua, David, ed. (2011). Encyclopedia of Parallel Computing. Volume 4. Springer Science & Business Media. pp. 249–256. ISBN 9780387097657.",
          "type": "unknown",
          "plid": 6
        },
        {
          "name": "Fortran",
          "details": "Fortran (formerly FORTRAN, derived from \"Formula Translation\"[2]) is a general-purpose, imperative programming language that is especially suited to numeric computation and scientific computing. Originally developed by IBM[3] in the 1950s for scientific and engineering applications, Fortran came to dominate this area of programming early on and has been in continuous use for over half a century in computationally intensive areas such as numerical weather prediction, finite element analysis, computational fluid dynamics, computational physics, crystallography and computational chemistry. It is a popular language for high-performance computing[4] and is used for programs that benchmark and rank the world's fastest supercomputers.[5]\n\nFortran encompasses a lineage of versions, each of which evolved to add extensions to the language while usually retaining compatibility with prior versions. Successive versions have added support for structured programming and processing of character-based data (FORTRAN 77), array programming, modular programming and generic programming (Fortran 90), high performance Fortran (Fortran 95), object-oriented programming (Fortran 2003) and concurrent programming (Fortran 2008).\n\nContents  [hide] \n1\tNaming\n2\tHistory\n2.1\tFORTRAN\n2.1.1\tFixed layout and punched cards\n2.2\tFORTRAN II\n2.2.1\tSimple FORTRAN II program\n2.3\tFORTRAN III\n2.4\tIBM 1401 FORTRAN\n2.5\tFORTRAN IV\n2.6\tFORTRAN 66\n2.7\tFORTRAN 77\n2.7.1\tVariants: Minnesota FORTRAN\n2.8\tTransition to ANSI Standard Fortran\n2.9\tFortran 90\n2.9.1\tObsolescence and deletions\n2.9.2\t\"Hello world\" example\n2.10\tFortran 95\n2.10.1\tConditional compilation and varying length strings\n2.11\tFortran 2003\n2.12\tFortran 2008\n2.13\tFortran 2015\n3\tFortran and supercomputers\n4\tLanguage features\n5\tPortability\n6\tVariants\n6.1\tFortran 5\n6.2\tFortran V\n6.3\tFortran 6\n6.4\tSpecific variants\n6.4.1\tFOR TRANSIT for the IBM 650\n6.5\tFortran-based languages\n7\tCode examples\n8\tHumor\n9\tSee also\n10\tReferences\n11\tFurther reading\n12\tExternal links\nNaming[edit]\nThe names of earlier versions of the language through FORTRAN 77 were conventionally spelled in all-capitals (FORTRAN 77 was the last version in which the use of lowercase letters in keywords was strictly non-standard). The capitalization has been dropped in referring to newer versions beginning with Fortran 90. The official language standards now refer to the language as \"Fortran\" rather than all-caps \"FORTRAN\".\n\nHistory[edit]\n\nAn IBM 704 mainframe computer\nIn late 1953, John W. Backus submitted a proposal to his superiors at IBM to develop a more practical alternative to assembly language for programming their IBM 704 mainframe computer. Backus' historic FORTRAN team consisted of programmers Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Lois Haibt, and David Sayre.[6] Its concepts included easier entry of equations into a computer, an idea developed by J. Halcombe Laning and demonstrated in the Laning and Zierler system of 1952.[7]\n\nA draft specification for The IBM Mathematical Formula Translating System was completed by mid-1954. The first manual for FORTRAN appeared in October 1956, with the first FORTRAN compiler delivered in April 1957. This was the first optimizing compiler, because customers were reluctant to use a high-level programming language unless its compiler could generate code with performance comparable to that of hand-coded assembly language.[8]\n\nWhile the community was skeptical that this new method could possibly outperform hand-coding, it reduced the number of programming statements necessary to operate a machine by a factor of 20, and quickly gained acceptance. John Backus said during a 1979 interview with Think, the IBM employee magazine, \"Much of my work has come from being lazy. I didn't like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs.\"[9]\n\nThe language was widely adopted by scientists for writing numerically intensive programs, which encouraged compiler writers to produce compilers that could generate faster and more efficient code. The inclusion of a complex number data type in the language made Fortran especially suited to technical applications such as electrical engineering.\n\nBy 1960, versions of FORTRAN were available for the IBM 709, 650, 1620, and 7090 computers. Significantly, the increasing popularity of FORTRAN spurred competing computer manufacturers to provide FORTRAN compilers for their machines, so that by 1963 over 40 FORTRAN compilers existed. For these reasons, FORTRAN is considered to be the first widely used programming language supported across a variety of computer architectures.\n\nThe development of FORTRAN paralleled the early evolution of compiler technology, and many advances in the theory and design of compilers were specifically motivated by the need to generate efficient code for FORTRAN programs.\n\nFORTRAN[edit]\nThe initial release of FORTRAN for the IBM 704 contained 32 statements, including:\n\nDIMENSION and EQUIVALENCE statements\nAssignment statements\nThree-way arithmetic IF statement, which passed control to one of three locations in the program depending on whether the result of the arithmetic statement was negative, zero, or positive\nIF statements for checking exceptions (ACCUMULATOR OVERFLOW, QUOTIENT OVERFLOW, and DIVIDE CHECK); and IF statements for manipulating sense switches and sense lights\nGO TO, computed GO TO, ASSIGN, and assigned GO TO\nDO loops\nFormatted I/O: FORMAT, READ, READ INPUT TAPE, WRITE, WRITE OUTPUT TAPE, PRINT, and PUNCH\nUnformatted I/O: READ TAPE, READ DRUM, WRITE TAPE, and WRITE DRUM\nOther I/O: END FILE, REWIND, and BACKSPACE\nPAUSE, STOP, and CONTINUE\nFREQUENCY statement (for providing optimization hints to the compiler).\nThe arithmetic IF statement was similar to a three-way branch instruction on the IBM 704. However, the 704 branch instructions all contained only one destination address (e.g., TZE – Transfer AC Zero, TNZ – Transfer AC Not Zero, TPL – Transfer AC Plus, TMI – Transfer AC Minus). The machine (and its successors in the 700/7000 series) did have a three-way skip instruction (CAS – Compare AC with Storage), but using this instruction to implement the IF would consume 4 instruction words, require the constant Zero in a word of storage, and take 3 machine cycles to execute; using the Transfer instructions to implement the IF could be done in 1 to 3 instruction words, required no constants in storage, and take 1 to 3 machine cycles to execute. An optimizing compiler like FORTRAN would most likely select the more compact and usually faster Transfers instead of the Compare (use of Transfers also allowed the FREQUENCY statement to optimize IFs, which could not be done using the Compare). Also the Compare considered −0 and +0 to be different values while the Transfer Zero and Transfer Not Zero considered them to be the same. The FREQUENCY statement in FORTRAN was used originally (and optionally) to give branch probabilities for the three branch cases of the arithmetic IF statement. The first FORTRAN compiler used this weighting to perform at compile time a Monte Carlo simulation of the generated code, the results of which were used to optimize the placement of basic blocks in memory – a very sophisticated optimization for its time. The Monte Carlo technique is documented in Backus et al.'s paper on this original implementation, The FORTRAN Automatic Coding System:\n\nThe fundamental unit of program is the basic block; a basic block is a stretch of program which has one entry point and one exit point. The purpose of section 4 is to prepare for section 5 a table of predecessors (PRED table) which enumerates the basic blocks and lists for every basic block each of the basic blocks which can be its immediate predecessor in flow, together with the absolute frequency of each such basic block link. This table is obtained by running the program once in Monte-Carlo fashion, in which the outcome of conditional transfers arising out of IF-type statements and computed GO TO'S is determined by a random number generator suitably weighted according to whatever FREQUENCY statements have been provided.[10]\n\nMany years later, the FREQUENCY statement had no effect on the code, and was treated as a comment statement, since the compilers no longer did this kind of compile-time simulation. A similar fate has befallen compiler hints in several other programming languages; for example C's register keyword.[citation needed]\n\nFixed layout and punched cards[edit]\n\nFORTRAN code on a punched card, showing the specialized uses of columns 1–5, 6 and 73–80\nFurther information: Computer programming in the punched card era\nBefore the development of disk files, text editors and terminals, programs were most often entered on a keypunch keyboard onto 80-column punched cards, one line to a card. The resulting deck of cards would be fed into a card reader to be compiled. Punched-card codes included no lower-case letters or many special characters, and special versions of the IBM 026 keypunch were offered that would correctly print the repurposed special characters used in Fortran.\n\nReflecting punched-card input practice, Fortran programs were originally written in a fixed-column format, with the first 72 columns read into twelve 36-bit words.\n\nA letter \"C\" in column 1 caused the entire card to be treated as a comment and ignored by the compiler. Otherwise, the columns of the card were divided into four fields:\n\n1 to 5 were the label field: a sequence of digits here was taken as a label for use in DO or GO TO control statements, or to refer to a FORMAT in a WRITE or READ statement.\n6 was a continuation field: a character other than a blank or a zero here caused the card to be taken as a continuation of the statement on the prior card.\n7 to 72 served as the statement field.\n73 to 80 were ignored (the IBM 704's card reader only used 72 columns).[11]\nColumns 73 to 80 could therefore be used for identification information, such as punching a sequence number, which could be used to re-order cards if a stack of cards was dropped; though in practice this was reserved for stable, production programs. An IBM 519 could be used to copy a program deck and add sequence numbers. Some early compilers, e.g., the IBM 650's, had additional restrictions due to limitations on their card readers.[12] Keypunches could be programmed to tab to column 7 and skip out after column 72. Later compilers relaxed most fixed-format restrictions, and the requirement was eliminated in the Fortran 90 standard.\n\nWithin the statement field, whitespace characters (blanks) were ignored outside a text literal. This allowed omitting spaces between tokens for brevity or including spaces within identifiers for clarity. For example, AVG OF X was a valid identifier, equivalent to AVGOFX, and 101010DO101I=1,101 was a valid statement, equivalent to 10101 DO 101 I = 1, 101 because the zero in column 6 is treated as if it were a space (!), while 101010DO101I=1.101 was instead 10101 DO101I = 1.101, the assignment of 1.101 to a variable called DO101I. Note the slight visual difference between a comma and a period.\n\nHollerith strings, originally allowed only in FORMAT and DATA statements, were prefixed by a character count and the letter H (e.g., 26HTHIS IS ALPHANUMERIC DATA.), allowing blanks to be retained within the character string. Miscounts were a problem.\n\nFORTRAN II[edit]\nIBM's FORTRAN II appeared in 1958. The main enhancement was to support procedural programming by allowing user-written subroutines and functions which returned values, with parameters passed by reference. The COMMON statement provided a way for subroutines to access common (or global) variables. Six new statements were introduced:\n\nSUBROUTINE, FUNCTION, and END\nCALL and RETURN\nCOMMON\nOver the next few years, FORTRAN II would also add support for the DOUBLE PRECISION and COMPLEX data types.\n\nEarly FORTRAN compilers supported no recursion in subroutines. Early computer architectures supported no concept of a stack, and when they did directly support subroutine calls, the return location was often stored in one fixed location adjacent to the subroutine code, which does not permit a subroutine to be called again before a prior call of the subroutine has returned. Although not specified in Fortran 77, many F77 compilers supported recursion as an option, while it became a standard in Fortran 90.[13]\n\nSimple FORTRAN II program[edit]\nThis program, for Heron's formula, reads data on a tape reel containing three 5-digit integers A, B, and C as input. If A, B, and C cannot represent the sides of a triangle in plane geometry, then the program's execution will end with an error code of \"STOP 1\". Otherwise, an output line will be printed showing the input values for A, B, and C, followed by the computed AREA of the triangle as a floating-point number with 2 digits after the decimal point.\n\nC AREA OF A TRIANGLE WITH A STANDARD SQUARE ROOT FUNCTION\nC INPUT - TAPE READER UNIT 5, INTEGER INPUT\nC OUTPUT - LINE PRINTER UNIT 6, REAL OUTPUT\nC INPUT ERROR DISPLAY ERROR OUTPUT CODE 1 IN JOB CONTROL LISTING\n      READ INPUT TAPE 5, 501, IA, IB, IC\n  501 FORMAT (3I5)\nC IA, IB, AND IC MAY NOT BE NEGATIVE\nC FURTHERMORE, THE SUM OF TWO SIDES OF A TRIANGLE\nC MUST BE GREATER THAN THE THIRD SIDE, SO WE CHECK FOR THAT, TOO\n      IF (IA) 777, 777, 701\n  701 IF (IB) 777, 777, 702\n  702 IF (IC) 777, 777, 703\n  703 IF (IA+IB-IC) 777, 777, 704\n  704 IF (IA+IC-IB) 777, 777, 705\n  705 IF (IB+IC-IA) 777, 777, 799\n  777 STOP 1\nC USING HERON'S FORMULA WE CALCULATE THE\nC AREA OF THE TRIANGLE\n  799 S = FLOATF (IA + IB + IC) / 2.0\n      AREA = SQRTF( S * (S - FLOATF(IA)) * (S - FLOATF(IB)) *\n     +     (S - FLOATF(IC)))\n      WRITE OUTPUT TAPE 6, 601, IA, IB, IC, AREA\n  601 FORMAT (4H A= ,I5,5H  B= ,I5,5H  C= ,I5,8H  AREA= ,F10.2,\n     +        13H SQUARE UNITS)\n      STOP\n      END\nFORTRAN III[edit]\n\nA FORTRAN coding form, printed on paper and intended to be used by programmers to prepare programs for punching onto cards by keypunch operators. Now obsolete.\nIBM also developed a FORTRAN III in 1958 that allowed for inline assembly code among other features; however, this version was never released as a product. Like the 704 FORTRAN and FORTRAN II, FORTRAN III included machine-dependent features that made code written in it unportable from machine to machine. Early versions of FORTRAN provided by other vendors suffered from the same disadvantage.\n\nIBM 1401 FORTRAN[edit]\nFORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (6-bit) characters. The compiler could be run from tape, or from a 2200-card deck; it used no further tape or disk storage. It kept the program in memory and loaded overlays that gradually transformed it, in place, into executable form, as described by Haines.[14] and in IBM document C24-1455. The executable form was not entirely machine language; rather, floating-point arithmetic, subscripting, input/output, and function references were interpreted, anticipating UCSD Pascal P-code by two decades.\n\nIBM later provided a FORTRAN IV compiler for the 1400 series of computers, described in IBM document C24-3322.\n\nFORTRAN IV[edit]\nStarting in 1961, as a result of customer demands, IBM began development of a FORTRAN IV that removed the machine-dependent features of FORTRAN II (such as READ INPUT TAPE), while adding new features such as a LOGICAL data type, logical Boolean expressions and the logical IF statement as an alternative to the arithmetic IF statement. FORTRAN IV was eventually released in 1962, first for the IBM 7030 (\"Stretch\") computer, followed by versions for the IBM 7090, IBM 7094, and later for the IBM 1401 in 1966.\n\nBy 1965, FORTRAN IV was supposed to be compliant with the standard being developed by the American Standards Association X3.4.3 FORTRAN Working Group.[15]\n\nAt about this time FORTRAN IV had started to become an important educational tool and implementations such as the University of Waterloo's WATFOR and WATFIV were created to simplify the complex compile and link processes of earlier compilers.\n\nFORTRAN 66[edit]\nPerhaps the most significant development in the early history of FORTRAN was the decision by the American Standards Association (now American National Standards Institute (ANSI)) to form a committee sponsored by BEMA, the Business Equipment Manufacturers Association, to develop an American Standard Fortran. The resulting two standards, approved in March 1966, defined two languages, FORTRAN (based on FORTRAN IV, which had served as a de facto standard), and Basic FORTRAN (based on FORTRAN II, but stripped of its machine-dependent features). The FORTRAN defined by the first standard, officially denoted X3.9-1966, became known as FORTRAN 66 (although many continued to term it FORTRAN IV, the language on which the standard was largely based). FORTRAN 66 effectively became the first industry-standard version of FORTRAN. FORTRAN 66 included:\n\nMain program, SUBROUTINE, FUNCTION, and BLOCK DATA program units\nINTEGER, REAL, DOUBLE PRECISION, COMPLEX, and LOGICAL data types\nCOMMON, DIMENSION, and EQUIVALENCE statements\nDATA statement for specifying initial values\nIntrinsic and EXTERNAL (e.g., library) functions\nAssignment statement\nGO TO, computed GO TO, assigned GO TO, and ASSIGN statements\nLogical IF and arithmetic (three-way) IF statements\nDO loop statement\nREAD, WRITE, BACKSPACE, REWIND, and ENDFILE statements for sequential I/O\nFORMAT statement and assigned format\nCALL, RETURN, PAUSE, and STOP statements\nHollerith constants in DATA and FORMAT statements, and as arguments to procedures\nIdentifiers of up to six characters in length\nComment lines\nEND line\nFORTRAN 77[edit]\n\nFORTRAN-77 program with compiler output, written on a CDC 175 at RWTH Aachen University, Germany, in 1987\n\n4.3 BSD for the Digital Equipment Corporation (DEC) VAX, displaying the manual for FORTRAN 77 (f77) compiler\nAfter the release of the FORTRAN 66 standard, compiler vendors introduced several extensions to Standard Fortran, prompting ANSI committee X3J3 in 1969 to begin work on revising the 1966 standard, under sponsorship of CBEMA, the Computer Business Equipment Manufacturers Association (formerly BEMA). Final drafts of this revised standard circulated in 1977, leading to formal approval of the new FORTRAN standard in April 1978. The new standard, called FORTRAN 77 and officially denoted X3.9-1978, added a number of significant features to address many of the shortcomings of FORTRAN 66:\n\nBlock IF and END IF statements, with optional ELSE and ELSE IF clauses, to provide improved language support for structured programming\nDO loop extensions, including parameter expressions, negative increments, and zero trip counts\nOPEN, CLOSE, and INQUIRE statements for improved I/O capability\nDirect-access file I/O\nIMPLICIT statement, to override implicit conventions that undeclared variables are INTEGER if their name begins with I, J, K, L, M, or N (and REAL otherwise)\nCHARACTER data type, replacing Hollerith strings with vastly expanded facilities for character input and output and processing of character-based data\nPARAMETER statement for specifying constants\nSAVE statement for persistent local variables\nGeneric names for intrinsic functions (e.g. SQRT also accepts arguments of other types, such as COMPLEX or REAL*16 ).\nA set of intrinsics (LGE, LGT, LLE, LLT) for lexical comparison of strings, based upon the ASCII collating sequence. (These ASCII functions were demanded by the U.S. Department of Defense, in their conditional approval vote.[citation needed])\nIn this revision of the standard, a number of features were removed or altered in a manner that might invalidate formerly standard-conforming programs. (Removal was the only allowable alternative to X3J3 at that time, since the concept of \"deprecation\" was not yet available for ANSI standards.) While most of the 24 items in the conflict list (see Appendix A2 of X3.9-1978) addressed loopholes or pathological cases permitted by the prior standard but rarely used, a small number of specific capabilities were deliberately removed, such as:\n\nHollerith constants and Hollerith data, such as GREET = 12HHELLO THERE!\nReading into an H edit (Hollerith field) descriptor in a FORMAT specification\nOverindexing of array bounds by subscripts\n      DIMENSION A(10,5)\n      Y=  A(11,1)\nTransfer of control out of and back into the range of a DO loop (also known as \"Extended Range\")\nVariants: Minnesota FORTRAN[edit]\nControl Data Corporation computers had another version of FORTRAN 77, called Minnesota FORTRAN (MNF), designed especially for student use, with variations in output constructs, special uses of COMMONs and DATA statements, optimizations code levels for compiling, and detailed error listings, extensive warning messages, and debugs.[16]\n\nTransition to ANSI Standard Fortran[edit]\nThe development of a revised standard to succeed FORTRAN 77 would be repeatedly delayed as the standardization process struggled to keep up with rapid changes in computing and programming practice. In the meantime, as the \"Standard FORTRAN\" for nearly fifteen years, FORTRAN 77 would become the historically most important dialect.\n\nAn important practical extension to FORTRAN 77 was the release of MIL-STD-1753 in 1978.[17] This specification, developed by the U.S. Department of Defense, standardized a number of features implemented by most FORTRAN 77 compilers but not included in the ANSI FORTRAN 77 standard. These features would eventually be incorporated into the Fortran 90 standard.\n\nDO WHILE and END DO statements\nINCLUDE statement\nIMPLICIT NONE variant of the IMPLICIT statement\nBit manipulation intrinsic functions, based on similar functions included in Industrial Real-Time Fortran (ANSI/ISA S61.1 (1976))\nThe IEEE 1003.9 POSIX Standard, released in 1991, provided a simple means for FORTRAN 77 programmers to issue POSIX system calls.[18] Over 100 calls were defined in the document – allowing access to POSIX-compatible process control, signal handling, file system control, device control, procedure pointing, and stream I/O in a portable manner.\n\nFortran 90[edit]\nThe much delayed successor to FORTRAN 77, informally known as Fortran 90 (and prior to that, Fortran 8X), was finally released as ISO/IEC standard 1539:1991 in 1991 and an ANSI Standard in 1992. In addition to changing the official spelling from FORTRAN to Fortran, this major revision added many new features to reflect the significant changes in programming practice that had evolved since the 1978 standard:\n\nFree-form source input, also with lowercase Fortran keywords\nIdentifiers up to 31 characters in length (In the previous standard, it was only 6 characters).\nInline comments\nAbility to operate on arrays (or array sections) as a whole, thus greatly simplifying math and engineering computations.\nwhole, partial and masked array assignment statements and array expressions, such as   X(1:N)=R(1:N)*COS(A(1:N))\nWHERE statement for selective array assignment\narray-valued constants and expressions,\nuser-defined array-valued functions and array constructors.\nRECURSIVE procedures\nModules, to group related procedures and data together, and make them available to other program units, including the capability to limit the accessibility to only specific parts of the module.\nA vastly improved argument-passing mechanism, allowing interfaces to be checked at compile time\nUser-written interfaces for generic procedures\nOperator overloading\nDerived (structured) data types\nNew data type declaration syntax, to specify the data type and other attributes of variables\nDynamic memory allocation by means of the ALLOCATABLE attribute and the ALLOCATE and DEALLOCATE statements\nPOINTER attribute, pointer assignment, and NULLIFY statement to facilitate the creation and manipulation of dynamic data structures\nStructured looping constructs, with an END DO statement for loop termination, and EXIT and CYCLE statements for terminating normal DO loop iterations in an orderly way\nSELECT . . . CASE construct for multi-way selection\nPortable specification of numerical precision under the user's control\nNew and enhanced intrinsic procedures.\nObsolescence and deletions[edit]\nUnlike the prior revision, Fortran 90 removed no features. (Appendix B.1 says, \"The list of deleted features in this standard is empty.\") Any standard-conforming FORTRAN 77 program is also standard-conforming under Fortran 90, and either standard should be usable to define its behavior.\n\nA small set of features were identified as \"obsolescent\" and expected to be removed in a future standard.\n\nObsolescent feature\tExample\tStatus/fate in Fortran 95\nArithmetic IF-statement\t\n      IF (X) 10, 20, 30\nNon-integer DO parameters or control variables\t\n      DO 9 X= 1.7, 1.6, -0.1\nDeleted\nShared DO-loop termination or\ntermination with a statement\nother than END DO or CONTINUE  \t\n      DO 9 J= 1, 10\n          DO 9 K= 1, 10\n  9       L=  J + K\nBranching to END IF\nfrom outside a block\n\n 66   GO TO 77 ; . . .\n      IF (E) THEN ;     . . .\n 77   END IF\nDeleted\nAlternate return\t\n      CALL SUBR( X, Y *100, *200 )\nPAUSE statement\t\n      PAUSE 600\nDeleted\nASSIGN statement\n  and assigned GO TO statement\t\n 100   . . .\n      ASSIGN 100 TO H\n       . . .\n      GO TO H . . .\nDeleted\nAssigned FORMAT specifiers\t\n      ASSIGN 606 TO F ... WRITE ( 6, F )...\nDeleted\nH edit descriptors\t\n 606  FORMAT ( 9H1GOODBYE. )\nDeleted\nComputed GO TO statement\t\n      GO TO (10, 20, 30, 40), index\n(obsolete)\nStatement functions\t\n      FOIL( X, Y )=  X**2 + 2*X*Y + Y**2\n(obsolete)\nDATA statements\n  among executable statements\t\n      X= 27.3\n      DATA  A, B, C  / 5.0, 12.0, 13.0 /\n      . . .\n(obsolete)\nCHARACTER* form of CHARACTER declaration\t\n      CHARACTER*8 STRING   ! Use CHARACTER(8)\n(obsolete)\nAssumed character length functions\t\n      CHARACTER*(*) STRING\n(obsolete)[19]\nFixed form source code\tColumn 1 contains C or * or ! for comments.\nColumn 6 for continuation.\t(obsolete)\n\"Hello world\" example[edit]\nprogram helloworld\n     print *, \"Hello world!\"\nend program helloworld\nFortran 95[edit]\nMain article: Fortran 95 language features\nFortran 95, published officially as ISO/IEC 1539-1:1997, was a minor revision, mostly to resolve some outstanding issues from the Fortran 90 standard. Nevertheless, Fortran 95 also added a number of extensions, notably from the High Performance Fortran specification:\n\nFORALL and nested WHERE constructs to aid vectorization\nUser-defined PURE and ELEMENTAL procedures\nDefault initialization of derived type components, including pointer initialization\nExpanded the ability to use initialization expressions for data objects\nInitialization of pointers to NULL()\nClearly defined that ALLOCATABLE arrays are automatically deallocated when they go out of scope.\nA number of intrinsic functions were extended (for example a dim argument was added to the maxloc intrinsic).\n\nSeveral features noted in Fortran 90 to be \"obsolescent\" were removed from Fortran 95:\n\nDO statements using REAL and DOUBLE PRECISION index variables\nBranching to an END IF statement from outside its block\nPAUSE statement\nASSIGN and assigned GO TO statement, and assigned format specifiers\nH edit descriptor.\nAn important supplement to Fortran 95 was the ISO technical report TR-15581: Enhanced Data Type Facilities, informally known as the Allocatable TR. This specification defined enhanced use of ALLOCATABLE arrays, prior to the availability of fully Fortran 2003-compliant Fortran compilers. Such uses include ALLOCATABLE arrays as derived type components, in procedure dummy argument lists, and as function return values. (ALLOCATABLE arrays are preferable to POINTER-based arrays because ALLOCATABLE arrays are guaranteed by Fortran 95 to be deallocated automatically when they go out of scope, eliminating the possibility of memory leakage. In addition, elements of allocatable arrays are contiguous, and aliasing is not an issue for optimization of array references, allowing compilers to generate faster code than in the case of pointers.[20])\n\nAnother important supplement to Fortran 95 was the ISO technical report TR-15580: Floating-point exception handling, informally known as the IEEE TR. This specification defined support for IEEE floating-point arithmetic and floating point exception handling.\n\nConditional compilation and varying length strings[edit]\nIn addition to the mandatory \"Base language\" (defined in ISO/IEC 1539-1 : 1997), the Fortran 95 language also includes two optional modules:\n\nVarying length character strings (ISO/IEC 1539-2 : 2000)\nConditional compilation (ISO/IEC 1539-3 : 1998)\nwhich, together, compose the multi-part International Standard (ISO/IEC 1539).\n\nAccording to the standards developers, \"the optional parts describe self-contained features which have been requested by a substantial body of users and/or implementors, but which are not deemed to be of sufficient generality for them to be required in all standard-conforming Fortran compilers.\" Nevertheless, if a standard-conforming Fortran does provide such options, then they \"must be provided in accordance with the description of those facilities in the appropriate Part of the Standard.\"\n\nFortran 2003[edit]\nFortran 2003, officially published as ISO/IEC 1539-1:2004, is a major revision introducing many new features.[21] A comprehensive summary of the new features of Fortran 2003 is available at the Fortran Working Group (ISO/IEC JTC1/SC22/WG5) official Web site.[22]\n\nFrom that article, the major enhancements for this revision include:\n\nDerived type enhancements: parameterized derived types, improved control of accessibility, improved structure constructors, and finalizers\nObject-oriented programming support: type extension and inheritance, polymorphism, dynamic type allocation, and type-bound procedures, providing complete support for abstract data types\nData manipulation enhancements: allocatable components (incorporating TR 15581), deferred type parameters, VOLATILE attribute, explicit type specification in array constructors and allocate statements, pointer enhancements, extended initialization expressions, and enhanced intrinsic procedures\nInput/output enhancements: asynchronous transfer, stream access, user specified transfer operations for derived types, user specified control of rounding during format conversions, named constants for preconnected units, the FLUSH statement, regularization of keywords, and access to error messages\nProcedure pointers\nSupport for IEEE floating-point arithmetic and floating point exception handling (incorporating TR 15580)\nInteroperability with the C programming language\nSupport for international usage: access to ISO 10646 4-byte characters and choice of decimal or comma in numeric formatted input/output\nEnhanced integration with the host operating system: access to command line arguments, environment variables, and processor error messages\nAn important supplement to Fortran 2003 was the ISO technical report TR-19767: Enhanced module facilities in Fortran. This report provided submodules, which make Fortran modules more similar to Modula-2 modules. They are similar to Ada private child subunits. This allows the specification and implementation of a module to be expressed in separate program units, which improves packaging of large libraries, allows preservation of trade secrets while publishing definitive interfaces, and prevents compilation cascades.\n\nFortran 2008[edit]\nThe most recent standard, ISO/IEC 1539-1:2010, informally known as Fortran 2008, was approved in September 2010.[23][24] As with Fortran 95, this is a minor upgrade, incorporating clarifications and corrections to Fortran 2003, as well as introducing a select few new capabilities. The new capabilities include:\n\nSubmodules – additional structuring facilities for modules; supersedes ISO/IEC TR 19767:2005\nCoarray Fortran – a parallel execution model\nThe DO CONCURRENT construct – for loop iterations with no interdependencies\nThe CONTIGUOUS attribute – to specify storage layout restrictions\nThe BLOCK construct – can contain declarations of objects with construct scope\nRecursive allocatable components – as an alternative to recursive pointers in derived types\nThe Final Draft international Standard (FDIS) is available as document N1830.[25]\n\nAn important supplement to Fortran 2008 is the ISO Technical Specification (TS) 29113 on Further Interoperability of Fortran with C,[26][27] which has been submitted to ISO in May 2012 for approval. The specification adds support for accessing the array descriptor from C and allows ignoring the type and rank of arguments.\n\nFortran 2015[edit]\nThe next revision of the language (Fortran 2015) is intended to be a minor revision and is planned for release in mid-2018.[28] It is currently planned to include further interoperability between Fortran and C, additional parallel features, and \"the removal of simple deficiencies in and discrepancies between existing facilities.\"[29][30]\n\nFortran and supercomputers[edit]\nAlthough a 1968 journal article by the authors of BASIC already described Fortran as \"old-fashioned\",[31] since Fortran has been in use for many decades, there is a vast body of Fortran software in daily use throughout the scientific and engineering communities.[32] Jay Pasachoff wrote in 1984 that \"physics and astronomy students simply have to learn Fortran. So much exists in Fortran that it seems unlikely that scientists will change to Pascal, Modula-2, or whatever.\"[33] In 1993, Cecil E. Leith called Fortran the \"mother tongue of scientific computing\" adding that its replacement by any other possible language \"may remain a forlorn hope.\"[34] It is the primary language for some of the most intensive supercomputing tasks, such as astronomy, weather and climate modeling, numerical linear algebra (LAPACK), numerical libraries (IMSL and NAG), structural engineering, hydrological modeling, optimization, satellite simulation and data analysis, computational fluid dynamics, computational chemistry, computational economics and computational physics. Many of the floating-point benchmarks to gauge the performance of new computer processors – such as CFP2006, the floating-point component of the SPEC CPU2006 benchmarks – are written in Fortran.\n\nOn the other hand, more modern code generally uses large program libraries such as PETSc or Trilinos for linear algebra capabilities, METIS for graph partitioning, deal.II or FEniCS for mesh and finite element support, and other generic libraries. Since the late 1990s, almost all of the most widely used support libraries have been written in C and, more often, C++. Consequently, a growing fraction of scientific code is also written in these languages. For this reason, facilities for interoperation with C were added to Fortran 2003, and enhanced by ISO/IEC technical specification 29113, which will be incorporated into Fortran 2015. This shift is also evident in the selection of applications between the SPEC CPU 2000 and SPEC CPU 2006 floating point benchmarks.\n\nLanguage features[edit]\nThe precise characteristics and syntax of Fortran 95 are discussed in Fortran 95 language features.\n\nPortability[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2011) (Learn how and when to remove this template message)\nPortability was a problem in the early days because there was no agreed standard – not even IBM's reference manual – and computer companies vied to differentiate their offerings from others by providing incompatible features. Standards have improved portability. The 1966 standard provided a reference syntax and semantics, but vendors continued to provide incompatible extensions. Although careful programmers were coming to realize that use of incompatible extensions caused expensive portability problems, and were therefore using programs such as The PFORT Verifier, it was not until after the 1977 standard, when the National Bureau of Standards (now NIST) published FIPS PUB 69, that processors purchased by the U.S. Government were required to diagnose extensions of the standard. Rather than offer two processors, essentially every compiler eventually had at least an option to diagnose extensions.\n\nIncompatible extensions were not the only portability problem. For numerical calculations, it is important to take account of the characteristics of the arithmetic. This was addressed by Fox et al. in the context of the 1966 standard by the PORT library. The ideas therein became widely used, and were eventually incorporated into the 1990 standard by way of intrinsic inquiry functions. The widespread (now almost universal) adoption of the IEEE 754 standard for binary floating-point arithmetic has essentially removed this problem.\n\nAccess to the computing environment (e.g., the program's command line, environment variables, textual explanation of error conditions) remained a problem until it was addressed by the 2003 standard.\n\nLarge collections of library software that could be described as being loosely related to engineering and scientific calculations, such as graphics libraries, have been written in C, and therefore access to them presented a portability problem. This has been addressed by incorporation of C interoperability into the 2003 standard.\n\nIt is now possible (and relatively easy) to write an entirely portable program in Fortran, even without recourse to a preprocessor.\n\nVariants[edit]\nFortran 5[edit]\nFortran 5 was marketed by Data General Corp in the late 1970s and early 1980s, for the Nova, Eclipse, and MV line of computers. It had an optimizing compiler that was quite good for minicomputers of its time. The language most closely resembles Fortran 66. The name is a pun on the earlier Fortran IV.\n\nFortran V[edit]\nFortran V was distributed by Control Data Corporation in 1968 for the CDC 6600 series. The language was based upon Fortran IV.[35]\n\nUnivac also offered a compiler for the 1100 series known as Fortran V. A spinoff of Univac Fortran V was Athena Fortran.\n\nFortran 6[edit]\nFortran 6 or Visual Fortran 2001 was licensed to Compaq by Microsoft. They have licensed Compaq Visual Fortran and have provided the Visual Studio 5 environment interface for Compaq v6 up to v6.1.[36]\n\nSpecific variants[edit]\nVendors of high-performance scientific computers (e.g., Burroughs, Control Data Corporation (CDC), Cray, Honeywell, IBM, Texas Instruments, and UNIVAC) added extensions to Fortran to take advantage of special hardware features such as instruction cache, CPU pipelines, and vector arrays. For example, one of IBM's FORTRAN compilers (H Extended IUP) had a level of optimization which reordered the machine code instructions to keep multiple internal arithmetic units busy simultaneously. Another example is CFD, a special variant of Fortran designed specifically for the ILLIAC IV supercomputer, running at NASA's Ames Research Center. IBM Research Labs also developed an extended FORTRAN-based language called VECTRAN for processing vectors and matrices.\n\nObject-Oriented Fortran was an object-oriented extension of Fortran, in which data items can be grouped into objects, which can be instantiated and executed in parallel. It was available for Sun, Iris, iPSC, and nCUBE, but is no longer supported.\n\nSuch machine-specific extensions have either disappeared over time or have had elements incorporated into the main standards. The major remaining extension is OpenMP, which is a cross-platform extension for shared memory programming. One new extension, Coarray Fortran, is intended to support parallel programming.\n\nFOR TRANSIT for the IBM 650[edit]\nFOR TRANSIT was the name of a reduced version of the IBM 704 FORTRAN language, which was implemented for the IBM 650, using a translator program developed at Carnegie in the late 1950s.[37] The following comment appears in the IBM Reference Manual (FOR TRANSIT Automatic Coding System C28-4038, Copyright 1957, 1959 by IBM):\n\nThe FORTRAN system was designed for a more complex machine than the 650, and consequently some of the 32 statements found in the FORTRAN Programmer's Reference Manual are not acceptable to the FOR TRANSIT system. In addition, certain restrictions to the FORTRAN language have been added. However, none of these restrictions make a source program written for FOR TRANSIT incompatible with the FORTRAN system for the 704.\n\nThe permissible statements were:\n\nArithmetic assignment statements, e.g., a = b\nGO to n\nGO TO (n1, n2, ..., nm), i\nIF (a) n1, n2, n3\nPAUSE\nSTOP\nDO n i = m1, m2\nCONTINUE\nEND\nREAD n, list\nPUNCH n, list\nDIMENSION V, V, V, ...\nEQUIVALENCE (a,b,c), (d,c), ...\nUp to ten subroutines could be used in one program.\n\nFOR TRANSIT statements were limited to columns 7 through 56, only. Punched cards were used for input and output on the IBM 650. Three passes were required to translate source code to the \"IT\" language, then to compile the IT statements into SOAP assembly language, and finally to produce the object program, which could then be loaded into the machine to run the program (using punched cards for data input, and outputting results onto punched cards).\n\nTwo versions existed for the 650s with a 2000 word memory drum: FOR TRANSIT I (S) and FOR TRANSIT II, the latter for machines equipped with indexing registers and automatic floating point decimal (bi-quinary) arithmetic. Appendix A of the manual included wiring diagrams for the IBM 533 card reader/punch control panel.\n\nFortran-based languages[edit]\nPrior to FORTRAN 77, a number of preprocessors were commonly used to provide a friendlier language, with the advantage that the preprocessed code could be compiled on any machine with a standard FORTRAN compiler. These preprocessors would typically support structured programming, variable names longer than six characters, additional data types, conditional compilation, and even macro capabilities. Popular preprocessors included FLECS, iftran, MORTRAN, SFtran, S-Fortran, Ratfor, and Ratfiv. Ratfor and Ratfiv, for example, implemented a C-like language, outputting preprocessed code in standard FORTRAN 66. Despite advances in the Fortran language, preprocessors continue to be used for conditional compilation and macro substitution.\n\nOne of the earliest versions of FORTRAN, introduced in the 60's, was popularly used in colleges and universities. Developed, supported, and distributed by the University of Waterloo, WATFOR was based largely on FORTRAN IV. A WATFOR student could submit their batch FORTRAN job and, if there were no syntax errors, the program would move straight to execution. This simplification allowed students to concentrate on their program's syntax and semantics, or execution logic flow, rather than dealing with submission Job Control Language (JCL), the compile/link-edit/execution successive process(es), or other complexities of the mainframe/minicomputer environment. A down side to this simplified environment was that WATFOR was not a good choice for programmers needing the expanded abilities of their host processor(s), e.g., WATFOR typically had very limited access to I/O devices. WATFOR was succeeded by WATFIV and its later versions.\n\nprogram; s=0  i=1,n;  s=s+1;  stop i;  s='s'  Stop\n(line programing)\n\nLRLTRAN was developed at the Lawrence Radiation Laboratory to provide support for vector arithmetic and dynamic storage, among other extensions to support systems programming. The distribution included the LTSS operating system.\n\nThe Fortran-95 Standard includes an optional Part 3 which defines an optional conditional compilation capability. This capability is often referred to as \"CoCo\".\n\nMany Fortran compilers have integrated subsets of the C preprocessor into their systems.\n\nSIMSCRIPT is an application specific Fortran preprocessor for modeling and simulating large discrete systems.\n\nThe F programming language was designed to be a clean subset of Fortran 95 that attempted to remove the redundant, unstructured, and deprecated features of Fortran, such as the EQUIVALENCE statement. F retains the array features added in Fortran 90, and removes control statements that were made obsolete by structured programming constructs added to both Fortran 77 and Fortran 90. F is described by its creators as \"a compiled, structured, array programming language especially well suited to education and scientific computing.\"[38]\n\nLahey and Fujitsu teamed up to create Fortran for the Microsoft .NET Framework.[39] Silverfrost FTN95 is also capable of creating .NET code.[40]\n\nCode examples[edit]\nFor more details on this topic, see Wikibooks:Fortran/Fortran examples.\nThe following program illustrates dynamic memory allocation and array-based operations, two features introduced with Fortran 90. Particularly noteworthy is the absence of DO loops and IF/THEN statements in manipulating the array; mathematical operations are applied to the array as a whole. Also apparent is the use of descriptive variable names and general code formatting that conform with contemporary programming style. This example computes an average over data entered interactively.\n\nprogram average\n\n  ! Read in some numbers and take the average\n  ! As written, if there are no data points, an average of zero is returned\n  ! While this may not be desired behavior, it keeps this example simple\n\n  implicit none\n\n  real, dimension(:), allocatable :: points\n  integer                         :: number_of_points\n  real                            :: average_points=0., positive_average=0., negative_average=0.\n\n  write (*,*) \"Input number of points to average:\"\n  read  (*,*) number_of_points\n\n  allocate (points(number_of_points))\n\n  write (*,*) \"Enter the points to average:\"\n  read  (*,*) points\n\n  ! Take the average by summing points and dividing by number_of_points\n  if (number_of_points > 0) average_points = sum(points) / number_of_points\n\n  ! Now form average over positive and negative points only\n  if (count(points > 0.) > 0) then\n     positive_average = sum(points, points > 0.) / count(points > 0.)\n  end if\n\n  if (count(points < 0.) > 0) then\n     negative_average = sum(points, points < 0.) / count(points < 0.)\n  end if\n\n  deallocate (points)\n\n  ! Print result to terminal\n  write (*,'(a,g12.4)') 'Average = ', average_points\n  write (*,'(a,g12.4)') 'Average of positive points = ', positive_average\n  write (*,'(a,g12.4)') 'Average of negative points = ', negative_average\n\nend program average\nHumor[edit]\nDuring the same Fortran standards committee meeting at which the name \"FORTRAN 77\" was chosen, a satirical technical proposal was incorporated into the official distribution bearing the title \"Letter O Considered Harmful\". This proposal purported to address the confusion that sometimes arises between the letter \"O\" and the numeral zero, by eliminating the letter from allowable variable names. However, the method proposed was to eliminate the letter from the character set entirely (thereby retaining 48 as the number of lexical characters, which the colon had increased to 49). This was considered beneficial in that it would promote structured programming, by making it impossible to use the notorious GO TO statement as before. (Troublesome FORMAT statements would also be eliminated.) It was noted that this \"might invalidate some existing programs\" but that most of these \"probably were non-conforming, anyway\".[41][42]\n\nDuring the standards committee battle over whether the \"minimum trip count\" for the FORTRAN 77 DO statement should be zero (allowing no execution of the block) or one (the \"plunge-ahead\" DO), another facetious alternative was proposed (by Loren Meissner) to have the minimum be two – since there is no need for a loop if it is only executed once.[citation needed]\n\nWhen assumed-length arrays were being added, there was a dispute as to the appropriate character to separate upper and lower bounds. In a comment examining these arguments, Dr. Walt Brainerd penned an article entitled \"Astronomy vs. Gastroenterology\" because some proponents had suggested using the star or asterisk (\"*\"), while others favored the colon (\":\").[citation needed]\n\nIn Fortran 77, variable names beginning with the letters I–N had a default type of integer, while variables starting with any other letters defaulted to real, although programmers could override the defaults with an explicit declaration.[43] This led to the joke: \"In Fortran, GOD is REAL (unless declared INTEGER).\"\n\nSee also[edit]\nicon\tComputer programming portal\nf2c\nList of Fortran compilers\nList of Fortran numerical libraries\nList of programming languages\nMatrix representation\nRow-major order",
          "type": "compiled",
          "plid": 7
        },
        {
          "name": "Apache Ant",
          "details": "Apache Ant is a software tool for automating software build processes, which originated from the Apache Tomcat project in early 2000. It was a replacement for the unix make build tool, and was created due to a number of problems with the unix make.[1] It is similar to Make but is implemented using the Java language, requires the Java platform, and is best suited to building Java projects.\n\nThe most immediately noticeable difference between Ant and Make is that Ant uses XML to describe the build process and its dependencies, whereas Make uses Makefile format. By default the XML file is named build.xml.\n\nAnt is an Apache project. It is open source software, and is released under the Apache License.\n\nContents  [hide] \n1\tHistory\n2\tExtensions\n3\tExample\n4\tPortability\n5\tLimitations\n6\tSee also\n7\tReferences\n8\tBibliography\n9\tExternal links\nHistory[edit]\nAnt (\"Another Neat Tool\"[2]) was conceived by James Duncan Davidson while preparing Sun's reference JSP/Servlet engine, later Apache Tomcat, for release as open source. A proprietary version of make was used to build it on Solaris, but in the open source world there was no way of controlling which platform was used to build Tomcat; so Ant was created as a simple platform-independent tool to build Tomcat from directives in an XML \"build file\". Ant (version 1.1) was officially released as a stand-alone product on July 19, 2000.\n\nSeveral proposals for an Ant version 2 have been made, such as AntEater by James Duncan Davidson, Myrmidon by Peter Donald and Mutant by Conor MacNeill, none of which were able to find large acceptance with the developer community.[3]\n\nAt one time (2002), Ant was the build tool used by most Java development projects.[4] For example, most open source Java developers include build.xml files with their distribution.[citation needed]\n\nBecause Ant made it trivial to integrate JUnit tests with the build process, Ant made it easy for willing developers to adopt test-driven development, and even Extreme Programming.\n\nExtensions[edit]\nWOProject-Ant[5] is just one of many examples of a task extension written for Ant. These extensions are put to use by copying their jar files into ant's lib directory. Once this is done, these extension tasks can be invoked directly in the typical build.xml file. The WOProject extensions allow WebObjects developers to use ant in building their frameworks and applications, instead of using Apple's Xcode suite.\n\nAntcontrib[6] provides a collection of tasks such as conditional statements and operations on properties as well as other useful tasks.[7]\n\nAnt-contrib.unkrig.de[8] implements tasks and types for networking, Swing user interfaces, JSON processing and other.\n\nOther task extensions exist for Perforce, .Net, EJB, and filesystem manipulations, just to name a few.[9]\n\nExample[edit]\nBelow is listed a sample build.xml file for a simple Java \"Hello, world\" application. It defines four targets - clean , clobber , compile and jar , each of which has an associated description. The jar target lists the compile target as a dependency. This tells Ant that before it can start the jar target it must first complete the compile target.\n\n<?xml version=\"1.0\"?>\n<project name=\"Hello\" default=\"compile\">\n    <target name=\"clean\" description=\"remove intermediate files\">\n        <delete dir=\"classes\"/>\n    </target>\n    <target name=\"clobber\" depends=\"clean\" description=\"remove all artifact files\">\n        <delete file=\"hello.jar\"/>\n    </target>\n    <target name=\"compile\" description=\"compile the Java source code to class files\">\n        <mkdir dir=\"classes\"/>\n        <javac srcdir=\".\" destdir=\"classes\"/>\n    </target>\n    <target name=\"jar\" depends=\"compile\" description=\"create a Jar file for the application\">\n        <jar destfile=\"hello.jar\">\n            <fileset dir=\"classes\" includes=\"**/*.class\"/>\n            <manifest>\n                <attribute name=\"Main-Class\" value=\"HelloProgram\"/>\n            </manifest>\n        </jar>\n    </target>\n</project>\nWithin each target are the actions that Ant must take to build that target; these are performed using built-in tasks. For example, to build the compile target Ant must first create a directory called classes (which Ant will do only if it does not already exist) and then invoke the Java compiler. Therefore, the tasks used are mkdir and javac . These perform a similar task to the command-line utilities of the same name.\n\nAnother task used in this example is named jar :\n\n<jar destfile=\"hello.jar\">\nThis Ant task has the same name as the common Java command-line utility, JAR, but is really a call to the Ant program's built-in JAR/ZIP file support. This detail is not relevant to most end users, who just get the JAR they wanted, with the files they asked for.\n\nMany Ant tasks delegate their work to external programs, either native or Java. They use Ant's own <exec> and <java> tasks to set up the command lines, and handle all the details of mapping from information in the build file to the program's arguments and interpreting the return value. Users can see which tasks do this (e.g. <cvs> , <signjar> , <chmod> , <rpm> ), by trying to execute the task on a system without the underlying program on the path, or without a full Java Development Kit (JDK) installed.\n\nPortability[edit]\nOne of the primary aims of Ant was to solve Make's portability problems. The first portability issue in a Makefile is that the actions required to create a target are specified as shell commands which are specific to the platform on which Make runs. Different platforms require different shell commands. Ant solves this problem by providing a large amount of built-in functionality that is designed to behave the same on all platforms. For example, in the sample build.xml file above, the clean target deletes the classes directory and everything in it. In a Makefile this would typically be done with the command:\n\nrm -rf classes/\nrm is a Unix-specific command unavailable in some other environments. Microsoft Windows, for example, would use:\n\nrmdir /S /Q classes\nIn an Ant build file the same goal would be accomplished using a built-in command:\n\n <delete dir=\"classes\"/>\nA second portability issue is a result of the fact that the symbol used to delimit elements of file system directory path components differs from one platform to another. Unix uses a forward slash (/) to delimit components whereas Windows uses a backslash (\\). Ant build files let authors choose their favorite convention: forward slash or backslash for directories; semicolon or colon for path separators. It converts each to the symbol appropriate to the platform on which it executes.\n\nLimitations[edit]\nUnbalanced scales.svg\nThis article's Criticism or Controversy section may compromise the article's neutral point of view of the subject. Please integrate the section's contents into the article as a whole, or rewrite the material. (September 2011)\n\nThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (September 2011) (Learn how and when to remove this template message)\nAnt build files, which are written in XML, can be complex and verbose. The complex structure (hierarchical, partly ordered, and pervasively cross-linked) of Ant documents can be a barrier to learning. The build files of large or complex projects can become unmanageably large. Good design and modularization of build files can improve readability but not necessarily reduce size. Other build tools, such as Gradle or Maven, use more concise scripts at the expense of generality and flexibility.\nMany of the older tasks—the core ones that are used every day, such as <javac>, <exec> and <java>—use default values for options that are not consistent with more recent versions of the tasks. Changing those defaults would break existing Ant scripts.\nWhen expanding properties in a string or text element, undefined properties are not raised as an error, but left as an unexpanded reference (e.g. ${unassigned.property}).\nAnt has limited fault handling rules, and no persistence of state, so it cannot be used as a workflow tool for any workflow other than classic build and test processes.\nLazy property evaluation is not supported. For instance, when working within an Antcontrib <for> loop, a property cannot be re-evaluated for a sub-value which may be part of the iteration. (Some third-party extensions facilitate a workaround; AntXtras flow-control tasksets do provide for cursor redefinition for loops.)\nIn makefiles, any rule to create one file type from another can be written inline within the makefile. For example, you may transform a document into some other format by using rules to execute another tool. Creating a similar task in Ant is more complex: a separate task must be written in Java and included with the Ant build file in order to handle the same type of functionality. However, this separation can enhance the readability of the Ant script by hiding some of the details of how a task is executed on different platforms.\nThere exists a myriad of third-party Ant extensions (called antlibs) that provide much of the missing functionality. Also, the Eclipse IDE can build and execute Ant scripts, while the NetBeans IDE uses Ant for its internal build system. As both these IDEs are very popular development platforms, they can simplify Ant use significantly. (As a bonus, Ant scripts generated by NetBeans can be used outside that IDE as standalone scripts.)",
          "type": "compiled",
          "plid": 8
        },
        {
          "name": "Distributed Application Specification Language",
          "details": "The DASL Programming Language (Distributed Application Specification Language) is a high-level, strongly typed programming language originally developed at Sun Microsystems Laboratories between 1999 and 2003 as part of the Ace Project. The goals of the project were to enable rapid development of web-based applications based on Sun's J2EE architecture, and to eliminate the steep learning curve of platform-specific details.\n\nDASL defines an application as a domain model with one or more logical presentation models, where a logical presentation model consists of a choreography of the domain model objects described in a set of forms with attached actions. DASL generates the graphical user interface directly from the logical presentation.\n\nDASL is unique among modern application programming languages in its ability to generate a modern graphic user interface for an application without requiring the programmer to define the user interface explicitly, while allowing the programmer to control the look and feel of the generated graphic user interface.\n\nThe DASL language is partially declarative and partially procedural. Description of object/data structures and persistence, and the description of the logical presentation, are declarative. Basic object constraints and behavior are declarative, while additional object behaviors are specified procedurally as methods. Queries can be defined either declaratively or by writing methods.\n\nThe language and development environment are a practical realization of the model-driven architecture (MDA) approach. The programmer uses DASL to produce the platform-independent model or PIM, and the language code generators automatically produce and deploy the platform-specific model or PSM. New PSMs may be introduced by writing new code generators.\n\nContents  [hide] \n1\tBenefits of the approach\n2\tLanguage traits\n3\tFull language description\n4\tInside Sun Microsystems\n5\tOutside Sun Microsystems\n6\tContinued Development outside of Sun Microsystems\nBenefits of the approach[edit]\nA key benefit of the DASL language approach over 3rd generation (3GL) programming languages is that enterprise applications can be specified in a very concise and precise way that expresses the application logic clearly. A small enterprise application in DASL can typically be implemented in 8-10K lines of DASL code, which the DASL compiler then typically translates into 200K lines of Java, XML, SQL, and other implementation artifacts. The 200K line figure is typical of equivalent applications written using 3GLs.\n\nThe conciseness of DASL can be seen also in terms of the content of the two representations (DASL vs. the generated application code in Java/XML/SQL etc.). Most of the DASL code describes business logic and business processes specific to the application, independent of the deployment middleware, frameworks, and presentation mechanisms. This core business logic typically represents only 2-5% of the generated application code. Thus, writing, understanding, and maintaining the application code is much easier at the DASL level than it is at the level of the generated code, in which the business logic is scattered within various implementation artifacts.\n\nAnother advantage of using DASL to write applications, rather than conventional 3rd generation languages and IDEs is that the DASL code is independent of middleware, GUI presentation frameworks, network topology, and other implementation technologies. As new middleware and frameworks are developed and evolve, existing DASL programs can be migrated to them without the need to re-implement them.\n\nFor example, the original DASL code generators produced traditional HTML screens. Later, DASL code generators were written to use frameworks such as Apache Struts, and technologies such as JavaScript in the browser, to produce a more interactive experience. Today, new DASL generators being written that produce Rich Internet applications. Existing DASL applications can thus be converted to rich internet applications by recompiling them with the latest code generators.\n\nLanguage traits[edit]\nDASL combines a declarative syntax with a Java-like procedural syntax. The declarative part of the language enables defining applications at a higher level of abstraction than 3rd generation languages such as Java. In DASL, the programmer does not describe inter-process communication between client processes, web servers, application servers, databases, or details of the user interface of the desired application. Rather, the programmer describes the application as a set of related domain objects (including their behavior), and as a set of forms and actions annotated with basic layout properties.\n\nIn contrast to highly specialized DSLs, DASL is Turing-complete. The behavior of domain objects can be expressed using a combination of declarative and procedural syntax. For example, constraints on objects and object attributes are expressed declaratively, but the constraint itself can be defined either as a declarative expression or procedurally.\n\nA DASL application has two primary components: A business object specification (BOS) that describes the object domain model, consisting of persistent and transient objects representing the domain of the application, and an application usage specification (AUS) that describes the actions or use cases that may be performed on the domain model. The AUS is essentially the choreography of the domain objects into a series of forms and actions.\n\nThe DASL programmer models the graphical user interface of the application by annotating the logical AUS forms and actions with properties that describe the basic layout of the data on the page. Rather than user interface considerations dominating the application specification, in DASL the logical interaction between the user and domain model is central, and the user interface is derived from the logical interaction. It is believed that DASL is unique among other languages in this respect.\n\nFull language description[edit]\nThe DASL language is described in a published Sun Labs technical report called The DASL Language: Programmer's Guide and Reference Manual. This technical report is also available as part of the ACM Digital Library as http://dl.acm.org/citation.cfm?id=1698172&dl=ACM&coll=DL&CFID=800729170&CFTOKEN=37871889\n\nInside Sun Microsystems[edit]\nAround 1999, two Sun researchers, Bruce Daniels and Bob Goldberg, started a research project in Sun Labs called the 'Ace Project', with the goal of simplifying the creation of Java web-based enterprise applications. The Ace language, now known as DASL, was developed by Goldberg, Daniels, and several other colleagues as part of this project.\n\nThe Ace project and language were featured in an article that appeared in June, 2002 on Sun's website, as well as in the January 2003 edition of Computing Research News entitled Sun Microsystems Laboratories: License to Innovate.\n\n'Project Ace', the Ace DASL development environment, was demonstrated by Bruce Daniels as part of James Gosling's keynote address at the JavaONE conference in March, 2002.\n\nOn the business side of Sun Microsystems, the DASL language was used to implement the public interface to the Sun Grid Compute Utility, known as the GridPortal.\n\nOutside Sun Microsystems[edit]\nAlthough Sun Microsystems did not release a commercial implementation of the DASL language, it has in the past made the technology available to selected partners and conducted trials using the technology. DASL is referenced on the Association for Computing Machinery Portal, including a paper presented at the OOPSLA 2004 Conference, in papers on Model-Driven Software Development, and on the Sun/Oracle website describing Project Ace. A demonstration of Ace DASL was given as part of James Gosling's keynote address at the 2004 Java One Conference.\n\nContinued Development outside of Sun Microsystems[edit]\nResearch and development on the DASL language continued at RD3 Software between 2006 and 2010. The RD3 language enhancements and code generators for DASL have made the language extensible and have broadened the scope of the language from dynamic HTTP-style web applications to rich internet applications. In addition to producing HTML front ends, DASL code generators were created to produce Adobe Flex (Flash) front ends. With modest resources, HTML5 could easily be generated, as well as iPhone- and Android-specific implementations.\n\nThe language has been extended so it can define dynamic websites and domain-specific web portals that include cloud applications. For example, the RD3 website was implemented a DASL \"program\".\n\nThe RD3 DASL language supports the concept of forms and nested forms as part of the logical presentation. Information and actions can be logically grouped, and the user interface can be controlled using annotations. Presentation and navigation of recursive relationships, such as organization charts and family trees, has been added to the language.\n\nThe DASL language has been made extensible. A third party programmer can write a code generator plugin in Java targeted to a specific DASL object class or form, allowing that object or form to be presented using a custom widget. Customized widgets may do computation within the user's browser, so the application presentation produced by DASL is now Turing complete.\n\nDASL is unique among modern application programming languages in its ability to generate a modern graphic user interface for an application without requiring the programmer to define the user interface explicitly. Using simple annotations, the programmer can modify the user interface that is produced by DASL. Using code generator plugins, the programmer can extend DASL to generate a specific desired user interface.",
          "type": "unknown",
          "plid": 9
        },
        {
          "name": "Mercury ",
          "details": "Mercury is a functional logic programming language geared towards real-world applications. It was initially developed at the University of Melbourne Computer Science department under the supervision of Zoltan Somogyi. The first version was developed by Fergus Henderson, Thomas Conway and Zoltan Somogyi and was released on April 8, 1995.\n\nMercury is a purely declarative logic language. It is related to both Prolog and Haskell.[1] It features a strong, static, polymorphic type system, as well as a strong mode and determinism system.\n\nThe official implementation, the Melbourne Mercury Compiler, is available for most Unix platforms, including Mac OS X, as well as for Microsoft Windows.\n\nContents  [hide] \n1\tOverview\n2\tBack-ends\n3\tExamples\n4\tRelease schedule\n5\tSee also\n6\tReferences\n7\tExternal links\nOverview[edit]\nMercury is based on the logic programming language Prolog. It has the same syntax, and the same basic concepts such as the SLD resolution algorithm. It can be viewed as a pure subset of Prolog with strong types and modes. As such, it is often compared to its predecessor, both in terms of features, and run-time efficiency.\n\nThe language is designed with software engineering principles in mind. Unlike the original implementations of Prolog, it has a separate compilation phase, rather than being directly interpreted, which allows a much wider range of errors to be caught before running a program. It features a strict static type and mode system[1] and a module system.\n\nDue to the use of information obtained at compile time (such as type and mode information), programs written in Mercury typically perform significantly faster than equivalent programs written in Prolog.[2][3] Its authors claim that Mercury is the fastest logic language in the world, by a wide margin.[1]\n\nMercury is a purely declarative language, unlike Prolog, since it lacks \"extra-logical\" Prolog statements such as \"cut\" and imperative I/O. This enables advanced static analysis and program optimization, including compile-time garbage collection,[4] but can make certain programming constructs (such as a switch over a number of options, with a default[dubious – discuss]) harder to express. (Note that while Mercury does allow impure functionality, this serves primarily as a way of calling foreign language code. All impure code must be explicitly marked.) Operations which would typically be impure (such as input/output) are expressed using pure constructs in Mercury using linear types, by threading a dummy \"world\" value through all relevant code.\n\nNotable programs written in Mercury include the Mercury compiler itself and the Prince XML formatter. Mission Critical IT [1], a software company, has also been using Mercury since 2000 to develop enterprise applications and its Ontology-Driven software development platform ODASE.\n\nBack-ends[edit]\nMercury has several back-ends, which means it is possible to compile Mercury code into the following languages:\n\nProduction level:\n\nAssembly via the GCC back-end\nHigh-level C\nJava\nC#\nErlang\nAlpha quality (may not work well, or even be completely broken):\n\nLow-level C for GCC (the original Mercury back-end)\nPast back-ends:\n\nAditi, a deductive database system also developed at the University of Melbourne. Mercury-0.12.2 is the last version of Mercury that supports Aditi.[citation needed]\nIL for Microsoft's .NET\nMercury also features a foreign language interface, allowing code in other languages (depending on the chosen back-end) to be linked with Mercury code. The following foreign languages are possible:\n\nBack-end\tForeign language(s)\nC (both levels) and ASM\tC\nJava\tJava\nErlang\tErlang\nIL\tIL or C#\nOther languages can then be interfaced to by calling them from these languages. However, this means that foreign language code may need to be written several times for the different backends, otherwise portability between backends will be lost.\n\nThe most commonly used back-end is the original low-level C back-end.\n\nExamples[edit]\nHello World:\n\n :- module hello.\n :- interface.\n :- import_module io.\n :- pred main(io::di, io::uo) is det.\n\n :- implementation.\n main(!IO) :-\n \tio.write_string(\"Hello, World!\\n\", !IO).\nCalculating the 10th Fibonacci number (in the most obvious way):[5]\n\n :- module fib.\n :- interface.\n :- import_module io.\n :- pred main(io::di, io::uo) is det.\n \n :- implementation.\n :- import_module int.\n\n :- func fib(int) = int.\n fib(N) = (if N =< 2 then 1 else fib(N - 1) + fib(N - 2)).\n\n main(!IO) :-\n        io.write_string(\"fib(10) = \", !IO),\n        io.write_int(fib(10), !IO),\n        io.nl(!IO).\n        % Could instead use io.format(\"fib(10) = %d\\n\", [i(fib(10))], !IO).\nRelease schedule[edit]\nReleases are named according to the year and month of the release. The current stable release is 14.01.1 (September 2014). Previously releases were numbered 0.12, 0.13, etc. and the period between stable releases can be very large (3 years).\n\nThere is also a snapshot release consisting of the latest features and bug fixes added to the last stable release.",
          "type": "compiled",
          "plid": 10
        },
        {
          "name": "Prolog",
          "details": "Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.[1][2][3]\n\nProlog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is declarative: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations.[4]\n\nThe language was first conceived by a group around Alain Colmerauer in Marseille, France, in the early 1970s and the first Prolog system was developed in 1972 by Colmerauer with Philippe Roussel.[5][6]\n\nProlog was one of the first logic programming languages,[7] and remains the most popular among such languages today, with several free and commercial implementations available. The language has been used for theorem proving,[8] expert systems,[9] as well as its original intended field of use, natural language processing.[10][11] Modern Prolog environments support creating graphical user interfaces, as well as administrative and networked applications.\n\nProlog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.\n\nContents  [hide] \n1\tSyntax and semantics\n1.1\tData types\n1.2\tRules and facts\n1.3\tExecution\n1.4\tLoops and recursion\n1.5\tNegation\n2\tProgramming in Prolog\n2.1\tHello world\n2.2\tCompiler optimization\n2.3\tQuicksort\n3\tDesign patterns\n4\tHigher-order programming\n5\tModules\n6\tParsing\n7\tMeta-interpreters and reflection\n8\tTuring completeness\n9\tImplementation\n9.1\tISO Prolog\n9.2\tCompilation\n9.3\tTail recursion\n9.4\tTerm indexing\n9.5\tHashing\n9.6\tTabling\n9.7\tImplementation in hardware\n10\tLimitations\n11\tExtensions\n11.1\tTypes\n11.2\tModes\n11.3\tConstraints\n11.4\tObject-orientation\n11.5\tGraphics\n11.6\tConcurrency\n11.7\tWeb programming\n11.8\tAdobe Flash\n11.9\tOther\n12\tInterfaces to other languages\n13\tHistory\n14\tUse in industry\n15\tSee also\n15.1\tRelated languages\n16\tReferences\n17\tFurther reading\n18\tExternal links\nSyntax and semantics[edit]\nMain article: Prolog syntax and semantics\nIn Prolog, program logic is expressed in terms of relations, and a computation is initiated by running a query over these relations. Relations and queries are constructed using Prolog's single data type, the term.[4] Relations are defined by clauses. Given a query, the Prolog engine attempts to find a resolution refutation of the negated query. If the negated query can be refuted, i.e., an instantiation for all free variables is found that makes the union of clauses and the singleton set consisting of the negated query false, it follows that the original query, with the found instantiation applied, is a logical consequence of the program. This makes Prolog (and other logic programming languages) particularly useful for database, symbolic mathematics, and language parsing applications. Because Prolog allows impure predicates, checking the truth value of certain special predicates may have some deliberate side effect, such as printing a value to the screen. Because of this, the programmer is permitted to use some amount of conventional imperative programming when the logical paradigm is inconvenient. It has a purely logical subset, called \"pure Prolog\", as well as a number of extralogical features.\n\nData types[edit]\nProlog's single data type is the term. Terms are either atoms, numbers, variables or compound terms.\n\nAn atom is a general-purpose name with no inherent meaning. Examples of atoms include x, red, 'Taco', and 'some atom'.\nNumbers can be floats or integers.\nVariables are denoted by a string consisting of letters, numbers and underscore characters, and beginning with an upper-case letter or underscore. Variables closely resemble variables in logic in that they are placeholders for arbitrary terms.\nA compound term is composed of an atom called a \"functor\" and a number of \"arguments\", which are again terms. Compound terms are ordinarily written as a functor followed by a comma-separated list of argument terms, which is contained in parentheses. The number of arguments is called the term's arity. An atom can be regarded as a compound term with arity zero. Examples of compound terms are truck_year('Mazda', 1986) and 'Person_Friends'(zelda,[tom,jim]).\nSpecial cases of compound terms:\n\nA List is an ordered collection of terms. It is denoted by square brackets with the terms separated by commas or in the case of the empty list, []. For example, [1,2,3] or [red,green,blue].\nStrings: A sequence of characters surrounded by quotes is equivalent to a list of (numeric) character codes, generally in the local character encoding, or Unicode if the system supports Unicode. For example, \"to be, or not to be\".\nRules and facts[edit]\nProlog programs describe relations, defined by means of clauses. Pure Prolog is restricted to Horn clauses. There are two types of clauses: facts and rules. A rule is of the form\n\nHead :- Body.\nand is read as \"Head is true if Body is true\". A rule's body consists of calls to predicates, which are called the rule's goals. The built-in predicate ,/2 (meaning a 2-arity operator with name ,) denotes conjunction of goals, and ;/2 denotes disjunction. Conjunctions and disjunctions can only appear in the body, not in the head of a rule.\n\nClauses with empty bodies are called facts. An example of a fact is:\n\ncat(tom).\nwhich is equivalent to the rule:\n\ncat(tom) :- true.\nThe built-in predicate true/0 is always true.\n\nGiven the above fact, one can ask:\n\nis tom a cat?\n\n ?- cat(tom).\n Yes\nwhat things are cats?\n\n ?- cat(X).\n X = tom\nClauses with bodies are called rules. An example of a rule is:\n\nanimal(X) :- cat(X).\nIf we add that rule and ask what things are animals?\n\n ?- animal(X).\n X = tom\nDue to the relational nature of many built-in predicates, they can typically be used in several directions. For example, length/2 can be used to determine the length of a list (length(List, L), given a list List) as well as to generate a list skeleton of a given length (length(X, 5)), and also to generate both list skeletons and their lengths together (length(X, L)). Similarly, append/3 can be used both to append two lists (append(ListA, ListB, X) given lists ListA and ListB) as well as to split a given list into parts (append(X, Y, List), given a list List). For this reason, a comparatively small set of library predicates suffices for many Prolog programs.\n\nAs a general purpose language, Prolog also provides various built-in predicates to perform routine activities like input/output, using graphics and otherwise communicating with the operating system. These predicates are not given a relational meaning and are only useful for the side-effects they exhibit on the system. For example, the predicate write/1 displays a term on the screen.\n\nExecution[edit]\nExecution of a Prolog program is initiated by the user's posting of a single goal, called the query. Logically, the Prolog engine tries to find a resolution refutation of the negated query. The resolution method used by Prolog is called SLD resolution. If the negated query can be refuted, it follows that the query, with the appropriate variable bindings in place, is a logical consequence of the program. In that case, all generated variable bindings are reported to the user, and the query is said to have succeeded. Operationally, Prolog's execution strategy can be thought of as a generalization of function calls in other languages, one difference being that multiple clause heads can match a given call. In that case, the system creates a choice-point, unifies the goal with the clause head of the first alternative, and continues with the goals of that first alternative. If any goal fails in the course of executing the program, all variable bindings that were made since the most recent choice-point was created are undone, and execution continues with the next alternative of that choice-point. This execution strategy is called chronological backtracking. For example:\n\nmother_child(trude, sally).\n \nfather_child(tom, sally).\nfather_child(tom, erica).\nfather_child(mike, tom).\n \nsibling(X, Y)      :- parent_child(Z, X), parent_child(Z, Y).\n \nparent_child(X, Y) :- father_child(X, Y).\nparent_child(X, Y) :- mother_child(X, Y).\nThis results in the following query being evaluated as true:\n\n ?- sibling(sally, erica).\n Yes\nThis is obtained as follows: Initially, the only matching clause-head for the query sibling(sally, erica) is the first one, so proving the query is equivalent to proving the body of that clause with the appropriate variable bindings in place, i.e., the conjunction (parent_child(Z,sally), parent_child(Z,erica)). The next goal to be proved is the leftmost one of this conjunction, i.e., parent_child(Z, sally). Two clause heads match this goal. The system creates a choice-point and tries the first alternative, whose body is father_child(Z, sally). This goal can be proved using the fact father_child(tom, sally), so the binding Z = tom is generated, and the next goal to be proved is the second part of the above conjunction: parent_child(tom, erica). Again, this can be proved by the corresponding fact. Since all goals could be proved, the query succeeds. Since the query contained no variables, no bindings are reported to the user. A query with variables, like:\n\n?- father_child(Father, Child).\nenumerates all valid answers on backtracking.\n\nNotice that with the code as stated above, the query ?- sibling(sally, sally). also succeeds. One would insert additional goals to describe the relevant restrictions, if desired.\n\nLoops and recursion[edit]\nIterative algorithms can be implemented by means of recursive predicates.\n\nNegation[edit]\nThe built-in Prolog predicate \\+/1 provides negation as failure, which allows for non-monotonic reasoning. The goal \\+ illegal(X) in the rule\n\nlegal(X) :- \\+ illegal(X).\nis evaluated as follows: Prolog attempts to prove illegal(X). If a proof for that goal can be found, the original goal (i.e., \\+ illegal(X)) fails. If no proof can be found, the original goal succeeds. Therefore, the \\+/1 prefix operator is called the \"not provable\" operator, since the query ?- \\+ Goal. succeeds if Goal is not provable. This kind of negation is sound if its argument is \"ground\" (i.e. contains no variables). Soundness is lost if the argument contains variables and the proof procedure is complete. In particular, the query ?- legal(X). can now not be used to enumerate all things that are legal.\n\nProgramming in Prolog[edit]\nIn Prolog, loading code is referred to as consulting. Prolog can be used interactively by entering queries at the Prolog prompt ?-. If there is no solution, Prolog writes no. If a solution exists then it is printed. If there are multiple solutions to the query, then these can be requested by entering a semi-colon ;. There are guidelines on good programming practice to improve code efficiency, readability and maintainability.[12]\n\nHere follow some example programs written in Prolog.\n\nHello world[edit]\nAn example of a query:\n\n?- write('Hello world!'), nl.\nHello world!\ntrue.\n\n?-\nCompiler optimization[edit]\nAny computation can be expressed declaratively as a sequence of state transitions. As an example, an optimizing compiler with three optimization passes could be implemented as a relation between an initial program and its optimized form:\n\nprogram_optimized(Prog0, Prog) :-\n    optimization_pass_1(Prog0, Prog1),\n    optimization_pass_2(Prog1, Prog2),\n    optimization_pass_3(Prog2, Prog).\nor equivalently using DCG notation:\n\nprogram_optimized --> optimization_pass_1, optimization_pass_2, optimization_pass_3.\nQuicksort[edit]\nThe quicksort sorting algorithm, relating a list to its sorted version:\n\npartition([], _, [], []).\npartition([X|Xs], Pivot, Smalls, Bigs) :-\n    (   X @< Pivot ->\n        Smalls = [X|Rest],\n        partition(Xs, Pivot, Rest, Bigs)\n    ;   Bigs = [X|Rest],\n        partition(Xs, Pivot, Smalls, Rest)\n    ).\n \nquicksort([])     --> [].\nquicksort([X|Xs]) -->\n    { partition(Xs, X, Smaller, Bigger) },\n    quicksort(Smaller), [X], quicksort(Bigger).\nDesign patterns[edit]\nA design pattern is a general reusable solution to a commonly occurring problem in software design. In Prolog, design patterns go under various names: skeletons and techniques,[13][14] cliches,[15] program schemata,[16] and logic description schemata.[17] An alternative to design patterns is higher order programming.[18]\n\nHigher-order programming[edit]\nMain articles: Higher-order logic and Higher-order programming\nA higher-order predicate is a predicate that takes one or more other predicates as arguments. Although support for higher-order programming takes Prolog outside the domain of first-order logic, which does not allow quantification over predicates,[19] ISO Prolog now has some built-in higher-order predicates such as call/1, call/2, call/3, findall/3, setof/3, and bagof/3.[20] Furthermore, since arbitrary Prolog goals can be constructed and evaluated at run-time, it is easy to write higher-order predicates like maplist/2, which applies an arbitrary predicate to each member of a given list, and sublist/3, which filters elements that satisfy a given predicate, also allowing for currying.[18]\n\nTo convert solutions from temporal representation (answer substitutions on backtracking) to spatial representation (terms), Prolog has various all-solutions predicates that collect all answer substitutions of a given query in a list. This can be used for list comprehension. For example, perfect numbers equal the sum of their proper divisors:\n\n perfect(N) :-\n     between(1, inf, N), U is N // 2,\n     findall(D, (between(1,U,D), N mod D =:= 0), Ds),\n     sumlist(Ds, N).\nThis can be used to enumerate perfect numbers, and also to check whether a number is perfect.\n\nAs another example, the predicate maplist applies a predicate P to all corresponding positions in a pair of lists:\n\nmaplist(_, [], []).\nmaplist(P, [X|Xs], [Y|Ys]) :-\n   call(P, X, Y),\n   maplist(P, Xs, Ys).\nWhen P is a predicate that for all X, P(X,Y) unifies Y with a single unique value, maplist(P, Xs, Ys) is equivalent to applying the map function in functional programming as Ys = map(Function, Xs).\n\nHigher-order programming style in Prolog was pioneered in HiLog and λProlog.\n\nModules[edit]\nFor programming in the large, Prolog provides a module system. The module system is standardised by ISO.[21] However, not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers.[22] Consequently, modules written on one Prolog compiler will not necessarily work on others.\n\nParsing[edit]\nMain articles: Prolog syntax and semantics § Definite clause grammars, and Definite clause grammar\nThere is a special notation called definite clause grammars (DCGs). A rule defined via -->/2 instead of :-/2 is expanded by the preprocessor (expand_term/2, a facility analogous to macros in other languages) according to a few straightforward rewriting rules, resulting in ordinary Prolog clauses. Most notably, the rewriting equips the predicate with two additional arguments, which can be used to implicitly thread state around,[clarification needed] analogous to monads in other languages. DCGs are often used to write parsers or list generators, as they also provide a convenient interface to difference lists.\n\nMeta-interpreters and reflection[edit]\nProlog is a homoiconic language and provides many facilities for reflection. Its implicit execution strategy makes it possible to write a concise meta-circular evaluator (also called meta-interpreter) for pure Prolog code:\n\nsolve(true).\nsolve((Subgoal1,Subgoal2)) :- \n    solve(Subgoal1),\n    solve(Subgoal2).\nsolve(Head) :- \n    clause(Head, Body),\n    solve(Body).\nwhere true represents an empty conjunction, and clause(Head, Body) unifies with clauses in the database of the form Head :- Body.\n\nSince Prolog programs are themselves sequences of Prolog terms (:-/2 is an infix operator) that are easily read and inspected using built-in mechanisms (like read/1), it is possible to write customized interpreters that augment Prolog with domain-specific features. For example, Sterling and Shapiro present a meta-interpreter that performs reasoning with uncertainty, reproduced here with slight modifications:[23]:330\n\nsolve(true, 1) :- !.\nsolve((Subgoal1,Subgoal2), Certainty) :-\n    !,\n    solve(Subgoal1, Certainty1),\n    solve(Subgoal2, Certainty2),\n    Certainty is min(Certainty1, Certainty2).\nsolve(Goal, 1) :-\n    builtin(Goal), !, \n    Goal.\nsolve(Head, Certainty) :-\n    clause_cf(Head, Body, Certainty1),\n    solve(Body, Certainty2),\n    Certainty is Certainty1 * Certainty2.\nThis interpreter uses a table of built-in Prolog predicates of the form[23]:327\n\nbuiltin(A is B).\nbuiltin(read(X)).\n% etc.\nand clauses represented as clause_cf(Head, Body, Certainty). Given those, it can be called as solve(Goal, Certainty) to execute Goal and obtain a measure of certainty about the result.\n\nTuring completeness[edit]\nPure Prolog is based on a subset of first-order predicate logic, Horn clauses, which is Turing-complete. Turing completeness of Prolog can be shown by using it to simulate a Turing machine:\n\nturing(Tape0, Tape) :-\n    perform(q0, [], Ls, Tape0, Rs),\n    reverse(Ls, Ls1),\n    append(Ls1, Rs, Tape).\n \nperform(qf, Ls, Ls, Rs, Rs) :- !.\nperform(Q0, Ls0, Ls, Rs0, Rs) :-\n    symbol(Rs0, Sym, RsRest),\n    once(rule(Q0, Sym, Q1, NewSym, Action)),\n    action(Action, Ls0, Ls1, [NewSym|RsRest], Rs1),\n    perform(Q1, Ls1, Ls, Rs1, Rs).\n \nsymbol([], b, []).\nsymbol([Sym|Rs], Sym, Rs).\n \naction(left, Ls0, Ls, Rs0, Rs) :- left(Ls0, Ls, Rs0, Rs).\naction(stay, Ls, Ls, Rs, Rs).\naction(right, Ls0, [Sym|Ls0], [Sym|Rs], Rs).\n \nleft([], [], Rs0, [b|Rs0]).\nleft([L|Ls], Ls, Rs, [L|Rs]).\nA simple example Turing machine is specified by the facts:\n\nrule(q0, 1, q0, 1, right).\nrule(q0, b, qf, 1, stay).\nThis machine performs incrementation by one of a number in unary encoding: It loops over any number of \"1\" cells and appends an additional \"1\" at the end. Example query and result:\n\n?- turing([1,1,1], Ts).\nTs = [1, 1, 1, 1] ;\nThis illustrates how any computation can be expressed declaratively as a sequence of state transitions, implemented in Prolog as a relation between successive states of interest.\n\nImplementation[edit]\nFurther information: Comparison of Prolog implementations\nISO Prolog[edit]\nThe ISO Prolog standard consists of two parts. ISO/IEC 13211-1,[20][24] published in 1995, aims to standardize the existing practices of the many implementations of the core elements of Prolog. It has clarified aspects of the language that were previously ambiguous and leads to portable programs. There are two corrigenda: Cor.1:2007[25] and Cor.2:2012.[26] ISO/IEC 13211-2,[20] published in 2000, adds support for modules to the standard. The standard is maintained by the ISO/IEC JTC1/SC22/WG17[27] working group. ANSI X3J17 is the US Technical Advisory Group for the standard.[28]\n\nCompilation[edit]\nFor efficiency, Prolog code is typically compiled to abstract machine code, often influenced by the register-based Warren Abstract Machine (WAM) instruction set.[29] Some implementations employ abstract interpretation to derive type and mode information of predicates at compile time, or compile to real machine code for high performance.[30] Devising efficient implementation methods for Prolog code is a field of active research in the logic programming community, and various other execution methods are employed in some implementations. These include clause binarization and stack-based virtual machines.[citation needed]\n\nTail recursion[edit]\nProlog systems typically implement a well-known optimization method called tail call optimization (TCO) for deterministic predicates exhibiting tail recursion or, more generally, tail calls: A clause's stack frame is discarded before performing a call in a tail position. Therefore, deterministic tail-recursive predicates are executed with constant stack space, like loops in other languages.\n\nTerm indexing[edit]\nMain article: Term indexing\nFinding clauses that are unifiable with a term in a query is linear in the number of clauses. Term indexing uses a data structure that enables sub-linear-time lookups.[31] Indexing only affects program performance, it does not affect semantics. Most Prologs only use indexing on the first term, as indexing on all terms is expensive, but techniques based on field-encoded words or superimposed codewords provide fast indexing across the full query and head.[32][33]\n\nHashing[edit]\nSome Prolog systems, such as LPA Prolog and SWI-Prolog, now implement hashing to help handle large datasets more efficiently. This tends to yield very large performance gains when working with large corpora such as WordNet.\n\nTabling[edit]\nSome Prolog systems, (BProlog, XSB Yap, B-Prolog and Ciao), implement a memoization method called tabling, which frees the user from manually storing intermediate results.[34][35]\n\nSubgoals encountered in a query evaluation are maintained in a table, along with answers to these subgoals. If a subgoal is re-encountered, the evaluation reuses information from the table rather than re-performing resolution against program clauses.[36]\n\nTabling is a space-time tradeoff; execution time can be reduced by using more memory to store intermediate results.\n\nImplementation in hardware[edit]\nDuring the Fifth Generation Computer Systems project, there were attempts to implement Prolog in hardware with the aim of achieving faster execution with dedicated architectures.[37][38][39] Furthermore, Prolog has a number of properties that may allow speed-up through parallel execution.[40] A more recent approach has been to compile restricted Prolog programs to a field programmable gate array.[41] However, rapid progress in general-purpose hardware has consistently overtaken more specialised architectures.\n\nLimitations[edit]\nAlthough Prolog is widely used in research and education, Prolog and other logic programming languages have not had a significant impact on the computer industry in general.[42] Most applications are small by industrial standards, with few exceeding 100,000 lines of code.[42][43] Programming in the large is considered to be complicated because not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers.[22] Portability of Prolog code across implementations has also been a problem, but developments since 2007 have meant: \"the portability within the family of Edinburgh/Quintus derived Prolog implementations is good enough to allow for maintaining portable real-world applications.\"[44]\n\nSoftware developed in Prolog has been criticised for having a high performance penalty compared to conventional programming languages. In particular, Prolog's non-deterministic evaluation strategy can be problematic when programming deterministic computations, or when even using \"don't care non-determinism\" (where a single choice is made instead of backtracking over all possibilities). Cuts and other language constructs may have to be used to achieve desirable performance, destroying one of Prolog's main attractions, the ability to run programs \"backwards and forwards\".[45]\n\nProlog is not purely declarative: because of constructs like the cut operator, a procedural reading of a Prolog program is needed to understand it.[46] The order of clauses in a Prolog program is significant, as the execution strategy of the language depends on it.[47] Other logic programming languages, such as Datalog, are truly declarative but restrict the language. As a result, many practical Prolog programs are written to conform to Prolog's depth-first search order, rather than as purely declarative logic programs.[45]\n\nExtensions[edit]\nVarious implementations have been developed from Prolog to extend logic programming capabilities in numerous directions. These include types, modes, constraint logic programming (CLP), object-oriented logic programming (OOLP), concurrency, linear logic (LLP), functional and higher-order logic programming capabilities, plus interoperability with knowledge bases:\n\nTypes[edit]\nProlog is an untyped language. Attempts to introduce types date back to the 1980s,[48][49] and as of 2008 there are still attempts to extend Prolog with types.[50] Type information is useful not only for type safety but also for reasoning about Prolog programs.[51]\n\nModes[edit]\nMode specifier\tInterpretation\n+\tnonvar on entry\n-\tvar on entry\n?\tNot specified\nThe syntax of Prolog does not specify which arguments of a predicate are inputs and which are outputs.[52] However, this information is significant and it is recommended that it be included in the comments.[53] Modes provide valuable information when reasoning about Prolog programs[51] and can also be used to accelerate execution.[54]\n\nConstraints[edit]\nConstraint logic programming extends Prolog to include concepts from constraint satisfaction.[55][56] A constraint logic program allows constraints in the body of clauses, such as: A(X,Y) :- X+Y>0. It is suited to large-scale combinatorial optimisation problems.[57] and is thus useful for applications in industrial settings, such as automated time-tabling and production scheduling. Most Prolog systems ship with at least one constraint solver for finite domains, and often also with solvers for other domains like rational numbers.\n\nObject-orientation[edit]\nFlora-2 is an object-oriented knowledge representation and reasoning system based on F-logic and incorporates HiLog, Transaction logic, and defeasible reasoning.\n\nLogtalk is an object-oriented logic programming language that can use most Prolog implementations as a back-end compiler. As a multi-paradigm language, it includes support for both prototypes and classes.\n\nOblog is a small, portable, object-oriented extension to Prolog by Margaret McDougall of EdCAAD, University of Edinburgh.\n\nObjlog was a frame-based language combining objects and Prolog II from CNRS, Marseille, France.\n\nProlog++ was developed by Logic Programming Associates and first released in 1989 for MS-DOS PCs. Support for other platforms was added, and a second version was released in 1995. A book about Prolog++ by Chris Moss was published by Addison-Wesley in 1994.\n\nGraphics[edit]\nProlog systems that provide a graphics library are SWI-prolog,[58] Visual-prolog, LPA Prolog for Windows and B-Prolog.\n\nConcurrency[edit]\nProlog-MPI is an open-source SWI-Prolog extension for distributed computing over the Message Passing Interface.[59] Also there are various concurrent Prolog programming languages.[60]\n\nWeb programming[edit]\nSome Prolog implementations, notably SWI-Prolog and Ciao, support server-side web programming with support for web protocols, HTML and XML.[61] There are also extensions to support semantic web formats such as RDF and OWL.[62][63] Prolog has also been suggested as a client-side language.[64]\n\nAdobe Flash[edit]\nCedar is a free and basic Prolog interpreter. From version 4 and above Cedar has a FCA (Flash Cedar App) support. This provides a new platform to programming in Prolog through ActionScript.\n\nOther[edit]\nF-logic extends Prolog with frames/objects for knowledge representation.\nTransaction logic extends Prolog with a logical theory of state-changing update operators. It has both a model-theoretic and procedural semantics.\nOW Prolog has been created in order to answer Prolog's lack of graphics and interface.\nInterfaces to other languages[edit]\nFrameworks exist which can bridge between Prolog and other languages:\n\nThe LPA Intelligence Server allows the embedding of LPA Prolog within C, C#, C++, Java, VB, Delphi, .Net, Lua, Python and other languages. It exploits the dedicated string data-type which LPA Prolog provides\nThe Logic Server API allows both the extension and embedding of Prolog in C, C++, Java, VB, Delphi, .NET and any language/environment which can call a .dll or .so. It is implemented for Amzi! Prolog Amzi! Prolog + Logic Server but the API specification can be made available for any implementation.\nJPL is a bi-directional Java Prolog bridge which ships with SWI-Prolog by default, allowing Java and Prolog to call each other (recursively). It is known to have good concurrency support and is under active development.\nInterProlog, a programming library bridge between Java and Prolog, implementing bi-directional predicate/method calling between both languages. Java objects can be mapped into Prolog terms and vice versa. Allows the development of GUIs and other functionality in Java while leaving logic processing in the Prolog layer. Supports XSB, with support for SWI-Prolog and YAP planned for 2013.\nProva provides native syntax integration with Java, agent messaging and reaction rules. Prova positions itself as a rule-based scripting (RBS) system for middleware. The language breaks new ground in combining imperative and declarative programming.\nPROL An embeddable Prolog engine for Java. It includes a small IDE and a few libraries.\nGNU Prolog for Java is an implementation of ISO Prolog as a Java library (gnu.prolog)\nCiao provides interfaces to C, C++, Java, and relational databases.\nC#-Prolog is a Prolog interpreter written in (managed) C#. Can easily be integrated in C# programs. Characteristics: reliable and fairly fast interpreter, command line interface, Windows-interface, builtin DCG, XML-predicates, SQL-predicates, extendible. The complete source code is available, including a parser generator that can be used for adding special purpose extensions.\nJekejeke Prolog API provides tightly coupled concurrent call-in and call-out facilities between Prolog and Java or Android, with the marked possibility to create individual knowledge base objects. It can be used to embed the ISO Prolog interpreter in standalones, applets, servlets, APKs, etc..\nA Warren Abstract Machine for PHP A Prolog compiler and interpreter in PHP 5.3. A library that can be used standalone or within Symfony2.1 framework\nHistory[edit]\nThe name Prolog was chosen by Philippe Roussel as an abbreviation for programmation en logique (French for programming in logic). It was created around 1972 by Alain Colmerauer with Philippe Roussel, based on Robert Kowalski's procedural interpretation of Horn clauses. It was motivated in part by the desire to reconcile the use of logic as a declarative knowledge representation language with the procedural representation of knowledge that was popular in North America in the late 1960s and early 1970s. According to Robert Kowalski, the first Prolog system was developed in 1972 by Colmerauer and Phillipe Roussel.[5] The first implementations of Prolog were interpreters. However, David H. D. Warren created the Warren Abstract Machine, an early and influential Prolog compiler which came to define the \"Edinburgh Prolog\" dialect which served as the basis for the syntax of most modern implementations.\n\nEuropean AI researchers favored Prolog while Americans favored Lisp, reportedly causing many nationalistic debates on the merits of the languages.[65] Much of the modern development of Prolog came from the impetus of the Fifth Generation Computer Systems project (FGCS), which developed a variant of Prolog named Kernel Language for its first operating system.\n\nPure Prolog was originally restricted to the use of a resolution theorem prover with Horn clauses of the form:\n\nH :- B1, ..., Bn.\nThe application of the theorem-prover treats such clauses as procedures:\n\nto show/solve H, show/solve B1 and ... and Bn.\nPure Prolog was soon extended, however, to include negation as failure, in which negative conditions of the form not(Bi) are shown by trying and failing to solve the corresponding positive conditions Bi.\n\nSubsequent extensions of Prolog by the original team introduced constraint logic programming abilities into the implementations.\n\nUse in industry[edit]\nProlog has been used in Watson. Watson uses IBM's DeepQA software and the Apache UIMA (Unstructured Information Management Architecture) framework. The system was written in various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using Apache Hadoop framework to provide distributed computing. Prolog is used for pattern matching over natural language parse trees. The developers have stated: \"We required a language in which we could conveniently express pattern matching rules over the parse trees and other annotations (such as named entity recognition results), and a technology that could execute these rules very efficiently. We found that Prolog was the ideal choice for the language due to its simplicity and expressiveness.\"[11]\n\nSee also[edit]\nComparison of Prolog implementations\nLogico-linguistic modeling. A method for building knowledge-based system that uses Prolog.\nAnswer set programming. A fully declarative approach to logic programming.\nAssociation for Logic Programming\nRelated languages[edit]\nThe Gödel language is a strongly typed implementation of concurrent constraint logic programming. It is built on SICStus Prolog.\nVisual Prolog, formerly known as PDC Prolog and Turbo Prolog, is a strongly typed object-oriented dialect of Prolog, which is very different from standard Prolog. As Turbo Prolog, it was marketed by Borland, but it is now developed and marketed by the Danish firm PDC (Prolog Development Center) that originally produced it.\nDatalog is a subset of Prolog. It is limited to relationships that may be stratified and does not allow compound terms. In contrast to Prolog, Datalog is not Turing-complete.\nMercury is an offshoot of Prolog geared toward software engineering in the large with a static, polymorphic type system, as well as a mode and determinism system.\nCSC GraphTalk is a proprietary implementation of Warren's Abstract Machine, with additional object-oriented properties.\nIn some ways[which?] Prolog is a subset of Planner. The ideas in Planner were later further developed in the Scientific Community Metaphor.\nAgentSpeak is a variant of Prolog for programming agent behavior in multi-agent systems.\nErlang began life with a Prolog-based implementation and maintains much of Prolog's unification-based syntax.",
          "type": "compiled",
          "plid": 11
        },
        {
          "name": "SQL",
          "details": "SQL (Listeni/ˈɛs kjuː ˈɛl/,[4] or Listeni/ˈsiːkwəl/;[5] Structured Query Language[6][7][8][9]) is a special-purpose programming language designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS).\n\nOriginally based upon relational algebra and tuple relational calculus, SQL consists of a data definition language, data manipulation language, and Data Control Language. The scope of SQL includes data insert, query, update and delete, schema creation and modification, and data access control. Although SQL is often described as, and to a great extent is, a declarative language (4GL), it also includes procedural elements.\n\nSQL was one of the first commercial languages for Edgar F. Codd's relational model, as described in his influential 1970 paper, \"A Relational Model of Data for Large Shared Data Banks.\"[10] Despite not entirely adhering to the relational model as described by Codd, it became the most widely used database language.[11][12]\n\nSQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987.[13] Since then, the standard has been revised to include a larger set of features. Despite the existence of such standards, most SQL code is not completely portable among different database systems without adjustments.\n\nContents  [hide] \n1\tHistory\n2\tDesign\n3\tSyntax\n3.1\tLanguage elements\n3.2\tOperators\n3.3\tQueries\n3.4\tData manipulation\n3.5\tTransaction controls\n3.6\tData definition\n3.7\tData types\n3.8\tData control\n4\tProcedural extensions\n5\tInteroperability and standardization\n6\tAlternatives\n7\tDistributed SQL processing\n8\tSee also\n9\tNotes\n10\tReferences\n11\tExternal links\nHistory[edit]\nSQL was initially developed at IBM by Donald D. Chamberlin and Raymond F. Boyce in the early 1970s.[14] This version, initially called SEQUEL (Structured English Query Language), was designed to manipulate and retrieve data stored in IBM's original quasi-relational database management system, System R, which a group at IBM San Jose Research Laboratory had developed during the 1970s.[14] The acronym SEQUEL was later changed to SQL because \"SEQUEL\" was a trademark of the UK-based Hawker Siddeley aircraft company.[15]\n\nIn the late 1970s, Relational Software, Inc. (now Oracle Corporation) saw the potential of the concepts described by Codd, Chamberlin, and Boyce, and developed their own SQL-based RDBMS with aspirations of selling it to the U.S. Navy, Central Intelligence Agency, and other U.S. government agencies. In June 1979, Relational Software, Inc. introduced the first commercially available implementation of SQL, Oracle V2 (Version2) for VAX computers.\n\nAfter testing SQL at customer test sites to determine the usefulness and practicality of the system, IBM began developing commercial products based on their System R prototype including System/38, SQL/DS, and DB2, which were commercially available in 1979, 1981, and 1983, respectively.[16]\n\nDesign[edit]\nSQL deviates in several ways from its theoretical foundation, the relational model and its tuple calculus. In that model, a table is a set of tuples, while in SQL, tables and query results are lists of rows: the same row may occur multiple times, and the order of rows can be employed in queries (e.g. in the LIMIT clause).\n\nCritics argue that SQL should be replaced with a language that strictly returns to the original foundation: for example, see The Third Manifesto.\n\nSyntax[edit]\n\nIt has been suggested that this section be split out into another article titled SQL syntax. (Discuss) (June 2015)\nLanguage elements[edit]\n\nA chart showing several of the SQL language elements that compose a single statement\nThe SQL language is subdivided into several language elements, including:\n\nClauses, which are constituent components of statements and queries. (In some cases, these are optional.)[17]\nExpressions, which can produce either scalar values, or tables consisting of columns and rows of data\nPredicates, which specify conditions that can be evaluated to SQL three-valued logic (3VL) (true/false/unknown) or Boolean truth values and are used to limit the effects of statements and queries, or to change program flow.\nQueries, which retrieve the data based on specific criteria. This is an important element of SQL.\nStatements, which may have a persistent effect on schemata and data, or may control transactions, program flow, connections, sessions, or diagnostics.\nSQL statements also include the semicolon (\";\") statement terminator. Though not required on every platform, it is defined as a standard part of the SQL grammar.\nInsignificant whitespace is generally ignored in SQL statements and queries, making it easier to format SQL code for readability.\nOperators[edit]\nOperator\tDescription\tExample\n=\tEqual to\tAuthor = 'Alcott'\n<>\tNot equal to (many DBMSs accept != in addition to <>)\tDept <> 'Sales'\n>\tGreater than\tHire_Date > '2012-01-31'\n<\tLess than\tBonus < 50000.00\n>=\tGreater than or equal\tDependents >= 2\n<=\tLess than or equal\tRate <= 0.05\nBETWEEN\tBetween an inclusive range\tCost BETWEEN 100.00 AND 500.00\nLIKE\tMatch a character pattern\tFirst_Name LIKE 'Will%'\nIN\tEqual to one of multiple possible values\tDeptCode IN (101, 103, 209)\nIS or IS NOT\tCompare to null (missing data)\tAddress IS NOT NULL\nIS NOT DISTINCT FROM\tIs equal to value or both are nulls (missing data)\tDebt IS NOT DISTINCT FROM - Receivables\nAS\tUsed to change a field name when viewing results\tSELECT employee AS 'department1'\nOther operators have at times been suggested and/or implemented, such as the skyline operator (for finding only those records that are not 'worse' than any others).\n\nSQL has the case/when/then/else/end expression, which was introduced in SQL-92. In its most general form, which is called a \"searched case\" in the SQL standard, it works like else if in other programming languages:\n\nCASE WHEN n > 0\n          THEN 'positive'\n     WHEN n < 0\n          THEN 'negative'\n     ELSE 'zero'\nEND\nSQL tests WHEN conditions in the order they appear in the source. If the source does not specify an ELSE expression, SQL defaults to ELSE NULL. An abbreviated syntax—called \"simple case\" in the SQL standard—mirrors switch statements:\n\nCASE n WHEN 1\n            THEN 'one'\n       WHEN 2\n            THEN 'two'\n       ELSE 'I cannot count that high'\nEND\nThis syntax uses implicit equality comparisons, with the usual caveats for comparing with NULL.\n\nFor the Oracle-SQL dialect, the latter can be shortened to an equivalent DECODE construct:\n\nSELECT DECODE(n, 1, 'one',\n                 2, 'two',\n                    'i cannot count that high')\nFROM   some_table;\nThe last value is the default; if none is specified, it also defaults to NULL. However, unlike the standard's \"simple case\", Oracle's DECODE considers two NULLs equal with each other.[18]\n\nQueries[edit]\nThe most common operation in SQL, the query, makes use of the declarative SELECT statement. SELECT retrieves data from one or more tables, or expressions. Standard SELECT statements have no persistent effects on the database. Some non-standard implementations of SELECT can have persistent effects, such as the SELECT INTO syntax provided in some databases.[19]\n\nQueries allow the user to describe desired data, leaving the database management system (DBMS) to carry out planning, optimizing, and performing the physical operations necessary to produce that result as it chooses.\n\nA query includes a list of columns to include in the final result, normally immediately following the SELECT keyword. An asterisk (\"*\") can be used to specify that the query should return all columns of the queried tables. SELECT is the most complex statement in SQL, with optional keywords and clauses that include:\n\nThe FROM clause, which indicates the table(s) to retrieve data from. The FROM clause can include optional JOIN subclauses to specify the rules for joining tables.\nThe WHERE clause includes a comparison predicate, which restricts the rows returned by the query. The WHERE clause eliminates all rows from the result set where the comparison predicate does not evaluate to True.\nThe GROUP BY clause projects rows having common values into a smaller set of rows. GROUP BY is often used in conjunction with SQL aggregation functions or to eliminate duplicate rows from a result set. The WHERE clause is applied before the GROUP BY clause.\nThe HAVING clause includes a predicate used to filter rows resulting from the GROUP BY clause. Because it acts on the results of the GROUP BY clause, aggregation functions can be used in the HAVING clause predicate.\nThe ORDER BY clause identifies which column[s] to use to sort the resulting data, and in which direction to sort them (ascending or descending). Without an ORDER BY clause, the order of rows returned by an SQL query is undefined.\nThe DISTINCT keyword[20] eliminates duplicate data.[21]\nThe following example of a SELECT query returns a list of expensive books. The query retrieves all rows from the Book table in which the price column contains a value greater than 100.00. The result is sorted in ascending order by title. The asterisk (*) in the select list indicates that all columns of the Book table should be included in the result set.\n\nSELECT *\n FROM  Book\n WHERE price > 100.00\n ORDER BY title;\nThe example below demonstrates a query of multiple tables, grouping, and aggregation, by returning a list of books and the number of authors associated with each book.\n\nSELECT Book.title AS Title,\n       count(*) AS Authors\n FROM  Book\n JOIN  Book_author\n   ON  Book.isbn = Book_author.isbn\n GROUP BY Book.title;\nExample output might resemble the following:\n\nTitle                  Authors\n---------------------- -------\nSQL Examples and Guide 4\nThe Joy of SQL         1\nAn Introduction to SQL 2\nPitfalls of SQL        1\nUnder the precondition that isbn is the only common column name of the two tables and that a column named title only exists in the Book table, one could re-write the query above in the following form:\n\nSELECT title,\n       count(*) AS Authors\n FROM  Book\n NATURAL JOIN Book_author\n GROUP BY title;\nHowever, many[quantify] vendors either do not support this approach, or require certain column-naming conventions for natural joins to work effectively.\n\nSQL includes operators and functions for calculating values on stored values. SQL allows the use of expressions in the select list to project data, as in the following example, which returns a list of books that cost more than 100.00 with an additional sales_tax column containing a sales tax figure calculated at 6% of the price.\n\nSELECT isbn,\n       title,\n       price,\n       price * 0.06 AS sales_tax\n FROM  Book\n WHERE price > 100.00\n ORDER BY title;\nSubqueries[edit]\nQueries can be nested so that the results of one query can be used in another query via a relational operator or aggregation function. A nested query is also known as a subquery. While joins and other table operations provide computationally superior (i.e. faster) alternatives in many cases, the use of subqueries introduces a hierarchy in execution that can be useful or necessary. In the following example, the aggregation function AVG receives as input the result of a subquery:\n\nSELECT isbn,\n       title,\n       price\n FROM  Book\n WHERE price < (SELECT AVG(price) FROM Book)\n ORDER BY title;\nA subquery can use values from the outer query, in which case it is known as a correlated subquery.\n\nSince 1999 the SQL standard allows named subqueries called common table expressions (named and designed after the IBM DB2 version 2 implementation; Oracle calls these subquery factoring). CTEs can also be recursive by referring to themselves; the resulting mechanism allows tree or graph traversals (when represented as relations), and more generally fixpoint computations.\n\nInline view[edit]\nAn Inline view is the use of referencing an SQL subquery in a FROM clause. Essentially, the inline view is a subquery that can be selected from or joined to. Inline View functionality allows the user to reference the subquery as a table. The inline view also is referred to as a derived table or a subselect. Inline view functionality was introduced in Oracle 9i.[22]\n\nIn the following example, the SQL statement involves a join from the initial Books table to the Inline view \"Sales\". This inline view captures associated book sales information using the ISBN to join to the Books table. As a result, the inline view provides the result set with additional columns (the number of items sold and the company that sold the books):\n\nSELECT b.isbn, b.title, b.price, sales.items_sold, sales.company_nm\nFROM Book b\n  JOIN (SELECT SUM(Items_Sold) Items_Sold, Company_Nm, ISBN\n        FROM Book_Sales\n        GROUP BY Company_Nm, ISBN) sales\n  ON sales.isbn = b.isbn\nNull or three-valued logic (3VL)[edit]\nMain article: Null (SQL)\nThe concept of Null was introduced[by whom?] into SQL to handle missing information in the relational model. The word NULL is a reserved keyword in SQL, used to identify the Null special marker. Comparisons with Null, for instance equality (=) in WHERE clauses, results in an Unknown truth value. In SELECT statements SQL returns only results for which the WHERE clause returns a value of True; i.e., it excludes results with values of False and also excludes those whose value is Unknown.\n\nAlong with True and False, the Unknown resulting from direct comparisons with Null thus brings a fragment of three-valued logic to SQL. The truth tables SQL uses for AND, OR, and NOT correspond to a common fragment of the Kleene and Lukasiewicz three-valued logic (which differ in their definition of implication, however SQL defines no such operation).[23]\n\np AND q\tp\nTrue\tFalse\tUnknown\nq\tTrue\tTrue\tFalse\tUnknown\nFalse\tFalse\tFalse\tFalse\nUnknown\tUnknown\tFalse\tUnknown\np OR q\tp\nTrue\tFalse\tUnknown\nq\tTrue\tTrue\tTrue\tTrue\nFalse\tTrue\tFalse\tUnknown\nUnknown\tTrue\tUnknown\tUnknown\np = q\tp\nTrue\tFalse\tUnknown\nq\tTrue\tTrue\tFalse\tUnknown\nFalse\tFalse\tTrue\tUnknown\nUnknown\tUnknown\tUnknown\tUnknown\nq\tNOT q\nTrue\tFalse\nFalse\tTrue\nUnknown\tUnknown\nThere are however disputes about the semantic interpretation of Nulls in SQL because of its treatment outside direct comparisons. As seen in the table above, direct equality comparisons between two NULLs in SQL (e.g. NULL = NULL) return a truth value of Unknown. This is in line with the interpretation that Null does not have a value (and is not a member of any data domain) but is rather a placeholder or \"mark\" for missing information. However, the principle that two Nulls aren't equal to each other is effectively violated in the SQL specification for the UNION and INTERSECT operators, which do identify nulls with each other.[24] Consequently, these set operations in SQL may produce results not representing sure information, unlike operations involving explicit comparisons with NULL (e.g. those in a WHERE clause discussed above). In Codd's 1979 proposal (which was basically adopted by SQL92) this semantic inconsistency is rationalized by arguing that removal of duplicates in set operations happens \"at a lower level of detail than equality testing in the evaluation of retrieval operations\".[23] However, computer-science professor Ron van der Meyden concluded that \"The inconsistencies in the SQL standard mean that it is not possible to ascribe any intuitive logical semantics to the treatment of nulls in SQL.\"[24]\n\nAdditionally, because SQL operators return Unknown when comparing anything with Null directly, SQL provides two Null-specific comparison predicates: IS NULL and IS NOT NULL test whether data is or is not Null.[25] SQL does not explicitly support universal quantification, and must work it out as a negated existential quantification.[26][27][28] There is also the \"<row value expression> IS DISTINCT FROM <row value expression>\" infixed comparison operator, which returns TRUE unless both operands are equal or both are NULL. Likewise, IS NOT DISTINCT FROM is defined as \"NOT (<row value expression> IS DISTINCT FROM <row value expression>)\". SQL:1999 also introduced BOOLEAN type variables, which according to the standard can also hold Unknown values. In practice, a number of systems (e.g. PostgreSQL) implement the BOOLEAN Unknown as a BOOLEAN NULL.\n\nData manipulation[edit]\nThe Data Manipulation Language (DML) is the subset of SQL used to add, update and delete data:\n\nINSERT adds rows (formally tuples) to an existing table, e.g.:\nINSERT INTO example\n (field1, field2, field3)\n VALUES\n ('test', 'N', NULL);\nUPDATE modifies a set of existing table rows, e.g.:\nUPDATE example\n SET field1 = 'updated value'\n WHERE field2 = 'N';\nDELETE removes existing rows from a table, e.g.:\nDELETE FROM example\n WHERE field2 = 'N';\nMERGE is used to combine the data of multiple tables. It combines the INSERT and UPDATE elements. It is defined in the SQL:2003 standard; prior to that, some databases provided similar functionality via different syntax, sometimes called \"upsert\".\n MERGE INTO table_name USING table_reference ON (condition)\n WHEN MATCHED THEN\n UPDATE SET column1 = value1 [, column2 = value2 ...]\n WHEN NOT MATCHED THEN\n INSERT (column1 [, column2 ...]) VALUES (value1 [, value2 ...])\nTransaction controls[edit]\nTransactions, if available, wrap DML operations:\n\nSTART TRANSACTION (or BEGIN WORK, or BEGIN TRANSACTION, depending on SQL dialect) marks the start of a database transaction, which either completes entirely or not at all.\nSAVE TRANSACTION (or SAVEPOINT) saves the state of the database at the current point in transaction\nCREATE TABLE tbl_1(id int);\n INSERT INTO tbl_1(id) VALUES(1);\n INSERT INTO tbl_1(id) VALUES(2);\nCOMMIT;\n UPDATE tbl_1 SET id=200 WHERE id=1;\nSAVEPOINT id_1upd;\n UPDATE tbl_1 SET id=1000 WHERE id=2;\nROLLBACK to id_1upd;\n SELECT id from tbl_1;\nCOMMIT makes all data changes in a transaction permanent.\nROLLBACK discards all data changes since the last COMMIT or ROLLBACK, leaving the data as it was prior to those changes. Once the COMMIT statement completes, the transaction's changes cannot be rolled back.\nCOMMIT and ROLLBACK terminate the current transaction and release data locks. In the absence of a START TRANSACTION or similar statement, the semantics of SQL are implementation-dependent. The following example shows a classic transfer of funds transaction, where money is removed from one account and added to another. If either the removal or the addition fails, the entire transaction is rolled back.\n\nSTART TRANSACTION;\n UPDATE Account SET amount=amount-200 WHERE account_number=1234;\n UPDATE Account SET amount=amount+200 WHERE account_number=2345;\n\nIF ERRORS=0 COMMIT;\nIF ERRORS<>0 ROLLBACK;\nData definition[edit]\nThe Data Definition Language (DDL) manages table and index structure. The most basic items of DDL are the CREATE, ALTER, RENAME, DROP and TRUNCATE statements:\n\nCREATE creates an object (a table, for example) in the database, e.g.:\nCREATE TABLE example(\n column1 INTEGER,\n column2 VARCHAR(50),\n column3 DATE NOT NULL,\n PRIMARY KEY (column1, column2)\n);\nALTER modifies the structure of an existing object in various ways, for example, adding a column to an existing table or a constraint, e.g.:\nALTER TABLE example ADD column4 NUMBER(3) NOT NULL;\nTRUNCATE deletes all data from a table in a very fast way, deleting the data inside the table and not the table itself. It usually implies a subsequent COMMIT operation, i.e., it cannot be rolled back (data is not written to the logs for rollback later, unlike DELETE).\nTRUNCATE TABLE example;\nDROP deletes an object in the database, usually irretrievably, i.e., it cannot be rolled back, e.g.:\nDROP TABLE example;\nData types[edit]\nEach column in an SQL table declares the type(s) that column may contain. ANSI SQL includes the following data types.[29]\n\nCharacter strings\nCHARACTER(n) or CHAR(n): fixed-width n-character string, padded with spaces as needed\nCHARACTER VARYING(n) or VARCHAR(n): variable-width string with a maximum size of n characters\nNATIONAL CHARACTER(n) or NCHAR(n): fixed width string supporting an international character set\nNATIONAL CHARACTER VARYING(n) or NVARCHAR(n): variable-width NCHAR string\nBit strings\nBIT(n): an array of n bits\nBIT VARYING(n): an array of up to n bits\nNumbers\nINTEGER, SMALLINT and BIGINT\nFLOAT, REAL and DOUBLE PRECISION\nNUMERIC(precision, scale) or DECIMAL(precision, scale)\nFor example, the number 123.45 has a precision of 5 and a scale of 2. The precision is a positive integer that determines the number of significant digits in a particular radix (binary or decimal). The scale is a non-negative integer. A scale of 0 indicates that the number is an integer. For a decimal number with scale S, the exact numeric value is the integer value of the significant digits divided by 10S.\n\nSQL provides a function to round numerics or dates, called TRUNC (in Informix, DB2, PostgreSQL, Oracle and MySQL) or ROUND (in Informix, SQLite, Sybase, Oracle, PostgreSQL and Microsoft SQL Server)[30]\n\nTemporal (date/time)\nDATE: for date values (e.g. 2011-05-03)\nTIME: for time values (e.g. 15:51:36). The granularity of the time value is usually a tick (100 nanoseconds).\nTIME WITH TIME ZONE or TIMETZ: the same as TIME, but including details about the time zone in question.\nTIMESTAMP: This is a DATE and a TIME put together in one variable (e.g. 2011-05-03 15:51:36).\nTIMESTAMP WITH TIME ZONE or TIMESTAMPTZ: the same as TIMESTAMP, but including details about the time zone in question.\nSQL provides several functions for generating a date / time variable out of a date / time string (TO_DATE, TO_TIME, TO_TIMESTAMP), as well as for extracting the respective members (seconds, for instance) of such variables. The current system date / time of the database server can be called by using functions like NOW. The IBM Informix implementation provides the EXTEND and the FRACTION functions to increase the accuracy of time, for systems requiring sub-second precision.[31]\n\nData control[edit]\nThe Data Control Language (DCL) authorizes users to access and manipulate data. Its two main statements are:\n\nGRANT authorizes one or more users to perform an operation or a set of operations on an object.\nREVOKE eliminates a grant, which may be the default grant.\nExample:\n\nGRANT SELECT, UPDATE\n ON example\n TO some_user, another_user;\n\nREVOKE SELECT, UPDATE\n ON example\n FROM some_user, another_user;\nProcedural extensions[edit]\nSQL is designed for a specific purpose: to query data contained in a relational database. SQL is a set-based, declarative programming language, not an imperative programming language like C or BASIC. However, extensions to Standard SQL add procedural programming language functionality, such as control-of-flow constructs. These include:\n\nSource\tCommon name\tFull name\nANSI/ISO Standard\tSQL/PSM\tSQL/Persistent Stored Modules\nInterbase / Firebird\tPSQL\tProcedural SQL\nIBM DB2\tSQL PL\tSQL Procedural Language (implements SQL/PSM)\nIBM Informix\tSPL\tStored Procedural Language\nIBM Netezza\tNZPLSQL [2]\t(based on Postgres PL/pgSQL)\nMicrosoft / Sybase\tT-SQL\tTransact-SQL\nMimer SQL\tSQL/PSM\tSQL/Persistent Stored Module (implements SQL/PSM)\nMySQL\tSQL/PSM\tSQL/Persistent Stored Module (implements SQL/PSM)\nMonetDB\tSQL/PSM\tSQL/Persistent Stored Module (implements SQL/PSM)\nNuoDB\tSSP\tStarkey Stored Procedures\nOracle\tPL/SQL\tProcedural Language/SQL (based on Ada)\nPostgreSQL\tPL/pgSQL\tProcedural Language/PostgreSQL Structured Query Language (implements SQL/PSM)\nSybase\tWatcom-SQL\tSQL Anywhere Watcom-SQL Dialect\nTeradata\tSPL\tStored Procedural Language\nSAP\tSAP HANA\tSQL Script\nIn addition to the standard SQL/PSM extensions and proprietary SQL extensions, procedural and object-oriented programmability is available on many SQL platforms via DBMS integration with other languages. The SQL standard defines SQL/JRT extensions (SQL Routines and Types for the Java Programming Language) to support Java code in SQL databases. SQL Server 2005 uses the SQLCLR (SQL Server Common Language Runtime) to host managed .NET assemblies in the database, while prior versions of SQL Server were restricted to unmanaged extended stored procedures primarily written in C. PostgreSQL lets users write functions in a wide variety of languages—including Perl, Python, Tcl, and C.[32]\n\nInteroperability and standardization[edit]\nSQL implementations are incompatible between vendors and do not necessarily completely follow standards. In particular date and time syntax, string concatenation, NULLs, and comparison case sensitivity vary from vendor to vendor. A particular exception is PostgreSQL, which strives for standards compliance.[33]\n\nPopular implementations of SQL commonly omit support for basic features of Standard SQL, such as the DATE or TIME data types. The most obvious such examples, and incidentally the most popular commercial and proprietary SQL DBMSs, are Oracle (whose DATE behaves as DATETIME,[34][35] and lacks a TIME type)[36] and MS SQL Server (before the 2008 version). As a result, SQL code can rarely be ported between database systems without modifications.\n\nThere are several reasons for this lack of portability between database systems:\n\nThe complexity and size of the SQL standard means that most implementors do not support the entire standard.\nThe standard does not specify database behavior in several important areas (e.g. indexes, file storage...), leaving implementations to decide how to behave.\nThe SQL standard precisely specifies the syntax that a conforming database system must implement. However, the standard's specification of the semantics of language constructs is less well-defined, leading to ambiguity.\nMany database vendors have large existing customer bases; where the newer version of the SQL standard conflicts with the prior behavior of the vendor's database, the vendor may be unwilling to break backward compatibility.\nThere is little commercial incentive for vendors to make it easier for users to change database suppliers (see vendor lock-in).\nUsers evaluating database software tend to place other factors such as performance higher in their priorities than standards conformance.\nSQL was adopted as a standard by the American National Standards Institute (ANSI) in 1986 as SQL-86[37] and the International Organization for Standardization (ISO) in 1987. Nowadays the standard is subject to continuous improvement by the Joint Technical Committee ISO/IEC JTC 1, Information technology, Subcommittee SC 32, Data management and interchange, which affiliate to ISO as well as IEC. It is commonly denoted by the pattern: ISO/IEC 9075-n:yyyy Part n: title, or, as a shortcut, ISO/IEC 9075.\n\nISO/IEC 9075 is complemented by ISO/IEC 13249: SQL Multimedia and Application Packages (SQL/MM), which defines SQL based interfaces and packages to widely spread applications like video, audio and spatial data.\n\nUntil 1996, the National Institute of Standards and Technology (NIST) data management standards program certified SQL DBMS compliance with the SQL standard. Vendors now self-certify the compliance of their products.[38]\n\nThe original standard declared that the official pronunciation for \"SQL\" was an initialism: /ˈɛs kjuː ˈɛl/ (\"es queue el\").[11] Regardless, many English-speaking database professionals (including Donald Chamberlin himself[39]) use the acronym-like pronunciation of /ˈsiːkwəl/ (\"sequel\"),[40] mirroring the language's pre-release development name of \"SEQUEL\".[14][15]\n\nThe SQL standard has gone through a number of revisions:\n\nYear\tName\tAlias\tComments\n1986\tSQL-86\tSQL-87\tFirst formalized by ANSI.\n1989\tSQL-89\tFIPS 127-1\tMinor revision that added integrity constraints, adopted as FIPS 127-1.\n1992\tSQL-92\tSQL2, FIPS 127-2\tMajor revision (ISO 9075), Entry Level SQL-92 adopted as FIPS 127-2.\n1999\tSQL:1999\tSQL3\tAdded regular expression matching, recursive queries (e.g. transitive closure), triggers, support for procedural and control-of-flow statements, non-scalar types, and some object-oriented features (e.g. structured types). Support for embedding SQL in Java (SQL/OLB) and vice versa (SQL/JRT).\n2003\tSQL:2003\tSQL 2003\tIntroduced XML-related features (SQL/XML), window functions, standardized sequences, and columns with auto-generated values (including identity-columns).\n2006\tSQL:2006\tSQL 2006\tISO/IEC 9075-14:2006 defines ways that SQL can be used with XML. It defines ways of importing and storing XML data in an SQL database, manipulating it within the database, and publishing both XML and conventional SQL-data in XML form. In addition, it lets applications integrate queries into their SQL code with XQuery, the XML Query Language published by the World Wide Web Consortium (W3C), to concurrently access ordinary SQL-data and XML documents.[41]\n2008\tSQL:2008\tSQL 2008\tLegalizes ORDER BY outside cursor definitions. Adds INSTEAD OF triggers. Adds the TRUNCATE statement.[42]\n2011\tSQL:2011\t\t\nInterested parties may purchase SQL standards documents from ISO,[43] IEC or ANSI. A draft of SQL:2008 is freely available as a zip archive.[44]\n\nThe SQL standard is divided into nine parts.\n\nISO/IEC 9075-1:2011 Part 1: Framework (SQL/Framework). It provides logical concepts.\nISO/IEC 9075-2:2011 Part 2: Foundation (SQL/Foundation). It contains the most central elements of the language and consists of both mandatory and optional features.\nISO/IEC 9075-3:2008 Part 3: Call-Level Interface (SQL/CLI). It defines interfacing components (structures, procedures, variable bindings) that can be used to execute SQL statements from applications written in Ada, C respectively C++, COBOL, Fortran, MUMPS, Pascal or PL/I. (For Java see part 10.) SQL/CLI is defined in such a way that SQL statements and SQL/CLI procedure calls are treated as separate from the calling application's source code. Open Database Connectivity is a well-known superset of SQL/CLI. This part of the standard consists solely of mandatory features.\nISO/IEC 9075-4:2011 Part 4: Persistent Stored Modules (SQL/PSM) It standardizes procedural extensions for SQL, including flow of control, condition handling, statement condition signals and resignals, cursors and local variables, and assignment of expressions to variables and parameters. In addition, SQL/PSM formalizes declaration and maintenance of persistent database language routines (e.g., \"stored procedures\"). This part of the standard consists solely of optional features.\nISO/IEC 9075-9:2008 Part 9: Management of External Data (SQL/MED). It provides extensions to SQL that define foreign-data wrappers and datalink types to allow SQL to manage external data. External data is data that is accessible to, but not managed by, an SQL-based DBMS. This part of the standard consists solely of optional features.\nISO/IEC 9075-10:2008 Part 10: Object Language Bindings (SQL/OLB). It defines the syntax and semantics of SQLJ, which is SQL embedded in Java (see also part 3). The standard also describes mechanisms to ensure binary portability of SQLJ applications, and specifies various Java packages and their contained classes. This part of the standard consists solely of optional features, as opposed to SQL/OLB JDBC, which is not part of the SQL standard, which defines an API.[citation needed]\nISO/IEC 9075-11:2011 Part 11: Information and Definition Schemas (SQL/Schemata). It defines the Information Schema and Definition Schema, providing a common set of tools to make SQL databases and objects self-describing. These tools include the SQL object identifier, structure and integrity constraints, security and authorization specifications, features and packages of ISO/IEC 9075, support of features provided by SQL-based DBMS implementations, SQL-based DBMS implementation information and sizing items, and the values supported by the DBMS implementations.[45] This part of the standard contains both mandatory and optional features.\nISO/IEC 9075-13:2008 Part 13: SQL Routines and Types Using the Java Programming Language (SQL/JRT). It specifies the ability to invoke static Java methods as routines from within SQL applications ('Java-in-the-database'). It also calls for the ability to use Java classes as SQL structured user-defined types. This part of the standard consists solely of optional features.\nISO/IEC 9075-14:2011 Part 14: XML-Related Specifications (SQL/XML). It specifies SQL-based extensions for using XML in conjunction with SQL. The XML data type is introduced, as well as several routines, functions, and XML-to-SQL data type mappings to support manipulation and storage of XML in an SQL database.[41] This part of the standard consists solely of optional features.[citation needed]\nISO/IEC 9075 is complemented by ISO/IEC 13249 SQL Multimedia and Application Packages. This closely related but separate standard is developed by the same committee. It defines interfaces and packages based on SQL. The aim is a unified access to typical database applications like text, pictures, data mining or spatial data.\n\nISO/IEC 13249-1:2007 Part 1: Framework\nISO/IEC 13249-2:2003 Part 2: Full-Text\nISO/IEC 13249-3:2011 Part 3: Spatial\nISO/IEC 13249-5:2003 Part 5: Still image\nISO/IEC 13249-6:2006 Part 6: Data mining\nISO/IEC 13249-8:xxxx Part 8: Metadata registries (MDR) (work in progress)\nAlternatives[edit]\nA distinction should be made between alternatives to SQL as a language, and alternatives to the relational model itself. Below are proposed relational alternatives to the SQL language. See navigational database and NoSQL for alternatives to the relational model.\n\n.QL: object-oriented Datalog\n4D Query Language (4D QL)\nBQL: a superset that compiles down to SQL\nDatalog: critics suggest that Datalog has two advantages over SQL: it has cleaner semantics, which facilitates program understanding and maintenance, and it is more expressive, in particular for recursive queries.[46]\nHTSQL: URL based query method\nIBM Business System 12 (IBM BS12): one of the first fully relational database management systems, introduced in 1982\nISBL\njOOQ: SQL implemented in Java as an internal domain-specific language\nJava Persistence Query Language (JPQL): The query language used by the Java Persistence API and Hibernate persistence library\nLINQ: Runs SQL statements written like language constructs to query collections directly from inside .Net code.\nObject Query Language\nQBE (Query By Example) created by Moshè Zloof, IBM 1977\nQuel introduced in 1974 by the U.C. Berkeley Ingres project.\nTutorial D\nXQuery\nDistributed SQL processing[edit]\nDistributed Relational Database Architecture (DRDA) was designed by a work group within IBM in the period 1988 to 1994. DRDA enables network connected relational databases to cooperate to fulfill SQL requests.[47][48]\n\nAn interactive user or program can issue SQL statements to a local RDB and receive tables of data and status indicators in reply from remote RDBs. SQL statements can also be compiled and stored in remote RDBs as packages and then invoked by package name. This is important for the efficient operation of application programs that issue complex, high-frequency queries. It is especially important when the tables to be accessed are located in remote systems.\n\nThe messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.\n\nSee also[edit]\nBook icon\t\nBook: SQL\nComparison of object-relational database management systems\nComparison of relational database management systems\nD (data language specification)\nD4 (programming language)\nHierarchical model\nList of relational database management systems\nMUMPS\nNoSQL\nTransact-SQL\nOnline analytical processing (OLAP)\nOnline transaction processing (OLTP)\nData warehouse\nrelational data stream management system\nStar schema\nSnowflake schema\nDB2 SQL return codes",
          "type": "unknown",
          "plid": 12
        },
        {
          "name": "Clojure",
          "details": "(pronunciation: /ˈkloʊʒɜːr/, like \"closure\"[6]) is a dialect of the Lisp programming language created by Rich Hickey.[7] Clojure is a general-purpose programming language with an emphasis on functional programming.[8] It runs on the Java virtual machine, Common Language Runtime,[9] and JavaScript[10] engines. Like other Lisps, Clojure treats code as data and has a macro system.[11] The current development process is community-driven,[12] overseen by Rich Hickey as its benevolent dictator for life (BDFL).[13]\n\nClojure encourages immutability and immutable data structures. While its type system is entirely dynamic, recent efforts have also sought the implementation of gradual typing.[14] Clojure encourages programmers to be explicit about managing state and identity.[15] This focus on programming with immutable values and explicit progression-of-time constructs is intended to facilitate developing more robust programs, especially multithreaded ones.[16][17]\n\nClojure is used in industry by firms such as Walmart,[18] Puppet Labs,[19] and other large software firms.[20] Commercial support for Clojure is provided by Cognitect.[20] Annual Clojure conferences are organised every year across the globe, the most famous of them being Clojure/conj (US east coast),[21] Clojure/West (US west coast),[22] and EuroClojure (Europe).[23]\n\nThe latest stable version of Clojure is 1.8,[24] released on January 19, 2016. The first stable release was version 1.0, released on May 4, 2009.[25] Clojure is free software released under the Eclipse Public License.[26]\n\nContents  [hide] \n1\tHistory and development process\n2\tDesign philosophy\n3\tFeatures\n4\tPlatforms and popularity\n5\tExamples\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory and development process[edit]\n\nRich Hickey in San Francisco\nRich Hickey is the creator of the Clojure language.[7] Before Clojure, he developed dotLisp, a similar project based on the .NET Framework,[27] and three earlier attempts to provide interoperability between Lisp and Java: a Java foreign language interface for Common Lisp (jfli),[28] A Foreign Object Interface for Lisp (FOIL),[29] and a Lisp-friendly interface to Java Servlets (Lisplets).[30]\n\nHickey spent about 2½ years working on Clojure before releasing it publicly, much of that time working exclusively on Clojure with no outside funding. At the end of this time, Hickey sent an email announcing the language to some friends in the Common Lisp community.\n\nThe development process is community-driven[12] and is managed at the Clojure Community website.[31] The website contains planning documents and an issue tracker where bugs may be filed. General development discussion occurs at the Clojure Dev Google Group.[32] While anyone can submit bug reports and ideas, to contribute patches one must sign the Clojure Contributor agreement.[33] JIRA tickets are processed by a team of screeners and finally Rich Hickey approves the changes.[34]\n\nDesign philosophy[edit]\nRich Hickey developed Clojure because he wanted a modern Lisp for functional programming, symbiotic with the established Java platform, and designed for concurrency.[16][17][35]\n\nClojure's approach to state is characterized by the concept of identities,[36] which are represented as a series of immutable states over time. Since states are immutable values, any number of workers can operate on them in parallel, and concurrency becomes a question of managing changes from one state to another. For this purpose, Clojure provides several mutable reference types, each having well-defined semantics for the transition between states.[15][36]\n\nFeatures[edit]\nVersion\tRelease date\tMajor features added\n2007-10-16[37]\tInitial public release\n1.0\t2009-05-04[38]\tFirst stable release[39]\n1.1\t2009-12-31[38]\tFutures[40]\n1.2\t2010-08-19[38]\tProtocols[41]\n1.3\t2011-09-23[38]\tEnhanced primitive support[42]\n1.4\t2012-04-15[38]\tReader literals\n1.5\t2013-03-01[38]\tReducers\n1.6\t2014-03-25[43]\tJava API, improved hashing algorithms\n1.7\t2015-06-30[24]\tTransducers, reader conditionals\n1.8\t2016-01-19[44]\tAdditional string functions, direct linking, socket server\n1.9\tfuture\nLatest versionFuture release\nClojure runs on the Java virtual machine and as a result integrates with Java and fully supports calling Java code from Clojure,[45] and Clojure code can be called from Java also.[46] The community uses Leiningen[47] for project automation, providing support for Maven integration. Leiningen handles project package management and dependencies and is configured using Clojure syntax.[47]\n\nLike most other Lisps, Clojure's syntax is built on S-expressions that are first parsed into data structures by a reader before being compiled.[48] Clojure's reader supports literal syntax for maps, sets and vectors in addition to lists, and these are compiled to the mentioned structures directly.[48] Clojure is a Lisp-1 and is not intended to be code-compatible with other dialects of Lisp, since it uses its own set of data structures incompatible with other Lisps.[48][49]\n\nAs a Lisp dialect, Clojure supports functions as first-class objects, a read–eval–print loop (REPL), and a macro system.[50] Clojure's macro system is very similar to that in Common Lisp with the exception that Clojure's version of the backquote (called \"syntax quote\") qualifies symbols with their namespace. This helps prevent unintended name capture, as binding to namespace-qualified names is forbidden. It is possible to force a capturing macro expansion, but it must be done explicitly. Clojure does not allow user-defined reader macros, but the reader supports a more constrained form of syntactic extension.[51] Clojure supports multimethods[52] and for interface-like abstractions has a protocol[53] based polymorphism and data type system using records,[54] providing high-performance and dynamic polymorphism designed to avoid the expression problem.\n\nClojure has support for lazy sequences and encourages the principle of immutability and persistent data structures. As a functional language, emphasis is placed on recursion and higher-order functions instead of side-effect-based looping. Automatic tail call optimization is not supported as the JVM does not support it natively;[55] it is possible to do so explicitly by using the recur keyword.[56] For parallel and concurrent programming Clojure provides software transactional memory[57] a reactive agent system,[58] and channel-based concurrent programming.[59]\n\nRecently Clojure introduced reader conditionals by allowing the embedding of Clojure and ClojureScript code in the same namespace.[24][60] Transducers have been added as a way for composing transformations. Transducers enable higher-order functions such as map and fold to generalize over any source of input data, as traditionally these functions operate on sequences, transducers allow them to work on channels and let the user define their own models for transduction.[61][62][63]\n\nPlatforms and popularity[edit]\nThe primary platform of Clojure is the JVM,[8][45] but other target implementations exist. The most notable of these are ClojureScript,[10] which compiles to JavaScript, and ClojureCLR,[64] a full port to the Common Language Runtime, interoperable with the .NET ecosystem. A survey of the Clojure community with 1,060 respondents conducted in 2013[65] found that 47% of respondents used both Clojure and ClojureScript when working with Clojure. In 2014 this number had increased to 55%,[66] in 2015, based on 2,445 respondents, to 66%.[67] Popular ClojureScript projects include implementations of the React library such as Reagent and Om.[68]\n\nClojure has also been used for creative computing, including visual art, music, games, and poetry.[69]\n\nVariant implementations of the Clojure language have been developed for platforms other than the above:\n\nlas3r,[70] a subset of Clojure that runs on the ActionScript Virtual Machine (the Adobe Flash Player platform)\nclojure-py,[71] Clojure in pure Python\nrouge,[72] Clojure atop YARV in Ruby\nCljPerl,[73] Clojure atop Perl\nPixie, Clojure-inspired Lisp dialect written in RPython\nExamples[edit]\nHello world:\n\n(println \"Hello world!\")\nDefining a function:\n\n(defn square [x]\n  (* x x))\nGUI \"Hello world\" by calling the Java Swing library:\n\n(javax.swing.JOptionPane/showMessageDialog nil \"Hello World\")\nUsing Unicode (Hello 世 (\"World\") using the CJK code point for that word):\n\n(println (str \"Hello, \" \\u4e16)) ; to the console\n(javax.swing.JOptionPane/showMessageDialog nil (str \"Hello, \" \\u4e16 \"!\")) ; using Java GUI\nA thread-safe generator of unique serial numbers (though, like many other Lisp dialects, Clojure has a built-in gensym function that it uses internally):\n\n(let [i (atom 0)]\n  (defn generate-unique-id\n    \"Returns a distinct numeric ID for each call.\"\n    []\n    (swap! i inc)))\nAn anonymous subclass of java.io.Writer that doesn't write to anything, and a macro using it to silence all prints within it:\n\n(def bit-bucket-writer\n  (proxy [java.io.Writer] []\n    (write [buf] nil)\n    (close []    nil)\n    (flush []    nil)))\n\n(defmacro noprint\n  \"Evaluates the given expressions with all printing to *out* silenced.\"\n  [& forms]\n  `(binding [*out* bit-bucket-writer]\n     ~@forms))\n\n(noprint\n  (println \"Hello, nobody!\"))\n10 threads manipulating one shared data structure, which consists of 100 vectors each one containing 10 (initially sequential) unique numbers. Each thread then repeatedly selects two random positions in two random vectors and swaps them. All changes to the vectors occur in transactions by making use of Clojure's software transactional memory system.\n\n(defn run [nvecs nitems nthreads niters]\n  (let [vec-refs (->> (range (* nvecs nitems)) (partition nitems) (map (comp ref vec)) vec)\n        swap #(let [v1 (rand-int nvecs)\n                    v2 (rand-int nvecs)\n                    i1 (rand-int nitems)\n                    i2 (rand-int nitems)]\n                (dosync\n                 (let [tmp (nth @(vec-refs v1) i1)]\n                   (alter (vec-refs v1) assoc i1 (nth @(vec-refs v2) i2))\n                   (alter (vec-refs v2) assoc i2 tmp))))\n        report #(let [derefed (map deref vec-refs)]\n                  (prn derefed)\n                  (println \"Distinct:\" (->> derefed (apply concat) distinct count)))]\n    (report)\n    (dorun (apply pcalls (repeat nthreads #(dotimes [_ niters] (swap)))))\n    (report)))\n\n(run 100 10 10 100000)\nOutput of prior example:\n\n([0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] ...\n[990 991 992 993 994 995 996 997 998 999])\nDistinct: 1000\n([382 318 466 963 619 22 21 273 45 596] [808 639 804 471 394 904 952 75 289 778] ...\n[484 216 622 139 651 592 379 228 242 355])\nDistinct: 1000",
          "type": "functional",
          "plid": 13
        },
        {
          "name": "Erlang",
          "details": "Erlang (/ˈɜːrlæŋ/ er-lang) is a general-purpose, concurrent, functional programming language. It is also a garbage-collected runtime system. The sequential subset of Erlang supports eager evaluation, single assignment, and dynamic typing. Erlang is known for its designs that are well suited for systems with the following characteristics:\n\nDistributed\nFault-tolerant\nSoft real-time,\nHighly available, non-stop applications\nHot swapping, where code can be changed without stopping a system.[3]\nIt was originally a proprietary language within Ericsson, developed by Joe Armstrong, Robert Virding and Mike Williams in 1986,[4] but was released as open source in 1998.[5][6] Erlang, along with OTP, a collection of middleware and libraries in Erlang, are now supported and maintained by the OTP product unit at Ericsson and have been widely referred to as Erlang/OTP.\n\nContents  [hide] \n1\tHistory\n1.1\tErlang Worldview\n1.2\tUsage\n2\tFunctional programming examples\n3\tData types\n4\tConcurrency and distribution orientation\n5\tImplementation\n6\tHot code loading and modules\n7\tDistribution\n8\tErlang in Industry\n8.1\tSoftware projects written in Erlang\n8.2\tCompanies using Erlang\n9\tVariants\n10\tReferences\n11\tFurther reading\n12\tExternal links\nHistory[edit]\nThe name \"Erlang\", attributed to Bjarne Däcker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang or the ubiquitous use of the unit named for him, and (initially at least) simultaneously as a syllabic abbreviation of \"Ericsson Language\".[4][7]\n\nErlang was designed with the aim of improving the development of telephony applications. The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. By 1988 Erlang had proven that it was suitable for prototyping telephone exchanges, but the Prolog interpreter was far too slow. One group within Ericsson estimated that it would need to be 40 times faster in order to be suitable for production use. In 1992 work began on the BEAM virtual machine which compiles Erlang to C using a mix of natively compiled code and threaded code to strike a balance between performance and disk space.[8] According to Armstrong, the language went from lab product to real applications following the collapse of the next-generation AXE exchange named AXE-N in 1995. As a result, Erlang was chosen for the next ATM exchange AXD.[4]\n\nIn 1998 Ericsson announced the AXD301 switch, containing over a million lines of Erlang and reported to achieve a high availability of nine \"9\"s.[9] Shortly thereafter, Ericsson Radio Systems banned the in-house use of Erlang for new products, citing a preference for non-proprietary languages. The ban caused Armstrong and others to leave Ericsson.[10] The implementation was open-sourced at the end of the year.[4] Ericsson eventually lifted the ban; it re-hired Armstrong in 2004.[10]\n\nIn 2006, native symmetric multiprocessing support was added to the runtime system and virtual machine.[4]\n\nErlang Worldview[edit]\nThe Erlang view of the world, as Joe Armstrong, co-inventor of Erlang summarized in his PhD thesis:[11]\n\nEverything is a process.\nProcesses are strongly isolated.\nProcess creation and destruction is a lightweight operation.\nMessage passing is the only way for processes to interact.\nProcesses have unique names.\nIf you know the name of a process you can send it a message.\nProcesses share no resources.\nError handling is non-local.\nProcesses do what they are supposed to do or fail.\nJoe Armstrong pointed out in an interview with Rackspace in 2013:[12] “If Java is the right one to run anywhere, then Erlang is the right one to run forever.”\n\nUsage[edit]\nErlang has now been adopted by companies worldwide, including Nortel and T-Mobile. Erlang is used in Ericsson’s support nodes, and in GPRS, 3G and LTE mobile networks worldwide.[13]\n\nAs Tim Bray, director of Web Technologies at Sun Microsystems, expressed in his keynote at OSCON in July 2008:\n\nIf somebody came to me and wanted to pay me a lot of money to build a large scale message handling system that really had to be up all the time, could never afford to go down for years at a time, I would unhesitatingly choose Erlang to build it in.\n\nFunctional programming examples[edit]\nAn Erlang function that uses recursion to count to ten:[14]\n\n1 -module(count_to_ten).\n2 -export([count_to_ten/0]).\n3  \n4 count_to_ten() -> do_count(0).\n5  \n6 do_count(10) -> 10;\n7 do_count(code) -> do_count(code + 1).\nA factorial algorithm implemented in Erlang:\n\n-module(fact).    % This is the file 'fact.erl', the module and the filename must match\n-export([fac/1]). % This exports the function 'fac' of arity 1 (1 parameter, no type, no name)\n\nfac(0) -> 1; % If 0, then return 1, otherwise (note the semicolon ; meaning 'else')\nfac(N) when N > 0, is_integer(N) -> N * fac(N-1).\n% Recursively determine, then return the result\n% (note the period . meaning 'endif' or 'function end')\n%% This function will crash if anything other than a nonnegative integer is given.\n%% It illustrates the \"Let it crash\" philosophy of Erlang.\nA Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. This algorithm is rather slow.[15]):\n\n-module(fib).    % This is the file 'fib.erl', the module and the filename must match\n-export([fib/1]). % This exports the function 'fib' of arity 1\n\nfib(1) -> 1; % If 1, then return 1, otherwise (note the semicolon ; meaning 'else')\nfib(2) -> 1; % If 2, then return 1, otherwise\nfib(N) -> fib(N - 2) + fib(N - 1).\nQuicksort in Erlang, using list comprehension:[16]\n\n%% qsort:qsort(List)\n%% Sort a list of items\n-module(qsort).     % This is the file 'qsort.erl'\n-export([qsort/1]). % A function 'qsort' with 1 parameter is exported (no type, no name)\n\nqsort([]) -> []; % If the list [] is empty, return an empty list (nothing to sort)\nqsort([Pivot|Rest]) ->\n    % Compose recursively a list with 'Front' for all elements that should be before 'Pivot'\n    % then 'Pivot' then 'Back' for all elements that should be after 'Pivot'\n    qsort([Front || Front <- Rest, Front < Pivot]) ++ \n    [Pivot] ++\n    qsort([Back || Back <- Rest, Back >= Pivot]).\nThe above example recursively invokes the function qsort until nothing remains to be sorted. The expression [Front || Front <- Rest, Front < Pivot] is a list comprehension, meaning \"Construct a list of elements Front such that Front is a member of Rest, and Front is less than Pivot.\" ++ is the list concatenation operator.\n\nA comparison function can be used for more complicated structures for the sake of readability.\n\nThe following code would sort lists according to length:\n\n% This is file 'listsort.erl' (the compiler is made this way)\n-module(listsort).\n% Export 'by_length' with 1 parameter (don't care about the type and name)\n-export([by_length/1]).\n\nby_length(Lists) -> % Use 'qsort/2' and provides an anonymous function as a parameter\n   qsort(Lists, fun(A,B) -> length(A) < length(B) end).\n\nqsort([], _)-> []; % If list is empty, return an empty list (ignore the second parameter)\nqsort([Pivot|Rest], Smaller) ->\n    % Partition list with 'Smaller' elements in front of 'Pivot' and not-'Smaller' elements\n    % after 'Pivot' and sort the sublists.\n    qsort([X || X <- Rest, Smaller(X,Pivot)], Smaller)\n    ++ [Pivot] ++\n    qsort([Y || Y <- Rest, not(Smaller(Y, Pivot))], Smaller).\nHere again, a Pivot is taken from the first parameter given to qsort() and the rest of Lists is named Rest. Note that the expression\n\n[X || X <- Rest, Smaller(X,Pivot)]\nis no different in form from\n\n[Front || Front <- Rest, Front < Pivot]\n(in the previous example) except for the use of a comparison function in the last part, saying \"Construct a list of elements X such that X is a member of Rest, and Smaller is true\", with Smaller being defined earlier as\n\nfun(A,B) -> length(A) < length(B) end\nNote also that the anonymous function is named Smaller in the parameter list of the second definition of qsort so that it can be referenced by that name within that function. It is not named in the first definition of qsort, which deals with the base case of an empty list and thus has no need of this function, let alone a name for it.\n\nData types[edit]\nErlang has eight primitive data types:\n\nIntegers\nIntegers are written as sequences of decimal digits, for example, 12, 12375 and -23427 are integers. Integer arithmetic is exact and only limited by available memory on the machine. (This is called arbitrary-precision arithmetic.)\nAtoms\nAtoms are used within a program to denote distinguished values. They are written as strings of consecutive alphanumeric characters, the first character being lowercase. Atoms can contain any character if they are enclosed within single quotes and an escape convention exists which allows any character to be used within an atom.\nFloats\nFloating point numbers use the IEEE 754 64-bit representation.\nReferences\nReferences are globally unique symbols whose only property is that they can be compared for equality. They are created by evaluating the Erlang primitive make_ref().\nBinaries\nA binary is a sequence of bytes. Binaries provide a space-efficient way of storing binary data. Erlang primitives exist for composing and decomposing binaries and for efficient input/output of binaries.\nPids\nPid is short for process identifier – a Pid is created by the Erlang primitive spawn(...) Pids are references to Erlang processes.\nPorts\nPorts are used to communicate with the external world. Ports are created with the built-in function open_port. Messages can be sent to and received from ports, but these messages must obey the so-called \"port protocol.\"\nFuns\nFuns are function closures. Funs are created by expressions of the form: fun(...) -> ... end.\nAnd three compound data types:\n\nTuples\nTuples are containers for a fixed number of Erlang data types. The syntax {D1,D2,...,Dn} denotes a tuple whose arguments are D1, D2, ... Dn. The arguments can be primitive data types or compound data types. Any element of a tuple can be accessed in constant time.\nLists\nLists are containers for a variable number of Erlang data types. The syntax [Dh|Dt] denotes a list whose first element is Dh, and whose remaining elements are the list Dt. The syntax [] denotes an empty list. The syntax [D1,D2,..,Dn] is short for [D1|[D2|..|[Dn|[]]]]. The first element of a list can be accessed in constant time. The first element of a list is called the head of the list. The remainder of a list when its head has been removed is called the tail of the list.\nMaps\nMaps contain a variable number of key-value associations. The syntax is#{Key1=>Value1,...,KeyN=>ValueN}.\nTwo forms of syntactic sugar are provided:\n\nStrings\nStrings are written as doubly quoted lists of characters. This is syntactic sugar for a list of the integer ASCII codes for the characters in the string. Thus, for example, the string \"cat\" is shorthand for [99,97,116]. It has partial support for Unicode strings.[17]\nRecords\nRecords provide a convenient way for associating a tag with each of the elements in a tuple. This allows one to refer to an element of a tuple by name and not by position. A pre-compiler takes the record definition and replaces it with the appropriate tuple reference.\nErlang has no method of defining classes, although there are external libraries available.[18]\n\nConcurrency and distribution orientation[edit]\nErlang's main strength is support for concurrency. It has a small but powerful set of primitives to create processes and communicate among them. Erlang is conceptually similar to the occam programming language, though it recasts the ideas of communicating sequential processes (CSP) in a functional framework and uses asynchronous message passing.[19] Processes are the primary means to structure an Erlang application. They are neither operating system processes nor operating system threads, but lightweight processes that are scheduled by Erlang's BEAM VM. Like operating system processes (but unlike operating system threads), they share no state with each other. The estimated minimal overhead for each is 300 words.[20] Thus, many processes can be created without degrading performance. A benchmark with 20 million processes has been successfully performed.[21] Erlang has supported symmetric multiprocessing since release R11B of May 2006.\n\nWhile threads require external library support in most languages, Erlang provides language-level features for creating and managing processes with the aim of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using message passing instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM[22]).\n\nInter-process communication works via a shared-nothing asynchronous message passing system: every process has a \"mailbox\", a queue of messages that have been sent by other processes and not yet consumed. A process uses the receive primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.\n\nThe code example below shows the built-in support for distributed processes:\n\n % Create a process and invoke the function web:start_server(Port, MaxConnections)\n ServerProcess = spawn(web, start_server, [Port, MaxConnections]),\n\n % Create a remote process and invoke the function\n % web:start_server(Port, MaxConnections) on machine RemoteNode\n RemoteProcess = spawn(RemoteNode, web, start_server, [Port, MaxConnections]),\n\n % Send a message to ServerProcess (asynchronously). The message consists of a tuple\n % with the atom \"pause\" and the number \"10\".\n ServerProcess ! {pause, 10},\n\n % Receive messages sent to this process\n receive\n         a_message -> do_something;\n         {data, DataContent} -> handle(DataContent);\n         {hello, Text} -> io:format(\"Got hello message: ~s\", [Text]);\n         {goodbye, Text} -> io:format(\"Got goodbye message: ~s\", [Text])\n end.\nAs the example shows, processes may be created on remote nodes, and communication with them is transparent in the sense that communication with remote processes works exactly as communication with local processes.\n\nConcurrency supports the primary method of error-handling in Erlang. When a process crashes, it neatly exits and sends a message to the controlling process which can then take action, such as for instance starting a new process that takes over the old process's task.[23][24]\n\nImplementation[edit]\nThe Ericsson Erlang implementation loads virtual machine bytecode which is converted to threaded code at load time. It also includes a native code compiler on most platforms, developed by the High Performance Erlang Project (HiPE) at Uppsala University. Since October 2001 the HiPE system is fully integrated in Ericsson's Open Source Erlang/OTP system.[25] It also supports interpreting, directly from source code via abstract syntax tree, via script as of R11B-5 release of Erlang.\n\nHot code loading and modules[edit]\nErlang supports language-level Dynamic Software Updating. To implement this, code is loaded and managed as \"module\" units; the module is a compilation unit. The system can keep two versions of a module in memory at the same time, and processes can concurrently run code from each. The versions are referred to as the \"new\" and the \"old\" version. A process will not move into the new version until it makes an external call to its module.\n\nAn example of the mechanism of hot code loading:\n\n  %% A process whose only job is to keep a counter.\n  %% First version\n  -module(counter).\n  -export([start/0, codeswitch/1]).\n\n  start() -> loop(0).\n\n  loop(Sum) ->\n    receive\n       {increment, Count} ->\n          loop(Sum+Count);\n       {counter, Pid} ->\n          Pid ! {counter, Sum},\n          loop(Sum);\n       code_switch ->\n          ?MODULE:codeswitch(Sum)\n          % Force the use of 'codeswitch/1' from the latest MODULE version\n    end.\n\n  codeswitch(Sum) -> loop(Sum).\nFor the second version, we add the possibility to reset the count to zero.\n\n  %% Second version\n  -module(counter).\n  -export([start/0, codeswitch/1]).\n\n  start() -> loop(0).\n\n  loop(Sum) ->\n    receive\n       {increment, Count} ->\n          loop(Sum+Count);\n       reset ->\n          loop(0);\n       {counter, Pid} ->\n          Pid ! {counter, Sum},\n          loop(Sum);\n       code_switch ->\n          ?MODULE:codeswitch(Sum)\n    end.\n\n  codeswitch(Sum) -> loop(Sum).\nOnly when receiving a message consisting of the atom 'code_switch' will the loop execute an external call to codeswitch/1 (?MODULE is a preprocessor macro for the current module). If there is a new version of the \"counter\" module in memory, then its codeswitch/1 function will be called. The practice of having a specific entry-point into a new version allows the programmer to transform state to what is required in the newer version. In our example we keep the state as an integer.\n\nIn practice, systems are built up using design principles from the Open Telecom Platform which leads to more code upgradable designs. Successful hot code loading is a tricky subject; Code needs to be written with care to make use of Erlang's facilities.\n\nDistribution[edit]\nIn 1998, Ericsson released Erlang as open source to ensure its independence from a single vendor and to increase awareness of the language. Erlang, together with libraries and the real-time distributed database Mnesia, forms the Open Telecom Platform (OTP) collection of libraries. Ericsson and a few other companies offer commercial support for Erlang.\n\nSince the open source release, Erlang has been used by several firms worldwide, including Nortel and T-Mobile.[26] Although Erlang was designed to fill a niche and has remained an obscure language for most of its existence, its popularity is growing due to demand for concurrent services.[27][28] Erlang has found some use in fielding MMORPG servers.[29]\n\nErlang in Industry[edit]\nSoftware projects written in Erlang[edit]\nConfiguration management:\nChef (software), for which the core API server, originally written in Ruby, was completely re-written in version 11 in Erlang[30]\nContent Management System:\nZotonic, a content management system and web framework written in Erlang\nDistributed databases:\nCloudant, a database service based on the company's fork of CouchDB, BigCouch\nCouchDB, a document-based database that uses MapReduce\nCouchbase Server (née Membase), database management system optimized for storing data behind interactive web applications[31]\nMnesia, a distributed database\nRiak, a distributed database\nSimpleDB, a distributed database that is part of Amazon Web Services[32]\nMessage broker:\nRabbitMQ, an implementation of Advanced Message Queuing Protocol (AMQP)\nOnline messaging:\nejabberd, an Extensible Messaging and Presence Protocol (XMPP) instant messaging server\nMongooseIM, a massively scalable XMPP platform\nSolution stacks\nLYME (software bundle), to serve dynamic web pages\nLYCE (software bundle), to serve dynamic web pages\nTools\nWings 3D, a 3D subdivision modeler, used to model and texture polygon meshes\nWeb servers:\nCowboy (Erlang), a small, fast, modular HTTP server written in Erlang.\nYaws web server\nCompanies using Erlang[edit]\nCompanies using Erlang in their production systems include:\n\nAmazon.com uses Erlang to implement SimpleDB, providing database services as a part of the Amazon Web Services offering.[33]\nAOL's digital advertising business is using Erlang for its real time bidding exchange systems.[34]\nBattlestar Galactica Online game server by Bigpoint\nBet365, the online gambling firm uses the language in production to drive its InPlay betting service, pushing live odds of sporting events to millions of customers in near real-time.[35]\nCall of Duty server core[36]\nCisco acquired Tail-f Systems, a leading provider of multi-vendor network service orchestration solutions for traditional and virtualized networks.[37]\nDiscord[38]\nDNSimple, a DNS provider that uses Erlang to run DNS servers in a globally distributed Anycast network, managing billions of requests per day.\nEricsson uses Erlang in its support nodes, used in GPRS, 3G and LTE mobile networks worldwide.[39][40]\nFacebook uses Erlang to power the backend of its chat service, handling more than 200 million active users.[41] It can be observed in some of its HTTP response headers.[citation needed]\nFacebook Chat system was running on ejabberd based servers[42][43][44]\nGitHub, a web-based hosting service for software development projects that use the Git version control system. Erlang is used for RPC proxies to ruby processes.[45]\nGoldman Sachs, high-frequency trading programs\"ØMQ: Mission Accomplished\". July 2016.\nHuffington Post uses Erlang for its commenting system on HuffPost Live.[46]\nIssuu, an online digital publisher[47]\nKlarna, a Swedish e-commerce company, has been using Erlang to handle 9 million customers and 50 million transaction since 2005.[48]\nLeague of Legends chat system by Riot Games, based on ejabberd\nLinden Lab uses Erlang in its games.[49]\nMachine Zone, a developer of Free-to-play games, uses Erlang in Game of War: Fire Age.[50]\nSmarkets, sports betting exchange\nTuenti chat is based on ejabberd[51]\nTwitterfall, a service to view trends and patterns from Twitter[52][53]\nRackspace uses Erlang in some of its internal applications to manage networking devices.[54]\nRakuten uses Erlang for its distributed file system.[55]\nVendetta Online Naos game server[56]\nWorld of Tanks uses Erlang for message delivery and communication between game players.[57]\nWhatsApp uses Erlang to run messaging servers, achieving up to 2 million connected users per server.[58][59][60][61]\nWhisper, an anonymous social network on mobile[62]\nYahoo! uses it in its social bookmarking service, Delicious, which has more than 5 million users and 150 million bookmarked URLs.[63]\nVariants[edit]\nElixir: a functional, concurrent, general-purpose programming language that runs on the Erlang Virtual Machine (BEAM).\nLisp Flavored Erlang: a LISP based programming language that runs on the Erlang Virtual Machine (BEAM).",
          "type": "compiled",
          "plid": 14
        },
        {
          "name": "Go",
          "details": "Go (often referred to as golang) is a free and open source[12] programming language created at Google[13] in 2007 by Robert Griesemer, Rob Pike, and Ken Thompson.[10] It is a compiled, statically typed language in the tradition of Algol and C, with garbage collection, limited structural typing,[3] memory safety features and CSP-style concurrent programming features added.[14]\n\nThe language was announced in November 2009; it is used in some of Google's production systems,[15] as well as by other firms. Two major implementations exist: Google's Go compiler, \"gc\", is developed as open source software and targets various platforms including Linux, OS X, Windows, various BSD and Unix versions, and since 2015 also mobile devices, including smartphones.[16] A second compiler, gccgo, is a GCC frontend.[17][18] The \"gc\" toolchain is self-hosting since version 1.5.[19]\n\nContents  [hide] \n1\tHistory\n2\tLanguage design\n2.1\tCriticism\n2.2\tSyntax\n2.3\tTypes\n2.3.1\tInterface system\n2.4\tPackage system\n2.5\tConcurrency: goroutines and channels\n2.5.1\tSuitability for parallel programming\n2.5.2\tLack of race condition safety\n2.6\tOmissions\n3\tConventions and code style\n4\tLanguage tools\n5\tExamples\n5.1\tHello world\n5.2\tConcurrency example\n6\tProjects using Go\n7\tReception\n8\tNaming dispute\n9\tSee also\n10\tNotes\n11\tReferences\n12\tExternal links\n12.1\tCommunity and conferences\nHistory[edit]\nGo originated as an experiment by Google engineers Robert Griesemer, Rob Pike, and Ken Thompson to design a new programming language that would resolve common criticisms of other languages while maintaining their positive characteristics.[20] The new language was to:\n\nbe statically typed, scalable to large systems (as Java and C++);\nbe productive and readable, without too many mandatory keywords and repetition[21] (\"light on the page\" like dynamic languages);\nnot require tooling, but support it well,\nsupport networking and multiprocessing.\nIn later interviews, all three of the language designers cited their shared dislike of C++'s complexity as a primary motivation for designing a new language.[22][23][24]\n\nGo 1.0 was released in March 2012.[25]\n\nGo 1.7 added \"one tiny language change\"[26] and one port to macOS 10.12 Sierra plus some experimental ports, e.g. for Linux on z Systems (linux/s390x). Some library changes apply, and e.g. Unicode 9.0 is now supported.\n\nLanguage design[edit]\nGo is recognizably in the tradition of C, but makes many changes to improve conciseness, simplicity, and safety. The following is a brief overview of the features which define Go:\n\nA syntax and environment adopting patterns more common in dynamic languages:[27]\nOptional concise variable declaration and initialization through type inference (x := 0 not var x int = 0;).\nFast compilation times.[28]\nRemote package management (go get)[29] and online package documentation.[30]\nDistinctive approaches to particular problems:\nBuilt-in concurrency primitives: light-weight processes (goroutines), channels, and the select statement.\nAn interface system in place of virtual inheritance, and type embedding instead of non-virtual inheritance.\nA toolchain that, by default, produces statically linked native binaries without external dependencies.\nA desire to keep the language specification simple enough to hold in a programmer's head,[31] in part by omitting features common to similar languages.\nCriticism[edit]\nGo critics assert that:\n\nlack of compile-time generics leads to code duplication, metaprogramming cannot be statically checked[32][33] and standard library cannot offer generic algorithms[34]\nlack of language extensibility (through, for instance, operator overloading) makes certain tasks more verbose[32][35]\nthe type system's lack of Hindley-Milner typing inhibiting safety and/or expressiveness[36][37]\nthe pauses and overhead of garbage collection (GC) limit Go's use in systems programming compared to languages with manual memory management.[32][36]\nThe language designers argue that these trade-offs are important to Go's success,[38] and explain some particular decisions at length,[39] though they do express openness to adding some form of generic programming in the future, and to pragmatic improvements in areas like standardizing ways to apply code generation.[40] Regarding GC, Go defenders point to pause-time reduction in later versions[41] (e.g. Go 1.6), while acknowledging their GC algorithm is not hard real-time.\n\nSyntax[edit]\nGo's syntax includes changes from C aimed at keeping code concise and readable. A combined declaration/initialization operator was introduced that allows the programmer to write i := 3 or s := \"some words\", without specifying the types of variables. This contrasts with C's int i = 3; and const char *s = \"some words\";. Semicolons still terminate statements, but are implicit when they would occur at the end of a line. Functions may return multiple values, and returning a result, err pair is the conventional way a function indicates an error to its caller in Go.[a] Go adds literal syntaxes for initializing struct parameters by name, and for initializing maps and slices. As an alternative to C's three-statement for loop, Go's range expressions allow concise iteration over arrays, slices, strings, maps and channels.\n\nTypes[edit]\nGo has a number of built-in types, including numeric ones (byte, int64, float32, etc.), booleans, and character strings (string). Strings are immutable; built-in operators and keywords (rather than functions) provide concatenation, comparison, and UTF-8 encoding and decoding.[42] Record types can be defined with the struct keyword.\n\nFor each type T and each non-negative integer constant n, there is an array type denoted [n]T; arrays of differing lengths are thus of different types. Dynamic arrays are available as \"slices\", denoted []T for some type T. These have a length and a capacity specifying when new memory needs to be allocated to expand the array. Several slices may share their underlying memory.[43][44][45]\n\nPointers are available for all types, and the pointer-to-T type is denoted *T. Address-taking and indirection use the & and * operators as in C, or happen implicitly through the method call or attribute access syntax.[46] There is no pointer arithmetic, except via the special unsafe.Pointer type in the standard library.\n\nFor a pair of types K, V, the type map[K]V is the type of hash tables mapping type-K keys to type-V values. Hash tables are built into the language, with special syntax and built-in functions. chan T is a channel that allows sending values of type T between concurrent Go processes.\n\nAside from its support for interfaces, Go's type system is nominal: the type keyword can be used to define a new named type, which is distinct from other named types that have the same layout (in the case of a struct, the same members in the same order). Some conversions between types (e.g., between the various integer types) are pre-defined and adding a new type may define additional conversions, but conversions between named types must always be invoked explicitly.[47] For example, the type keyword can be used to define a type for IPv4 addresses, which are 32-bit unsigned integers:\n\ntype ipv4addr uint32\nWith this type definition, ipv4addr(x) interprets the uint32 value x as an IP address. Simply assigning x to a variable of type ipv4addr is a type error.\n\nConstant expressions may be either typed or \"untyped\"; they are given a type when assigned to a typed variable, if the value they represent passes a compile-time check.[48]\n\nFunction types are indicated by the func keyword; they take zero or more parameters and return zero or more values, all of which are typed. The parameter and return values determine a function type; thus, func(string, int32) (int, error) is the type of functions that take a string and a 32-bit signed integer, and return a signed integer (of default width) and a value of the built-in interface type error.\n\nAny named type has a method set associated with it. The IP address example above can be extended with a method for converting an address to a human-readable representation, viz.,\n\n// Is this the zero broadcast address 255.255.255.255?\nfunc (addr ipv4addr) ZeroBroadcast() bool {\n    return addr == 0xFFFFFFFF\n}\nDue to nominal typing, this method definition adds a method to ipv4addr, but not on uint32. While methods have special definition and call syntax, there is no distinct method type.[49]\n\nInterface system[edit]\nGo provides two features that replace class inheritance.\n\nThe first is embedding, which can be viewed as an automated form of composition[50] or delegation.[51]:255\n\nThe second are its interfaces, which provides runtime polymorphism.[52]:266 Interfaces provide a limited form of structural typing in the otherwise nominal type system of Go. Any type that implements all methods of an interface conforms to that interface. Go interfaces were designed after protocols from the Smalltalk programming language.[53] Multiple sources use the term duck typing when describing Go interface.[54][55] Although the term duck typing is not precisely defined and therefore not wrong, it usually implies that type conformance is not statically checked. Since conformance to a Go interface is checked statically by the Go compiler (except when performing a type assertion), the Go authors prefer to use the term structural typing.\n\nAn interface specifies a set of types by listing required methods and their types, and is satisfied by any type that has the required methods. Implementing types do not need to specify their implementing of interfaces, so if Shape, Square and Circle are defined as:\n\nimport \"math\"\n\ntype Shape interface {\n    Area() float64\n}\n\ntype Square struct { // Note: no \"implements\" declaration\n    side float64\n}\n\nfunc (sq Square) Area() float64 { return sq.side * sq.side }\n\ntype Circle struct { // No \"implements\" declaration here either\n    radius float64\n}\n\nfunc (c Circle) Area() float64 { return math.Pi * math.Pow(c.radius, 2) }\nthen both Square and Circle are implicitly a Shape and can be assigned to a Shape-typed variable.[52]:263–268 In formal language, Go's interface system provides structural rather than nominal typing. Interfaces can embed other interfaces with the effect of creating a combined interface that is satisfied by exactly the types that implement the embedded interface and any methods that the newly defined interface adds.[52]:270\n\nThe Go standard library uses interfaces to provide genericity in several places, including the input/output system that is based on the concepts of Reader and Writer.[52]:282–283\n\nBesides calling methods via interfaces, Go allows converting interface values to other types with a run-time type check. The language constructs to do so are the type assertion,[56] which checks against a single potential type, and the type switch,[57] which checks against multiple types.\n\nThe empty interface interface{} is an important corner case because it can refer to an item of any concrete type. It is similar to the Object class in Java or C#, but with the difference that the empty interface is satisfied by any type, including built-in types like int (while in Java and C#, an Object variable can only hold instances of reference type).[52]:284 Code using the empty interface cannot simply call methods (or built-in operators) on the referred-to object, but it can store the interface{} value, try to convert it to a more useful type via a type assertion or type switch, or inspect it with Go's reflect package.[58] Because interface{} can refer to any value, it is a limited way to escape the restrictions of static typing, like void* in C but with additional run-time type checks.\n\nInterface values are implemented using pointer to data and a second pointer to run-time type information.[59] Like some other types implemented using pointers in Go, interface values are nil if uninitialized.[60]\n\nPackage system[edit]\nIn Go's package system, each package has a path (e.g., \"compress/bzip2\" or \"golang.org/x/net/html\") and a name (e.g., bzip2 or html). References to other packages' definitions must always be prefixed with the other package's name, and only the capitalized names from other packages are accessible: io.Reader is public but bzip2.reader is not.[61] The go get command can retrieve packages stored in a remote repository such as GitHub.,[62] and developers are encouraged to develop packages inside a base path corresponding to a source repository (such as github.com/user_name/package_name) to reduce the likelihood of name collision with future additions to the standard library or other external libraries.[63]\n\nProposals exist to introduce a proper package management solution for Go similar to Rust's cargo system or Node's npm system.[64]\n\nConcurrency: goroutines and channels[edit]\nThe Go language has built-in facilities, as well as library support, for writing concurrent programs. Concurrency refers not only to CPU parallelism, but also to asynchrony: letting slow operations like a database or network-read run while the program does other work, as is common in event-based servers.[65]\n\nThe primary concurrency construct is the goroutine, a type of light-weight process. A function call prefixed with the go keyword starts a function in a new goroutine. The language specification does not specify how goroutines should be implemented, but current implementations multiplex a Go process's goroutines onto a smaller set of operating system threads, similar to the scheduling performed in Erlang.[66]:10\n\nWhile a standard library package featuring most of the classical concurrency control structures (mutex locks, etc.) is available,[66]:151–152 idiomatic concurrent programs instead prefer channels, which provide send messages between goroutines.[67] Optional buffers store messages in FIFO order[51]:43 and allow sending goroutines to proceed before their messages are received.\n\nChannels are typed, so that a channel of type chan T can only be used to transfer messages of type T. Special syntax is used to operate on them; <-ch is an expression that causes the executing goroutine to block until a value comes in over the channel ch, while ch <- x sends the value x (possibly blocking until another goroutine receives the value). The built-in switch-like select statement can be used to implement non-blocking communication on multiple channels; see below for an example. Go has a memory model describing how goroutines must use channels or other operations to safely share data.\n\nThe existence of channels sets Go apart from actor model-style concurrent languages like Erlang, where messages are addressed directly to actors (corresponding to goroutines); the actor style can be simulated in Go by maintaining a one-to-one correspondence between goroutines and channels, but the language allows multiple goroutines to share a channel, or a single goroutine to send and receive on multiple channels.[66]:147\n\nFrom these tools one can build concurrent constructs like worker pools, pipelines (in which, say, a file is decompressed and parsed as it downloads), background calls with timeout, \"fan-out\" parallel calls to a set of services, and others.[68] Channels have also found uses further from the usual notion of interprocess communication, like serving as a concurrency-safe list of recycled buffers,[69] implementing coroutines (which helped inspire the name goroutine),[70] and implementing iterators.[71]\n\nConcurrency-related structural conventions of Go (channels and alternative channel inputs) are derived from Tony Hoare's communicating sequential processes model. Unlike previous concurrent programming languages such as Occam or Limbo (a language on which Go co-designer Rob Pike worked),[72] Go does not provide any built-in notion of safe or verifiable concurrency.[73] While the communicating-processes model is favored in Go, it is not the only one: all goroutines in a program share a single address space. This means that mutable objects and pointers can be shared between goroutines; see § Lack of race condition safety, below.\n\nSuitability for parallel programming[edit]\nAlthough Go's concurrency features are not aimed primarily at parallel processing,[65] they can be used to program shared memory multi-processor machines. Various studies have been done into the effectiveness of this approach.[74] One of these studies compared the size (in lines of code) and speed of programs written by a seasoned programmer not familiar with the language and corrections to these programs by a Go expert (from Google's development team), doing the same for Chapel, Cilk and Intel TBB. The study found that the non-expert tended to write divide-and-conquer algorithms with one go statement per recursion, while the expert wrote distribute-work-synchronize programs using one goroutine per processor. The expert's programs were usually faster, but also longer.[75]\n\nLack of race condition safety[edit]\nThere are no restrictions on how goroutines access shared data, making race conditions possible. Specifically, unless a program explicitly synchronizes via channels or other means, writes from one goroutine might be partly, entirely, or not at all visible to another, often with no guarantees about ordering of writes.[73] Furthermore, Go's internal data structures like interface values, slice headers, hash tables, and string headers are not immune to race conditions, so type and memory safety can be violated in multithreaded programs that modify shared instances of those types without synchronization.[76][77]\n\nInstead of language support, safe concurrent programming thus relies on conventions; for example, Chisnall recommends an idiom called \"aliases xor mutable\", meaning that passing a mutable value (or pointer) over a channel signals a transfer of ownership over the value to its receiver.[66]:155\n\nOmissions[edit]\nGo deliberately omits certain features common in other languages, including (implementation) inheritance, generic programming, assertions, pointer arithmetic, and implicit type conversions.\n\nOf these language features, the Go authors express an openness to generic programming, explicitly argue against assertions and pointer arithmetic, while defending the choice to omit type inheritance as giving a more useful language, encouraging instead the use of interfaces to achieve dynamic dispatch[b] and composition to reuse code. Composition and delegation are in fact largely automated by struct embedding; according to researchers Schmager et al., this feature \"has many of the drawbacks of inheritance: it affects the public interface of objects, it is not fine-grained (i.e, no method-level control over embedding), methods of embedded objects cannot be hidden, and it is static\", making it \"not obvious\" whether programmers will not overuse it to the extent that programmers in other languages are reputed to overuse inheritance.[50]\n\nRegarding generic programming, some built-in functions are in fact type-generic, but these are treated as special cases; Rob Pike calls this a weakness of the language that may at some point be changed.[43] The Google team that designs the language built at least one compiler for an experimental Go dialect with generics, but did not release it.[78]\n\nAfter initially omitting exceptions, the exception-like panic/recover mechanism was eventually added to the language, which the Go authors advise using for unrecoverable errors such as those that should halt an entire program or server request, or as a shortcut to propagate errors up the stack within a package (but not across package boundaries; there, error returns are the standard API).[79][80][81][82]\n\nConventions and code style[edit]\nThe Go authors put substantial effort into molding the style and design of Go programs:\n\nIndentation, spacing, and other surface-level details of code are automatically standardized by the gofmt tool. golint does additional style checks automatically.\nTools and libraries distributed with Go suggest standard approaches to things like API documentation (godoc[83]), testing (go test), building (go build), package management (go get), and so on.\nGo enforces rules that are recommendations in other languages, for example banning cyclic dependencies, unused variables or imports, and implicit type conversions.\nThe omission of certain features (for example, functional-programming shortcuts like map and C++-style try/finally blocks) tends to encourage a particular explicit, concrete, and imperative programming style.\nOn day one the Go team published a collection of Go idioms, and later also collected code review comments, talks, official blog posts to teach Go style and coding philosophy.\nLanguage tools[edit]\nGo includes the same sort of debugging, testing, and code-vetting tools as many language distributions. The Go distribution includes, among other tools,\n\ngo build, which builds Go binaries using only information in the source files themselves, no separate makefiles\ngo test, for unit testing and microbenchmarks\ngo fmt, for formatting code\ngo get, for retrieving and installing remote packages\ngo vet, a static analyzer looking for potential errors in code\ngo run, a shortcut for building and executing code\ngodoc, for displaying documentation or serving it via HTTP\ngorename, for renaming variables, functions, and so on in a type-safe way\ngo generate, a standard way to invoke code generators\nIt also includes profiling and debugging support, runtime instrumentation (to, for example, track garbage collection pauses), and a race condition tester.\n\nThere is an ecosystem of third-party tools that add to the standard distribution, such as gocode, which enables code autocompletion in many text editors, goimports (by a Go team member), which automatically adds/removes package imports as needed, errcheck, which detects code that might unintentionally ignore errors, and more. Plugins exist to add language support in widely used text editors, and at least one IDE, LiteIDE, is branded as \"a simple, open source, cross-platform Go IDE.\"[84]\n\nExamples[edit]\nHello world[edit]\nHere is a Hello world program in Go:\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello, World\")\n}\nConcurrency example[edit]\nThe following simple program demonstrates Go's concurrency features to implement an asynchronous program. It launches two \"goroutines\" (lightweight threads): one waits for the user to type some text, while the other implements a timeout. The select statement waits for either of these goroutines to send a message to the main routine, and acts on the first message to arrive (example adapted from Chisnall).[66]:152\n\npackage main\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\nfunc readword(ch chan string) {\n    fmt.Println(\"Type a word, then hit Enter.\")\n    var word string\n    fmt.Scanf(\"%s\", &word)\n    ch <- word\n}\n\nfunc timeout(t chan bool) {\n    time.Sleep(5 * time.Second)\n    t <- true\n}\n\nfunc main() {\n    t := make(chan bool)\n    go timeout(t)\n\n    ch := make(chan string)\n    go readword(ch)\n\n    select {\n    case word := <-ch:\n        fmt.Println(\"Received\", word)\n    case <-t:\n        fmt.Println(\"Timeout.\")\n    }\n}\nProjects using Go[edit]\nQuestion book-new.svg\nThis section relies too much on references to primary sources. Please improve this section by adding secondary or tertiary sources. (November 2015) (Learn how and when to remove this template message)\nSome notable open-source applications in Go include:\n\nDocker, a set of tools for deploying Linux containers\nDoozer, a lock service by managed hosting provider Heroku[14]\nEthereum, a shared world computing platform.\nJuju, a service orchestration tool by Canonical, packagers of Ubuntu Linux\nPacker, a tool for creating identical machine images for multiple platforms from a single source configuration\nSnappy, a package manager developed by Canonical for Ubuntu.\nSyncthing, an open-source file synchronization client/server application\nSome notable open-source frameworks using Go:\n\nBeego, high-performance web framework in Go, used for web apps and backend services.[citation needed]\nMartini, package for web applications/services.[citation needed]\nGorilla, a web toolkit for Go.[citation needed]\nOther notable companies and sites using Go (generally together with other languages, not exclusively) include:[85][self-published source?][86]\n\nAeroFS, private cloud filesync appliance provider which migrated some microservices from Java to Go with major memory footprint improvements[87]\nChango, a programmatic advertising company uses Go in its real-time bidding systems.[88]\nCloud Foundry, a platform as a service[citation needed]\nCloudFlare, for their delta-coding proxy Railgun, their distributed DNS service, as well as tools for cryptography, logging, stream processing, and accessing SPDY sites.[89][90]\nCoreOS, a Linux-based operating system that utilizes Docker containers.[91]\nCouchbase, Query and Indexing services within the Couchbase Server[92]\nDropbox, migrated some of their critical components from Python to Go[93]\nGoogle, for many projects, notably including download server dl.google.com[94][95][96]\nMercadoLibre, for several public APIs.[citation needed]\nMongoDB, tools for administering MongoDB instances[97]\nNetflix, for two portions of their server architecture[98]\nNovartis, for an internal inventory system[citation needed]\nPlug.dj, an interactive online social music streaming website.[99]\nReplicated, Docker based PaaS for creating enterprise, installable software.[100]\nSendGrid, a Boulder, Colorado-based transactional email delivery and management service.[101]\nSoundCloud, for \"dozens of systems\"[102]\nSplice, for the entire backend (API and parsers) of their online music collaboration platform.[103]\nThoughtWorks, some tools and applications around continuous delivery and instant messages (CoyIM).[104]\nTwitch.tv, for their IRC-based chat system (migrated from Python).[105]\nUber, for handling high volumes of geofence-based queries.[106]\nZerodha, for realtime peering and streaming of market data[citation needed]\nReception[edit]\nGo's initial release led to much discussion.\n\nThe interface system, and the deliberate omission of inheritance, were praised by Michele Simionato, who likened these language characteristics to those of Standard ML, calling it \"a shame that no popular language has followed [this] particular route in the design space\".[107]\n\nDave Astels at Engine Yard wrote:[108]\n\nGo is extremely easy to dive into. There are a minimal number of fundamental language concepts and the syntax is clean and designed to be clear and unambiguous. Go is still experimental and still a little rough around the edges.\n\nArs Technica interviewed Rob Pike, one of the authors of Go, and asked why a new language was needed. He replied that:[109]\n\nIt wasn't enough to just add features to existing programming languages, because sometimes you can get more in the long run by taking things away. They wanted to start from scratch and rethink everything. ... [But they did not want] to deviate too much from what developers already knew because they wanted to avoid alienating Go's target audience.\n\nGo was named Programming Language of the Year by the TIOBE Programming Community Index in its first year, 2009, for having a larger 12-month increase in popularity (in only 2 months, after its introduction in November) than any other language that year, and reached 13th place by January 2010,[110] surpassing established languages like Pascal. By June of 2015, its ranking had dropped to below 50th in the index, placing it lower than COBOL and Fortran.[111] But as of September 2016, its ranking had surged to 19th, indicating significant growth in popularity and adoption.[112]\n\nRegarding Go, Bruce Eckel has stated:[113]\n\nThe complexity of C++ (even more complexity has been added in the new C++), and the resulting impact on productivity, is no longer justified. All the hoops that the C++ programmer had to jump through in order to use a C-compatible language make no sense anymore -- they're just a waste of time and effort. Go makes much more sense for the class of problems that C++ was originally intended to solve.\n\nA 2011 evaluation of the language and its gc implementation in comparison to C++ (GCC), Java and Scala by a Google engineer found that:\n\nGo offers interesting language features, which also allow for a concise and standardized notation. The compilers for this language are still immature, which reflects in both performance and binary sizes.\n\n— R. Hundt[114]\nThe evaluation got a rebuttal from the Go development team. Ian Lance Taylor, who had improved the Go code for Hundt's paper, had not been aware of the intention to publish his code, and says that his version was \"never intended to be an example of idiomatic or efficient Go\"; Russ Cox then did optimize the Go code, as well as the C++ code, and got the Go code to run slightly faster than C++ and more than an order of magnitude faster than the \"optimized\" code in the paper.[115]\n\nNaming dispute[edit]\nOn 10 November 2009, the day of the general release of the language, Francis McCabe, developer of the Go! programming language (note the exclamation point), requested a name change of Google's language to prevent confusion with his language, which he had spent 10 years developing.[116] McCabe raised concerns that \"the 'big guy' will end up steam-rollering over\" him, and this concern resonated with the more than 120 developers who commented on Google's official issues thread saying they should change the name, with some[117] even saying the issue contradicts Google's motto of: Don't be evil.[118] The issue was closed by a Google developer on 12 October 2010 with the custom status \"Unfortunate\" and with the following comment: \"there are many computing products and services named Go. In the 11 months since our release, there has been minimal confusion of the two languages.\"\n\nSee also[edit]\n\tFree software portal\nComparison of programming languages\nDart, another Google programming language\nUFCS, a way of having 'open methods' in other languages\nNotes[edit]\nJump up ^ Usually, exactly one of the result and error values has a value other than the type's zero value; sometimes both do, as when a read or write can only be partially completed, and sometimes neither, as when a read returns 0 bytes. See Semipredicate problem: Multivalued return.\nJump up ^ Questions \"How do I get dynamic dispatch of methods?\" and \"Why is there no type inheritance?\" in the language FAQ.[10]",
          "type": "compiled",
          "plid": 15
        },
        {
          "name": "Java",
          "details": "Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers \"write once, run anywhere\" (WORA),[15] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[16] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of computer architecture. As of 2016, Java is one of the most popular programming languages in use,[17][18][19][20] particularly for client-server web applications, with a reported 9 million developers.[21] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them.\n\nThe original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licences. As of May 2007, in compliance with the specifications of the Java Community Process, Sun relicensed most of its Java technologies under the GNU General Public License. Others have also developed alternative implementations of these Sun technologies, such as the GNU Compiler for Java (bytecode compiler), GNU Classpath (standard libraries), and IcedTea-Web (browser plugin for applets).\n\nThe latest version is Java 8, which is the only version currently supported for free by Oracle, although earlier versions are supported both by Oracle and other companies on a commercial basis.\n\nContents  [hide] \n1\tHistory\n1.1\tPrinciples\n1.2\tVersions\n2\tPractices\n2.1\tJava platform\n2.1.1\tImplementations\n2.1.2\tPerformance\n2.2\tAutomatic memory management\n3\tSyntax\n4\tExamples\n4.1\t\"Hello, world!\" program\n4.2\tComprehensive example\n5\tSpecial classes\n5.1\tApplet\n5.2\tServlet\n5.3\tJavaServer Pages\n5.4\tSwing application\n5.5\tGenerics\n6\tCriticism\n7\tUse outside of the Java platform\n7.1\tGoogle\n8\tClass libraries\n9\tDocumentation\n10\tEditions\n11\tSee also\n11.1\tComparison of Java with other languages\n12\tNotes\n13\tReferences\n14\tExternal links\nHistory\nSee also: Java (software platform) § History\n\nDuke, the Java mascot\n\nJames Gosling, the creator of Java (2008)\n\nThe TIOBE programming language popularity index graph from 2002 to 2015. Over the course of a decade Java (blue) and C (black) competing for the top position.\nJames Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[22] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[23] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee.[24] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[25]\n\nSun Microsystems released the first public implementation as Java 1.0 in 1995.[26] It promised \"Write Once, Run Anywhere\" (WORA), providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular, while mostly outside of browsers, that wasn't the original plan. In January 2016, Oracle announced that Java runtime environments based on JDK 9 will discontinue the browser plugin.[27] The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[28] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 – 1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.\n\nIn 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[29][30][31] Java remains a de facto standard, controlled through the Java Community Process.[32] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.\n\nOn November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software, (FOSS), under the terms of the GNU General Public License (GPL). On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[33]\n\nSun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an \"evangelist\".[34] Following Oracle Corporation's acquisition of Sun Microsystems in 2009–10, Oracle has described itself as the \"steward of Java technology with a relentless commitment to fostering a community of participation and transparency\".[35] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see Google section below). Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[36] On April 2, 2010, James Gosling resigned from Oracle.[37]\n\nPrinciples\nThere were five primary goals in the creation of the Java language:[16]\n\nIt must be \"simple, object-oriented, and familiar\".\nIt must be \"robust and secure\".\nIt must be \"architecture-neutral and portable\".\nIt must execute with \"high performance\".\nIt must be \"interpreted, threaded, and dynamic\".\nVersions\nMain article: Java version history\nAs of 2015, only Java 8 is supported (\"publicly\"). Major release versions of Java, along with their release dates:\n\nJDK 1.0 (January 21, 1996)\nJDK 1.1 (February 19, 1997)\nJ2SE 1.2 (December 8, 1998)\nJ2SE 1.3 (May 8, 2000)\nJ2SE 1.4 (February 6, 2002)\nJ2SE 5.0 (September 30, 2004)\nJava SE 6 (December 11, 2006)\nJava SE 7 (July 28, 2011)\nJava SE 8 (March 18, 2014)\nPractices\nJava platform\nMain articles: Java (software platform) and Java virtual machine\n\nJava Control Panel, version 7\nOne design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate runtime support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End users commonly use a Java Runtime Environment (JRE) installed on their own machine for standalone Java applications, or in a web browser for Java applets.\n\nStandard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.\n\nThe use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions makes interpreted programs almost always run more slowly than native executables. However, just-in-time (JIT) compilers that compile bytecodes to machine code during runtime were introduced from an early stage. Java itself is platform-independent, and is adapted to the particular platform it is to run on by a Java virtual machine for it, which translates the Java bytecode into the platform's machine language.[38]\n\nImplementations\nSee also: Free Java implementations\nOracle Corporation is the current owner of the official implementation of the Java SE platform, following their acquisition of Sun Microsystems on January 27, 2010. This implementation is based on the original implementation of Java by Sun. The Oracle implementation is available for Microsoft Windows (still works for XP, while only later versions currently \"publicly\" supported), Mac OS X, Linux and Solaris. Because Java lacks any formal standardization recognized by Ecma International, ISO/IEC, ANSI, or other third-party standards organization, the Oracle implementation is the de facto standard.\n\nThe Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger.\n\nOpenJDK is another notable Java SE implementation that is licensed under the GNU GPL. The implementation started when Sun began releasing the Java source code under the GPL. As of Java SE 7, OpenJDK is the official Java reference implementation.\n\nThe goal of Java is to make all implementations of Java compatible. Historically, Sun's trademark license for usage of the Java brand insists that all implementations be \"compatible\". This resulted in a legal dispute with Microsoft after Sun claimed that the Microsoft implementation did not support RMI or JNI and had added platform-specific features of their own. Sun sued in 1997, and in 2001 won a settlement of US$20 million, as well as a court order enforcing the terms of the license from Sun.[39] As a result, Microsoft no longer ships Java with Windows.\n\nPlatform-independent Java is essential to Java EE, and an even more rigorous validation is required to certify an implementation. This environment enables portable server-side applications.\n\nPerformance\nMain article: Java performance\nPrograms written in Java have a reputation for being slower and requiring more memory than those written in C++.[40][41] However, Java programs' execution speed improved significantly with the introduction of just-in-time compilation in 1997/1998 for Java 1.1,[42] the addition of language features supporting better code analysis (such as inner classes, the StringBuilder class, optional assertions, etc.), and optimizations in the Java virtual machine, such as HotSpot becoming the default for Sun's JVM in 2000. With Java 1.5, the performance was improved with the addition of the java.util.concurrent package, including Lock free implementations of the ConcurrentMaps and other multi-core collections, and it was improved further Java 1.6.\n\nSome platforms offer direct hardware support for Java; there are microcontrollers that can run Java in hardware instead of a software Java virtual machine, and ARM based processors can have hardware support for executing Java bytecode through their Jazelle option (while its support is mostly dropped in current implementations of ARM).\n\nAutomatic memory management\nJava uses an automatic garbage collector to manage memory in the object lifecycle. The programmer determines when objects are created, and the Java runtime is responsible for recovering the memory once objects are no longer in use. Once no references to an object remain, the unreachable memory becomes eligible to be freed automatically by the garbage collector. Something similar to a memory leak may still occur if a programmer's code holds a reference to an object that is no longer needed, typically when objects that are no longer needed are stored in containers that are still in use. If methods for a nonexistent object are called, a \"null pointer exception\" is thrown.[43][44]\n\nOne of the ideas behind Java's automatic memory management model is that programmers can be spared the burden of having to perform manual memory management. In some languages, memory for the creation of objects is implicitly allocated on the stack, or explicitly allocated and deallocated from the heap. In the latter case the responsibility of managing memory resides with the programmer. If the program does not deallocate an object, a memory leak occurs. If the program attempts to access or deallocate memory that has already been deallocated, the result is undefined and difficult to predict, and the program is likely to become unstable and/or crash. This can be partially remedied by the use of smart pointers, but these add overhead and complexity. Note that garbage collection does not prevent \"logical\" memory leaks, i.e., those where the memory is still referenced but never used.\n\nGarbage collection may happen at any time. Ideally, it will occur when a program is idle. It is guaranteed to be triggered if there is insufficient free memory on the heap to allocate a new object; this can cause a program to stall momentarily. Explicit memory management is not possible in Java.\n\nJava does not support C/C++ style pointer arithmetic, where object addresses and unsigned integers (usually long integers) can be used interchangeably. This allows the garbage collector to relocate referenced objects and ensures type safety and security.\n\nAs in C++ and some other object-oriented languages, variables of Java's primitive data types are either stored directly in fields (for objects) or on the stack (for methods) rather than on the heap, as is commonly true for non-primitive data types (but see escape analysis). This was a conscious decision by Java's designers for performance reasons.\n\nJava contains multiple types of garbage collectors. By default,[citation needed] HotSpot uses the parallel scavenge garbage collector. However, there are also several other garbage collectors that can be used to manage the heap. For 90% of applications in Java, the Concurrent Mark-Sweep (CMS) garbage collector is sufficient.[45] Oracle aims to replace CMS with the Garbage-First collector (G1).[46]\n\nSyntax\nMain article: Java syntax\nThe syntax of Java is largely influenced by C++. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[16] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, i.e. integers, floating-point numbers, boolean values, and characters, which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as printf() method).\n\nUnlike C++, Java does not support operator overloading[47] or multiple inheritance for classes, though multiple inheritance is supported for interfaces.[48] This simplifies the language and aids in preventing potential errors and anti-pattern design.[citation needed]\n\nJava uses comments similar to those of C++. There are three different styles of comments: a single line style marked with two slashes (//), a multiple line style opened with /* and closed with */, and the Javadoc commenting style opened with /** and closed with */. The Javadoc style of commenting allows the user to run the Javadoc executable to create documentation for the program.\n\nExample:\n\n// This is an example of a single line comment using two slashes\n\n/* This is an example of a multiple line comment using the slash and asterisk.\n This type of comment can be used to hold a lot of information or deactivate\n code, but it is very important to remember to close the comment. */\n\npackage fibsandlies;\nimport java.util.HashMap;\n\n/**\n * This is an example of a Javadoc comment; Javadoc can compile documentation\n * from this text. Javadoc comments must immediately precede the class, method, or field being documented.\n */\npublic class FibCalculator extends Fibonacci implements Calculator {\n    private static Map<Integer, Integer> memoized = new HashMap<Integer, Integer>();\n\n    /*\n     * The main method written as follows is used by the JVM as a starting point for the program.\n     */\n    public static void main(String[] args) {\n        memoized.put(1, 1);\n        memoized.put(2, 1);\n        System.out.println(fibonacci(12)); //Get the 12th Fibonacci number and print to console\n    }\n\n    /**\n     * An example of a method written in Java, wrapped in a class.\n     * Given a non-negative number FIBINDEX, returns\n     * the Nth Fibonacci number, where N equals FIBINDEX.\n     * @param fibIndex The index of the Fibonacci number\n     * @return The Fibonacci number\n     */\n    public static int fibonacci(int fibIndex) {\n        if (memoized.containsKey(fibIndex)) {\n            return memoized.get(fibIndex);\n        } else {\n            int answer = fibonacci(fibIndex - 1) + fibonacci(fibIndex - 2);\n            memoized.put(fibIndex, answer);\n            return answer;\n        }\n    }\n}\nExamples\n\"Hello, world!\" program\nThe traditional \"Hello, world!\" program can be written in Java as:[49]\n\nclass HelloWorldApp {\n    public static void main(String[] args) {\n        System.out.println(\"Hello World!\"); // Prints the string to the console.\n    }\n}\nSource files must be named after the public class they contain, appending the suffix .java, for example, HelloWorldApp.java. It must first be compiled into bytecode, using a Java compiler, producing a file named HelloWorldApp.class. Only then can it be executed, or \"launched\". The Java source file may only contain one public class, but it can contain multiple classes with other than public access and any number of public inner classes. When the source file contains multiple classes, make one class \"public\" and name the source file with that public class name.\n\nA class that is not declared public may be stored in any .java file. The compiler will generate a class file for each class defined in the source file. The name of the class file is the name of the class, with .class appended. For class file generation, anonymous classes are treated as if their name were the concatenation of the name of their enclosing class, a $, and an integer.\n\nThe keyword public denotes that a method can be called from code in other classes, or that a class may be used by classes outside the class hierarchy. The class hierarchy is related to the name of the directory in which the .java file is located. This is called an access level modifier. Other access level modifiers include the keywords private and protected.\n\nThe keyword static in front of a method indicates a static method, which is associated only with the class and not with any specific instance of that class. Only static methods can be invoked without a reference to an object. Static methods cannot access any class members that are not also static. Methods that are not designated static are instance methods, and require a specific instance of a class to operate.\n\nThe keyword void indicates that the main method does not return any value to the caller. If a Java program is to exit with an error code, it must call System.exit() explicitly.\n\nThe method name \"main\" is not a keyword in the Java language. It is simply the name of the method the Java launcher calls to pass control to the program. Java classes that run in managed environments such as applets and Enterprise JavaBeans do not use or need a main() method. A Java program may contain multiple classes that have main methods, which means that the VM needs to be explicitly told which class to launch from.\n\nThe main method must accept an array of String objects. By convention, it is referenced as args although any other legal identifier name can be used. Since Java 5, the main method can also use variable arguments, in the form of public static void main(String... args), allowing the main method to be invoked with an arbitrary number of String arguments. The effect of this alternate declaration is semantically identical (the args parameter is still an array of String objects), but it allows an alternative syntax for creating and passing the array.\n\nThe Java launcher launches Java by loading a given class (specified on the command line or as an attribute in a JAR) and starting its public static void main(String[]) method. Stand-alone programs must declare this method explicitly. The String[] args parameter is an array of String objects containing any arguments passed to the class. The parameters to main are often passed by means of a command line.\n\nPrinting is part of a Java standard library: The System class defines a public static field called out. The out object is an instance of the PrintStream class and provides many methods for printing data to standard out, including println(String) which also appends a new line to the passed string.\n\nThe string \"Hello World!\" is automatically converted to a String object by the compiler.\n\nComprehensive example\n\n[hide]This section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)\nThis section does not cite any sources. (May 2013)\nThis section contains instructions, advice, or how-to content. (May 2013)\n// OddEven.java\nimport javax.swing.JOptionPane;\n\npublic class OddEven {\n\n    private int userInput; // a whole number(\"int\" means integer)\n\n    /**\n     * This is the constructor method. It gets called when an object of the OddEven type\n     * is being created.\n     */\n    public OddEven() {\n        /*\n         * In most Java programs constructors can initialize objects with default values, or create\n         * other objects that this object might use to perform its functions. In some Java programs, the\n         * constructor may simply be an empty function if nothing needs to be initialized prior to the\n         * functioning of the object. In this program's case, an empty constructor would suffice.\n         * A constructor must exist; however, if the user doesn't put one in then the compiler\n         * will create an empty one.\n         */\n    }\n\n    /**\n     * This is the main method. It gets called when this class is run through a Java interpreter.\n     * @param args command line arguments (unused)\n     */\n    public static void main(final String[] args) {\n       /*\n        * This line of code creates a new instance of this class called \"number\" (also known as an\n        * Object) and initializes it by calling the constructor. The next line of code calls\n        * the \"showDialog()\" method, which brings up a prompt to ask you for a number.\n        */\n       OddEven number = new OddEven();\n       number.showDialog();\n    }\n\n    public void showDialog() {\n        /*\n         * \"try\" makes sure nothing goes wrong. If something does,\n         * the interpreter skips to \"catch\" to see what it should do.\n         */\n        try {\n            /*\n             * The code below brings up a JOptionPane, which is a dialog box\n             * The String returned by the \"showInputDialog()\" method is converted into\n             * an integer, making the program treat it as a number instead of a word.\n             * After that, this method calls a second method, calculate() that will\n             * display either \"Even\" or \"Odd.\"\n             */\n            userInput = Integer.parseInt(JOptionPane.showInputDialog(\"Please enter a number.\"));\n            calculate();\n        } catch (final NumberFormatException e) {\n            /*\n             * Getting in the catch block means that there was a problem with the format of\n             * the number. Probably some letters were typed in instead of a number.\n             */\n            System.err.println(\"ERROR: Invalid input. Please type in a numerical value.\");\n        }\n    }\n\n    /**\n     * When this gets called, it sends a message to the interpreter.\n     * The interpreter usually shows it on the command prompt (For Windows users)\n     * or the terminal (For *nix users).(Assuming it's open)\n     */\n    private void calculate() {\n        if ((userInput % 2) == 0) {\n            JOptionPane.showMessageDialog(null, \"Even\");\n        } else {\n            JOptionPane.showMessageDialog(null, \"Odd\");\n        }\n    }\n}\nThe import statement imports the JOptionPane class from the javax.swing package.\nThe OddEven class declares a single private field of type int named userInput. Every instance of the OddEven class has its own copy of the userInput field. The private declaration means that no other class can access (read or write) the userInput field.\nOddEven() is a public constructor. Constructors have the same name as the enclosing class they are declared in, and unlike a method, have no return type. A constructor is used to initialize an object that is a newly created instance of the class.\nThe calculate() method is declared without the static keyword. This means that the method is invoked using a specific instance of the OddEven class. (The reference used to invoke the method is passed as an undeclared parameter of type OddEven named this.) The method tests the expression userInput % 2 == 0 using the if keyword to see if the remainder of dividing the userInput field belonging to the instance of the class by two is zero. If this expression is true, then it prints Even; if this expression is false it prints Odd. (The calculate method can be equivalently accessed as this.calculate and the userInput field can be equivalently accessed as this.userInput, which both explicitly use the undeclared this parameter.)\nOddEven number = new OddEven(); declares a local object reference variable in the main method named number. This variable can hold a reference to an object of type OddEven. The declaration initializes number by first creating an instance of the OddEven class, using the new keyword and the OddEven() constructor, and then assigning this instance to the variable.\nThe statement number.showDialog(); calls the calculate method. The instance of OddEven object referenced by the number local variable is used to invoke the method and passed as the undeclared this parameter to the calculate method.\nuserInput = Integer.parseInt(JOptionPane.showInputDialog(\"Please Enter A Number\")); is a statement that converts the type of String to the primitive data type int by using a utility function in the primitive wrapper class Integer.\nSpecial classes\n\nThis section contains instructions, advice, or how-to content. The purpose of Wikipedia is to present facts, not to train. Please help improve this article either by rewriting the how-to content or by moving it to Wikiversity, Wikibooks or Wikivoyage. (January 2012)\nApplet\nMain article: Java applet\nJava applets are programs that are embedded in other applications, typically in a Web page displayed in a web browser.\n\n// Hello.java\nimport javax.swing.JApplet;\nimport java.awt.Graphics;\n\npublic class Hello extends JApplet {\n    public void paintComponent(final Graphics g) {\n        g.drawString(\"Hello, world!\", 65, 95);\n    }\n}\nThe import statements direct the Java compiler to include the javax.swing.JApplet and java.awt.Graphics classes in the compilation. The import statement allows these classes to be referenced in the source code using the simple class name (i.e. JApplet) instead of the fully qualified class name (FQCN, i.e. javax.swing.JApplet).\n\nThe Hello class extends (subclasses) the JApplet (Java Applet) class; the JApplet class provides the framework for the host application to display and control the lifecycle of the applet. The JApplet class is a JComponent (Java Graphical Component) which provides the applet with the capability to display a graphical user interface (GUI) and respond to user events.\n\nThe Hello class overrides the paintComponent(Graphics) method (additionally indicated with the annotation, supported as of JDK 1.5, Override) inherited from the Container superclass to provide the code to display the applet. The paintComponent() method is passed a Graphics object that contains the graphic context used to display the applet. The paintComponent() method calls the graphic context drawString(String, int, int) method to display the \"Hello, world!\" string at a pixel offset of (65, 95) from the upper-left corner in the applet's display.\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n\"http://www.w3.org/TR/html4/strict.dtd\">\n<!-- Hello.html -->\n<html>\n    <head>\n        <title>Hello World Applet</title>\n    </head>\n    <body>\n        <applet code=\"Hello.class\" width=\"200\" height=\"200\">\n        </applet>\n    </body>\n</html>\nAn applet is placed in an HTML document using the <applet> HTML element. The applet tag has three attributes set: code=\"Hello\" specifies the name of the JApplet class and width=\"200\" height=\"200\" sets the pixel width and height of the applet. Applets may also be embedded in HTML using either the object or embed element,[50] although support for these elements by web browsers is inconsistent.[51] However, the applet tag is deprecated, so the object tag is preferred where supported.\n\nThe host application, typically a Web browser, instantiates the Hello applet and creates an AppletContext for the applet. Once the applet has initialized itself, it is added to the AWT display hierarchy. The paintComponent() method is called by the AWT event dispatching thread whenever the display needs the applet to draw itself.\n\nServlet\nMain article: Java Servlet\nJava Servlet technology provides Web developers with a simple, consistent mechanism for extending the functionality of a Web server and for accessing existing business systems. Servlets are server-side Java EE components that generate responses (typically HTML pages) to requests (typically HTTP requests) from clients. A servlet can almost be thought of as an applet that runs on the server side—without a face.\n\n// Hello.java\nimport java.io.*;\nimport javax.servlet.*;\n\npublic class Hello extends GenericServlet {\n    public void service(final ServletRequest request, final ServletResponse response)\n    throws ServletException, IOException {\n        response.setContentType(\"text/html\");\n        final PrintWriter pw = response.getWriter();\n        try {\n            pw.println(\"Hello, world!\");\n        } finally {\n            pw.close();\n        }\n    }\n}\nThe import statements direct the Java compiler to include all the public classes and interfaces from the java.io and javax.servlet packages in the compilation. Packages make Java well suited for large scale applications.\n\nThe Hello class extends the GenericServlet class; the GenericServlet class provides the interface for the server to forward requests to the servlet and control the servlet's lifecycle.\n\nThe Hello class overrides the service(ServletRequest, ServletResponse) method defined by the Servlet interface to provide the code for the service request handler. The service() method is passed: a ServletRequest object that contains the request from the client and a ServletResponse object used to create the response returned to the client. The service() method declares that it throws the exceptions ServletException and IOException if a problem prevents it from responding to the request.\n\nThe setContentType(String) method in the response object is called to set the MIME content type of the returned data to \"text/html\". The getWriter() method in the response returns a PrintWriter object that is used to write the data that is sent to the client. The println(String) method is called to write the \"Hello, world!\" string to the response and then the close() method is called to close the print writer, which causes the data that has been written to the stream to be returned to the client.\n\nJavaServer Pages\nMain article: JavaServer Pages\nJavaServer Pages (JSP) are server-side Java EE components that generate responses, typically HTML pages, to HTTP requests from clients. JSPs embed Java code in an HTML page by using the special delimiters <% and %>. A JSP is compiled to a Java servlet, a Java application in its own right, the first time it is accessed. After that, the generated servlet creates the response.\n\nSwing application\nMain article: Swing (Java)\nSwing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+ and Motif are supplied by Sun. Apple also provides an Aqua look and feel for Mac OS X. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.\n\nThis example Swing application creates a single window with \"Hello, world!\" inside:\n\n// Hello.java (Java SE 5)\nimport javax.swing.*;\n\npublic class Hello extends JFrame {\n    public Hello() {\n        super(\"hello\");\n        super.setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);\n        super.add(new JLabel(\"Hello, world!\"));\n        super.pack();\n        super.setVisible(true);\n    }\n\n    public static void main(final String[] args) {\n        new Hello();\n    }\n}\nThe first import includes all the public classes and interfaces from the javax.swing package.\n\nThe Hello class extends the JFrame class; the JFrame class implements a window with a title bar and a close control.\n\nThe Hello() constructor initializes the frame by first calling the superclass constructor, passing the parameter \"hello\", which is used as the window's title. It then calls the setDefaultCloseOperation(int) method inherited from JFrame to set the default operation when the close control on the title bar is selected to WindowConstants.EXIT_ON_CLOSE – this causes the JFrame to be disposed of when the frame is closed (as opposed to merely hidden), which allows the Java virtual machine to exit and the program to terminate. Next, a JLabel is created for the string \"Hello, world!\" and the add(Component) method inherited from the Container superclass is called to add the label to the frame. The pack() method inherited from the Window superclass is called to size the window and lay out its contents.\n\nThe main() method is called by the Java virtual machine when the program starts. It instantiates a new Hello frame and causes it to be displayed by calling the setVisible(boolean) method inherited from the Component superclass with the boolean parameter true. Once the frame is displayed, exiting the main method does not cause the program to terminate because the AWT event dispatching thread remains active until all of the Swing top-level windows have been disposed.\n\nGenerics\nMain article: Generics in Java\nIn 2004, generics were added to the Java language, as part of J2SE 5.0. Prior to the introduction of generics, each variable declaration had to be of a specific type. For container classes, for example, this is a problem because there is no easy way to create a container that accepts only specific types of objects. Either the container operates on all subtypes of a class or interface, usually Object, or a different container class has to be created for each contained class. Generics allow compile-time type checking without having to create many container classes, each containing almost identical code. In addition to enabling more efficient code, certain runtime exceptions are converted to compile-time errors, a characteristic known as type safety.\n\nCriticism\nMain article: Criticism of Java\nCriticisms directed at Java include the implementation of generics,[52] speed,[53] the handling of unsigned numbers,[54] the implementation of floating-point arithmetic,[55] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[56]\n\nUse outside of the Java platform\nThe Java programming language requires the presence of a software platform in order for compiled programs to be executed. Oracle supplies the Java platform for use with Java. The Android SDK, is an alternative software platform, used primarily for developing Android applications. It supports Java 6 and some Java 7 features, offering a compatible implementation of a significant part of the standard library (Apache Harmony). The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers.\n\n\nThe Android operating system makes extensive use of Java-related technology.\nGoogle\nSee also: Oracle America, Inc. v. Google, Inc.\nThe Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, was written largely in C, the Android SDK uses the Java language as the basis for Android applications. However, Android does not use the standard Java virtual machine, instead using Java bytecode as an intermediate step which is transformed into Dalvik bytecode. Depending on the Android version, this is then either interpreted by the Dalvik virtual machine, or compiled into native code by the Android Runtime.\n\nAndroid also does not provide the full Java SE standard library, although the Android class library does include an independent implementation of a large subset of it. This led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[57] District Judge William Haskell Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[58] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[59][60][61]\n\nClass libraries\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2014) (Learn how and when to remove this template message)\nMain article: Java Class Library\nThe Java Class Library is the standard library, developed to support application development in Java. It is controlled by Sun Microsystems in cooperation with others through the Java Community Process program. Companies or individuals participating in this process can influence the design and development of the APIs. This process has been a subject of controversy.[when?] The class library contains features such as:\n\nThe core libraries, which include:\nIO/NIO\nNetworking\nReflection\nConcurrency\nGenerics\nScripting/Compiler\nFunctional Programming (Lambda, Streaming)\nCollection libraries that implement data structures such as lists, dictionaries, trees, sets, queues and double-ended queue, or stacks[62]\nXML Processing (Parsing, Transforming, Validating) libraries\nSecurity[63]\nInternationalization and localization libraries[64]\nThe integration libraries, which allow the application writer to communicate with external systems. These libraries include:\nThe Java Database Connectivity (JDBC) API for database access\nJava Naming and Directory Interface (JNDI) for lookup and discovery\nRMI and CORBA for distributed application development\nJMX for managing and monitoring applications\nUser interface libraries, which include:\nThe (heavyweight, or native) Abstract Window Toolkit (AWT), which provides GUI components, the means for laying out those components and the means for handling events from those components\nThe (lightweight) Swing libraries, which are built on AWT but provide (non-native) implementations of the AWT widgetry\nAPIs for audio capture, processing, and playback\nJavaFX\nA platform dependent implementation of the Java virtual machine that is the means by which the bytecodes of the Java libraries and third party applications are executed\nPlugins, which enable applets to be run in web browsers\nJava Web Start, which allows Java applications to be efficiently distributed to end users across the Internet\nLicensing and documentation\nDocumentation\nMain article: Javadoc\nJavadoc is a comprehensive documentation system, created by Sun Microsystems, used by many Java developers[by whom?]. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are /** and */, whereas the normal multi-line comments in Java are set off with the delimiters /* and */.[65]\n\nEditions\nSee also: Free Java implementations § Class library\nJava editions\nWave.svg\nJava Card\nMicro Edition (ME)\nStandard Edition (SE)\nEnterprise Edition (EE)\nJavaFX (Merged to Java SE 8)\nPersonalJava (discontinued)\nv t e\nSun has defined and supports four editions of Java targeting different application environments and segmented many of its APIs so that they belong to one of the platforms. The platforms are:\n\nJava Card for smartcards.[66]\nJava Platform, Micro Edition (Java ME) – targeting environments with limited resources.[67]\nJava Platform, Standard Edition (Java SE) – targeting workstation environments.[68]\nJava Platform, Enterprise Edition (Java EE) – targeting large distributed enterprise or Internet environments.[69]\nThe classes in the Java APIs are organized into separate groups called packages. Each package contains a set of related interfaces, classes and exceptions. Refer to the separate platforms for a description of the packages available.[relevant to this section? – discuss]\n\nSun also provided an edition called PersonalJava that has been superseded by later, standards-based Java ME configuration-profile pairings.\n\nSee also\nicon\tJava portal\nicon\tComputer programming portal\nBook icon\t\nBook: Programming for Students\nDalvik – used in old Android versions, replaced by non-JIT Android Runtime\nJavaOne\nJavapedia\nList of Java virtual machines\nList of Java APIs\nList of JVM languages\nGraal (compiler), a project aiming to implement a high performance Java dynamic compiler and interpreter\nSpring Framework\nComparison of Java with other languages\nComparison of programming languages\nComparison of Java and C++\nComparison of C# and Java",
          "type": "compiled",
          "plid": 16
        },
        {
          "name": "C",
          "details": "C (/ˈsiː/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.\n\nC was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs,[5] and used to re-implement the Unix operating system.[6] It has since become one of the most widely used programming languages of all time,[7][8] with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).\n\nContents  [hide] \n1\tDesign\n2\tOverview\n2.1\tRelations to other languages\n3\tHistory\n3.1\tEarly developments\n3.2\tK&R C\n3.3\tANSI C and ISO C\n3.4\tC99\n3.5\tC11\n3.6\tEmbedded C\n4\tSyntax\n4.1\tCharacter set\n4.2\tReserved words\n4.3\tOperators\n5\t\"Hello, world\" example\n6\tData types\n6.1\tPointers\n6.2\tArrays\n6.3\tArray–pointer interchangeability\n7\tMemory management\n8\tLibraries\n9\tLanguage tools\n10\tUses\n11\tRelated languages\n12\tSee also\n13\tNotes\n14\tReferences\n15\tSources\n16\tFurther reading\n17\tExternal links\nDesign[edit]\nC is an imperative procedural language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. Therefore, C was useful for many applications that had formerly been coded in assembly language, for example in system programming.\n\nDespite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.\n\nOverview[edit]\nLike most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations. In C, all executable code is contained within subroutines, which are called \"functions\" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.\n\nThe C language also exhibits the following characteristics:\n\nThere is a small, fixed number of keywords, including a full set of flow of control primitives: for, if/else, while, switch, and do/while. User-defined names are not distinguished from keywords by any kind of sigil.\nThere are a large number of arithmetical and logical operators, such as +, +=, ++, &, ~, etc.\nMore than one assignment may be performed in a single statement.\nFunction return values can be ignored when not needed.\nTyping is static, but weakly enforced: all data has a type, but implicit conversions may be performed.\nDeclaration syntax mimics usage context. C has no \"define\" keyword; instead, a statement beginning with the name of a type is taken as a declaration. There is no \"function\" keyword; instead, a function is indicated by the parentheses of an argument list.\nUser-defined (typedef) and compound types are possible.\nHeterogeneous aggregate data types (struct) allow related data elements to be accessed and assigned as a unit.\nArray indexing is a secondary notation, defined in terms of pointer arithmetic. Unlike structs, arrays are not first-class objects; they cannot be assigned or compared using single built-in operators. There is no \"array\" keyword, in use or definition; instead, square brackets indicate arrays syntactically, for example month[11].\nEnumerated types are possible with the enum keyword. They are not tagged, and are freely interconvertible with integers.\nStrings are not a separate data type, but are conventionally implemented as null-terminated arrays of characters.\nLow-level access to computer memory is possible by converting machine addresses to typed pointers.\nProcedures (subroutines not returning values) are a special case of function, with an untyped return type void.\nFunctions may not be defined within the lexical scope of other functions.\nFunction and data pointers permit ad hoc run-time polymorphism.\nA preprocessor performs macro definition, source code file inclusion, and conditional compilation.\nThere is a basic form of modularity: files can be compiled separately and linked together, with control over which functions and data objects are visible to other files via static and extern attributes.\nComplex functionality such as I/O, string manipulation, and mathematical functions are consistently delegated to library routines.\nWhile C does not include some features found in some other languages, such as object orientation or garbage collection, such features can be implemented or emulated in C, often by way of external libraries (e.g., the Boehm garbage collector or the GLib Object System).\n\nRelations to other languages[edit]\nMany later languages have borrowed directly or indirectly from C, including C++, D, Go, Rust, Java, JavaScript, Limbo, LPC, C#, Objective-C, Perl, PHP, Python, Verilog (hardware description language),[4] and Unix's C shell. These languages have drawn many of their control structures and other basic features from C. Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.\n\nHistory[edit]\nEarly developments[edit]\n\nKen Thompson (left) with Dennis Ritchie (right, the inventor of the C programming language)\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Ritchie and Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL.[9] However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C. The name of C was chosen simply as the next after B.[10]\n\nThe development of C started in 1972 on the PDP-11 Unix system[11] and first appeared in Version 2 Unix.[12] The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.[1][11]\n\nAlso in 1972, a large part of Unix was rewritten in C.[13] By 1973, with the addition of struct types, the C language had become powerful enough that most of the Unix's kernel was now in C.\n\nUnix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system which was written in PL/I), and Master Control Program (MCP) for the Burroughs B5000 written in ALGOL in 1961. In around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[11]\n\nK&R C[edit]\n\nThe cover of the book, The C Programming Language, first edition by Brian Kernighan and Dennis Ritchie\nIn 1978, Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[1] This book, known to C programmers as \"K&R\", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as K&R C. The second edition of the book[14] covers the later ANSI C standard, described below.\n\nK&R introduced several language features:\n\nStandard I/O library\nlong int data type\nunsigned int data type\nCompound assignment operators of the form =op (such as =-) were changed to the form op= (that is, -=) to remove the semantic ambiguity created by constructs such as i=-10, which had been interpreted as i =- 10 (decrement i by 10) instead of the possibly intended i = -10 (let i be -10).\nEven after the publication of the 1989 ANSI standard, for many years K&R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.\n\nIn early versions of C, only functions that return types other than int must be declared if used before the function definition; functions used without prior declaration were presumed to return type int.\n\nFor example:\n\nlong some_function();\n/* int */ other_function();\n\n/* int */ calling_function()\n{\n    long test1;\n    register /* int */ test2;\n\n    test1 = some_function();\n    if (test1 > 0)\n          test2 = 0;\n    else\n          test2 = other_function();\n    return test2;\n}\nThe int type specifiers which are commented out could be omitted in K&R C, but are required in later standards.\n\nSince K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.\n\nIn the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC[15]) and some other vendors. These included:\n\nvoid functions (i.e., functions with no return value)\nfunctions returning struct or union types (rather than pointers)\nassignment for struct data types\nenumerated types\nThe large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.\n\nANSI C and ISO C[edit]\nMain article: ANSI C\n\nThe cover of the book, The C Programming Language, second edition by Brian Kernighan and Dennis Ritchie covering ANSI C\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\n\nIn 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\n\nIn 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\n\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\n\nOne of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.\n\nC89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\n\nIn cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.\n\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.[citation needed]\n\nC99[edit]\nMain article: C99\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.[16]\n\nC99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with //, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\n\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.[17]\n\nC11[edit]\nMain article: C11 (C standard revision)\nIn 2007, work began on another revision of the C standard, informally called \"C1X\" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\n\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro __STDC_VERSION__ is defined as 201112L to indicate that C11 support is available.\n\nEmbedded C[edit]\nMain article: Embedded C\nHistorically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\n\nIn 2008, the C Standards Committee published a technical report extending the C language[18] to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\n\nSyntax[edit]\nMain article: C syntax\nC has a formal grammar specified by the C standard.[19] Unlike languages such as FORTRAN 77, C source code is free-form which allows arbitrary use of whitespace to format code, rather than column-based or text-line-based restrictions; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters /* and */, or (since C99) following // until the end of the line. Comments delimited by /* and */ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.[20]\n\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\n\nAs an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if(-else) conditional execution and by do-while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression.\n\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\n\nKernighan and Ritchie say in the Introduction of The C Programming Language: \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\"[21] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\n\nCharacter set[edit]\nThe basic C source character set includes the following characters:\n\nLowercase and uppercase letters of ISO Basic Latin Alphabet: a–z A–Z\nDecimal digits: 0–9\nGraphic characters: ! \" # % & ' ( ) * + , - . / : ; < = > ? [ \\ ] ^ _ { | } ~\nWhitespace characters: space, horizontal tab, vertical tab, form feed, newline\nNewline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.\n\nAdditional multi-byte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using \\uXXXX or \\UXXXXXXXX encoding (where the X denotes a hexadecimal character), although this feature is not yet widely implemented.\n\nThe basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.\n\nReserved words[edit]\nC89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:\n\nauto\nbreak\ncase\nchar\nconst\ncontinue\ndefault\ndo\ndouble\nelse\nenum\nextern\nfloat\nfor\ngoto\nif\nint\nlong\nregister\nreturn\nshort\nsigned\nsizeof\nstatic\nstruct\nswitch\ntypedef\nunion\nunsigned\nvoid\nvolatile\nwhile\nC99 reserved five more words:\n\n_Bool\n_Complex\n_Imaginary\ninline\nrestrict\nC11 reserved seven more words:[22]\n\n_Alignas\n_Alignof\n_Atomic\n_Generic\n_Noreturn\n_Static_assert\n_Thread_local\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called entry, but this was seldom implemented, and has now been removed as a reserved word.[23]\n\nOperators[edit]\nMain article: Operators in C and C++\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\n\narithmetic: +, -, *, /, %\nassignment: =\naugmented assignment: +=, -=, *=, /=, %=, &=, |=, ^=, <<=, >>=\nbitwise logic: ~, &, |, ^\nbitwise shifts: <<, >>\nboolean logic: !, &&, ||\nconditional evaluation: ? :\nequality testing: ==, !=\ncalling functions: ( )\nincrement and decrement: ++, --\nmember selection: ., ->\nobject size: sizeof\norder relations: <, <=, >, >=\nreference and dereference: &, *, [ ]\nsequencing: ,\nsubexpression grouping: ( )\ntype conversion: (typename)\nC uses the operator = (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator == to test for equality. The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression if(a==b+1) might mistakenly be written as if(a=b+1), which will be evaluated as true if a is not zero after the assignment.[24]\n\nThe C operator precedence is not always intuitive. For example, the operator == binds more tightly than (is executed prior to) the operators & (bitwise AND) and | (bitwise OR) in expressions such as x & 1 == 0, which must be written as (x & 1) == 0 if that is the coder's intent.[25]\n\n\"Hello, world\" example[edit]\nThe \"hello, world\" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints \"hello, world\" to the standard output, which is usually a terminal or screen display.\n\nThe original version was:[26]\n\nmain()\n{\n    printf(\"hello, world\\n\");\n}\nA standard-conforming \"hello, world\" program is:[a]\n\n#include <stdio.h>\n\nint main(void)\n{\n    printf(\"hello, world\\n\");\n    return 0;\n}\nThe first line of the program contains a preprocessing directive, indicated by #include. This causes the compiler to replace that line with the entire text of the stdio.h standard header, which contains declarations for standard input and output functions such as printf. The angle brackets surrounding stdio.h indicate that stdio.h is located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.\n\nThe next line indicates that a function named main is being defined. The main function serves a special purpose in C programs; the run-time environment calls the main function to begin program execution. The type specifier int indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the main function, is an integer. The keyword void as a parameter list indicates that this function takes no arguments.[b]\n\nThe opening curly brace indicates the beginning of the definition of the main function.\n\nThe next line calls (diverts execution to) a function named printf, which in this case is supplied from a system library. In this call, the printf function is passed (provided with) a single argument, the address of the first character in the string literal \"hello, world\\n\". The string literal is an unnamed array with elements of type char, set up automatically by the compiler with a final 0-valued character to mark the end of the array (printf needs to know this). The \\n is an escape sequence that C translates to a newline character, which on output signifies the end of the current line. The return value of the printf function is of type int, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the printf function succeeded.) The semicolon ; terminates the statement.\n\nThe closing curly brace indicates the end of the code for the main function. According to the C99 specification and newer, the main function, unlike any other function, will implicitly return a status of 0 upon reaching the } that terminates the function. This is interpreted by the run-time system as an exit code indicating successful execution.[27]\n\nData types[edit]\nMain article: C variable types and declarations\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal.[28] There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (enum). Integer type char is often used for single-byte characters. C99 added a boolean datatype. There are also derived types including arrays, pointers, records (struct), and untagged unions (union).\n\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\n\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)[29]\n\nC's usual arithmetic conversions allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\n\nPointers[edit]\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be dereferenced to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated struct objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.[27]\n\nA null pointer value explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a null pointer constant can be written as 0, with or without explicit casting to a pointer type, or as the NULL macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.\n\nVoid pointers (void *) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.[27]\n\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\n\nArrays[edit]\nSee also: C string\nArray types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's malloc function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.\n\nSince arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option.[30] Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.\n\nC does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multi-dimensional array\" can be thought of as increasing in row-major order.\n\nMulti-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional \"row vector\" of pointers to the columns.)\n\nC99 introduced \"variable-length arrays\" which address some, but not all, of the issues with ordinary C arrays.\n\nArray–pointer interchangeability[edit]\nThe subscript notation x[i] (where x designates a pointer) is syntactic sugar for *(x+i).[31] Taking advantage of the compiler's knowledge of the pointer type, the address that x + i points to is not the base address (pointed to by x) incremented by i bytes, but rather is defined to be the base address incremented by i multiplied by the size of an element that x points to. Thus, x[i] designates the i+1th element of the array.\n\nFurthermore, in most expression contexts (a notable exception is as operand of sizeof), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\n\nThe size of an element can be determined by applying the operator sizeof to any dereferenced element of x, as in n = sizeof *x or n = sizeof x[0], and the number of elements in a declared array A can be determined as sizeof A / sizeof A[0]. The latter only applies to array names: variables declared with subscripts (int A[20]). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (malloc); code such as sizeof arr / sizeof arr[0] (where arr designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested.[32][33] Since array name arguments to sizeof are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same sizeof issues as array pointers.\n\nThus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array \"points to\" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the memcpy function, or by accessing the individual elements.\n\nMemory management[edit]\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:[27]\n\nStatic memory allocation: space for the object is provided in the binary at compile-time; these objects have an extent (or lifetime) as long as the binary which contains them is loaded into memory.\nAutomatic memory allocation: temporary objects can be stored on the stack, and this space is automatically freed and reusable after the block in which they are declared is exited.\nDynamic memory allocation: blocks of memory of arbitrary size can be requested at run-time using library functions such as malloc from a region of memory called the heap; these blocks persist until subsequently freed for reuse by calling the library function realloc or free\nThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\n\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[27] Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\n\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\n\nAnother issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before free() is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)\n\nLibraries[edit]\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for \"link the math library\").[27]\n\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.\n\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\n\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.[27]\n\nLanguage tools[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2014) (Learn how and when to remove this template message)\nA number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler. The tool lint was the first such, leading to many others.\n\nAutomated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.[34]\n\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.\n\nTools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.\n\nUses[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)\n\nThe TIOBE index graph from 2002 to 2015, showing a comparison of the popularity of various programming languages[35]\nC is widely used for \"system programming\", including implementing operating systems and embedded system applications, because C code, when written for portability, can be used for most purposes, yet when needed, system-specific code can be used to access specific hardware addresses and to perform type punning to match externally imposed interface requirements, with a low run-time demand on system resources. C can also be used for website programming using CGI as a \"gateway\" for information between the Web application, the server, and the browser.[36] C is often chosen over interpreted languages because of its speed, stability, and near-universal availability.[37]\n\nOne consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The primary implementations of Python, Perl 5 and PHP, for example, are all written in C.\n\nBecause the layer of abstraction is thin and the overhead is low, C enables programmers to create efficient implementations of algorithms and data structures, useful for computationally intense programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.\n\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.\n\nC has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.\n\nRelated languages[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2016) (Learn how and when to remove this template message)\nC has directly or indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical: all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.\n\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\n\nWhen object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\n\nThe C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[38] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\n\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\n\nIn addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.\n\nSee also[edit]\nicon\tComputer programming portal\n\tInformation technology portal\nComparison of Pascal and C\nComparison of programming languages\nInternational Obfuscated C Code Contest\nList of C-based programming languages\nList of C compilers",
          "type": "compiled",
          "plid": 17
        },
        {
          "name": "C++",
          "details": "C++ (pronounced cee plus plus, /ˈsiː plʌs plʌs/) is a general-purpose programming language. It has imperative, object-oriented and generic programming features, while also providing facilities for low-level memory manipulation.\n\nIt was designed with a bias toward system programming and embedded, resource-constrained and large systems, with performance, efficiency and flexibility of use as its design highlights.[5] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[5] including desktop applications, servers (e.g. e-commerce, web search or SQL servers), and performance-critical applications (e.g. telephone switches or space probes).[6] C++ is a compiled language, with implementations of it available on many platforms and provided by various organizations, including the Free Software Foundation (FSF's GCC), LLVM, Microsoft, Intel and IBM.\n\nC++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2014 as ISO/IEC 14882:2014 (informally known as C++14).[7] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, ISO/IEC 14882:2003, standard. The current C++14 standard supersedes these and C++11, with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Bjarne Stroustrup at Bell Labs since 1979, as an extension of the C language as he wanted an efficient and flexible language similar to C, which also provided high-level features for program organization.\n\nMany other programming languages have been influenced by C++, including C#, D, Java, and newer versions of C (after 1998).\n\nContents  [hide] \n1\tHistory\n1.1\tEtymology\n1.2\tPhilosophy\n1.3\tStandardization\n2\tLanguage\n2.1\tObject storage\n2.1.1\tStatic storage duration objects\n2.1.2\tThread storage duration objects\n2.1.3\tAutomatic storage duration objects\n2.1.4\tDynamic storage duration objects\n2.2\tTemplates\n2.3\tObjects\n2.3.1\tEncapsulation\n2.3.2\tInheritance\n2.4\tOperators and operator overloading\n2.5\tPolymorphism\n2.5.1\tStatic polymorphism\n2.5.2\tDynamic polymorphism\n2.5.2.1\tInheritance\n2.5.2.2\tVirtual member functions\n2.6\tLambda expressions\n2.7\tException handling\n3\tStandard library\n4\tCompatibility\n4.1\tWith C\n5\tCriticism\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory[edit]\n\nBjarne Stroustrup, the creator of C++\nIn 1979, Bjarne Stroustrup, a Danish computer scientist, began work on the predecessor to C++, \"C with Classes\".[8] The motivation for creating a new language originated from Stroustrup's experience in programming for his Ph.D. thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his Ph.D. experience, Stroustrup set out to enhance the C language with Simula-like features.[9] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced C++, including ALGOL 68, Ada, CLU and ML.\n\nInitially, Stroustrup's \"C with Classes\" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining and default arguments.[10]\n\nIn 1983, C with Classes was renamed to C++ (\"++\" being the increment operator in C), adding new features that included virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL style single-line comments with two forward slashes (//). Furthermore, it included the development of a standalone compiler for C++, Cfront.\n\nIn 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[11] The first commercial implementation of C++ was released in October of the same year.[8]\n\nIn 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[12] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a boolean type.\n\nAfter the 2.0 update, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions are planned for 2017 and 2020.[13]\n\nEtymology[edit]\nAccording to Stroustrup: \"the name signifies the evolutionary nature of the changes from C\".[14] This name is credited to Rick Mascitti (mid-1983)[10] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's \"++\" operator (which increments the value of a variable) and a common naming convention of using \"+\" to indicate an enhanced computer program.\n\nDuring C++'s development period, the language had been referred to as \"new C\" and \"C with Classes\"[10][15] before acquiring its final name.\n\nPhilosophy[edit]\nThroughout C++'s life, its development and evolution has been informally governed by a set of rules that its evolution should follow:[9]\n\nIt must be driven by actual problems and its features should be useful immediately in real world programs.\nEvery feature should be implementable (with a reasonably obvious way to do so).\nProgrammers should be free to pick their own programming style, and that style should be fully supported by C++.\nAllowing a useful feature is more important than preventing every possible misuse of C++.\nIt should provide facilities for organising programs into well-defined separate parts, and provide facilities for combining separately developed parts.\nNo implicit violations of the type system (but allow explicit violations; that is, those explicitly requested by the programmer).\nUser-created types need to have the same support and performance as built-in types.\nUnused features should not negatively impact created executables (e.g. in lower performance).\nThere should be no language beneath C++ (except assembly language).\nC++ should work alongside other existing programming languages, rather than fostering its own separate and incompatible programming environment.\nIf the programmer's intent is unknown, allow the programmer to specify it by providing manual control.\nStandardization[edit]\nYear\tC++ Standard\tInformal name\n1998\tISO/IEC 14882:1998[16]\tC++98\n2003\tISO/IEC 14882:2003[17]\tC++03\n2011\tISO/IEC 14882:2011[7]\tC++11\n2014\tISO/IEC 14882:2014[18]\tC++14\n2017\tto be determined\tC++17\n2020\tto be determined\tC++20[13]\nC++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published four revisions of the C++ standard and is currently working on the next revision, C++17.\n\nIn 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.\n\nThe next major revision of the standard was informally referred to as \"C++0x\", but it was not released until 2011.[19] C++11 (14882:2011) included many additions to both the core language and the standard library.[7]\n\nIn 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[20] The Draft International Standard ballot procedures completed in mid-August 2014.[21]\n\nAfter C++14, a major revision, informally known as C++17 or C++1z, is planned for 2017,[20] which is almost feature-complete.[22]\n\nAs part of the standardization process, ISO also publishes technical reports and specifications:\n\nISO/IEC TR 18015:2006[23] on the use of C++ in embedded systems and on performance implications of C++ language and library features,\nISO/IEC TR 19768:2007[24] (also known as the C++ Technical Report 1) on library extensions mostly integrated into C++11,\nISO/IEC TR 29124:2010[25] on special mathematical functions,\nISO/IEC TR 24733:2011[26] on decimal floating point arithmetic,\nISO/IEC TS 18822:2015[27] on the standard filesystem library,\nISO/IEC TS 19570:2015[28] on parallel versions of the standard library algorithms,\nISO/IEC TS 19841:2015[29] on software transactional memory,\nISO/IEC TS 19568:2015[30] on a new set of library extensions, some of which are already integrated into C++17,\nISO/IEC TS 19217:2015[31] on the C++ Concepts\nMore technical specifications are in development and pending approval, including concurrency library extensions, a networking standard library, ranges, and modules.[32]\n\nLanguage[edit]\nThe C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as \"a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions\";[5] and \"offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages\".[33]\n\nC++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[34][35]\n\n#include <iostream>\n\nint main()\n{\n\tstd::cout << \"Hello, world!\\n\";\n}\nWithin functions that define a non-void return type, failure to return a value before control reaches the end of the function results in undefined behaviour (compilers typically provide the means to issue a diagnostic in such a case).[36] The sole exception to this rule is the main function, which implicitly returns a value of zero.[37]\n\nObject storage[edit]\nAs in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[38]\n\nStatic storage duration objects[edit]\nStatic storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that \"shall last for the duration of the program\".[39]\n\nStatic storage duration objects are initialized in two phases. First, \"static initialization\" is performed, and only after all static initialization is performed, \"dynamic initialization\" is performed. In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable. Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.\n\nThread storage duration objects[edit]\nVariables of this type are very similar to static storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[40]\n\nAutomatic storage duration objects[edit]\nThe most common variable types in C++ are local variables inside a function or block, and temporary variables.[41] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left.\n\nLocal variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.\n\nMember variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an \"automatic object\" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.\n\nTemporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).\n\nDynamic storage duration objects[edit]\nMain article: new and delete (C++)\nThese objects have a dynamic lifespan and are created with a call to new and destroyed explicitly with a call to delete.[42]\n\nTemplates[edit]\nSee also: Template metaprogramming and Generic programming\nC++ templates enable generic programming. C++ supports function, class, alias and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase \"Substitution failure is not an error\" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code was written by hand.[43] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.\n\nTemplates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.\n\nIn addition, templates are a compile time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.\n\nIn summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.\n\nObjects[edit]\nMain article: C++ classes\nC++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.\n\nEncapsulation[edit]\nEncapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class (\"friends\"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.\n\nThe OO principle is that all of the functions (and only the functions) that access the internal representation of a type should be encapsulated within the type definition. C++ supports this (via member functions and friend functions), but does not enforce it: the programmer can declare parts or all of the representation of a type to be public, and is allowed to make public entities that are not part of the representation of the type. Therefore, C++ supports not just OO programming, but other weaker decomposition paradigms, like modular programming.\n\nIt is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[44][45]\n\nInheritance[edit]\nInheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by \"inheritance\". The other two forms are much less frequently used. If the access specifier is omitted, a \"class\" inherits privately, while a \"struct\" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.\n\nMultiple inheritance is a C++ feature not found in most other languages, allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a \"Flying Cat\" class can inherit from both \"Cat\" and \"Flying Mammal\". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or \"ABC\". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.\n\nOperators and operator overloading[edit]\nOperators that cannot be overloaded\nOperator\tSymbol\nScope resolution operator\t::\nConditional operator\t?:\ndot operator\t.\nMember selection operator\t.*\n\"sizeof\" operator\tsizeof\n\"typeid\" operator\ttypeid\nMain article: Operators in C and C++\nC++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.\n\nOverloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded \"&&\" and \"||\" operators lose their short-circuit evaluation property.\n\nPolymorphism[edit]\nSee also: Polymorphism in object-oriented programming\nPolymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.\n\nC++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.\n\nStatic polymorphism[edit]\nSee also: Parametric polymorphism and ad hoc polymorphism\nFunction overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and would result in a compile-time error message.\n\nWhen declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.\n\nTemplates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the Curiously Recurring Template Pattern, it's possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[43]\n\nDynamic polymorphism[edit]\nInheritance[edit]\nSee also: Subtyping\nVariable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently depending on their (actual, derived) types.\n\nC++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.\n\nVirtual member functions[edit]\nOrdinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[46] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.\n\nIn addition to standard member functions, operator overloads and destructors can be virtual. As a rule of thumb, if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.\n\nA member function can also be made \"pure virtual\" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract data type. Objects cannot be created from abstract data types; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.\n\nLambda expressions[edit]\nC++ provides support for anonymous functions, which are also known as lambda expressions and have the following form:\n\n[capture](parameters) -> return_type { function_body }\nThe [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object. An example lambda function may be defined as follows:\n\n[](int x, int y) -> int { return x + y; }\nException handling[edit]\nException handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[47] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[48] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[49] At the same time, an exception is presented as an object carrying the data about the detected problem.[50]\n\nThe exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[51]\n\n#include <iostream>\n#include <vector>\n#include <stdexcept>\n\nint main() {\n    try {\n        std::vector<int> vec{3,4,3,1};\n        int i{vec.at(4)}; // Throws an exception, std::out_of_range (indexing for vec is from 0-3 not 1-4)\n    }\n\n    // An exception handler, catches std::out_of_range, which is thrown by vec.at(4)\n    catch (std::out_of_range& e) {\n        std::cerr << \"Accessing a non-existent element: \" << e.what() << '\\n';\n    }\n\n    // To catch any other standard library exceptions (they derive from std::exception)\n    catch (std::exception& e) {\n        std::cerr << \"Exception thrown: \" << e.what() << '\\n';\n    }\n\n    // Catch any unrecognised exceptions (i.e. those which don't derive from std::exception)\n    catch (...) {\n        std::cerr << \"Some fatal error\\n\";\n    }\n}\nIt is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the minimum time required for an exception to be handled.[52]\n\nStandard library[edit]\nMain article: C++ Standard Library\nThe C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes vectors, lists, maps, algorithms (find, for_each, binary_search, random_shuffle, etc.), sets, queues, stacks, arrays, tuples, input/output facilities (iostream, for reading from and writing to the console and files), smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that doesn't use C++ exceptions into C++ exceptions, a random number generator and a slightly modified version of the C standard library (to make it comply with the C++ type system).\n\nA large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.\n\nFurthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. C++ provides 105 standard headers, of which 27 are deprecated.\n\nThe standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as \"STL\", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[53]\n\nMost C++ compilers, and all major ones, provide a standards conforming implementation of the C++ standard library.\n\nCompatibility[edit]\nTo give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There were, however, attempts to standardize compilers for particular machines or operating systems (for example C++ ABI),[54] though they seem to be largely abandoned now.\n\nWith C[edit]\nFor more details on this topic, see Compatibility of C and C++.\nC++ is often considered to be a superset of C, but this is not strictly true.[55] Most C code can easily be made to compile correctly in C++, but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types, but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.\n\nSome incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//), and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support, were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code-compatible), designated initializers, compound literals, and the restrict keyword.[56] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[57][58][59] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.\n\nTo intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern \"C\" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).\n\nCriticism[edit]\nMain article: Criticism of C++\nDespite its widespread adoption, many programmers have criticized the C++ language, including Linus Torvalds,[60] Richard Stallman,[61] and Ken Thompson.[62] Issues include a lack of reflection or garbage collection, slow compilation times, perceived feature creep,[63] and verbose error messages, particularly from template metaprogramming.[64]\n\nTo avoid the problems that exist in C++, and to increase productivity,[65] some people suggest alternative languages newer than C++, such as D, Go, Rust and Vala.[66]",
          "type": "compiled",
          "plid": 18
        },
        {
          "name": "Dart",
          "details": "Dart is a general-purpose programming language originally developed by Google and later approved as a standard by Ecma (ECMA-408).[4] It is used to build web, server and mobile applications, and for Internet of Things (IoT) devices.[5] It is open-source software under a BSD license.\n\nDart is a class-based, single inheritance, object-oriented language with C-style syntax which can optionally transcompile into JavaScript. It supports interfaces, mixins, abstract classes, reified generics, and optional typing.\n\nContents  [hide] \n1\tHistory\n2\tUsage\n3\tRuntime modes\n4\tIsolates\n5\tSnapshots\n6\tNative mobile apps\n7\tCompiling to JavaScript\n8\tEditors\n8.1\tChrome Dev Editor\n8.2\tDartPad\n9\tSIMD on the web\n10\tExample\n11\tInfluences from other languages\n12\tCriticism\n13\tSee also\n14\tReferences\n15\tBibliography\n16\tExternal links\nHistory[edit]\nDart was unveiled at the GOTO conference in Aarhus, Denmark, October 10–12, 2011.[6] The project was founded by Lars Bak and Kasper Lund.[7]\n\nStandardization\nEcma International has formed technical committee TC52[8] to work on standardizing Dart, and inasmuch as Dart can be compiled to standard JavaScript, it works effectively in any modern browser. Ecma International approved the Dart language specification first edition in July 2014, at its 107th General Assembly,[9] and a second edition in December 2014.[10]\n\nUsage[edit]\nThere are four main ways to run Dart code:\n\nCompiled as JavaScript\nTo run in mainstream web browsers, Dart relies on a source-to-source compiler to JavaScript. According to the project site, Dart was \"designed to be easy to write development tools for, well-suited to modern app development, and capable of high-performance implementations.\"[11] When running Dart code in a web browser the code is precompiled into JavaScript using the dart2js compiler. Compiled as JavaScript, Dart code is compatible with all major browsers with no need for browsers to adopt Dart. Through optimizing the compiled JavaScript output to avoid expensive checks and operations, code written in Dart can, in some cases, run faster than equivalent code hand-written using JavaScript idioms.[12]\nIn the Dartium Browser\nThe Dart software development kit (SDK) ships with a version of the Chromium web browser modified to include a Dart virtual machine (VM). This browser can run Dart code directly without compiling to JavaScript. It is intended as a development tool for applications written in this language, rather than as a general purpose web browser.[13] There were originally plans to include Dart support directly in Chrome, but these were cancelled.[14]\nStand-alone\nThe Dart SDK also ships with a stand-alone Dart VM, allowing dart code to run in a command-line interface environment. As the language tools included in the Dart SDK are written mostly in Dart, the stand-alone Dart VM is a critical part of the SDK. These tools include the dart2js compiler, and a package manager suite called pub. Dart ships with a complete standard library allowing users to write fully working system apps, such as custom web servers.[15]\nAhead-of-time compiled\nDart code can be AOT-compiled into machine code (native instruction sets). Apps built with Flutter, a mobile app SDK built with Dart, are deployed to app stores as AOT-compiled Dart code. [16]\nRuntime modes[edit]\nDart programs run in one of two modes. In checked mode, which is not the default mode and must be turned on, dynamic type assertions are enabled. These type assertions can turn on if static types are provided in the code, and can catch some errors when types do not match. For example, if a method is annotated to return a String, but instead returns an integer, the dynamic type assertion will catch this and throw an exception. Running in checked mode is recommended for development and testing.\n\nDart programs run by default in production mode, which runs with all dynamic type assertions turned off. This is the default mode because it is the fastest way to run a Dart program.\n\nIsolates[edit]\nTo achieve concurrency, Dart uses isolates, which are independent workers that do not share memory, but instead use message passing. This is similar to Erlang actors. Every Dart program uses at least one isolate, which is the main isolate. When compiled to JavaScript, isolates are transformed into Web workers.\n\nSnapshots[edit]\nSnapshots are a core part of the Dart VM. Snapshots are files which store objects and other runtime data.\n\nScript snapshots\nDart programs can be compiled into snapshot files. These files contain all of the program code and dependencies preparsed and ready to execute. This allows fast startups.\n\nFull snapshots\nThe Dart core libraries can be compiled into a snapshot file which allows fast loading of the libraries. Most standard distributions of the main Dart VM have a prebuilt snapshot for the core libraries which is loaded at runtime.\n\nObject snapshots\nDart is a very asynchronous language. With this, it uses isolates for concurrency. Since these are workers which pass messages, it needs a way to serialize a message. This is done using a snapshot, which is generated from a given object, and then this is transferred to another isolate for deserializing.\n\nNative mobile apps[edit]\nGoogle is working on full Dart stacks for native mobile app development on both Android and iOS.[17]\n\nCompiling to JavaScript[edit]\nThe first compiler to generate JavaScript from Dart code was dartc, but it was deprecated. The second Dart-to-JavaScript compiler was Frog. It was written in Dart, but never implemented the full semantics of the language. As of 2015, the third Dart-to-JavaScript compiler is dart2js, from Google. An evolution of earlier compilers, it is written in Dart, and intended to implement the full Dart language specification and semantics.\n\nOn March 28, 2013, the Dart team posted an update on their blog addressing Dart code compiled to JavaScript with the dart2js compiler,[18] stating that it now runs faster than handwritten JavaScript on Chrome's V8 JavaScript engine for the DeltaBlue benchmark.[19]\n\nEditors[edit]\nOn November 18, 2011, Google released Dart Editor, an open-source program based on Eclipse components, for Mac OS X, Windows, and Linux-based operating systems.[20] The editor supports syntax highlighting, code completion, JavaScript compiling, running web and server Dart applications, and debugging.\n\nOn August 13, 2012, Google announced the release of an Eclipse plugin for Dart development.[21]\n\nOn April 18, 2015, Google announced that the Dart Editor would be retired in favor of the JetBrains integrated development environment (IDE),[22] which is now the recommended IDE for the language. The Dart plugin[23] is available for IntelliJ IDEA, PyCharm, PhpStorm and WebStorm. This plugin supports many features such as syntax highlighting, code completion, analysis, refactoring, debugging, and more. Other plugins are available for editors like Sublime Text, Atom, Emacs, Vim and Visual Studio Code.[24]\n\nChrome Dev Editor[edit]\nAmbox current red.svg\nThis section needs to be updated. Please update this article to reflect recent events or newly available information. (October 2016)\nIt has been known since November 2013[25] that the Chromium team is working on an open source, Chrome App-based development environment with a reusable library of GUI widgets, codenamed Spark, later renamed as Chrome Dev Editor.[26] It is built in Dart, and contains Spark which is powered by Polymer.[27] A developer preview version is available in the Chrome Web Store.\n\nDartPad[edit]\nThe Dart team created DartPad at the start of 2015, to provide an easier way to start using Dart. It is a fully online editor from which users can experiment with Dart application programming interfaces (APIs), and run Dart code. It provides syntax highlighting, code analysis, code completion, documentation, and HTML and CSS editing.[28]\n\nSIMD on the web[edit]\nIn 2013, John McCutchan announced that he had created a performant interface to single instruction, multiple data (SIMD) instruction sets for Dart, bringing the benefits of SIMD to web programs for the first time, for users running Google's experimental Dartium browser.[29] The interface consists of two types:\n\nFloat32×4, 4× single precision floating point values\nUint32×4, 4× 32-bit unsigned integer values\nInstances of these types are immutable and in optimized code are mapped directly to SIMD registers. Operations expressed in Dart typically are compiled into one instruction with no overhead. This is similar to C and C++ intrinsics. Benchmarks for 4×4 matrix multiplication, 3D vertex transformation, and Mandelbrot set visualization show near 400% speedup compared to scalar code written in Dart.\n\nExample[edit]\nA Hello World example:\n\nmain() {\n  print('Hello World!');\n}\nA function to calculate the nth Fibonacci number:\n\nint fib(int n) => (n > 2) ? (fib(n - 1) + fib(n - 2)) : 1;\n\nvoid main() {\n  print('fib(20) = ${fib(20)}');\n}\nA simple class:\n\n// Import the math library to get access to the sqrt function.\nimport 'dart:math' as math;\n\n// Create a class for Point.\nclass Point {\n\n  // Final variables cannot be changed once they are assigned.\n  // Create two instance variables.\n  final num x, y;\n\n  // A constructor, with syntactic sugar for setting instance variables.\n  Point(this.x, this.y);\n\n  // A named constructor with an initializer list.\n  Point.origin()\n      : x = 0,\n        y = 0;\n\n  // A method.\n  num distanceTo(Point other) {\n    var dx = x - other.x;\n    var dy = y - other.y;\n    return math.sqrt(dx * dx + dy * dy);\n  }\n\n  // Example of Operator Overloading\n  Point operator +(Point other) => new Point(x + other.x, y + other.y);\n}\n\n// All Dart programs start with main().\nvoid main() {\n  // Instantiate point objects.\n  var p1 = new Point(10, 10);\n  var p2 = new Point.origin();\n  var distance = p1.distanceTo(p2);\n  print(distance);\n}\nInfluences from other languages[edit]\nDart is a descendant of the ALGOL language family,[30] alongside C, Java, C#, JavaScript, and others.\n\nThe method cascade syntax, which provides a syntactic shortcut for invoking several methods one after another on the same object, is adopted from Smalltalk.\n\nDart's mixins were influenced by Strongtalk[citation needed][31] and Ruby.\n\nDart makes use of isolates as a concurrency and security unit when structuring applications.[32] The Isolate concept builds upon the Actor model, which is most famously implemented in Erlang.\n\nThe Mirror API for performing controlled and secure reflection was first proposed in a paper[33] by Gilad Bracha (who is a member of the Dart team) and David Ungar and originally implemented in Self.\n\nCriticism[edit]\nDart initially had a mixed reception and the Dart initiative has been criticized by some for fragmenting the web, due to the original plans to include a Dart VM in Chrome. Those plans were dropped to focus instead on compiling Dart to JavaScript.[14]\n\nSee also[edit]\nicon\tComputer programming portal\n\tFree software portal\nCoffeeScript\nFantom (programming language)\nGo, another language developed by Google\nGoogle Web Toolkit\nHaxe, a language that can be compiled to JavaScript and several other languages\nOpa (programming language)\nPyjamas (software)\nTypeScript\nAtScript",
          "type": "compiled",
          "plid": 19
        },
        {
          "name": "Elixir",
          "details": "lixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM). Elixir builds on top of Erlang and shares the same abstractions for building distributed, fault-tolerant applications. Elixir also provides a productive tooling and an extensible design. The latter is supported by compile-time metaprogramming with macros and polymorphism via protocols.[4]\n\nElixir is successfully used in the industry by companies such as Pinterest[5] and Moz.[6] Elixir is also used for web development, by companies such as Bleacher Report and Inverse,[7] and for building embedded-systems.[8][9] The community organizes yearly events in both United States[10][11][12] and Europe[13] as well as minor local events and conferences.[14][15]\n\nContents  [hide] \n1\tHistory\n2\tFeatures\n3\tExamples\n4\tReferences\n5\tExternal links\nHistory[edit]\nJosé Valim is the creator of the Elixir programming language, an R&D project of Plataformatec. His goals were to enable higher extensibility and productivity in the Erlang VM while keeping compatibility with Erlang's ecosystem.[16]\n\nFeatures[edit]\nA language that compiles to bytecode for the Erlang Virtual Machine (BEAM)[17]\nEverything is an expression[17]\nErlang functions can be called from Elixir without run time impact, due to compilation to Erlang bytecode, and vice versa\nMeta programming allowing direct manipulation of AST[17]\nPolymorphism via a mechanism called protocols. Like in Clojure, protocols provide a dynamic dispatch mechanism. However, this is not to be confused with multiple dispatch as Elixir protocols dispatch on a single type.\nSupport for documentation via Python-like docstrings in the Markdown formatting language[17]\nShared nothing concurrent programming via message passing (Actor model)[18]\nEmphasis on recursion and higher-order functions instead of side-effect-based looping\nLightweight concurrency utilizing Erlang's mechanisms.[17]\nLazy and async collections with streams\nPattern matching[17]\nUnicode support and UTF-8 strings\nExamples[edit]\nThe following examples can be run in an iex shell or saved in a file and run from the command line by typing elixir <filename>.\n\nClassic Hello world example:\n\niex> IO.puts \"Hello World!\"\nHello World!\nComprehensions\n\niex> for n <- [1,2,3,4,5], rem(n,2) == 1, do: n*n\n[1, 9, 25]\nPattern Matching\n\niex> [1, a] = [1, 2]\niex> a\n2\n\niex> {:ok, [hello: a]} = {:ok, [hello: \"world\"]}\niex> a\n\"world\"\nModules\n\ndefmodule Fun do\n  def fib(0), do: 0\n  def fib(1), do: 1\n  def fib(n) do \n    fib(n-2) + fib(n-1)  \n  end\nend\nSequentially spawning a thousand processes\n\nfor num <- 1..1000, do: spawn fn -> IO.puts \"#{num * 2}\" end\nAsynchronously performing a task\n\ntask = Task.async fn -> perform_complex_action() end\nother_time_consuming_action()\nTask.await task",
          "type": "dynamic",
          "plid": 20
        },
        {
          "name": "groovy",
          "details": "Apache Groovy is an object-oriented programming language for the Java platform. It is a dynamic language with features similar to those of Python, Ruby, Perl, and Smalltalk. It can be used as a scripting language for the Java Platform, is dynamically compiled to Java Virtual Machine (JVM) bytecode, and interoperates with other Java code and libraries. Groovy uses a Java-like curly-bracket syntax. Most Java code is also syntactically valid Groovy, although semantics may be different.\n\nGroovy 1.0 was released on January 2, 2007, and Groovy 2.0 in July, 2012. Since version 2, Groovy can also be compiled statically, offering type inference and performance very close to that of Java.[1][2] Groovy 2.4 was the last major release under Pivotal Software's sponsorship which ended in March 2015.[3] Groovy has since changed its governance structure to a Project Management Committee (PMC) in the Apache Software Foundation.[4]\n\nContents  [hide] \n1\tHistory\n2\tFeatures\n2.1\tGroovyBeans / Properties\n2.2\tPrototype extension\n2.3\tDot and parentheses\n2.4\tFunctional programming\n2.4.1\tClosures\n2.4.2\tCurry\n2.5\tXML and JSON processing\n2.6\tString interpolation\n2.7\tAST (Abstract Syntax Tree) Transformation\n2.8\tTraits\n3\tAdoption\n4\tIDE support\n5\tSee also\n6\tReferences\n6.1\tCitations\n6.2\tSources\n7\tExternal links\nHistory[edit]\nJames Strachan first talked about the development of Groovy on his blog in August 2003.[5] Several versions were released between 2004 and 2006. After the Java Community Process (JCP) standardization process began, the version numbering changed and a version called \"1.0\" was released on January 2, 2007. After various betas and release candidates numbered 1.1, on December 7, 2007, Groovy 1.1 Final was released and immediately rebranded as Groovy 1.5 as a reflection of the many changes made.\n\nIn 2007, Groovy won the first prize at JAX 2007 innovation award.[6] In 2008, Grails, a Groovy web framework, won the second prize at JAX 2008 innovation award.[7]\n\nIn November 2008, SpringSource acquired the Groovy and Grails company (G2One).[8] In August 2009 VMWare acquired SpringSource.[9]\n\nIn July 2009, Strachan wrote on his blog, \"I can honestly say if someone had shown me the Programming in Scala book by Martin Odersky, Lex Spoon & Bill Venners back in 2003 I'd probably have never created Groovy.\"[10] Strachan had left the project silently a year before the Groovy 1.0 release in 2007.[citation needed]\n\nIn March 2004, Groovy had been submitted to the JCP as JSR 241[11] and accepted by ballot. After 8 years of inactivity, the Spec Lead changed its status to dormant in April 2012.\n\nOn July 2, 2012, Groovy 2.0 was released, which, among other new features, added static compilation and a static type checker to Groovy.\n\nWhen the Pivotal joint venture was spun-off by EMC and VMware in April 2013, Groovy and Grails formed part of its product portfolio. Pivotal ceased sponsoring Groovy and Grails from April 2015.[3] That same month, Groovy changed its governance structure from a Codehaus repository to a Project Management Committee (PMC) in the Apache Software Foundation via its incubator.[4] Groovy graduated from Apache's incubator and become a top-level project in November 2015.[12]\n\nFeatures[edit]\nMost valid Java files are also valid Groovy files. Although the two languages are similar, Groovy code can be more compact, because it does not require all the elements that Java requires.[13] This makes it possible for Java programmers to learn Groovy gradually by starting with familiar Java syntax before acquiring more Groovy idioms.[14]\n\nGroovy features not available in Java include both static and dynamic typing (with the def keyword), operator overloading, native syntax for lists and associative arrays (maps), native support for regular expressions, polymorphic iteration, expressions embedded inside strings, additional helper methods, and the safe navigation operator \"?.\" to check automatically for nulls (for example, \"variable?.method()\", or \"variable?.field\").[15]\n\nSince version 2 Groovy also supports modularity (being able to ship only the needed jars according to the project needs, thus reducing the size of groovy's lib), type checking, static compilation, Project Coin syntax enhancements, multicatch blocks and ongoing performance enhancements using JDK7's invoke dynamic instruction.[16]\n\nGroovy provides native support for various markup languages such as XML and HTML, accomplished via an inline DOM syntax. This feature enables the definition and manipulation of many types of heterogeneous data assets with a uniform and concise syntax and programming methodology.[citation needed]\n\nUnlike Java, a Groovy source code file can be executed as an (uncompiled) script if it contains code outside any class definition, if it is a class with a main method, or if it is a Runnable or GroovyTestCase. A Groovy script is fully parsed, compiled, and generated before execution (similar to Perl and Ruby). This occurs under the hood, and the compiled version is not saved as an artifact of the process.[17]\n\nGroovyBeans / Properties[edit]\nGroovyBeans are Groovy's version of JavaBeans. Groovy implicitly generates getters and setters. In the following code, setColor(String color) and getColor() are implicitly generated; and the last two lines, which appear to access color directly, are actually calling the implicitly generated methods.[18]\n\nclass AGroovyBean {\n  String color\n}\n\ndef myGroovyBean = new AGroovyBean()\n\nmyGroovyBean.setColor('baby blue')\nassert myGroovyBean.getColor() == 'baby blue'\n\nmyGroovyBean.color = 'pewter'\nassert myGroovyBean.color == 'pewter'\nGroovy offers simple, consistent syntax for handling lists and maps, reminiscent of Java's array syntax.[19]\n\ndef movieList = ['Dersu Uzala', 'Ran', 'Seven Samurai']  //looks like an array, but is a list\nassert movieList[2] == 'Seven Samurai'\nmovieList[3] = 'Casablanca'  // adds an element to the list\nassert movieList.size() == 4\n\ndef monthMap = [ 'January' : 31, 'February' : 28, 'March' : 31 ]  //declares a map\nassert monthMap['March'] == 31  // accesses an entry\nmonthMap['April'] = 30  // adds an entry to the map\nassert monthMap.size() == 4\nPrototype extension[edit]\nGroovy offers support for prototype extension through ExpandoMetaClass, Extension Modules (only in Groovy 2), Objective-C-like Categories and DelegatingMetaClass.[20]\n\nExpandoMetaClass offers a domain-specific language (DSL) to express the changes in the class easily, similar to Ruby's open class concept:\n\nNumber.metaClass {\n  sqrt = { Math.sqrt(delegate) }\n}\n\nassert 9.sqrt() == 3\nassert 4.sqrt() == 2\nGroovy's changes in code through prototyping are not visible in Java, since each attribute/method invocation in Groovy goes through the metaclass registry. The changed code can only be accessed from Java by going to the metaclass registry.\n\nGroovy also allows overriding methods as getProperty(), propertyMissing() among others, enabling the developer to intercept calls to an object and specify an action for them, in a simplified aspect-oriented way. The following code enables the class java.lang.String to respond to the hex property:\n\nenum Color {\n  BLACK('#000000'), WHITE('#FFFFFF'), RED('#FF0000'), BLUE('#0000FF')\n  String hex\n  Color(String hex) { \n    this.hex = hex \n  }\n}\n\nString.metaClass.getProperty = { String property ->\n  def stringColor = delegate\n  if (property == 'hex') {\n    Color.values().find { it.name().equalsIgnoreCase stringColor }?.hex\n  }\n}\n\nassert \"WHITE\".hex == \"#FFFFFF\"\nassert \"BLUE\".hex == \"#0000FF\"\nassert \"BLACK\".hex == \"#000000\"\nassert \"GREEN\".hex == null\nThe Grails framework uses metaprogramming extensively to enable GORM dynamic finders, like User.findByName('Josh') and others.[21]\n\nDot and parentheses[edit]\nGroovy's syntax permits omitting parentheses and dots in some situations. The following groovy code\n\ntake(coffee).with(sugar, milk).and(liquor)\ncan be written as\n\ntake coffee with sugar, milk and liquor\nenabling the development of domain-specific languages (DSLs) which look like plain English.\n\nFunctional programming[edit]\nAlthough Groovy is mostly an object-oriented language, it also offers functional features.\n\nClosures[edit]\nAccording to Groovy's documentation: \"Closures in Groovy work similar to a 'method pointer', enabling code to be written and run in a later point in time\".[22] Groovy's closures support free variables, i.e. variables which have not been explicitly passed as a parameter to it, but exist in its declaration context, partial application (which it terms 'currying'[23]), delegation, implicit, typed and untyped parameters.\n\nWhen working on Collections of a determined type, the closure passed to an operation on the collection can be inferred:\n\nlist = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n/* \n * Non-zero numbers are coerced to true, so when it % 2 == 0 (even), it is false.\n * The type of the implicit \"it\" parameter can be inferred as an Integer by the IDE.\n * It could also be written as:\n * list.findAll { Integer i -> i % 2 }\n * list.findAll { i -> i % 2 }\n */\ndef odds = list.findAll { it % 2 }\n\nassert odds == [1, 3, 5, 7, 9]\nA group of expressions can be written in a closure block without reference to an implementation and the responding object can be assigned at a later point using delegation:\n\n// This block of code contains expressions without reference to an implementation\ndef operations = {\n  declare 5\n  sum 4\n  divide 3\n  print\n}\n/* \n * This class will handle the operations that can be used in the closure above. We could declare \n * another class having the same methods, but using, for example, webservice operations in the\n * calculations.\n */\nclass Expression {\n  BigDecimal value\n\n  /* \n   * Though an Integer is passed as a parameter, it is coerced into a BigDecimal, as we defined. \n   * If the class had a 'declare(Integer value)' method, it would be used instead.\n   */\n  def declare(BigDecimal value) {\n    this.value = value\n  }\n  \n  def sum(BigDecimal valueToAdd) {\n    this.value += valueToAdd\n  }\n  \n  def divide(BigDecimal divisor) {\n    this.value /= divisor\n  }\n  \n  def propertyMissing(String property) {\n    if (property == \"print\") println value\n  }\n}\n// Here we define who is going to respond the expressions in the block\noperations.delegate = new Expression()\noperations()\nCurry[edit]\nUsually called partial application,[23] this Groovy feature allows closures' parameters to be set to a default parameter in any of their arguments, creating a new closure with the bound value. If you supply one argument to the curry() method you will fix the first argument. If you supply N arguments you will fix arguments 1..N.\n\ndef joinTwoWordsWithSymbol = { symbol, first, second -> first + symbol + second }\nassert joinTwoWordsWithSymbol('#', 'Hello', 'World') == 'Hello#World'\n\ndef concatWords = joinTwoWordsWithSymbol.curry(' ')\nassert concatWords('Hello', 'World') == 'Hello World'\n\ndef prependHello = concatWords.curry('Hello')\n// def prependHello = joinTwoWordsWithSymbol.curry(' ', 'Hello')\nassert prependHello('World') == 'Hello World'\nCurry can also be used in the reversed direction (fixing arguments N to N-1) using rcurry.\n\ndef power = { BigDecimal value, BigDecimal power ->\n  value ** power\n}\n\ndef square = power.rcurry(2)\ndef cube = power.rcurry(3)\n\nassert power(2, 2) == 4\nassert square(4) == 16\nassert cube(3) == 27\nGroovy also supports lazy evaluation,[24][25] reduce/fold,[26] infinite structures and immutability,[27] among others.[28]\n\nXML and JSON processing[edit]\nOn XML and JSON processing Groovy employs the Builder pattern, making the production of the data structure less verbose. For example, the following XML:\n\n<languages>\n  <language year=\"1995\">\n    <name>java</name>\n    <paradigm>Object oriented</paradigm>\n    <typing>Static</typing>\n  </language>\n  <language year=\"1995\">\n    <name>ruby</name>\n    <paradigm>Object oriented, Functional</paradigm>\n    <typing>Dynamic, duck typing</typing>\n  </language>  \n  <language year=\"2003\">\n    <name>groovy</name>\n    <paradigm>Object oriented, Functional</paradigm>\n    <typing>Dynamic, Static, Duck typing</typing>\n  </language>\n</languages>\nCan be generated through the following Groovy code:\n\ndef writer = new StringWriter()\ndef builder = new groovy.xml.MarkupBuilder(writer)\nbuilder.languages {\n  language(year: 1995) {\n    name \"java\"\n    paradigm \"Object oriented\"\n    typing \"Static\"\n  }\n  language (year: 1995) {\n    name \"ruby\"\n    paradigm \"Object oriented, Functional\"\n    typing \"Dynamic, Duck typing\"\n  }\n  language (year: 2003) {\n    name \"groovy\"\n    paradigm \"Object oriented, Functional\"\n    typing \"Dynamic, Static, Duck typing\"\n  }\n}\nAnd also can be processed in a streaming way through StreamingMarkupBuilder. To change the implementation to JSON, we can just swap the MarkupBuilder to JsonBuilder.[29]\n\nTo parse it and search for a functional language we can use Groovy's findAll method:\n\ndef languages = new XmlSlurper().parseText writer.toString()\n\n// Here we employ groovy's regex syntax for a matcher (=~) which will be coerced to a boolean value: \n// either true if the value contains our string, or false otherwise.\ndef functional = languages.language.findAll { it.paradigm =~ \"functional\" }\nassert functional.collect { it.name } == [\"ruby\", \"groovy\"]\nString interpolation[edit]\nIn Groovy we can interpolate the string with variables and expressions by using GStrings:[30]\n\nBigDecimal account = 10.0\ndef text = \"Your account shows currently a balance of $account\"\nassert text == \"Your account shows currently a balance of 10.0\"\nGStrings containing variables and expressions must be declared using double quotes.\n\nA complex expression must be enclosed in curly brackets. This prevents parts of it from being interpreted as belonging to the surrounding string instead of to the expression:\n\nBigDecimal minus = 4.0\ntext = \"Your account shows currently a balance of ${account - minus}\"\nassert text == \"Your account shows currently a balance of 6.0\"\n\n// Without the brackets to isolate the expression, we would have the following:\ntext = \"Your account shows currently a balance of $account - minus\"\nassert text == \"Your account shows currently a balance of 10.0 - minus\"\nExpression evaluation can be deferred by employing arrow syntax:\n\nBigDecimal tax = 0.15\ntext = \"Your account shows currently a balance of ${->account - account * tax}\"\ntax = 0.10\n\n// The tax value was changed AFTER declaration of the GString. The expression \n// variables are bound only when the expression must actually be evaluated:\n\nassert text == \"Your account shows currently a balance of 9.000\"\nAST (Abstract Syntax Tree) Transformation[edit]\nAccording to Groovy's own documentation, \"When the Groovy compiler compiles Groovy scripts and classes, at some point in the process, the source code will end up being represented in memory in the form of a Concrete Syntax Tree, then transformed into an Abstract Syntax Tree. The purpose of AST Transformations is to let developers hook into the compilation process to be able to modify the AST before it is turned into bytecode that will be run by the JVM. AST Transformations provides Groovy with improved compile-time metaprogramming capabilities allowing powerful flexibility at the language level, without a runtime performance penalty.\".[31]\n\nExamples of ASTs in Groovy are:\n\nSingleton transformation;\nCategory and Mixin transformation;\nImmutable AST Macro;\nNewify transformation;\nAmong others.\n\nTraits[edit]\nAccording to Groovy's documentation, \"Traits are a structural construct of the language which allow: composition of behaviors, runtime implementation of interfaces, behavior overriding, and compatibility with static type checking/compilation.\"\n\nTraits can be seen as interfaces carrying both default implementations and state. A trait is defined using the trait keyword:\n\ntrait FlyingAbility { /* declaration of a trait */\n    String fly() { \"I'm flying!\" } /* declaration of a method inside a trait */\n}\nThen it can be used like a normal interface using the implements keyword:\n\nclass Bird implements FlyingAbility {} /* Adds the trait FlyingAbility to the Bird class capabilities */\ndef b = new Bird() /* instantiate a new Bird */\nassert b.fly() == \"I'm flying!\" /* the Bird class automatically gets the behavior of the FlyingAbility trait */\nTraits allow a wide range of capabilities, from simple composition to testing.\n\nAdoption[edit]\neXo Platform, an Open Source Enterprise Social Collaboration Platform uses Groovy.\nWired.com uses Groovy and Grails for the Product Reviews standalone section of the website.[32]\nThough Groovy can be integrated into any JVM environment, the JBoss Seam framework provides Groovy, besides Java, as a development language, out of the box.[33]\nThe European Patent Office (EPO) developed a Data Flow Language in Groovy \"to leverage similarities in the processes for communicating with each individual country’s patent office, and transform them into a single, universal process.\"[citation needed]\nLinkedin uses Groovy and Grails for some of their subsystems.[34]\nXWiki SAS uses Groovy as scripting language in their collaborative open-source product.[35]\nSoapUI provides Groovy as a language for webservice tests development.[36]\nSky.com uses Groovy and Grails to serve massive online media content.[37]\nDataMelt integrates Groovy into a numeric and statistic data-analysis framework\nvCalc.com uses Groovy for all of the user defined mathematics in its math crowd-sourcing engine.[38]\nEucalyptus, a cloud management system, uses a significant amount of Groovy.\nApache OFBiz, the open source enterprise resource planning (ERP) system, uses Groovy.[39][40]\nSmartThings, an open platform for smart homes and the consumer Internet of Things, uses a security-oriented subset of Groovy[41]\nSurvata, a market research startup, uses Groovy and Grails.[citation needed]\nJenkins, a platform for continuous integration. With version 2, Jenkins includes a Pipeline plugin that allows for build instructions to be written in Groovy.[42]\nIDE support[edit]\nMany integrated development environments and text editors support Groovy:\n\nEclipse, through Groovy-Eclipse\nEmacs, using the groovy-emacs-mode project's groovy-mode.\nIntelliJ IDEA, Community Edition, Grails/Griffon in the Ultimate Edition only\nNetBeans, since version 6.5\nTextMate\nDataMelt Java IDE\nSublime Text 2, a cross platform text editor\nJDeveloper, for use with Oracle ADF\njEdit, an advanced text editor for the Java Platform\nKate, an advanced text editor for KDE supports Groovy and over 200 other file formats\nNotepad++, an advanced text editor for Microsoft Windows\nAndroid Studio, IDE used for making Android apps\nAtom IDE\nSee also[edit]\n\tFree software portal\nGriffon – a desktop framework\nGradle – a build automation tool\nProject Zero\nComparison of programming languages",
          "type": "compiled",
          "plid": 21
        },
        {
          "name": "JavaScript",
          "details": "In computing, JavaScript (/ˈdʒævəˌskrɪpt/[5]) is a high-level, dynamic, untyped, and interpreted programming language.[6] It has been standardized in the ECMAScript language specification.[7] Alongside HTML and CSS, JavaScript is one of the three core technologies of World Wide Web content production; the majority of websites employ it, and all modern Web browsers support it without the need for plug-ins.[6] JavaScript is prototype-based with first-class functions, making it a multi-paradigm language, supporting object-oriented,[8] imperative, and functional programming styles.[6] It has an API for working with text, arrays, dates and regular expressions, but does not include any I/O, such as networking, storage, or graphics facilities, relying for these upon the host environment in which it is embedded.[7]\n\nAlthough there are strong outward similarities between JavaScript and Java, including language name, syntax, and respective standard libraries, the two are distinct languages and differ greatly in their design. JavaScript was influenced by programming languages such as Self and Scheme.[9]\n\nJavaScript is also used in environments that are not Web-based, such as PDF documents, site-specific browsers, and desktop widgets. Newer and faster JavaScript virtual machines (VMs) and platforms built upon them have also increased the popularity of JavaScript for server-side Web applications. On the client side, developers have traditionally implemented JavaScript as an interpreted language, but more recent browsers perform just-in-time compilation. Programmers also use JavaScript in video-game development, in crafting desktop and mobile applications, and in server-side network programming with run-time environments such as Node.js.\n\nContents  [hide] \n1\tHistory\n1.1\tBeginnings at Netscape\n1.2\tServer-side JavaScript\n1.3\tAdoption by Microsoft\n1.4\tStandardization\n1.5\tLater developments\n2\tTrademark\n3\tFeatures\n3.1\tImperative and structured\n3.2\tDynamic\n3.3\tPrototype-based (Object-oriented)\n3.4\tFunctional\n3.5\tDelegative\n3.6\tMiscellaneous\n3.7\tVendor-specific extensions\n4\tSyntax\n4.1\tSimple examples\n4.2\tMore advanced example\n5\tUse in Web pages\n5.1\tExample script\n5.2\tCompatibility considerations\n6\tSecurity\n6.1\tCross-site vulnerabilities\n6.1.1\tMisplaced trust in the client\n6.1.2\tBrowser and plugin coding errors\n6.1.3\tSandbox implementation errors\n7\tUses outside Web pages\n7.1\tEmbedded scripting language\n7.2\tScripting engine\n7.3\tApplication platform\n8\tDevelopment tools\n9\tBenchmark tools for developers\n10\tVersion history\n11\tRelated languages and features\n11.1\tUse as an intermediate language\n11.2\tJavaScript and Java\n12\tReferences\n13\tFurther reading\n14\tExternal links\nHistory[edit]\nBeginnings at Netscape[edit]\nIn 1993, the National Center for Supercomputing Applications (NCSA), a unit of the University of Illinois at Urbana-Champaign, released NCSA Mosaic, the first popular graphical Web browser, which played an important part in expanding the growth of the nascent World Wide Web. In 1994, a company called Mosaic Communications was founded in Mountain View, California and employed many of the original NCSA Mosaic authors to create Mosaic Netscape. However, it intentionally shared no code with NCSA Mosaic. The internal codename for the company's browser was Mozilla, which stood for \"Mosaic killer\", as the company's goal was to displace NCSA Mosaic as the world's number one web browser. The first version of the Web browser, Mosaic Netscape 0.9, was released in late 1994. Within four months it had already taken three-quarters of the browser market and became the main browser for Internet in the 1990s. To avoid trademark ownership problems with the NCSA, the browser was subsequently renamed Netscape Navigator in the same year, and the company took the name Netscape Communications.\n\nNetscape Communications realized that the Web needed to become more dynamic. Marc Andreessen, the founder of the company believed that HTML needed a \"glue language\" that was easy to use by Web designers and part-time programmers to assemble components such as images and plugins, where the code could be written directly in the Web page markup. In 1995, the company recruited Brendan Eich with the goal of embedding the Scheme programming language into its Netscape Navigator. Before he could get started, Netscape Communications collaborated with Sun Microsystems to include in Netscape Navigator Sun's more static programming language Java, in order to compete with Microsoft for user adoption of Web technologies and platforms.[10] Netscape Communications then decided that the scripting language they wanted to create would complement Java and should have a similar syntax, which excluded adopting other languages such as Perl, Python, TCL, or Scheme. To defend the idea of JavaScript against competing proposals, the company needed a prototype. Eich wrote one in 10 days, in May 1995.\n\nAlthough it was developed under the name Mocha, the language was officially called LiveScript when it first shipped in beta releases of Netscape Navigator 2.0 in September 1995, but it was renamed JavaScript[11] when it was deployed in the Netscape Navigator 2.0 beta 3 in December.[12] The final choice of name caused confusion, giving the impression that the language was a spin-off of the Java programming language, and the choice has been characterized as a marketing ploy by Netscape to give JavaScript the cachet of what was then the hot new Web programming language.\n\nThere is a common misconception that JavaScript was influenced by an earlier Web page scripting language developed by Nombas named C-- (not to be confused with the later C-- created in 1997).[13][14] Brendan Eich, however, had never heard of C-- before he created LiveScript.[15] Nombas did pitch their embedded Web page scripting to Netscape, though Web page scripting was not a new concept, as shown by the ViolaWWW Web browser.[16] Nombas later switched to offering JavaScript instead of C-- in their ScriptEase product and was part of the TC39 group that standardized ECMAScript.[17]\n\nServer-side JavaScript[edit]\nNetscape introduced an implementation of the language for server-side scripting with Netscape Enterprise Server in December 1995, soon after releasing JavaScript for browsers.[18] Since the mid-2000s, there has been a resurgence of server-side JavaScript implementations, such as Node.js.[19] and MarkLogic.[20]\n\nAdoption by Microsoft[edit]\nMicrosoft script technologies including VBScript and JScript were released in 1996. JScript, a reverse-engineered implementation of Netscape's JavaScript, was part of Internet Explorer 3 as well as being available server-side in Internet Information Server. Internet Explorer 3 also included Microsoft's first support for CSS and various extensions to HTML, but in each case the implementation was noticeably different to that found in Netscape Navigator at the time.[21][22] These differences made it difficult for designers and programmers to make a single website work well in both browsers, leading to the use of \"best viewed in Netscape\" and \"best viewed in Internet Explorer\" logos that characterized these early years of the browser wars.[23] JavaScript began to acquire a reputation for being one of the roadblocks to a cross-platform and standards-driven Web. Some developers took on the difficult task of trying to make their sites work in both major browsers, but many could not afford the time.[21] With the release of Internet Explorer 4, Microsoft introduced the concept of Dynamic HTML, but the differences in language implementations and the different and proprietary Document Object Models remained and were obstacles to widespread take-up of JavaScript on the Web.[21]\n\nStandardization[edit]\nIn November 1996, Netscape submitted JavaScript to Ecma International to carve out a standard specification, which other browser vendors could then implement based on the work done at Netscape. This led to the official release of the language specification ECMAScript published in the first edition of the ECMA-262 standard in June 1997, with JavaScript being the most well known of the implementations. ActionScript and JScript are other well-known implementations of ECMAScript, with extensions.\n\nThe standards process continued in cycles, with the release of ECMAScript 2 in June 1998, which brings some modifications to conform to the ISO/IEC 16262 international standard. The release of ECMAScript 3 followed in December 1999, which is the baseline for modern day JavaScript. The original ECMAScript 4 work led by Waldemar Horwat (then at Netscape, now at Google) started in 2000 and at first, Microsoft seemed to participate and even implemented some of the proposals in their JScript .NET language.\n\nOver time it was clear though that Microsoft had no intention of cooperating or implementing proper JavaScript in Internet Explorer, even though they had no competing proposal and they had a partial (and diverged at this point) implementation on the .NET server side. So by 2003, the original ECMAScript 4 work was mothballed.\n\nThe next major event was in 2005, with two major happenings in JavaScript's history. First, Brendan Eich and Mozilla rejoined Ecma International as a not-for-profit member and work started on ECMAScript for XML (E4X), the ECMA-357 standard, which came from ex-Microsoft employees at BEA Systems (originally acquired as Crossgain). This led to working jointly with Macromedia (later acquired by Adobe Systems), who were implementing E4X in ActionScript 3 (ActionScript 3 was a fork of original ECMAScript 4).\n\nSo, along with Macromedia, work restarted on ECMAScript 4 with the goal of standardizing what was in ActionScript 3. To this end, Adobe Systems released the ActionScript Virtual Machine 2, code named Tamarin, as an open source project. But Tamarin and ActionScript 3 were too different from web JavaScript to converge, as was realized by the parties in 2007 and 2008.\n\nAlas, there was still turmoil between the various players; Douglas Crockford—then at Yahoo!—joined forces with Microsoft in 2007 to oppose ECMAScript 4, which led to the ECMAScript 3.1 effort. The development of ECMAScript 4 was never completed, but that work influenced subsequent versions.[24]\n\nWhile all of this was happening, the open source and developer communities set to work to revolutionize what could be done with JavaScript. This community effort was sparked in 2005 when Jesse James Garrett released a white paper in which he coined the term Ajax, and described a set of technologies, of which JavaScript was the backbone, used to create web applications where data can be loaded in the background, avoiding the need for full page reloads and leading to more dynamic applications. This resulted in a renaissance period of JavaScript usage spearheaded by open source libraries and the communities that formed around them, with libraries such as Prototype, jQuery, Dojo Toolkit, MooTools and others being released.\n\nIn July 2008, the disparate parties on either side came together in Oslo. This led to the eventual agreement in early 2009 to rename ECMAScript 3.1 to ECMAScript 5 and drive the language forward using an agenda that is known as Harmony. ECMAScript 5 was finally released in December 2009.\n\nIn June 2011, ECMAScript 5.1 was released to fully align with the third edition of the ISO/IEC 16262 international standard. ECMAScript 2015 was released in June 2015. The current version is ECMAScript 2016, released in June 2016.[25]\n\nLater developments[edit]\nJavaScript has become one of the most popular programming languages on the Web. Initially, however, many professional programmers denigrated the language because, among other reasons, its target audience consisted of Web authors and other such \"amateurs\".[26] The advent of Ajax returned JavaScript to the spotlight and brought more professional programming attention. The result was a proliferation of comprehensive frameworks and libraries, improved JavaScript programming practices, and increased usage of JavaScript outside Web browsers, as seen by the proliferation of server-side JavaScript platforms.\n\nIn January 2009, the CommonJS project was founded with the goal of specifying a common standard library mainly for JavaScript development outside the browser.[27]\n\nWith the rise of single-page applications and JavaScript-heavy sites, it is increasingly being used as a compile target for source-to-source compilers from both dynamic languages and static languages.\n\nTrademark[edit]\n\"JavaScript\" is a trademark of Oracle Corporation.[28] It is used under license for technology invented and implemented by Netscape Communications and current entities such as the Mozilla Foundation.[29]\n\nFeatures[edit]\nThe following features are common to all conforming ECMAScript implementations, unless explicitly specified otherwise.\n\nImperative and structured[edit]\nJavaScript supports much of the structured programming syntax from C (e.g., if statements, while loops, switch statements, do while loops, etc.). One partial exception is scoping: JavaScript originally had only function scoping with var. ECMAScript 2015 added a let keyword for block scoping, meaning JavaScript now has both function and block scoping. Like C, JavaScript makes a distinction between expressions and statements. One syntactic difference from C is automatic semicolon insertion, which allows the semicolons that would normally terminate statements to be omitted.[30]\n\nDynamic[edit]\nTyping\nAs with most scripting languages, JavaScript is dynamically typed; a type is associated with each value, rather than just with each expression. For example, a variable that is at one time bound to a number may later be re-bound to a string.[31] JavaScript supports various ways to test the type of an object, including duck typing.[32]\nRun-time evaluation\nJavaScript includes an eval function that can execute statements provided as strings at run-time.\nPrototype-based (Object-oriented)[edit]\nJavaScript is almost entirely object-based. In JavaScript, an object is an associative array, augmented with a prototype (see below); each string key provides the name for an object property, and there are two syntactical ways to specify such a name: dot notation (obj.x = 10) and bracket notation (obj['x'] = 10). A property may be added, rebound, or deleted at run-time. Most properties of an object (and any property that belongs to an object's prototype inheritance chain) can be enumerated using a for...in loop.\n\nJavaScript has a small number of built-in objects, including Function and Date.\n\nPrototypes\nJavaScript uses prototypes where many other object-oriented languages use classes for inheritance.[33] It is possible to simulate many class-based features with prototypes in JavaScript.[34]\nFunctions as object constructors\nFunctions double as object constructors, along with their typical role. Prefixing a function call with new will create an instance of a prototype, inheriting properties and methods from the constructor (including properties from the Object prototype).[35] ECMAScript 5 offers the Object.create method, allowing explicit creation of an instance without automatically inheriting from the Object prototype (older environments can assign the prototype to null).[36] The constructor's prototype property determines the object used for the new object's internal prototype. New methods can be added by modifying the prototype of the function used as a constructor. JavaScript's built-in constructors, such as Array or Object, also have prototypes that can be modified. While it is possible to modify the Object prototype, it is generally considered bad practice because most objects in JavaScript will inherit methods and properties from the Object prototype, and they may not expect the prototype to be modified.[37]\nFunctions as methods\nUnlike many object-oriented languages, there is no distinction between a function definition and a method definition. Rather, the distinction occurs during function calling; when a function is called as a method of an object, the function's local this keyword is bound to that object for that invocation.\nFunctional[edit]\nA function is first-class; a function is considered to be an object. As such, a function may have properties and methods, such as .call() and .bind().[38] A nested function is a function defined within another function. It is created each time the outer function is invoked. In addition, each nested function forms a lexical closure: The lexical scope of the outer function (including any constant, local variable, or argument value) becomes part of the internal state of each inner function object, even after execution of the outer function concludes.[39] JavaScript also supports anonymous functions.\n\nDelegative[edit]\nJavaScript supports implicit and explicit delegation.\n\nFunctions as Roles (Traits and Mixins)\nJavaScript natively supports various function-based implementations of Role[40] patterns like Traits[41][42] and Mixins.[43] Such a function defines additional behavior by at least one method bound to the this keyword within its function body. A Role then has to be delegated explicitly via call or apply to objects that need to feature additional behavior that is not shared via the prototype chain.\nObject Composition and Inheritance\nWhereas explicit function-based delegation does cover composition in JavaScript, implicit delegation already happens every time the prototype chain is walked in order to, e.g., find a method that might be related to but is not directly owned by an object. Once the method is found it gets called within this object's context. Thus inheritance in JavaScript is covered by a delegation automatism that is bound to the prototype property of constructor functions.\nMiscellaneous[edit]\nRun-time environment\nJavaScript typically relies on a run-time environment (e.g., a Web browser) to provide objects and methods by which scripts can interact with the environment (e.g., a webpage DOM). It also relies on the run-time environment to provide the ability to include/import scripts (e.g., HTML <script> elements). This is not a language feature per se, but it is common in most JavaScript implementations.\nJavaScript processes messages from a queue one at a time. Upon loading a new message, JavaScript calls a function associated with that message, which creates a call stack frame (the function's arguments and local variables). The call stack shrinks and grows based on the function's needs. Upon function completion, when the stack is empty, JavaScript proceeds to the next message in the queue. This is called the event loop, described as \"run to completion\" because each message is fully processed before the next message is considered. However, the language's concurrency model describes the event loop as non-blocking: program input/output is performed using events and callback functions. This means, for instance, that JavaScript can process a mouse click while waiting for a database query to return information.[44]\nVariadic functions\nAn indefinite number of parameters can be passed to a function. The function can access them through formal parameters and also through the local arguments object. Variadic functions can also be created by using the bind method.\nArray and object literals\nLike many scripting languages, arrays and objects (associative arrays in other languages) can each be created with a succinct shortcut syntax. In fact, these literals form the basis of the JSON data format.\nRegular expressions\nJavaScript also supports regular expressions in a manner similar to Perl, which provide a concise and powerful syntax for text manipulation that is more sophisticated than the built-in string functions.[45]\nVendor-specific extensions[edit]\nJavaScript is officially managed by Mozilla Foundation, and new language features are added periodically. However, only some JavaScript engines support these new features:\n\nproperty getter and setter functions (supported by WebKit, Gecko, Opera,[46] ActionScript, and Rhino)[47]\nconditional catch clauses\niterator protocol (adopted from Python)\nshallow generators-coroutines (adopted from Python)\narray comprehensions and generator expressions (adopted from Python)\nproper block scope via the let keyword\narray and object destructuring (limited form of pattern matching)\nconcise function expressions (function(args) expr)\nECMAScript for XML (E4X), an extension that adds native XML support to ECMAScript (unsupported in Firefox since version 21[48])\nSyntax[edit]\nMain article: JavaScript syntax\nSimple examples[edit]\nVariables in JavaScript can be defined using the var keyword:[49]\n\nvar x; // defines the variable x and assigns to it the special value \"undefined\" (not to be confused with an undefined value)\nvar y = 2; // defines the variable y and assigns to it the value 2\nNote the comments in the example above, both of which were preceded with two forward slashes.\n\nThere is no built-in I/O functionality in JavaScript; the run-time environment provides that. The ECMAScript specification in edition 5.1 mentions:[50]\n\n… indeed, there are no provisions in this specification for input of external data or output of computed results.\n\nHowever, most runtime environments have a console object [51] that can be used to print output. Here is a minimalist Hello World program:\n\nconsole.log(\"Hello World!\");\nA simple recursive function:\n\nfunction factorial(n) {\n    if (n == 0) {\n        return 1;\n    }\n\n    return n * factorial(n - 1);\n}\n\nfactorial(3); // returns 6\nAn anonymous function (or lambda):\n\nfunction counter() {\n    var count = 0;\n    return function () {\n        return ++count;\n    };\n};\n\nvar closure = counter();\nclosure(); // returns 1\nclosure(); // returns 2\nclosure(); // returns 3\nThis example shows that in JavaScript, function closures captures their non-local variables by reference.\n\nVariadic function demonstration (arguments is a special variable):[52]\n\nfunction sum() {\n    var x = 0;\n\n    for (var i = 0; i < arguments.length; ++i) {\n        x += arguments[i];\n    }\n\n    return x;\n}\n\nsum(1, 2); // returns 3\nsum(1, 2, 3); // returns 6\nImmediately-invoked function expressions are often used to create modules, as before ECMAScript 2015 there was not built-in construct in the language. Modules allow gathering properties and methods in a namespace and making some of them private:\n\nvar counter = (function () {\n    var i = 0; // private property\n\n    return {   // public methods\n        get: function () {\n            alert(i);\n        },\n        set: function (value) {\n            i = value;\n        },\n        increment: function () {\n            alert(++i);\n        }\n    };\n})(); // module\n\ncounter.get();       // shows 0\ncounter.set(6);\ncounter.increment(); // shows 7\ncounter.increment(); // shows 8\nMore advanced example[edit]\nThis sample code displays various JavaScript features.\n\n/* Finds the lowest common multiple (LCM) of two numbers */\nfunction LCMCalculator(x, y) { // constructor function\n    var checkInt = function (x) { // inner function\n        if (x % 1 !== 0) {\n            throw new TypeError(x + \" is not an integer\"); // throw an exception\n        }\n        return x;\n    };\n    this.a = checkInt(x)\n    //   semicolons   ^^^^  are optional, a newline is enough\n    this.b = checkInt(y);\n}\n// The prototype of object instances created by a constructor is\n// that constructor's \"prototype\" property.\nLCMCalculator.prototype = { // object literal\n    constructor: LCMCalculator, // when reassigning a prototype, set the constructor property appropriately\n    gcd: function () { // method that calculates the greatest common divisor\n        // Euclidean algorithm:\n        var a = Math.abs(this.a), b = Math.abs(this.b), t;\n        if (a < b) {\n            // swap variables\n            t = b;\n            b = a;\n            a = t;\n        }\n        while (b !== 0) {\n            t = b;\n            b = a % b;\n            a = t;\n        }\n        // Only need to calculate GCD once, so \"redefine\" this method.\n        // (Actually not redefinition—it's defined on the instance itself,\n        // so that this.gcd refers to this \"redefinition\" instead of LCMCalculator.prototype.gcd.\n        // Note that this leads to a wrong result if the LCMCalculator object members \"a\" and/or \"b\" are altered afterwards.)\n        // Also, 'gcd' === \"gcd\", this['gcd'] === this.gcd\n        this['gcd'] = function () {\n            return a;\n        };\n        return a;\n    },\n    // Object property names can be specified by strings delimited by double (\") or single (') quotes.\n    lcm : function () {\n        // Variable names don't collide with object properties, e.g., |lcm| is not |this.lcm|.\n        // not using |this.a*this.b| to avoid FP precision issues\n        var lcm = this.a/this.gcd()*this.b;\n        // Only need to calculate lcm once, so \"redefine\" this method.\n        this.lcm = function () {\n            return lcm;\n        };\n        return lcm;\n    },\n    toString: function () {\n        return \"LCMCalculator: a = \" + this.a + \", b = \" + this.b;\n    }\n};\n\n// Define generic output function; this implementation only works for Web browsers\nfunction output(x) {\n    document.body.appendChild(document.createTextNode(x));\n    document.body.appendChild(document.createElement('br'));\n}\n\n// Note: Array's map() and forEach() are defined in JavaScript 1.6.\n// They are used here to demonstrate JavaScript's inherent functional nature.\n[[25, 55], [21, 56], [22, 58], [28, 56]].map(function (pair) { // array literal + mapping function\n    return new LCMCalculator(pair[0], pair[1]);\n}).sort(function (a, b) { // sort with this comparative function\n    return a.lcm() - b.lcm();\n}).forEach(function (obj) {\n    output(obj + \", gcd = \" + obj.gcd() + \", lcm = \" + obj.lcm());\n});\nThe following output should be displayed in the browser window.\n\nLCMCalculator: a = 28, b = 56, gcd = 28, lcm = 56\nLCMCalculator: a = 21, b = 56, gcd = 7, lcm = 168\nLCMCalculator: a = 25, b = 55, gcd = 5, lcm = 275\nLCMCalculator: a = 22, b = 58, gcd = 2, lcm = 638\nUse in Web pages[edit]\nSee also: Dynamic HTML and Ajax (programming)\nThe most common use of JavaScript is to add client-side behavior to HTML pages, also known as Dynamic HTML (DHTML). Scripts are embedded in or included from HTML pages and interact with the Document Object Model (DOM) of the page. Some simple examples of this usage are:\n\nLoading new page content or submitting data to the server via Ajax without reloading the page (for example, a social network might allow the user to post status updates without leaving the page).\nAnimation of page elements, fading them in and out, resizing them, moving them, etc.\nInteractive content, for example games, and playing audio and video.\nValidating input values of a Web form to make sure that they are acceptable before being submitted to the server.\nTransmitting information about the user's reading habits and browsing activities to various websites. Web pages frequently do this for Web analytics, ad tracking, personalization or other purposes.[53]\nBecause JavaScript code can run locally in a user's browser (rather than on a remote server), the browser can respond to user actions quickly, making an application more responsive. Furthermore, JavaScript code can detect user actions that HTML alone cannot, such as individual keystrokes. Applications such as Gmail take advantage of this: much of the user-interface logic is written in JavaScript, and JavaScript dispatches requests for information (such as the content of an e-mail message) to the server. The wider trend of Ajax programming similarly exploits this strength.\n\nA JavaScript engine (also known as JavaScript interpreter or JavaScript implementation) is an interpreter that interprets JavaScript source code and executes the script accordingly. The first JavaScript engine was created by Brendan Eich at Netscape, for the Netscape Navigator Web browser. The engine, code-named SpiderMonkey, is implemented in C. It has since been updated (in JavaScript 1.5) to conform to ECMAScript 3. The Rhino engine, created primarily by Norris Boyd (formerly at Netscape, now at Google) is a JavaScript implementation in Java. Rhino, like SpiderMonkey, is ECMAScript 3 compliant.\n\nA Web browser is by far the most common host environment for JavaScript. Web browsers typically create \"host objects\" to represent the DOM in JavaScript. The Web server is another common host environment. A JavaScript Web server would typically expose host objects representing HTTP request and response objects, which a JavaScript program could then interrogate and manipulate to dynamically generate Web pages.\n\nBecause JavaScript is the only language that the most popular browsers share support for, it has become a target language for many frameworks in other languages, even though JavaScript was never intended to be such a language.[54] Despite the performance limitations inherent to its dynamic nature, the increasing speed of JavaScript engines has made the language a surprisingly feasible compilation target.\n\nExample script[edit]\nBelow is a minimal example of a standards-conforming Web page containing JavaScript (using HTML 5 syntax) and the DOM:\n\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Example</title>\n  </head>\n  <body>\n    <button id=\"hellobutton\">Hello</button>\n    <script>\n        document.getElementById('hellobutton').onclick = function() {\n            alert('Hello world!');                     // Show a dialog\n            var myTextNode = document.createTextNode('Some new words.');\n            document.body.appendChild(myTextNode);     // Append \"Some new words\" to the page\n        }\n    </script>\n  </body>\n</html>\nCompatibility considerations[edit]\nMain article: Web interoperability\nBecause JavaScript runs in widely varying environments, an important part of testing and debugging is to test and verify that the JavaScript works across multiple browsers.\n\nThe DOM interfaces for manipulating Web pages are not part of the ECMAScript standard, or of JavaScript itself. Officially, the DOM interfaces are defined by a separate standardization effort by the W3C; in practice, browser implementations differ from the standards and from each other, and not all browsers execute JavaScript.\n\nTo deal with these differences, JavaScript authors can attempt to write standards-compliant code that will also be executed correctly by most browsers; failing that, they can write code that checks for the presence of certain browser features and behaves differently if they are not available.[55] In some cases, two browsers may both implement a feature but with different behavior, and authors may find it practical to detect what browser is running and change their script's behavior to match.[56][57] Programmers may also use libraries or toolkits that take browser differences into account.\n\nFurthermore, scripts may not work for some users. For example, a user may:\n\nuse an old or rare browser with incomplete or unusual DOM support;\nuse a PDA or mobile phone browser that cannot execute JavaScript;\nhave JavaScript execution disabled as a security precaution;\nuse a speech browser due to, for example, a visual disability.\nTo support these users, Web authors can try to create pages that degrade gracefully on user agents (browsers) that do not support the page's JavaScript. In particular, the page should remain usable albeit without the extra features that the JavaScript would have added. An alternative approach that many find preferable is to first author content using basic technologies that work in all browsers, then enhance the content for users that have JavaScript enabled. This is known as progressive enhancement.\n\nSecurity[edit]\nSee also: Browser security\nJavaScript and the DOM provide the potential for malicious authors to deliver scripts to run on a client computer via the Web. Browser authors contain this risk using two restrictions. First, scripts run in a sandbox in which they can only perform Web-related actions, not general-purpose programming tasks like creating files. Second, scripts are constrained by the same origin policy: scripts from one Web site do not have access to information such as usernames, passwords, or cookies sent to another site. Most JavaScript-related security bugs are breaches of either the same origin policy or the sandbox.\n\nThere are subsets of general JavaScript—ADsafe, Secure ECMAScript (SES)—that provide greater level of security, especially on code created by third parties (such as advertisements).[58][59] Caja is another project for safe embedding and isolation of third-party JavaScript and HTML.\n\nContent Security Policy is the main intended method of ensuring that only trusted code is executed on a Web page.\n\nSee also: Content Security Policy\nCross-site vulnerabilities[edit]\nMain articles: Cross-site scripting and Cross-site request forgery\nA common JavaScript-related security problem is cross-site scripting (XSS), a violation of the same-origin policy. XSS vulnerabilities occur when an attacker is able to cause a target Web site, such as an online banking website, to include a malicious script in the webpage presented to a victim. The script in this example can then access the banking application with the privileges of the victim, potentially disclosing secret information or transferring money without the victim's authorization. A solution to XSS vulnerabilities is to use HTML escaping whenever displaying untrusted data.\n\nSome browsers include partial protection against reflected XSS attacks, in which the attacker provides a URL including malicious script. However, even users of those browsers are vulnerable to other XSS attacks, such as those where the malicious code is stored in a database. Only correct design of Web applications on the server side can fully prevent XSS.\n\nXSS vulnerabilities can also occur because of implementation mistakes by browser authors.[60]\n\nAnother cross-site vulnerability is cross-site request forgery (CSRF). In CSRF, code on an attacker's site tricks the victim's browser into taking actions the user didn't intend at a target site (like transferring money at a bank). It works because, if the target site relies only on cookies to authenticate requests, then requests initiated by code on the attacker's site will carry the same legitimate login credentials as requests initiated by the user. In general, the solution to CSRF is to require an authentication value in a hidden form field, and not only in the cookies, to authenticate any request that might have lasting effects. Checking the HTTP Referrer header can also help.\n\n\"JavaScript hijacking\" is a type of CSRF attack in which a <script> tag on an attacker's site exploits a page on the victim's site that returns private information such as JSON or JavaScript. Possible solutions include:\n\nrequiring an authentication token in the POST and GET parameters for any response that returns private information.\nMisplaced trust in the client[edit]\nDevelopers of client-server applications must recognize that untrusted clients may be under the control of attackers. The application author cannot assume that his JavaScript code will run as intended (or at all) because any secret embedded in the code could be extracted by a determined adversary. Some implications are:\n\nWeb site authors cannot perfectly conceal how their JavaScript operates because the raw source code must be sent to the client. The code can be obfuscated, but obfuscation can be reverse-engineered.\nJavaScript form validation only provides convenience for users, not security. If a site verifies that the user agreed to its terms of service, or filters invalid characters out of fields that should only contain numbers, it must do so on the server, not only the client.\nScripts can be selectively disabled, so JavaScript can't be relied on to prevent operations such as right-clicking on an image to save it.[61]\nIt is extremely bad practice to embed sensitive information such as passwords in JavaScript because it can be extracted by an attacker.\nBrowser and plugin coding errors[edit]\nJavaScript provides an interface to a wide range of browser capabilities, some of which may have flaws such as buffer overflows. These flaws can allow attackers to write scripts that would run any code they wish on the user's system. This code is not by any means limited to another JavaScript application. For example, a buffer overrun exploit can allow an attacker to gain access to the operating system's API with superuser privileges.\n\nThese flaws have affected major browsers including Firefox,[62] Internet Explorer,[63] and Safari.[64]\n\nPlugins, such as video players, Adobe Flash, and the wide range of ActiveX controls enabled by default in Microsoft Internet Explorer, may also have flaws exploitable via JavaScript (such flaws have been exploited in the past).[65][66]\n\nIn Windows Vista, Microsoft has attempted to contain the risks of bugs such as buffer overflows by running the Internet Explorer process with limited privileges.[67] Google Chrome similarly confines its page renderers to their own \"sandbox\".\n\nSandbox implementation errors[edit]\nWeb browsers are capable of running JavaScript outside the sandbox, with the privileges necessary to, for example, create or delete files. Of course, such privileges aren't meant to be granted to code from the Web.\n\nIncorrectly granting privileges to JavaScript from the Web has played a role in vulnerabilities in both Internet Explorer[68] and Firefox.[69] In Windows XP Service Pack 2, Microsoft demoted JScript's privileges in Internet Explorer.[70]\n\nMicrosoft Windows allows JavaScript source files on a computer's hard drive to be launched as general-purpose, non-sandboxed programs (see: Windows Script Host). This makes JavaScript (like VBScript) a theoretically viable vector for a Trojan horse, although JavaScript Trojan horses are uncommon in practice.[71]\n\nUses outside Web pages[edit]\nIn addition to Web browsers and servers, JavaScript interpreters are embedded in a number of tools. Each of these applications provides its own object model that provides access to the host environment. The core JavaScript language remains mostly the same in each application.\n\nEmbedded scripting language[edit]\nGoogle's Chrome extensions, Opera's extensions, Apple's Safari 5 extensions, Apple's Dashboard Widgets, Microsoft's Gadgets, Yahoo! Widgets, Google Desktop Gadgets, and Serence Klipfolio are implemented using JavaScript.\nThe MongoDB database accepts queries written in JavaScript. MongoDB and NodeJS are the core components of MEAN: a solution stack for creating Web applications using just JavaScript.\nThe Clusterpoint database accept queries written in JS/SQL, which is a combination of SQL and JavaScript. Clusterpoint has built-in computing engine that allows execution of JavaScript code right inside the distributed database.\nAdobe's Acrobat and Adobe Reader support JavaScript in PDF files.[72]\nTools in the Adobe Creative Suite, including Photoshop, Illustrator, Dreamweaver, and InDesign, allow scripting through JavaScript.\nOpenOffice.org, an office application suite, as well as its popular fork LibreOffice, allows JavaScript to be used as a scripting language.\nThe visual programming language Max, released by Cycling '74, offers a JavaScript model of its environment for use by developers. It allows users to reduce visual clutter by using a object for a task rather than many.\nApple's Logic Pro X digital audio workstation (DAW) software can create custom MIDI effects plugins using JavaScript.[citation needed]\nECMAScript was included in the VRML97 standard for scripting nodes of VRML scene description files.[citation needed]\nThe Unity game engine supports a modified version of JavaScript for scripting via Mono.[73]\nDX Studio (3D engine) uses the SpiderMonkey implementation of JavaScript for game and simulation logic.[74]\nMaxwell Render (rendering software) provides an ECMA standard based scripting engine for tasks automation.[75]\nGoogle Apps Script in Google Spreadsheets and Google Sites allows users to create custom formulas, automate repetitive tasks and also interact with other Google products such as Gmail.[76]\nMany IRC clients, like ChatZilla or XChat, use JavaScript for their scripting abilities.[77][78]\nRPG Maker MV uses Javascript as its scripting language.[79]\nScripting engine[edit]\nMicrosoft's Active Scripting technology supports JScript as a scripting language.[80]\nJava introduced the javax.script package in version 6 that includes a JavaScript implementation based on Mozilla Rhino. Thus, Java applications can host scripts that access the application's variables and objects, much like Web browsers host scripts that access a webpage's Document Object Model (DOM).[81][82]\nThe Qt C++ toolkit includes a QtScript module to interpret JavaScript, analogous to Java's javax.script package.[83]\nOS X Yosemite introduced JavaScript for Automation (JXA), which is built upon JavaScriptCore and the Open Scripting Architecture. It features an Objective-C bridge that enables entire Cocoa applications to be programmed in JavaScript.\nLate Night Software's JavaScript OSA (also known as JavaScript for OSA, or JSOSA) is a freeware alternative to AppleScript for OS X. It is based on the Mozilla JavaScript 1.5 implementation, with the addition of a MacOS object for interaction with the operating system and third-party applications.[84]\nApplication platform[edit]\nActionScript, the programming language used in Adobe Flash, is another implementation of the ECMAScript standard.\nAdobe Integrated Runtime is a JavaScript runtime that allows developers to create desktop applications.\nAtom, the open-source text editor developed by Github, was implemented using Javascript, and has a special API on Javascript for packages that are developed for it.\nCA, Inc.'s AutoShell cross-application scripting environment is built on the SpiderMonkey JavaScript engine. It contains preprocessor-like extensions for command definition, as well as custom classes for various system-related tasks like file I/O, operation system command invocation and redirection, and COM scripting.\nGNOME Shell, the shell for the GNOME 3 desktop environment,[85] made JavaScript its default programming language in 2013.[86]\nThe Mozilla platform, which underlies Firefox, Thunderbird, and some other Web browsers, uses JavaScript to implement the graphical user interface (GUI) of its various products.\nQt Quick's markup language (available since Qt 4.7) uses JavaScript for its application logic. Its declarative syntax is also similar to JavaScript.\nTypeScript is a programming language based on JavaScript that adds support for optional type annotations and some other language extensions such as classes, interfaces and modules. A TS-script compiles into plain JavaScript and can be executed in any JS host supporting ECMAScript 3 or higher. The compiler is itself written in TypeScript.\nUbuntu Touch provides a JavaScript API for its unified usability interface.\nwebOS uses the WebKit implementation of JavaScript in its SDK to allow developers to create stand-alone applications solely in JavaScript.\nWinJS provides a special Windows Library for JavaScript functionality in Windows 8 that enables the development of Modern style (formerly Metro style) applications in HTML5 and JavaScript.\nDevelopment tools[edit]\nWithin JavaScript, access to a debugger becomes invaluable when developing large, non-trivial programs. Because there can be implementation differences between the various browsers (particularly within the DOM), it is useful to have access to a debugger for each of the browsers that a Web application targets.[87]\n\nScript debuggers are integrated within Internet Explorer, Firefox, Safari, Google Chrome, Opera and Node.js.[88][89][90]\n\nIn addition to the native Internet Explorer Developer Tools, three debuggers are available for Internet Explorer: Microsoft Visual Studio is the richest of the three, closely followed by Microsoft Script Editor (a component of Microsoft Office),[91] and finally the free Microsoft Script Debugger that is far more basic than the other two. The free Microsoft Visual Web Developer Express provides a limited version of the JavaScript debugging functionality in Microsoft Visual Studio. Internet Explorer has included developer tools since version 8.\n\nIn comparison to Internet Explorer, Firefox has a more comprehensive set of developer tools, which include a debugger as well. Old versions of Firefox without these tools used a Firefox addon called Firebug, or the older Venkman debugger. Also, WebKit's Web Inspector includes a JavaScript debugger,[92] which is used in Safari. A modified version called Blink DevTools is used in Google Chrome. Node.js has Node Inspector, an interactive debugger that integrates with the Blink DevTools, available in Google Chrome. Opera includes a set of tools called Dragonfly.[93]\n\nIn addition to the native computer software, there are online JavaScript IDEs, debugging aids that are themselves written in JavaScript and built to run on the Web. An example is the program JSLint, developed by Douglas Crockford who has written extensively on the language. JSLint scans JavaScript code for conformance to a set of standards and guidelines. Many libraries for JavaScript, such as three.js, provide links to demonstration code that can be edited by users. They are also used as a pedagogical tool by institutions such as Khan Academy[94] to allow students to experience writing code in an environment where they can see the output of their programs, without needing any setup beyond a Web browser.\n\nBenchmark tools for developers[edit]\nSince JavaScript is getting more important for web development (frontend overtakes many aspects which were done in backend before), there is also more consideration done about performance. Especially mobile devices could have problems with rendering and processing unoptimized complex logic.\n\nA library for doing benchmarks is benchmark.js. A benchmarking library that supports high-resolution timers and returns statistically significant results.\n\nAnother tool is jsben.ch. An online JavaScript benchmarking tool, where code snippets can be tested against each other.\n\nVersion history[edit]\nSee also: ECMAScript § Versions, and ECMAScript § Version correspondence\nJavaScript was initially developed in 1996 for use in the Netscape Navigator Web browser. In the same year Microsoft released an implementation for Internet Explorer. This implementation was called JScript due to trademark issues. In 1997 the first standardized version of the language was released under the name ECMAScript in the first edition of the ECMA-252 standard. The explicit versioning and opt-in of language features was Mozilla-specific and has been removed. Firefox 4 was the last version which referred to a JavaScript version (1.8.5). With new editions of the ECMA-262 standard, JavaScript language features are now often mentioned with their initial definition in the ECMA-262 editions.\n\nThe following table is based on information from multiple sources.[95][96][97]\n\nVersion\tRelease date\tEquivalent to\tNetscape\nNavigator\tMozilla\nFirefox\tInternet\nExplorer\tOpera\tSafari\tGoogle\nChrome\n1.0\tMarch 1996\t\t2.0\t\t3.0\t\t\t\n1.1\tAugust 1996\t\t3.0\t\t\t\t\t\n1.2\tJune 1997\t\t4.0-4.05\t\t\t3[98]\t\t\n1.3\tOctober 1998\tECMA-262 1st + 2nd edition\t4.06-4.7x\t\t4.0\t5[99]\t\t\n1.4\t\t\tNetscape\nServer\t\t\t6\t\t\n1.5\tNovember 2000\tECMA-262 3rd edition\t6.0\t1.0\t5.5 (JScript 5.5),\n6 (JScript 5.6),\n7 (JScript 5.7),\n8 (JScript 5.8)\t7.0\t3.0-5\t1.0-10.0.666\n1.6\tNovember 2005\t1.5 + array extras + array and string generics + E4X\t\t1.5\t\t\t\t\n1.7\tOctober 2006\t1.6 + Pythonic generators + iterators + let\t\t2.0\t\t\t\t28.0.1500.95\n1.8\tJune 2008\t1.7 + generator expressions + expression closures\t\t3.0\t\t11.50\t\t\n1.8.1\t\t1.8 + native JSON support + minor updates\t\t3.5\t\t\t\t\n1.8.2\tJune 22, 2009\t1.8.1 + minor updates\t\t3.6\t\t\t\t\n1.8.5\tJuly 27, 2010\t1.8.2 + new features for ECMA-262 5th edition compliance\t\t4.0\t\t\t\t\nRelated languages and features[edit]\nJSON, or JavaScript Object Notation, is a general-purpose data interchange format that is defined as a subset of JavaScript's object literal syntax. Like much of JavaScript (regexps and anonymous functions as 1st class elements, closures, flexible classes, 'use strict'), JSON, except for replacing Perl's key-value operator '=>' by an RFC 822[100] inspired ':', is syntactically pure Perl.\n\njQuery is a popular JavaScript library designed to simplify DOM-oriented client-side HTML scripting along with offering cross-browser compatibility because various browsers respond differently to certain vanilla JavaScript code.\n\nUnderscore.js is a utility JavaScript library for data manipulation that is used in both client-side and server-side network applications.\n\nAngularJS is a web application framework to use for developing single-page applications and also cross-platform mobile apps.\n\nReact (JavaScript library) is an open-source JavaScript library providing a views that is rendered using components specified as custom HTML tags.\n\nMozilla browsers currently support LiveConnect, a feature that allows JavaScript and Java to intercommunicate on the Web. However, Mozilla-specific support for LiveConnect is scheduled to be phased out in the future in favor of passing on the LiveConnect handling via NPAPI to the Java 1.6+ plug-in (not yet supported on the Mac as of March 2010).[101] Most browser inspection tools, such as Firebug in Firefox, include JavaScript interpreters that can act on the visible page's DOM.\n\nasm.js is a subset of JavaScript that can be run in any JavaScript engine or run faster in an ahead-of-time (AOT) compiling engine.[102]\n\nJSFuck is an esoteric programming language. Programs are written using only six different characters, but are still valid JavaScript code.\n\np5.js[103] is an object oriented JavaScript library designed for artists and designers. It is based on the ideas of the Processing project but is for the web.\n\njsben.ch is an online JavaScript benchmarking tool, where different code snippets can be tested against each other.\n\nUse as an intermediate language[edit]\nAs JavaScript is the most widely supported client-side language that can run within a Web browser, it has become an intermediate language for other languages to target. This has included both newly created languages and ports of existing languages. Some of these include:\n\nOberonScript, a full implementation of the Oberon programming language that compiles to high-level JavaScript.[104]\nObjective-J, a superset of JavaScript that compiles to standard JavaScript. It adds traditional inheritance and Smalltalk/Objective-C style dynamic dispatch and optional pseudo-static typing to JavaScript.\nProcessing.js, a JavaScript port of the Processing programming language designed to write visualizations, images, and interactive content. It allows Web browsers to display animations, visual applications, games and other graphical rich content without the need for a Java applet or Flash plugin.\nCoffeeScript, an alternate syntax for JavaScript intended to be more concise and readable. It adds features like array comprehensions (also available in JavaScript since version 1.7)[105] and pattern matching. Like Objective-J, it compiles to JavaScript. Ruby and Python have been cited as influential on CoffeeScript syntax.\nGoogle Web Toolkit translates a subset of Java to JavaScript.\nScala, an object-oriented and functional programming language, has a Scala-to-JavaScript compiler.[106]\nPyjamas, a port of Google Web Toolkit to Python translates a subset of Python to JavaScript.\nDart, an open-source programming language developed by Google, can be compiled to JavaScript.\nWhalesong,[107] a Racket-to-JavaScript compiler.\nEmscripten, a LLVM-backend for porting native libraries to JavaScript.\nFantom a programming language that runs on JVM, .NET and JavaScript.\nTypeScript, a free and open-source programming language developed by Microsoft. It is a superset of JavaScript, and essentially adds optional static typing and class-based object-oriented programming to the language.\nHaxe, an open-source high-level multiplatform programming language and compiler that can produce applications and source code for many different platforms including JavaScript.\nClojureScript,[108] a compiler for Clojure that targets JavaScript. It is designed to emit JavaScript code that is compatible with the advanced compilation mode of the Google Closure optimizing compiler.\nKotlin, a statically-typed language that also compiles to Java byte code.\nSqueakJS, a virtual machine and DOM environment for the open-source Squeak implementation of the Smalltalk programming language.\nAs JavaScript has unusual limitations – such as no separate integer type, using floating point – languages that compile to JavaScript commonly have slightly different behavior than in other environments.\n\nJavaScript and Java[edit]\nA common misconception is that JavaScript is similar or closely related to Java. It is true that both have a C-like syntax (the C language being their most immediate common ancestor language). They also are both typically sandboxed (when used inside a browser), and JavaScript was designed with Java's syntax and standard library in mind. In particular, all Java keywords were reserved in original JavaScript, JavaScript's standard library follows Java's naming conventions, and JavaScript's Math and Date objects are based on classes from Java 1.0,[109] but the similarities end there.\n\nJava and JavaScript both first appeared on 23 May 1995, but Java was developed by James Gosling of Sun Microsystems, and JavaScript by Brendan Eich of NetScape Communications.\n\nThe differences between the two languages are more prominent than their similarities. Java has static typing, while JavaScript's typing is dynamic. Java is loaded from compiled bytecode, while JavaScript is loaded as human-readable source code. Java's objects are class-based, while JavaScript's are prototype-based. Finally, Java did not support functional programming until Java 8, while JavaScript has done so from the beginning, being influenced by Scheme.",
          "type": "intepreted",
          "plid": 22
        },
        {
          "name": "OCaml",
          "details": "OCaml (/oʊˈkæməl/ oh-kam-əl), originally known as Objective Caml, is the main implementation of the Caml programming language, created by Xavier Leroy, Jérôme Vouillon, Damien Doligez, Didier Rémy, Ascánder Suárez and others in 1996. A member of the ML language family, OCaml extends the core Caml language with object-oriented constructs.\n\nOCaml's toolset includes an interactive top-level interpreter, a bytecode compiler, a reversible debugger, a package manager (OPAM), and an optimizing native code compiler. It has a large standard library that makes it useful for many of the same applications as Python or Perl, as well as robust modular and object-oriented programming constructs that make it applicable for large-scale software engineering. OCaml is the successor to Caml Light. The acronym \"CAML\" originally stood for Categorical Abstract Machine Language, although OCaml abandons this abstract machine.[1]\n\nOCaml is a free open-source project managed and principally maintained by INRIA. In recent years,[when?] many new languages have drawn elements from OCaml, most notably F# and Scala.\n\nContents  [hide] \n1\tPhilosophy\n2\tFeatures\n3\tDevelopment environment\n4\tCode examples\n4.1\tHello World\n4.2\tSumming a list of integers\n4.3\tQuicksort\n4.4\tBirthday paradox\n4.5\tChurch numerals\n4.6\tArbitrary-precision factorial function (libraries)\n4.7\tTriangle (graphics)\n4.8\tFibonacci Sequence\n4.9\tHigher-order functions\n5\tDerived languages\n5.1\tMetaOCaml\n5.2\tOther derived languages\n6\tSoftware written in OCaml\n7\tCommercial users of OCaml\n8\tSee also\n9\tReferences\n10\tExternal links\nPhilosophy[edit]\nML-derived languages are best known for their static type systems and type-inferring compilers. OCaml unifies functional, imperative, and object-oriented programming under an ML-like type system. This means the program author is not required to be overly familiar with the pure functional language paradigm in order to use OCaml.\n\nOCaml's static type system can help eliminate problems at runtime. However, it also forces the programmer to conform to the constraints of the type system, which can require careful thought and close attention. A type-inferring compiler greatly reduces the need for manual type annotations (for example, the data type of variables and the signature of functions usually do not need to be explicitly declared, as they do in Java). Nonetheless, effective use of OCaml's type system can require some sophistication on the part of the programmer.\n\nOCaml is perhaps most distinguished from other languages with origins in academia by its emphasis on performance. Firstly, its static type system renders runtime type mismatches impossible, and thus obviates runtime type and safety checks that burden the performance of dynamically typed languages, while still guaranteeing runtime safety (except when array bounds checking is turned off, or when certain type-unsafe features like serialization are used; these are rare enough that avoiding them is quite possible in practice).\n\nAside from type-checking overhead, functional programming languages are, in general, challenging to compile to efficient machine language code, due to issues such as the funarg problem. In addition to standard loop, register, and instruction optimizations, OCaml's optimizing compiler employs static program analysis techniques to optimize value boxing and closure allocation, helping to maximize the performance of the resulting code even if it makes extensive use of functional programming constructs.\n\nXavier Leroy has stated that \"OCaml delivers at least 50% of the performance of a decent C compiler\",[2] but a direct comparison is impossible. Some functions in the OCaml standard library are implemented with faster algorithms than equivalent functions in the standard libraries of other languages. For example, the implementation of set union in the OCaml standard library in theory is asymptotically faster than the equivalent function in the standard libraries of imperative languages (e.g. C++, Java) because the OCaml implementation exploits the immutability of sets in order to reuse parts of input sets in the output (persistence).\n\nFeatures[edit]\nOCaml features: a static type system, type inference, parametric polymorphism, tail recursion, pattern matching, first class lexical closures, functors (parametric modules), exception handling, and incremental generational automatic garbage collection.\n\nOCaml is particularly notable for extending ML-style type inference to an object system in a general-purpose language. This permits structural subtyping, where object types are compatible if their method signatures are compatible, regardless of their declared inheritance; an unusual feature in statically typed languages.\n\nA foreign function interface for linking to C primitives is provided, including language support for efficient numerical arrays in formats compatible with both C and FORTRAN. OCaml also supports the creation of libraries of OCaml functions that can be linked to a \"main\" program in C, so that one could distribute an OCaml library to C programmers who have no knowledge nor installation of OCaml.\n\nThe OCaml distribution contains:\n\nAn extensible parser and macro language named Camlp4, which permits the syntax of OCaml to be extended or even replaced\nLexer and parser tools called ocamllex and ocamlyacc\nDebugger that supports stepping backwards to investigate errors\nDocumentation generator\nProfiler — for measuring performance\nNumerous general-purpose libraries\nThe native code compiler is available for many platforms, including Unix, Microsoft Windows, and Apple Mac OS X. Portability is achieved through native code generation support for major architectures: IA-32, AMD64, Power, SPARC, ARM, and ARM64.[3]\n\nOCaml bytecode and native code programs can be written in a multithreaded style, with preemptive context switching. However, because the garbage collector of the INRIA OCaml system (which is the only currently available full implementation of the language) is not designed for concurrency, symmetric multiprocessing is not supported.[4] OCaml threads in the same process execute by time sharing only. There are however several libraries for distributed computing such as Functory and ocamlnet/Plasma.\n\nDevelopment environment[edit]\nSince 2011, a lot of new tools and libraries have been contributed to the OCaml development environment:\n\nOPAM, the OCaml Package Manager, developed by OCamlPro, is now an easy way to install OCaml and many of its tools and libraries\nOptimizing compilers for OCaml:\njs_of_ocaml, developed by the Ocsigen team, is an optimizing compiler from OCaml to JavaScript, to create webapps in OCaml.\nocamlcc is a compiler from OCaml to C, to complement the native code compiler for unsupported platforms.\nOCamlJava, developed by Inria, is a compiler from OCaml to the JVM.\nOCaPic, developed by Lip6, is a compiler from OCaml to PIC microcontroller.\nWeb sites:\nOCaml.org is a website managed by the OCaml community.\nTry-OCaml, developed by OCamlPro, is a website containing a complete OCaml REPL in a webpage.\nDevelopment Tools\nTypeRex is a set of open-source tools and libraries for OCaml, developed and maintained by OCamlPro.\nMerlin is an auto-completion tool for editing OCaml code in Emacs and Vim.\nCode examples[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2013) (Learn how and when to remove this template message)\nSnippets of OCaml code are most easily studied by entering them into the \"top-level\". This is an interactive OCaml session that prints the inferred types of resulting or defined expressions. The OCaml top-level is started by simply executing the OCaml program:\n\n  $ ocaml\n       Objective Caml version 3.09.0\n  #\nCode can then be entered at the \"#\" prompt. For example, to calculate 1+2*3:\n\n  # 1 + 2 * 3;;\n  - : int = 7\nOCaml infers the type of the expression to be \"int\" (a machine-precision integer) and gives the result \"7\".\n\nHello World[edit]\nThe following program \"hello.ml\":\n\nprint_endline \"Hello World!\"\ncan be compiled into a bytecode executable:\n\n$ ocamlc hello.ml -o hello\nor compiled into an optimized native-code executable:\n\n$ ocamlopt hello.ml -o hello\nand executed:\n\n$ ./hello\nHello World!\n$\nSumming a list of integers[edit]\nLists are one of the fundamental datatypes in OCaml. The following code example defines a recursive function sum that accepts one argument xs. (Notice the keyword rec). The function recursively iterates over a given list and provides a sum of integer elements. The match statement has similarities to C's switch element, though it is much more general.\n\nlet rec sum xs =\n  match xs with\n    | []       -> 0                  (* yield 0 if xs has the form [] *)\n    | x :: xs' -> x + sum xs';;      (* recursive call if xs has the form x::xs' for suitable x and xs' *)\n # sum [1;2;3;4;5];;\n - : int = 15\nAnother way is to use standard fold function that works with lists.\n\nlet sum xs =\n    List.fold_left (fun acc each_xs -> acc + each_xs) 0 xs;;\n # sum [1;2;3;4;5];;\n - : int = 15\nQuicksort[edit]\nOCaml lends itself to the concise expression of recursive algorithms. The following code example implements an algorithm similar to quicksort that sorts a list in increasing order.\n\n let rec qsort = function\n   | [] -> []\n   | pivot :: rest ->\n       let is_less x = x < pivot in\n       let left, right = List.partition is_less rest in\n       qsort left @ [pivot] @ qsort right\nBirthday paradox[edit]\nThe following program calculates the smallest number of people in a room for whom the probability of completely unique birthdays is less than 50% (the so-called birthday paradox, where for 1 person the probability is 365/365 (or 100%), for 2 it is 364/365, for 3 it is 364/365 × 363/365, etc.) (answer = 23).\n\n let year_size = 365.\n\n let rec birthday_paradox prob people =\n     let prob' = (year_size -. float people) /. year_size *. prob  in\n     if prob' < 0.5 then\n         Printf.printf \"answer = %d\\n\" (people+1)\n     else\n         birthday_paradox prob' (people+1) ;;\n\n birthday_paradox 1.0 1\nChurch numerals[edit]\nThe following code defines a Church encoding of natural numbers, with successor (succ) and addition (add). A Church numeral n is a higher-order function that accepts a function f and a value x and applies f to x exactly n times. To convert a Church numeral from a functional value to a string, we pass it a function that prepends the string \"S\" to its input and the constant string \"0\".\n\nlet zero f x = x\nlet succ n f x = f (n f x)\nlet one = succ zero\nlet two = succ (succ zero)\nlet add n1 n2 f x = n1 f (n2 f x)\nlet to_string n = n (fun k -> \"S\" ^ k) \"0\"\nlet _ = to_string (add (succ two) two)\nArbitrary-precision factorial function (libraries)[edit]\nA variety of libraries are directly accessible from OCaml. For example, OCaml has a built-in library for arbitrary-precision arithmetic. As the factorial function grows very rapidly, it quickly overflows machine-precision numbers (typically 32- or 64-bits). Thus, factorial is a suitable candidate for arbitrary-precision arithmetic.\n\nIn OCaml, the Num module provides arbitrary-precision arithmetic and can be loaded into a running top-level using:\n\n# #load \"nums.cma\";;\n# open Num;;\nThe factorial function may then be written using the arbitrary-precision numeric operators =/, */ and -/ :\n\n# let rec fact n =\n    if n =/ Int 0 then Int 1 else n */ fact(n -/ Int 1);;\nval fact : Num.num -> Num.num = <fun>\nThis function can compute much larger factorials, such as 120!:\n\n# string_of_num (fact (Int 120));;\n- : string =\n\"6689502913449127057588118054090372586752746333138029810295671352301633\n55724496298936687416527198498130815763789321409055253440858940812185989\n8481114389650005964960521256960000000000000000000000000000\"\nThe cumbersome syntax for Num operations can be alleviated thanks to the camlp4 syntax extension called Delimited overloading:\n\n# #require \"pa_do.num\";;\n# let rec fact n = Num.(if n = 0 then 1 else n * fact(n-1));;\nval fact : Num.num -> Num.num = <fun>\n# fact Num.(120);;\n- : Num.num =\n  <num 668950291344912705758811805409037258675274633313802981029567\n  135230163355724496298936687416527198498130815763789321409055253440\n  8589408121859898481114389650005964960521256960000000000000000000000000000>\nTriangle (graphics)[edit]\nThe following program \"simple.ml\" renders a rotating triangle in 2D using OpenGL:\n\nlet () =\n  ignore( Glut.init Sys.argv );\n  Glut.initDisplayMode ~double_buffer:true ();\n  ignore (Glut.createWindow ~title:\"OpenGL Demo\");\n  let angle t = 10. *. t *. t in\n  let render () =\n    GlClear.clear [ `color ];\n    GlMat.load_identity ();\n    GlMat.rotate ~angle: (angle (Sys.time ())) ~z:1. ();\n    GlDraw.begins `triangles;\n    List.iter GlDraw.vertex2 [-1., -1.; 0., 1.; 1., -1.];\n    GlDraw.ends ();\n    Glut.swapBuffers () in\n  GlMat.mode `modelview;\n  Glut.displayFunc ~cb:render;\n  Glut.idleFunc ~cb:(Some Glut.postRedisplay);\n  Glut.mainLoop ()\nThe LablGL bindings to OpenGL are required. The program may then be compiled to bytecode with:\n\n  $ ocamlc -I +lablGL lablglut.cma lablgl.cma simple.ml -o simple\nor to nativecode with:\n\n  $ ocamlopt -I +lablGL lablglut.cmxa lablgl.cmxa simple.ml -o simple\nand run:\n\n  $ ./simple\nFar more sophisticated, high-performance 2D and 3D graphical programs can be developed in OCaml. Thanks to the use of OpenGL and OCaml, the resulting programs can be cross-platform, compiling without any changes on many major platforms.\n\nFibonacci Sequence[edit]\nThe following code calculates the Fibonacci sequence of a number n inputted. It uses tail recursion and pattern matching.\n\nlet rec fib_aux n a b =\n  match n with\n  | 0 -> a\n  | _ -> fib_aux (n - 1) b (a+b) \nlet fib n = fib_aux n 0 1\nHigher-order functions[edit]\nFunctions may take functions as input and return functions as result. For example, applying twice to a function f yields a function that applies f two times to its argument.\n\nlet twice (f : 'a -> 'a) = fun (x : 'a) -> f (f x) ;;\nlet inc (x : int) : int = x + 1 ;;\nlet add2 = twice(inc);;\nlet inc_str (x : string) : string = x ^ \" \" ^ x ;;\nlet add_str = twice(inc_str);;\n  # add2 98;;\n  - : int = 100\n  # add_str \"Test\";;\n  - : string = \"Test Test Test Test\"\nThe function twice uses a type variable 'a to indicate that it can be applied to any function f mapping from a type 'a to itself, rather than only to int->int functions. In particular, twice can even be applied to itself.\n\n  # let fourtimes f = (twice twice) f;;\n  val fourtimes : ('a -> 'a) -> 'a -> 'a = <fun>\n  # let add4 = fourtimes inc;;\n  val add4 : int -> int = <fun>\n  # add4 98;;\n  - : int = 102\nDerived languages[edit]\nMetaOCaml[edit]\nMetaOCaml[5] is a multi-stage programming extension of OCaml enabling incremental compiling of new machine code during runtime. Under certain circumstances, significant speedups are possible using multi-stage programming, because more detailed information about the data to process is available at runtime than at the regular compile time, so the incremental compiler can optimize away many cases of condition checking etc.\n\nAs an example: if at compile time it is known that a certain power function x -> x^n is needed very frequently, but the value of n is known only at runtime, you can use a two-stage power function in MetaOCaml:\n\n let rec power n x =\n   if n = 0\n   then .<1>.\n   else\n     if even n\n     then sqr (power (n/2) x)\n     else .<.~x *. .~(power (n-1) x)>.\nAs soon as you know n at runtime, you can create a specialized and very fast power function:\n\n .<fun x -> .~(power 5 .<x>.)>.\nThe result is:\n\n fun x_1 -> (x_1 *\n     let y_3 = \n         let y_2 = (x_1 * 1)\n         in (y_2 * y_2)\n     in (y_3 * y_3))\nThe new function is automatically compiled.\n\nOther derived languages[edit]\nAtomCaml provides a synchronization primitive for atomic (transactional) execution of code.\nEmily is a subset of OCaml that uses a design rule verifier to enforce object-capability [security] principles.\nF# is a Microsoft .NET language based on OCaml.\nFresh OCaml facilitates the manipulation of names and binders.\nGCaml adds extensional polymorphism to OCaml, thus allowing overloading and type-safe marshalling.\nJoCaml integrates constructions for developing concurrent and distributed programs.\nOCamlDuce extends OCaml with features such as XML expressions and regular-expression types.\nOCamlP3l is a parallel programming system based on OCaml and the P3L language\nSoftware written in OCaml[edit]\nMirageOS, a unikernel programming framework written in pure OCaml\nHack, a new programming language created by Facebook, which is an extension of PHP with static typing. Hack's compiler is written in OCaml.\nFlow, a static analyzer for JavaScript created at Facebook that infers and verifies static types for JavaScript programs.\nInfer, a static analyzer for Java, C and Objective-C created at Facebook, which is used to detect bugs in iOS and Android apps.\n0Install, a multi-platform package manager\nXCP, The Xen Cloud Platform, an open source toolstack for the Xen Virtual Machine Hypervisor\nFFTW, a software library for computing discrete Fourier transforms. Several C routines have been generated by an OCaml program named genfft.\nUnison, a file synchronization program to synchronize files between two directories\nMldonkey, a peer to peer client based on the EDonkey network\nGeneWeb, free open source multi-platform genealogy software\nThe Haxe compiler, a free open source compiler for the Haxe programming language\nFrama-C, a framework for C programs analysis\nCoq, a formal proof management system\nOcsigen, web development framework\nOpa, an open source programming language for web development\nWebAssembly, an experimental, low-level scripting language for in-browser client-side scripting. Its WIP interpreter and parser in the specification repo on Github is written mostly in OCaml.\nCommercial users of OCaml[edit]\nThere are several dozen companies that use OCaml to some degree.[6] Notable examples include:\n\nJane Street Capital, a proprietary trading firm, which adopted OCaml as its preferred language in its early days[7]\nCitrix Systems, which uses OCaml in XenServer, a component of one of its products\nFacebook, which developed Hack, Flow, Infer and Pfff\nAhrefs Site Explorer, which uses OCaml for its back-end framework (database management, web-crawler and web-page parser)\nSee also[edit]\nCaml and Caml Light, languages from which OCaml evolved\nStandard ML, another popular dialect of ML\nExtensible ML, another object-oriented dialect of ML\nO'Haskell, an object-oriented extension to the functional language Haskell\nReason, a new interface to OCaml initiated by Facebook",
          "type": "compiled",
          "plid": 23
        },
        {
          "name": "Objective-C",
          "details": "Objective-C is a general-purpose, object-oriented programming language that adds Smalltalk-style messaging to the C programming language. It was the main programming language used by Apple for the OS X and iOS operating systems, and their respective application programming interfaces (APIs) Cocoa and Cocoa Touch prior to the introduction of Swift.\n\nThe programming language Objective-C was originally developed in the early 1980s. It was selected as the main language used by NeXT for its NeXTSTEP operating system, from which OS X and iOS are derived.[3] Portable Objective-C programs that do not use the Cocoa or Cocoa Touch libraries, or those using parts that may be ported or reimplemented for other systems, can also be compiled for any system supported by GNU Compiler Collection (GCC) or Clang.\n\nObjective-C source code 'implementation' program files usually have .m filename extensions, while Objective-C 'header/interface' files have .h extensions, the same as C header files. Objective-C++ files are denoted with a .mm file extension.\n\nContents  [hide] \n1\tHistory\n1.1\tPopularization through NeXT\n1.2\tApple development and Swift\n2\tSyntax\n2.1\tMessages\n2.2\tInterfaces and implementations\n2.2.1\tInterface\n2.2.2\tImplementation\n2.2.3\tInstantiation\n2.3\tProtocols\n2.4\tDynamic typing\n2.5\tForwarding\n2.5.1\tExample\n2.5.2\tNotes\n2.6\tCategories\n2.6.1\tExample usage of categories\n2.6.2\tNotes\n2.7\tPosing\n2.8\t#import\n3\tOther features\n4\tLanguage variants\n4.1\tObjective-C++\n4.2\tObjective-C 2.0\n4.2.1\tGarbage collection\n4.2.2\tProperties\n4.2.3\tNon-fragile instance variables\n4.2.4\tFast enumeration\n4.2.5\tClass extensions\n4.2.6\tImplications for Cocoa development\n4.3\tBlocks\n4.4\tModern Objective-C\n4.4.1\tAutomatic Reference Counting\n4.4.2\tLiterals\n4.4.3\tSubscripting\n4.5\t\"Modern\" Objective-C syntax (1997)\n4.6\tPortable Object Compiler\n4.7\tGEOS Objective-C\n4.8\tClang\n5\tLibrary use\n6\tAnalysis of the language\n6.1\tMemory management\n6.2\tPhilosophical differences between Objective-C and C++\n7\tSee also\n8\tReferences\n9\tFurther reading\n10\tExternal links\nHistory[edit]\nObjective-C was created primarily by Brad Cox and Tom Love in the early 1980s at their company Stepstone.[4] Both had been introduced to Smalltalk while at ITT Corporation's Programming Technology Center in 1981. The earliest work on Objective-C traces back to around that time.[5] Cox was intrigued by problems of true reusability in software design and programming. He realized that a language like Smalltalk would be invaluable in building development environments for system developers at ITT. However, he and Tom Love also recognized that backward compatibility with C was critically important in ITT's telecom engineering milieu.[6]\n\nCox began writing a pre-processor for C to add some of the abilities of Smalltalk. He soon had a working implementation of an object-oriented extension to the C language, which he called \"OOPC\" for Object-Oriented Pre-Compiler.[7] Love was hired by Schlumberger Research in 1982 and had the opportunity to acquire the first commercial copy of Smalltalk-80, which further influenced the development of their brainchild.\n\nIn order to demonstrate that real progress could be made, Cox showed that making interchangeable software components really needed only a few practical changes to existing tools. Specifically, they needed to support objects in a flexible manner, come supplied with a usable set of libraries, and allow for the code (and any resources needed by the code) to be bundled into one cross-platform format.\n\nLove and Cox eventually formed a new venture, Productivity Products International (PPI), to commercialize their product, which coupled an Objective-C compiler with class libraries. In 1986, Cox published the main description of Objective-C in its original form in the book Object-Oriented Programming, An Evolutionary Approach. Although he was careful to point out that there is more to the problem of reusability than just the language, Objective-C often found itself compared feature for feature with other languages.\n\nPopularization through NeXT[edit]\nIn 1988, NeXT licensed Objective-C from StepStone (the new name of PPI, the owner of the Objective-C trademark) and extended the GCC compiler to support Objective-C. NeXT developed the AppKit and Foundation Kit libraries on which the NeXTSTEP user interface and Interface Builder were based. While the NeXT workstations failed to make a great impact in the marketplace, the tools were widely lauded in the industry. This led NeXT to drop hardware production and focus on software tools, selling NeXTSTEP (and OpenStep) as a platform for custom programming.\n\nIn order to circumvent the terms of the GPL, NeXT had originally intended to ship the Objective-C frontend separately, allowing the user to link it with GCC to produce the compiler executable. After being initially accepted by Richard M. Stallman, this plan was rejected after Stallman consulted with GNU's lawyers and NeXT agreed to make Objective-C part of GCC.[8]\n\nThe work to extend GCC was led by Steve Naroff, who joined NeXT from StepStone. The compiler changes were made available as per GPL license terms, but the runtime libraries were not, rendering the open source contribution unusable to the general public. This led to other parties developing such runtime libraries under open source license. Later, Steve Naroff was also principal contributor to work at Apple to build the Objective-C frontend to Clang.\n\nThe GNU project started work on its free software implementation of Cocoa, named GNUstep, based on the OpenStep standard.[9] Dennis Glatting wrote the first GNU Objective-C runtime in 1992. The GNU Objective-C runtime, which has been in use since 1993, is the one developed by Kresten Krab Thorup when he was a university student in Denmark.[citation needed] Thorup also worked at NeXT from 1993 to 1996.[10]\n\nApple development and Swift[edit]\nAfter acquiring NeXT in 1996, Apple Computer used OpenStep in its new operating system, OS X. This included Objective-C, NeXT's Objective-C based developer tool, Project Builder, and its interface design tool, Interface Builder, both now merged into one Xcode application. Most of Apple's current Cocoa API is based on OpenStep interface objects and is the most significant Objective-C environment being used for active development.\n\nAt WWDC 2014, Apple introduced a new language, Swift, which was characterized as \"Objective-C without the C\".\n\nSyntax[edit]\nObjective-C is a thin layer atop C, and is a \"strict superset\" of C, meaning that it is possible to compile any C program with an Objective-C compiler, and to freely include C language code within an Objective-C class.[11][12][13][14][15][16]\n\nObjective-C derives its object syntax from Smalltalk. All of the syntax for non-object-oriented operations (including primitive variables, pre-processing, expressions, function declarations, and function calls) are identical to those of C, while the syntax for object-oriented features is an implementation of Smalltalk-style messaging.\n\nMessages[edit]\nThe Objective-C model of object-oriented programming is based on message passing to object instances. In Objective-C one does not call a method; one sends a message. This is unlike the Simula-style programming model used by C++. The difference between these two concepts is in how the code referenced by the method or message name is executed. In a Simula-style language, the method name is in most cases bound to a section of code in the target class by the compiler. In Smalltalk and Objective-C, the target of a message is resolved at runtime, with the receiving object itself interpreting the message. A method is identified by a selector or SEL — a NUL-terminated string representing its name — and resolved to a C method pointer implementing it: an IMP.[17] A consequence of this is that the message-passing system has no type checking. The object to which the message is directed — the receiver — is not guaranteed to respond to a message, and if it does not, it raises an exception.[18]\n\nSending the message method to the object pointed to by the pointer obj would require the following code in C++:\n\nobj->method(argument);\nIn Objective-C, this is written as follows:\n\n[obj method:argument];\nBoth styles of programming have their strengths and weaknesses. Object-oriented programming in the Simula (C++) style allows multiple inheritance and faster execution by using compile-time binding whenever possible, but it does not support dynamic binding by default. It also forces all methods to have a corresponding implementation unless they are abstract. The Smalltalk-style programming as used in Objective-C allows messages to go unimplemented, with the method resolved to its implementation at runtime. For example, a message may be sent to a collection of objects, to which only some will be expected to respond, without fear of producing runtime errors. Message passing also does not require that an object be defined at compile time. An implementation is still required for the method to be called in the derived object. (See the dynamic typing section below for more advantages of dynamic (late) binding.)\n\nInterfaces and implementations[edit]\nObjective-C requires that the interface and implementation of a class be in separately declared code blocks. By convention, developers place the interface in a header file and the implementation in a code file. The header files, normally suffixed .h, are similar to C header files while the implementation (method) files, normally suffixed .m, can be very similar to C code files.\n\nInterface[edit]\nIn other programming languages, this is called a \"class declaration\".\n\nThe interface of a class is usually defined in a header file. A common convention is to name the header file after the name of the class, e.g. Ball.h would contain the interface for the class Ball.\n\nAn interface declaration takes the form:\n\n@interface classname : superclassname {\n // instance variables\n}\n+ classMethod1;\n+ (return_type)classMethod2;\n+ (return_type)classMethod3:(param1_type)param1_varName;\n\n- (return_type)instanceMethod1With1Parameter:(param1_type)param1_varName;\n- (return_type)instanceMethod2With2Parameters:(param1_type)param1_varName param2_callName:(param2_type)param2_varName;\n@end\nIn the above, plus signs denote class methods, or methods that can be called on the class itself (not on an instance), and minus signs denote instance methods, which can only be called on a particular instance of the class. Class methods also have no access to instance variables.\n\nThe code above is roughly equivalent to the following C++ interface:\n\nclass classname : public superclassname {\n protected:\n // instance variables\n\n public:\n // Class (static) functions\n static void * classMethod1();\n static return_type classMethod2();\n static return_type classMethod3(param1_type param1_varName);\n\n // Instance (member) functions\n return_type instanceMethod1With1Parameter (param1_type param1_varName);\n return_type instanceMethod2With2Parameters (param1_type param1_varName, param2_type param2_varName=default);\n};\nNote that instanceMethod2With2Parameters:param2_callName: demonstrates the interleaving of selector segments with argument expressions, for which there is no direct equivalent in C/C++.\n\nReturn types can be any standard C type, a pointer to a generic Objective-C object, a pointer to a specific type of object such as NSArray *, NSImage *, or NSString *, or a pointer to the class to which the method belongs (instancetype). The default return type is the generic Objective-C type id.\n\nMethod arguments begin with a name labeling the argument that is part of the method name, followed by a colon followed by the expected argument type in parentheses and the argument name. The label can be omitted.\n\n- (void)setRangeStart:(int)start end:(int)end;\n- (void)importDocumentWithName:(NSString *)name withSpecifiedPreferences:\n(Preferences *)prefs beforePage:(int)insertPage;\nImplementation[edit]\nThe interface only declares the class interface and not the methods themselves: the actual code is written in the implementation file. Implementation (method) files normally have the file extension .m, which originally signified \"messages\".[19]\n\n@implementation classname\n+ (return_type)classMethod\n{\n // implementation\n}\n- (return_type)instanceMethod\n{\n // implementation\n}\n@end\nMethods are written using their interface declarations. Comparing Objective-C and C:\n\n- (int)method:(int)i\n{\n return [self square_root:i];\n}\nint function (int i)\n{\n return square_root(i);\n}\nThe syntax allows pseudo-naming of arguments.\n\n- (int)changeColorToRed:(float)red green:(float)green blue:(float)blue;\n\n[myColor changeColorToRed:5.0 green:2.0 blue:6.0];\nInternal representations of a method vary between different implementations of Objective-C. If myColor is of the class Color, instance method -changeColorToRed:green:blue: might be internally labeled _i_Color_changeColorToRed_green_blue. The i is to refer to an instance method, with the class and then method names appended and colons changed to underscores. As the order of parameters is part of the method name, it cannot be changed to suit coding style or expression as with true named parameters.\n\nHowever, internal names of the function are rarely used directly. Generally, messages are converted to function calls defined in the Objective-C runtime library. It is not necessarily known at link time which method will be called because the class of the receiver (the object being sent the message) need not be known until runtime.\n\nInstantiation[edit]\nOnce an Objective-C class is written, it can be instantiated. This is done by first allocating an uninitialized instance of the class (an object) and then by initializing it. An object is not fully functional until both steps have been completed. These steps should be accomplished with one line of code so that there is never an allocated object that hasn't undergone initialization (and because it is unwise to keep the intermediate result since -init can return a different object than that on which it is called).\n\nInstantiation with the default, no-parameter initializer:\n\nMyObject *o = [[MyObject alloc] init];\nInstantiation with a custom initializer:\n\nMyObject *o = [[MyObject alloc] initWithString:myString];\nIn the case where no custom initialization is being performed, the \"new\" method can often be used in place of the alloc-init messages:\n\nMyObject *o = [MyObject new];\nAlso, some classes implement class method initializers. Like +new, they combine +alloc and -init, but unlike +new, they return an autoreleased instance. Some class method initializers take parameters:\n\nMyObject *o = [MyObject object];\nMyObject *o2 = [MyObject objectWithString:myString];\nThe alloc message allocates enough memory to hold all the instance variables for an object, sets all the instance variables to zero values, and turns the memory into an instance of the class; at no point during the initialization is the memory an instance of the superclass.\n\nThe init message performs the set-up of the instance upon creation. The init method is often written as follows:\n\n- (id)init {\n    self = [super init];\n    if (self) {\n        // perform initialization of object here\n    }\n    return self;\n}\nIn the above example, notice the id return type. This type stands for \"pointer to any object\" in Objective-C (See the Dynamic typing section).\n\nThe initializer pattern is used to assure that the object is properly initialized by its superclass before the init method performs its initialization. It performs the following actions:\n\nself = [super init]\nSends the superclass instance an init message and assigns the result to self (pointer to the current object).\nif (self)\nChecks if the returned object pointer is valid before performing any initialization.\nreturn self\nReturns the value of self to the caller.\nA non-valid object pointer has the value nil; conditional statements like \"if\" treat nil like a null pointer, so the initialization code will not be executed if [super init] returned nil. If there is an error in initialization the init method should perform any necessary cleanup, including sending a \"release\" message to self, and return nil to indicate that initialization failed. Any checking for such errors must only be performed after having called the superclass initialization to ensure that destroying the object will be done correctly.\n\nIf a class has more than one initialization method, only one of them (the \"designated initializer\") needs to follow this pattern; others should call the designated initializer instead of the superclass initializer.\n\nProtocols[edit]\nIn other programming languages, these are called \"interfaces\".\n\nObjective-C was extended at NeXT to introduce the concept of multiple inheritance of specification, but not implementation, through the introduction of protocols. This is a pattern achievable either as an abstract multiple inherited base class in C++, or as an \"interface\" (as in Java and C#). Objective-C makes use of ad hoc protocols called informal protocols and compiler-enforced protocols called formal protocols.\n\nAn informal protocol is a list of methods that a class can opt to implement. It is specified in the documentation, since it has no presence in the language. Informal protocols are implemented as a category (see below) on NSObject and often include optional methods, which, if implemented, can change the behavior of a class. For example, a text field class might have a delegate that implements an informal protocol with an optional method for performing auto-completion of user-typed text. The text field discovers whether the delegate implements that method (via reflection) and, if so, calls the delegate's method to support the auto-complete feature.\n\nA formal protocol is similar to an interface in Java, C#, and Ada 2005. It is a list of methods that any class can declare itself to implement. Versions of Objective-C before 2.0 required that a class must implement all methods in a protocol it declares itself as adopting; the compiler will emit an error if the class does not implement every method from its declared protocols. Objective-C 2.0 added support for marking certain methods in a protocol optional, and the compiler will not enforce implementation of optional methods.\n\nA class must be declared to implement that protocol to be said to conform to it. This is detectable at runtime. Formal protocols cannot provide any implementations; they simply assure callers that classes that conform to the protocol will provide implementations. In the NeXT/Apple library, protocols are frequently used by the Distributed Objects system to represent the abilities of an object executing on a remote system.\n\nThe syntax\n\n@protocol NSLocking\n- (void)lock;\n- (void)unlock;\n@end\ndenotes that there is the abstract idea of locking. By stating in the class definition that the protocol is implemented,\n\n@interface NSLock : NSObject <NSLocking>\n//...\n@end\ninstances of NSLock claim that they will provide an implementation for the two instance methods.\n\nDynamic typing[edit]\nObjective-C, like Smalltalk, can use dynamic typing: an object can be sent a message that is not specified in its interface. This can allow for increased flexibility, as it allows an object to \"capture\" a message and send the message to a different object that can respond to the message appropriately, or likewise send the message on to another object. This behavior is known as message forwarding or delegation (see below). Alternatively, an error handler can be used in case the message cannot be forwarded. If an object does not forward a message, respond to it, or handle an error, then the system will generate a runtime exception.[20] If messages are sent to nil (the null object pointer), they will be silently ignored or raise a generic exception, depending on compiler options.\n\nStatic typing information may also optionally be added to variables. This information is then checked at compile time. In the following four statements, increasingly specific type information is provided. The statements are equivalent at runtime, but the extra information allows the compiler to warn the programmer if the passed argument does not match the type specified.\n\n- (void)setMyValue:(id)foo;\nIn the above statement, foo may be of any class.\n\n- (void)setMyValue:(id<NSCopying>)foo;\nIn the above statement, foo may be an instance of any class that conforms to the NSCopying protocol.\n\n- (void)setMyValue:(NSNumber *)foo;\nIn the above statement, foo must be an instance of the NSNumber class.\n\n- (void)setMyValue:(NSNumber<NSCopying> *)foo;\nIn the above statement, foo must be an instance of the NSNumber class, and it must conform to the NSCopying protocol.\n\nForwarding[edit]\nObjective-C permits the sending of a message to an object that may not respond. Rather than responding or simply dropping the message, an object can forward the message to an object that can respond. Forwarding can be used to simplify implementation of certain design patterns, such as the observer pattern or the proxy pattern.\n\nThe Objective-C runtime specifies a pair of methods in Object\n\nforwarding methods:\n- (retval_t)forward:(SEL)sel args:(arglist_t)args; // with GCC\n- (id)forward:(SEL)sel args:(marg_list)args; // with NeXT/Apple systems\naction methods:\n- (retval_t)performv:(SEL)sel args:(arglist_t)args; // with GCC\n- (id)performv:(SEL)sel args:(marg_list)args; // with NeXT/Apple systems\nAn object wishing to implement forwarding needs only to override the forwarding method with a new method to define the forwarding behavior. The action method performv:: need not be overridden, as this method merely performs an action based on the selector and arguments. Notice the SEL type, which is the type of messages in Objective-C.\n\nNote: in OpenStep, Cocoa, and GNUstep, the commonly used frameworks of Objective-C, one does not use the Object class. The - (void)forwardInvocation:(NSInvocation *)anInvocation method of the NSObject class is used to do forwarding.\n\nExample[edit]\nHere is an example of a program that demonstrates the basics of forwarding.\n\nForwarder.h\n# import <objc/Object.h>\n\n@interface Forwarder : Object {\n id recipient; //The object we want to forward the message to.\n}\n\n//Accessor methods.\n- (id)recipient;\n- (id)setRecipient:(id)_recipient;\n\n@end\nForwarder.m\n# import \"Forwarder.h\"\n\n@implementation Forwarder\n\n- (retval_t)forward:(SEL)sel args:(arglist_t) args {\n /*\n * Check whether the recipient actually responds to the message.\n * This may or may not be desirable, for example, if a recipient\n * in turn does not respond to the message, it might do forwarding\n * itself.\n */\n if([recipient respondsToSelector:sel]) {\n  return [recipient performv:sel args:args];\n } else {\n  return [self error:\"Recipient does not respond\"];\n }\n}\n\n- (id)setRecipient:(id)_recipient {\n [recipient autorelease];\n recipient = [_recipient retain];\n return self;\n}\n\n- (id) recipient {\n return recipient;\n}\n@end\nRecipient.h\n# import <objc/Object.h>\n\n// A simple Recipient object.\n@interface Recipient : Object\n- (id)hello;\n@end\nRecipient.m\n# import \"Recipient.h\"\n\n@implementation Recipient\n\n- (id)hello {\n printf(\"Recipient says hello!\\n\");\n\n return self;\n}\n\n@end\nmain.m\n# import \"Forwarder.h\"\n# import \"Recipient.h\"\n\nint main(void) {\n Forwarder *forwarder = [Forwarder new];\n Recipient *recipient = [Recipient new];\n\n [forwarder setRecipient:recipient]; //Set the recipient.\n /*\n * Observe forwarder does not respond to a hello message! It will\n * be forwarded. All unrecognized methods will be forwarded to\n * the recipient\n * (if the recipient responds to them, as written in the Forwarder)\n */\n [forwarder hello];\n\n [recipient release];\n [forwarder release];\n\n return 0;\n}\nNotes[edit]\nWhen compiled using gcc, the compiler reports:\n\n$ gcc -x objective-c -Wno-import Forwarder.m Recipient.m main.m -lobjc\nmain.m: In function `main':\nmain.m:12: warning: `Forwarder' does not respond to `hello'\n$\nThe compiler is reporting the point made earlier, that Forwarder does not respond to hello messages. In this circumstance, it is safe to ignore the warning since forwarding was implemented. Running the program produces this output:\n\n$ ./a.out\nRecipient says hello!\nCategories[edit]\nDuring the design of Objective-C, one of the main concerns was the maintainability of large code bases. Experience from the structured programming world had shown that one of the main ways to improve code was to break it down into smaller pieces. Objective-C borrowed and extended the concept of categories from Smalltalk implementations to help with this process.[21]\n\nFurthermore, the methods within a category are added to a class at run-time. Thus, categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code. For example, if a system does not contain a spell checker in its String implementation, it could be added without modifying the String source code.\n\nMethods within categories become indistinguishable from the methods in a class when the program is run. A category has full access to all of the instance variables within the class, including private variables.\n\nIf a category declares a method with the same method signature as an existing method in a class, the category's method is adopted. Thus categories can not only add methods to a class, but also replace existing methods. This feature can be used to fix bugs in other classes by rewriting their methods, or to cause a global change to a class's behavior within a program. If two categories have methods with the same name (not to be confused with method signature), it is undefined which category's method is adopted.\n\nOther languages have attempted to add this feature in a variety of ways. TOM took the Objective-C system a step further and allowed for the addition of variables also. Other languages have used prototype-based solutions instead, the most notable being Self.\n\nThe C# and Visual Basic.NET languages implement superficially similar functionality in the form of extension methods, but these lack access to the private variables of the class.[22] Ruby and several other dynamic programming languages refer to the technique as \"monkey patching\".\n\nLogtalk implements a concept of categories (as first-class entities) that subsumes Objective-C categories functionality (Logtalk categories can also be used as fine-grained units of composition when defining e.g. new classes or prototypes; in particular, a Logtalk category can be virtually imported by any number of classes and prototypes).\n\nExample usage of categories[edit]\nThis example builds up an Integer class, by defining first a basic class with only accessor methods implemented, and adding two categories, Arithmetic and Display, which extend the basic class. While categories can access the base class's private data members, it is often good practice to access these private data members through the accessor methods, which helps keep categories more independent from the base class. Implementing such accessors is one typical usage of categories. Another is to use categories to add methods to the base class. However, it is not regarded as good practice to use categories for subclass overriding, also known as monkey patching. Informal protocols are implemented as a category on the base NSObject class. By convention, files containing categories that extend base classes will take the name BaseClass+ExtensionClass.h.\n\nInteger.h\n# import <objc/Object.h>\n\n@interface Integer : Object {\n int integer;\n}\n\n- (int) integer;\n- (id) integer: (int) _integer;\n@end\nInteger.m\n# import \"Integer.h\"\n\n@implementation Integer\n- (int) integer {\n return integer;\n}\n\n- (id) integer: (int) _integer {\n integer = _integer;\n\n return self;\n}\n@end\nInteger+Arithmetic.h\n# import \"Integer.h\"\n\n@interface Integer (Arithmetic)\n- (id) add: (Integer *) addend;\n- (id) sub: (Integer *) subtrahend;\n@end\nInteger+Arithmetic.m\n# import \"Integer+Arithmetic.h\"\n\n@implementation Integer (Arithmetic)\n- (id) add: (Integer *) addend {\n return [self integer: [self integer] + [addend integer]];\n}\n\n- (id) sub: (Integer *) subtrahend {\n return [self integer: [self integer] - [subtrahend integer]];\n}\n@end\nInteger+Display.h\n# import \"Integer.h\"\n\n@interface Integer (Display)\n- (id) showstars;\n- (id) showint;\n@end\nInteger+Display.m\n# import \"Integer+Display.h\"\n\n@implementation Integer (Display)\n- (id) showstars {\n int i, x = [self integer];\n for (i = 0; i < x; i++) {\n printf(\"*\");\n }\n printf(\"\\n\");\n\n return self;\n}\n\n- (id) showint {\n printf(\"%d\\n\", [self integer]);\n\n return self;\n}\n@end\nmain.m\n# import \"Integer.h\"\n# import \"Integer+Arithmetic.h\"\n# import \"Integer+Display.h\"\n\nint main(void) {\n Integer *num1 = [Integer new], *num2 = [Integer new];\n int x;\n\n printf(\"Enter an integer: \");\n scanf(\"%d\", &x);\n\n [num1 integer:x];\n [num1 showstars];\n\n printf(\"Enter an integer: \");\n scanf(\"%d\", &x);\n\n [num2 integer:x];\n [num2 showstars];\n\n [num1 add:num2];\n [num1 showint];\n\n return 0;\n}\nNotes[edit]\nCompilation is performed, for example, by:\n\ngcc -x objective-c main.m Integer.m Integer+Arithmetic.m Integer+Display.m -lobjc\nOne can experiment by leaving out the #import \"Integer+Arithmetic.h\" and [num1 add:num2] lines and omitting Integer+Arithmetic.m in compilation. The program will still run. This means that it is possible to mix-and-match added categories if needed; if a category does not need to have some ability, it can simply not be compile in.\n\nPosing[edit]\nObjective-C permits a class to wholly replace another class within a program. The replacing class is said to \"pose as\" the target class.\n\nClass posing was declared deprecated with Mac OS X v10.5, and is unavailable in the 64-bit runtime. Similar functionality can be achieved by using method swizzling in categories, that swaps one method's implementation with another's that have the same signature.\n\nFor the versions still supporting posing, all messages sent to the target class are instead received by the posing class. There are several restrictions:\n\nA class may only pose as one of its direct or indirect superclasses.\nThe posing class must not define any new instance variables that are absent from the target class (though it may define or override methods).\nThe target class may not have received any messages prior to the posing.\nPosing, similarly with categories, allows global augmentation of existing classes. Posing permits two features absent from categories:\n\nA posing class can call overridden methods through super, thus incorporating the implementation of the target class.\nA posing class can override methods defined in categories.\nFor example,\n\n@interface CustomNSApplication : NSApplication\n@end\n\n@implementation CustomNSApplication\n- (void) setMainMenu: (NSMenu*) menu {\n // do something with menu\n}\n@end\n\nclass_poseAs ([CustomNSApplication class], [NSApplication class]);\nThis intercepts every invocation of setMainMenu to NSApplication.\n\n#import[edit]\nIn the C language, the #include pre-compile directive always causes a file's contents to be inserted into the source at that point. Objective-C has the #import directive, equivalent except that each file is included only once per compilation unit, obviating the need for include guards.\n\nOther features[edit]\nObjective-C's features often allow for flexible, and often easy, solutions to programming issues.\n\nDelegating methods to other objects and remote invocation can be easily implemented using categories and message forwarding.\nSwizzling of the isa pointer allows for classes to change at runtime. Typically used for debugging where freed objects are swizzled into zombie objects whose only purpose is to report an error when someone calls them. Swizzling was also used in Enterprise Objects Framework to create database faults[citation needed]. Swizzling is used today by Apple's Foundation Framework to implement Key-Value Observing.\nLanguage variants[edit]\nObjective-C++[edit]\nObjective-C++ is a language variant accepted by the front-end to the GNU Compiler Collection and Clang, which can compile source files that use a combination of C++ and Objective-C syntax. Objective-C++ adds to C++ the extensions that Objective-C adds to C. As nothing is done to unify the semantics behind the various language features, certain restrictions apply:\n\nA C++ class cannot derive from an Objective-C class and vice versa.\nC++ namespaces cannot be declared inside an Objective-C declaration.\nObjective-C declarations may appear only in global scope, not inside a C++ namespace\nObjective-C classes cannot have instance variables of C++ classes that lack a default constructor or that have one or more virtual methods,[citation needed] but pointers to C++ objects can be used as instance variables without restriction (allocate them with new in the -init method).\nC++ \"by value\" semantics cannot be applied to Objective-C objects, which are only accessible through pointers.\nAn Objective-C declaration cannot be within a C++ template declaration and vice versa. However, Objective-C types, (e.g., Classname *) can be used as C++ template parameters.\nObjective-C and C++ exception handling is distinct; the handlers of each cannot handle exceptions of the other type. This is mitigated in recent runtimes as Objective-C exceptions are either replaced by C++ exceptions completely (Apple runtime), or partly when Objective-C++ library is linked (GNUstep libobjc2).\nCare must be taken since the destructor calling conventions of Objective-C and C++'s exception run-time models do not match (i.e., a C++ destructor will not be called when an Objective-C exception exits the C++ object's scope). The new 64-bit runtime resolves this by introducing interoperability with C++ exceptions in this sense.[23]\nObjective-C blocks and C++11 lambdas are distinct entities, however a block is transparently generated on Mac OS X when passing a lambda where a block is expected.[24]\nObjective-C 2.0[edit]\nAt the 2006 Worldwide Developers Conference, Apple announced the release of \"Objective-C 2.0,\" a revision of the Objective-C language to include \"modern garbage collection, syntax enhancements,[25] runtime performance improvements,[26] and 64-bit support\". Mac OS X v10.5, released in October 2007, included an Objective-C 2.0 compiler. GCC 4.6 supports many new Objective-C features, such as declared and synthesized properties, dot syntax, fast enumeration, optional protocol methods, method/protocol/class attributes, class extensions and a new GNU Objective-C runtime API.[27]\n\nGarbage collection[edit]\nObjective-C 2.0 provided an optional conservative, generational garbage collector. When run in backwards-compatible mode, the runtime turned reference counting operations such as \"retain\" and \"release\" into no-ops. All objects were subject to garbage collection when garbage collection was enabled. Regular C pointers could be qualified with \"__strong\" to also trigger the underlying write-barrier compiler intercepts and thus participate in garbage collection.[28] A zero-ing weak subsystem was also provided such that pointers marked as \"__weak\" are set to zero when the object (or more simply, GC memory) is collected. The garbage collector does not exist on the iOS implementation of Objective-C 2.0.[29] Garbage collection in Objective-C runs on a low-priority background thread, and can halt on user events, with the intention of keeping the user experience responsive.[30]\n\nGarbage collection was deprecated in OS X v10.8 in favor of Automatic Reference Counting (ARC).[31] Objective-C on iOS 7 running on ARM64 uses 19 bits out of a 64-bit word to store the reference count, as a form of tagged pointers.[32][33]\n\nProperties[edit]\nObjective-C 2.0 introduces a new syntax to declare instance variables as properties, with optional attributes to configure the generation of accessor methods. Properties are, in a sense, public instance variables; that is, declaring an instance variable as a property provides external classes with access (possibly limited, e.g. read only) to that property. A property may be declared as \"readonly\", and may be provided with storage semantics such as assign, copy or retain. By default, properties are considered atomic, which results in a lock preventing multiple threads from accessing them at the same time. A property can be declared as nonatomic, which removes this lock.\n\n@interface Person : NSObject {\n @public\n NSString *name;\n @private\n int age;\n}\n\n@property(copy) NSString *name;\n@property(readonly) int age;\n\n-(id)initWithAge:(int)age;\n@end\nProperties are implemented by way of the @synthesize keyword, which generates getter (and setter, if not read-only) methods according to the property declaration. Alternatively, the getter and setter methods must be implemented explicitly, or the @dynamic keyword can be used to indicate that accessor methods will be provided by other means. When compiled using clang 3.1 or higher, all properties which are not explicitly declared with @dynamic, marked readonly or have complete user-implemented getter and setter will be automatically implicitly @synthesize'd.\n\n@implementation Person\n@synthesize name;\n\n-(id)initWithAge:(int)initAge {\n self = [super init];\n if (self) {\n age = initAge; // NOTE: direct instance variable assignment, not property setter\n }\n return self;\n}\n\n-(int)age {\n return age;\n}\n@end\nProperties can be accessed using the traditional message passing syntax, dot notation, or, in Key-Value Coding, by name via the \"valueForKey:\"/\"setValue:forKey:\" methods.\n\nPerson *aPerson = [[Person alloc] initWithAge: 53];\naPerson.name = @\"Steve\"; // NOTE: dot notation, uses synthesized setter,\n // equivalent to [aPerson setName: @\"Steve\"];\nNSLog(@\"Access by message (%@), dot notation(%@),\nproperty name(%@) and direct instance variable access (%@)\",\n [aPerson name], aPerson.name, [aPerson valueForKey:@\"name\"], aPerson->name);\nIn order to use dot notation to invoke property accessors within an instance method, the \"self\" keyword should be used:\n\n-(void) introduceMyselfWithProperties:(BOOL)useGetter {\n NSLog(@\"Hi, my name is %@.\", (useGetter ? self.name : name));\n// NOTE: getter vs. ivar access\n}\nA class or protocol's properties may be dynamically introspected.\n\nint i;\nint propertyCount = 0;\nobjc_property_t *propertyList = class_copyPropertyList([aPerson class], &propertyCount);\n\nfor (i = 0; i < propertyCount; i++) {\n objc_property_t *thisProperty = propertyList + i;\n const char* propertyName = property_getName(*thisProperty);\n NSLog(@\"Person has a property: '%s'\", propertyName);\n}\nNon-fragile instance variables[edit]\nObjective-C 2.0 provides non-fragile instance variables where supported by the runtime (i.e. when building code for 64-bit Mac OS X, and all iOS). Under the modern runtime, an extra layer of indirection is added to instance variable access, allowing the dynamic linker to adjust instance layout at runtime. This feature allows for two important improvements to Objective-C code:\n\nIt eliminates the fragile binary interface problem; superclasses can change sizes without affecting binary compatibility.\nIt allows instance variables that provide the backing for properties to be synthesized at runtime without them being declared in the class's interface.\nFast enumeration[edit]\nInstead of using an NSEnumerator object or indices to iterate through a collection, Objective-C 2.0 offers the fast enumeration syntax. In Objective-C 2.0, the following loops are functionally equivalent, but have different performance traits.\n\n// Using NSEnumerator\nNSEnumerator *enumerator = [thePeople objectEnumerator];\nPerson *p;\n\nwhile ((p = [enumerator nextObject]) != nil) {\n NSLog(@\"%@ is %i years old.\", [p name], [p age]);\n}\n// Using indexes\nfor (int i = 0; i < [thePeople count]; i++) {\n Person *p = [thePeople objectAtIndex:i];\n NSLog(@\"%@ is %i years old.\", [p name], [p age]);\n}\n// Using fast enumeration\nfor (Person *p in thePeople) {\n NSLog(@\"%@ is %i years old.\", [p name], [p age]);\n}\nFast enumeration generates more efficient code than standard enumeration because method calls to enumerate over objects are replaced by pointer arithmetic using the NSFastEnumeration protocol.[34]\n\nClass extensions[edit]\nA class extension has the same syntax as a category declaration with no category name, and the methods and properties declared in it are added directly to the main class. It is mostly used as an alternative to a category to add methods to a class without advertising them in the public headers, with the advantage that for class extensions the compiler checks that all the privately declared methods are actually implemented.[35]\n\nImplications for Cocoa development[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2012) (Learn how and when to remove this template message)\nAll Objective-C applications developed for Mac OS X that make use of the above improvements for Objective-C 2.0 are incompatible with all operating systems prior to 10.5 (Leopard). Since fast enumeration does not generate exactly the same binaries as standard enumeration, its use will cause an application to crash on OS X version 10.4 or earlier.\n\nBlocks[edit]\nMain article: Blocks (C language extension)\nBlocks is a nonstandard extension for Objective-C (and C and C++) that uses special syntax to create closures. Blocks are only supported in Mac OS X 10.6 \"Snow Leopard\" or later, iOS 4 or later, and GNUstep with libobjc2 1.7 and compiling with clang 3.1 or later.[36]\n\n#include <stdio.h>\n#include <Block.h>\ntypedef int (^IntBlock)();\n\nIntBlock MakeCounter(int start, int increment) {\n\t__block int i = start;\n\t\n\treturn Block_copy( ^ {\n\t\tint ret = i;\n\t\ti += increment;\n\t\treturn ret;\n\t});\n\t\n}\n\nint main(void) {\n\tIntBlock mycounter = MakeCounter(5, 2);\n\tprintf(\"First call: %d\\n\", mycounter());\n\tprintf(\"Second call: %d\\n\", mycounter());\n\tprintf(\"Third call: %d\\n\", mycounter());\n\t\n\t/* because it was copied, it must also be released */\n\tBlock_release(mycounter);\n\t\n\treturn 0;\n}\n/* Output:\n\tFirst call: 5\n\tSecond call: 7\n\tThird call: 9\n*/\nModern Objective-C[edit]\nAutomatic Reference Counting[edit]\nMain article: Automatic Reference Counting\nAutomatic Reference Counting (ARC) is a compile-time feature that eliminates the need for programmers to manually manage retain counts using retain and release.[37] Unlike garbage collection, which occurs at run time, ARC eliminates the overhead of a separate process managing retain counts. ARC and manual memory management are not mutually exclusive; programmers can continue to use non-ARC code in ARC-enabled projects by disabling ARC for individual code files. XCode can also attempt to automatically upgrade a project to ARC.\n\nLiterals[edit]\nNeXT and Apple Obj-C runtimes have long included a short-form way to create new strings, using the literal syntax @\"a new string\", or drop to CoreFoundation constants kCFBooleanTrue and kCFBooleanFalse for NSNumber with Boolean values. Using this format saves the programmer from having to use the longer initWithString or similar methods when doing certain operations.\n\nWhen using Apple LLVM compiler 4.0 or later, arrays, dictionaries, and numbers (NSArray, NSDictionary, NSNumber classes) can also be created using literal syntax instead of methods.[38]\n\nExample without literals:\n\nNSArray *myArray = [NSArray arrayWithObjects:object1,object2,object3,nil];\nNSDictionary *myDictionary1 = [NSDictionary dictionaryWithObject:someObject forKey:@\"key\"];\nNSDictionary *myDictionary2 = [NSDictionary dictionaryWithObjectsAndKeys:object1, key1, object2, key2, nil];\nNSNumber *myNumber = [NSNumber numberWithInt:myInt];\nNSNumber *mySumNumber= [NSNumber numberWithInt:(2 + 3)];\nNSNumber *myBoolNumber = [NSNumber numberWithBool:YES];\nExample with literals:\n\nNSArray *myArray = @[ object1, object2, object3 ];\nNSDictionary *myDictionary1 = @{ @\"key\" : someObject };\nNSDictionary *myDictionary2 = @{ key1: object1, key2: object2 };\nNSNumber *myNumber = @(myInt);\nNSNumber *mySumNumber = @(2+3);\nNSNumber *myBoolNumber = @YES;\nNSNumber *myIntegerNumber = @8;\nHowever, different from string literals, which compile to constants in the executable, these literals compile to code equivalent to the above method calls. In particular, under manually reference-counted memory management, these objects are autoreleased, which requires added care when e.g., used with function-static variables or other kinds of globals.\n\nSubscripting[edit]\nWhen using Apple LLVM compiler 4.0 or later, arrays and dictionaries (NSArray and NSDictionary classes) can be manipulated using subscripting.[38] Subscripting can be used to retrieve values from indexes (array) or keys (dictionary), and with mutable objects, can also be used to set objects to indexes or keys. In code, subscripting is represented using brackets [ ].[39]\n\nExample without subscripting:\n\nid object1 = [someArray objectAtIndex:0];\nid object2 = [someDictionary objectForKey:@\"key\"];\n[someMutableArray replaceObjectAtIndex:0 withObject:object3];\n[someMutableDictionary setObject:object4 forKey:@\"key\"];\nExample with subscripting:\n\nid object1 = someArray[0];\nid object2 = someDictionary[@\"key\"];\nsomeMutableArray[0] = object3;\nsomeMutableDictionary[@\"key\"] = object4;\n\"Modern\" Objective-C syntax (1997)[edit]\nAfter the purchase of NeXT by Apple, attempts were made to make the language more acceptable to programmers more familiar with Java than Smalltalk. One of these attempts was introducing what was dubbed \"Modern Syntax\" for Objective-C at the time[40] (as opposed to the current, \"classic\" syntax). There was no change in behaviour, this was merely an alternative syntax. Instead of writing a method invocation like\n\n   object = [[MyClass alloc] init];\n   [object firstLabel: param1 secondLabel: param2];\nIt was instead written as\n\n   object = (MyClass.alloc).init;\n   object.firstLabel ( param1, param2 );\nSimilarly, declarations went from the form\n\n   -(void) firstLabel: (int)param1 secondLabel: (int)param2;\nto\n\n   -(void) firstLabel ( int param1, int param2 );\nThis \"modern\" syntax is no longer supported in current dialects of the Objective-C language.\n\nPortable Object Compiler[edit]\nBesides the GCC/NeXT/Apple implementation, which added several extensions to the original Stepstone implementation, another free, open-source Objective-C implementation called the Portable Object Compiler[41] also exists. The set of extensions implemented by the Portable Object Compiler differs from the GCC/NeXT/Apple implementation; in particular, it includes Smalltalk-like blocks for Objective-C, while it lacks protocols and categories, two features used extensively in OpenStep and its derivatives and relatives. Overall, POC represents an older, pre-NeXT stage in the language's evolution, roughly conformant to Brad Cox's 1991 book.\n\nIt also includes a runtime library called ObjectPak, which is based on Cox's original ICPak101 library (which in turn derives from the Smalltalk-80 class library), and is quite radically different from the OpenStep FoundationKit.\n\nGEOS Objective-C[edit]\nThe PC GEOS system used a programming language known as GEOS Objective-C or goc;[42] despite the name similarity, the two languages are similar only in overall concept and the use of keywords prefixed with an @ sign.\n\nClang[edit]\nThe Clang compiler suite, part of the LLVM project, implements Objective-C, and other languages.\n\nLibrary use[edit]\nObjective-C today is often used in tandem with a fixed library of standard objects (often known as a \"kit\" or \"framework\"), such as Cocoa, GNUstep or ObjFW. These libraries often come with the operating system: the GNUstep libraries often come with Linux based distributions and Cocoa comes with OS X. The programmer is not forced to inherit functionality from the existing base class (NSObject / OFObject). Objective-C allows for the declaration of new root classes that do not inherit any existing functionality. Originally, Objective-C based programming environments typically offered an Object class as the base class from which almost all other classes inherited. With the introduction of OpenStep, NeXT created a new base class named NSObject, which offered additional features over Object (an emphasis on using object references and reference counting instead of raw pointers, for example). Almost all classes in Cocoa inherit from NSObject.\n\nNot only did the renaming serve to differentiate the new default behavior of classes within the OpenStep API, but it allowed code that used Object—the original base class used on NeXTSTEP (and, more or less, other Objective-C class libraries)—to co-exist in the same runtime with code that used NSObject (with some limitations). The introduction of the two letter prefix also became a simplistic form of namespaces, which Objective-C lacks. Using a prefix to create an informal packaging identifier became an informal coding standard in the Objective-C community, and continues to this day.\n\nMore recently, package managers have started appearing, such as CocoaPods, which aims to be both a package manager and a repository of packages. A lot of open-source Objective-C code that was written in the last few years can now be installed using CocoaPods.\n\nAnalysis of the language[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2011) (Learn how and when to remove this template message)\nObjective-C implementations use a thin runtime system written in C, which adds little to the size of the application. In contrast, most object-oriented systems at the time that it was created used large virtual machine runtimes. Programs written in Objective-C tend to be not much larger than the size of their code and that of the libraries (which generally do not need to be included in the software distribution), in contrast to Smalltalk systems where a large amount of memory was used just to open a window. Objective-C applications tend to be larger than similar C or C++ applications because Objective-C dynamic typing does not allow methods to be stripped or inlined. Since the programmer has such freedom to delegate, forward calls, build selectors on the fly and pass them to the runtime system, the Objective-C compiler cannot assume it is safe to remove unused methods or to inline calls.\n\nLikewise, the language can be implemented atop extant C compilers (in GCC, first as a preprocessor, then as a module) rather than as a new compiler. This allows Objective-C to leverage the huge existing collection of C code, libraries, tools, etc. Existing C libraries can be wrapped in Objective-C wrappers to provide an OO-style interface. In this aspect, it is similar to GObject library and Vala language, which are widely used in development of GTK applications.\n\nAll of these practical changes lowered the barrier to entry, likely the biggest problem for the widespread acceptance of Smalltalk in the 1980s.\n\nA common criticism is that Objective-C does not have language support for namespaces. Instead, programmers are forced to add prefixes to their class names, which are traditionally shorter than namespace names and thus more prone to collisions. As of 2007, all Mac OS X classes and functions in the Cocoa programming environment are prefixed with \"NS\" (e.g. NSObject, NSButton) to identify them as belonging to the Mac OS X or iOS core; the \"NS\" derives from the names of the classes as defined during the development of NeXTSTEP.\n\nSince Objective-C is a strict superset of C, it does not treat C primitive types as first-class objects.\n\nUnlike C++, Objective-C does not support operator overloading. Also unlike C++, Objective-C allows an object to directly inherit only from one class (forbidding multiple inheritance). However, in most cases, categories and protocols may be used as alternative ways to achieve the same results.\n\nBecause Objective-C uses dynamic runtime typing and because all method calls are function calls (or, in some cases, syscalls), many common performance optimizations cannot be applied to Objective-C methods (for example: inlining, constant propagation, interprocedural optimizations, and scalar replacement of aggregates). This limits the performance of Objective-C abstractions relative to similar abstractions in languages such as C++ where such optimizations are possible.\n\nMemory management[edit]\nThe first versions of Objective-C did not support garbage collection. At the time this decision was a matter of some debate, and many people considered long \"dead times\" (when Smalltalk performed collection) to render the entire system unusable. Some 3rd party implementations have added this feature (most notably GNUstep) and Apple has implemented it as of Mac OS X v10.5.[43] However, in more recent versions of Mac OS X and iOS, garbage collection has been deprecated in favor of Automatic Reference Counting (ARC), introduced in 2011.\n\nWith ARC, the compiler inserts retain and release calls automatically into Objective-C code based on static code analysis. The automation relieves the programmer of having to write in memory management code. ARC also adds weak references to the Objective-C language.[44]\n\nPhilosophical differences between Objective-C and C++[edit]\nThe design and implementation of C++ and Objective-C represent fundamentally different approaches to extending C.\n\nIn addition to C's style of procedural programming, C++ directly supports certain forms of object-oriented programming, generic programming, and metaprogramming. C++ also comes with a large standard library that includes several container classes. Similarly, Objective-C adds object-oriented programming, dynamic typing, and reflection to C. Objective-C does not provide a standard library per se, but in most places where Objective-C is used, it is used with an OpenStep-like library such as OPENSTEP, Cocoa, or GNUstep, which provides functionality similar to C++'s standard library.\n\nOne notable difference is that Objective-C provides runtime support for reflective features, whereas C++ adds only a small amount of runtime support to C. In Objective-C, an object can be queried about its own properties, e.g., whether it will respond to a certain message. In C++, this is not possible without the use of external libraries.\n\nThe use of reflection is part of the wider distinction between dynamic (run-time) features and static (compile-time) features of a language. Although Objective-C and C++ each employ a mix of both features, Objective-C is decidedly geared toward run-time decisions while C++ is geared toward compile-time decisions. The tension between dynamic and static programming involves many of the classic trade-offs in programming: dynamic features add flexibility, static features add speed and type checking.\n\nGeneric programming and metaprogramming can be implemented in both languages using runtime polymorphism. In C++ this takes the form of virtual functions and runtime type identification, while Objective-C offers dynamic typing and reflection. Objective-C lacks compile-time polymorphism (generic functions) entirely, while C++ supports it via function overloading and templates.\n\nSee also[edit]\nC (programming language)\nC++\nComparison of programming languages\nComparison with COM, GObject, SOM, Windows Runtime, XPCOM\nSwift\nXcode",
          "type": "compiled",
          "plid": 24
        },
        {
          "name": "Objective-J",
          "details": "Objective-J is a programming language developed as part of the Cappuccino web development framework. Its syntax is nearly identical to the Objective-C syntax and it shares with JavaScript the same relationship that Objective-C has with the C programming language: that of being a strict, but small, superset; adding traditional inheritance and Smalltalk/Objective-C style dynamic dispatch. Pure JavaScript, being a prototype-based language, already has a notion of object orientation and inheritance, but Objective-J adds the use of class-based programming to JavaScript.\n\nPrograms written in Objective-J need to be preprocessed before being run by a web browser's JavaScript virtual machine. This step can occur in the web browser at runtime or by a compiler which translates Objective-J programs into pure JavaScript code. The Objective-J compiler is written in JavaScript; consequently, deploying Objective-J programs does not require a web browser plug-in. Objective-J can be compiled and run on Node.js.\n\nContents  [hide] \n1\tApplications\n1.1\tApplications designed using the Cappuccino Framework[1]\n2\tSyntax\n3\tMemory management\n4\tSee also\n5\tReferences\n6\tExternal links\nApplications[edit]\nThe first widely known use of Objective-J was in the Cappuccino-based web application 280 Slides, which was developed by 280 North itself. Even though Objective-J can be used (and has been designed) independently from the Cappuccino framework, Objective-J has primarily been invented to support web development in Cappuccino.\n\nApplications designed using the Cappuccino Framework[1][edit]\nMockingbird\nPicEngine\nGithubIssues\nTimeTable\nEnstore (until October 2013, they rewrote it using Ember [2])\nAlmost At\nAkshell - Online JavaScript Web-App IDE\nArchipel Project - Virtual machine orchestrator\nSpot Specific - Mobile App SDK and IDE\nSyntax[edit]\nObjective-J is a superset of JavaScript, which means that any valid JavaScript code is also valid Objective-J code.\n\nThe following example shows the definition and implementation in Objective-J of a class named Address; this class extends the root object CPObject, which plays a role similar to the Objective-C's NSObject. This example differs from traditional Objective-C in that the root object reflects the underlying Cappuccino framework as opposed to Cocoa, Objective-J does not use pointers and, as such, type definitions do not contain asterisk characters. Instance variables are always defined in the @implementation.\n\n@implementation Address : CPObject\n{\n  CPString name;\n  CPString city;\n}\n\n- (id)initWithName:(CPString)aName city:(CPString)aCity\n{\n  self = [super init];\n\n  name = aName;\n  city = aCity;\n\n  return self;\n}\n\n- (void)setName:(CPString)aName\n{\n  name = aName;\n}\n\n- (CPString)name\n{\n  return name;\n}\n\n+ (id)newAddressWithName:(CPString)aName city:(CPString)aCity\n{\n  return [[self alloc] initWithName:aName city:aCity];\n}\n\n@end\nAs with Objective-C, class method definitions and instance method definitions start with '+' (plus) and '-' (dash), respectively.\n\nMemory management[edit]\nObjective-C uses ARC (Automatic Reference Counting) for deallocating unused objects. In Objective-J, objects are automatically deallocated by JavaScript's Garbage Collector.",
          "type": "compiler",
          "plid": 25
        },
        {
          "name": "Perl",
          "details": "Perl is a family of high-level, general-purpose, interpreted, dynamic programming languages. The languages in this family include Perl 5 and Perl 6.[6]\n\nThough Perl is not officially an acronym,[7] there are various backronyms in use, the best-known being \"Practical Extraction and Reporting Language\".[8] Perl was originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier.[9] Since then, it has undergone many changes and revisions. Perl 6, which began as a redesign of Perl 5 in 2000, eventually evolved into a separate language. Both languages continue to be developed independently by different development teams and liberally borrow ideas from one another.\n\nThe Perl languages borrow features from other programming languages including C, shell script (sh), AWK, and sed.[10] They provide powerful text processing facilities without the arbitrary data-length limits of many contemporary Unix commandline tools,[11] facilitating easy manipulation of text files. Perl 5 gained widespread popularity in the late 1990s as a CGI scripting language, in part due to its unsurpassed[12][13][14] regular expression and string parsing abilities.[15]\n\nIn addition to CGI, Perl 5 is used for graphics programming, system administration, network programming, finance, bioinformatics, and other applications. It has been nicknamed \"the Swiss Army chainsaw of scripting languages\" because of its flexibility and power,[16] and possibly also because of its \"ugliness\".[17] In 1998, it was also referred to as the \"duct tape that holds the Internet together\", in reference to both its ubiquitous use as a glue language and its perceived inelegance.[18]\n\nContents  [hide] \n1\tHistory\n1.1\tEarly versions\n1.2\tEarly Perl 5\n1.3\t2000–present\n1.4\tPONIE\n1.5\tName\n1.6\tCamel symbol\n1.7\tOnion symbol\n2\tOverview\n2.1\tFeatures\n2.2\tDesign\n2.3\tApplications\n2.4\tImplementation\n2.5\tAvailability\n2.5.1\tWindows\n3\tDatabase interfaces\n4\tComparative performance\n4.1\tOptimizing\n5\tPerl 6\n6\tFuture of Perl 5\n7\tPerl community\n7.1\tState of the Onion\n7.2\tPerl pastimes\n7.3\tPerl on IRC\n7.4\tCPAN Acme\n8\tExample code\n9\tCriticism\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory[edit]\nEarly versions[edit]\nLarry Wall began work on Perl in 1987, while working as a programmer at Unisys,[11] and released version 1.0 to the comp.sources.misc newsgroup on December 18, 1987.[19] The language expanded rapidly over the next few years.\n\nPerl 2, released in 1988, featured a better regular expression engine. Perl 3, released in 1989, added support for binary data streams.\n\nOriginally, the only documentation for Perl was a single (increasingly lengthy) man page. In 1991, Programming Perl, known to many Perl programmers as the \"Camel Book\" because of its cover, was published and became the de facto reference for the language. At the same time, the Perl version number was bumped to 4, not to mark a major change in the language but to identify the version that was well documented by the book.\n\nEarly Perl 5[edit]\nMain article: Perl 5 version history\nPerl 4 went through a series of maintenance releases, culminating in Perl 4.036 in 1993. At that point, Wall abandoned Perl 4 to begin work on Perl 5. Initial design of Perl 5 continued into 1994. The perl5-porters mailing list was established in May 1994 to coordinate work on porting Perl 5 to different platforms. It remains the primary forum for development, maintenance, and porting of Perl 5.[20]\n\nPerl 5.000 was released on October 17, 1994.[21] It was a nearly complete rewrite of the interpreter, and it added many new features to the language, including objects, references, lexical (my) variables, and modules. Importantly, modules provided a mechanism for extending the language without modifying the interpreter. This allowed the core interpreter to stabilize, even as it enabled ordinary Perl programmers to add new language features. Perl 5 has been in active development since then.\n\nPerl 5.001 was released on March 13, 1995. Perl 5.002 was released on February 29, 1996 with the new prototypes feature. This allowed module authors to make subroutines that behaved like Perl builtins. Perl 5.003 was released June 25, 1996, as a security release.\n\nOne of the most important events in Perl 5 history took place outside of the language proper and was a consequence of its module support. On October 26, 1995, the Comprehensive Perl Archive Network (CPAN) was established as a repository for Perl modules and Perl itself; as of June 2015, it carries over 150,775 modules in 31,896 distributions, written by more than 12,219 authors, and is mirrored worldwide at more than 253 locations.[22]\n\nPerl 5.004 was released on May 15, 1997, and included among other things the UNIVERSAL package, giving Perl a base object to which all classes were automatically derived and the ability to require versions of modules. Another significant development was the inclusion of the CGI.pm module,[23] which contributed to Perl's popularity as a CGI scripting language.[24]\n\nPerl is also now supported running under Microsoft Windows and several other operating systems.[23]\n\nPerl 5.005 was released on July 22, 1998. This release included several enhancements to the regex engine, new hooks into the backend through the B::* modules, the qr// regex quote operator, a large selection of other new core modules, and added support for several more operating systems, including BeOS.[25]\n\n2000–present[edit]\nMajor version\tLatest update\n5.5\t2004-02-23[26]\n5.6\t2003-11-15[26]\n5.8\t2008-12-14[26]\n5.10\t2009-08-23[26]\n5.12\t2012-11-10[26]\n5.14\t2013-03-10[26]\n5.16\t2013-03-11[26]\n5.18\t2014-10-02[26]\n5.20\t2015-09-12[26]\n5.22\t2016-04-29[26]\n5.24\t2016-05-09[26]\nOld version\nOlder version, still supported\nCurrent stable version\nFuture release\nPerl 5.6 was released on March 22, 2000. Major changes included 64-bit support, Unicode string representation, large file support (i.e. files over 2 GiB) and the \"our\" keyword.[27][28] When developing Perl 5.6, the decision was made to switch the versioning scheme to one more similar to other open source projects; after 5.005_63, the next version became 5.5.640, with plans for development versions to have odd numbers and stable versions to have even numbers.\n\nIn 2000, Wall put forth a call for suggestions for a new version of Perl from the community. The process resulted in 361 RFC (request for comments) documents that were to be used in guiding development of Perl 6. In 2001,[29] work began on the apocalypses for Perl 6, a series of documents meant to summarize the change requests and present the design of the next generation of Perl. They were presented as a digest of the RFCs, rather than a formal document. At this point, Perl 6 existed only as a description of a language.\n\nPerl 5.8 was first released on July 18, 2002, and had nearly yearly updates since then. Perl 5.8 improved Unicode support, added a new I/O implementation, added a new thread implementation, improved numeric accuracy, and added several new modules.[30] As of 2013 this version still remains the most popular version of Perl and is used by Red Hat 5, Suse 10, Solaris 10, HP-UX 11.31 and AIX 5.\n\nIn 2004, work began on the Synopses – documents that originally summarized the Apocalypses, but which became the specification for the Perl 6 language. In February 2005, Audrey Tang began work on Pugs, a Perl 6 interpreter written in Haskell.[31] This was the first concerted effort towards making Perl 6 a reality. This effort stalled in 2006.[32]\n\nOn December 18, 2007, the 20th anniversary of Perl 1.0, Perl 5.10.0 was released. Perl 5.10.0 included notable new features, which brought it closer to Perl 6. These included a switch statement (called \"given\"/\"when\"), regular expressions updates, and the smart match operator, \"~~\".[33][34] Around this same time, development began in earnest on another implementation of Perl 6 known as Rakudo Perl, developed in tandem with the Parrot virtual machine. As of November 2009, Rakudo Perl has had regular monthly releases and now is the most complete implementation of Perl 6.\n\nA major change in the development process of Perl 5 occurred with Perl 5.11; the development community has switched to a monthly release cycle of development releases, with a yearly schedule of stable releases. By that plan, bugfix point releases will follow the stable releases every three months.\n\nOn April 12, 2010, Perl 5.12.0 was released. Notable core enhancements include new package NAME VERSION syntax, the Yada Yada operator (intended to mark placeholder code that is not yet implemented), implicit strictures, full Y2038 compliance, regex conversion overloading, DTrace support, and Unicode 5.2.[35] On January 21, 2011, Perl 5.12.3 was released; it contains updated modules and some documentation changes.[36] Version 5.12.4 was released on June 20, 2011. The latest version of that branch, 5.12.5, was released on November 10, 2012.\n\nOn May 14, 2011, Perl 5.14 was released. JSON support is built-in as of 5.14.2. The latest version of that branch, 5.14.4, was released on March 10, 2013.\n\nOn May 20, 2012, Perl 5.16 was released. Notable new features include the ability to specify a given version of Perl that one wishes to emulate, allowing users to upgrade their version of Perl, but still run old scripts that would normally be incompatible.[37] Perl 5.16 also updates the core to support Unicode 6.1.[37]\n\nOn May 18, 2013, Perl 5.18 was released. Notable new features include the new dtrace hooks, lexical subs, more CORE:: subs, overhaul of the hash for security reasons, support for Unicode 6.2.[38]\n\nOn May 27, 2014, Perl 5.20 was released. Notable new features include subroutine signatures, hash slices/new slice syntax, postfix dereferencing (experimental), Unicode 6.3, rand() using consistent random number generator.[39]\n\nSome observers credit the release of Perl 5.10 with the start of the Modern Perl movement.[40] In particular, this phrase describes a style of development that embraces the use of the CPAN, takes advantage of recent developments in the language, and is rigorous about creating high quality code.[41] While the book \"Modern Perl\"[42] may be the most visible standard-bearer of this idea, other groups such as the Enlightened Perl Organization[43] have taken up the cause.\n\nIn late 2012 and 2013 several projects for alternative implementations for Perl 5 started: Perl5 in Perl6 by the Rakudo Perl team,[44] moe by Stevan Little and friends,[45] p2[46] by the Perl11 team under Reini Urban, gperl by goccy,[47] and rperl a kickstarter project led by Will Braswell and affiliated with the Perll11 project.[48]\n\nPONIE[edit]\nPONIE is an acronym for Perl On New Internal Engine. The PONIE Project existed from 2003 until 2006 and was to be a bridge between Perl 5 and Perl 6. It was an effort to rewrite the Perl 5 interpreter to run on Parrot, the Perl 6 virtual machine. The goal was to ensure the future of the millions of lines of Perl 5 code at thousands of companies around the world.[49]\n\nThe PONIE project ended in 2006 and is no longer being actively developed. Some of the improvements made to the Perl 5 interpreter as part of PONIE were folded into that project.[50]\n\nName[edit]\nPerl was originally named \"Pearl\". Wall wanted to give the language a short name with positive connotations; he claims that he considered (and rejected) every three- and four-letter word in the dictionary. He also considered naming it after his wife Gloria. Wall discovered the existing PEARL programming language before Perl's official release and changed the spelling of the name.[51]\n\nWhen referring to the language, the name is normally capitalized (Perl) as a proper noun. When referring to the interpreter program itself, the name is often uncapitalized (perl) because most Unix-like file systems are case-sensitive. Before the release of the first edition of Programming Perl, it was common to refer to the language as perl; Randal L. Schwartz, however, capitalized the language's name in the book to make it stand out better when typeset. This case distinction was subsequently documented as canonical.[52]\n\nThe name is occasionally expanded as Practical Extraction and Report Language, but this is a backronym.[53] Other expansions have been suggested as equally canonical, including Wall's own humorous Pathologically Eclectic Rubbish Lister.[54] Indeed, Wall claims that the name was intended to inspire many different expansions.[55]\n\nCamel symbol[edit]\n\nThe Camel symbol used by O'Reilly Media\nProgramming Perl, published by O'Reilly Media, features a picture of a dromedary camel on the cover and is commonly called the \"Camel Book\".[56] This image of a camel has become an unofficial symbol of Perl as well as a general hacker emblem, appearing on T-shirts and other clothing items.\n\nO'Reilly owns the image as a trademark but licenses it for non-commercial use, requiring only an acknowledgement and a link to www.perl.com. Licensing for commercial use is decided on a case by case basis.[57] O'Reilly also provides \"Programming Republic of Perl\" logos for non-commercial sites and \"Powered by Perl\" buttons for any site that uses Perl.[57]\n\nOnion symbol[edit]\n\nThe onion logo used by The Perl Foundation\nThe Perl Foundation owns an alternative symbol, an onion, which it licenses to its subsidiaries, Perl Mongers, PerlMonks, Perl.org, and others.[58] The symbol is a visual pun on pearl onion.[59]\n\nOverview[edit]\nMain article: Perl language structure\nAccording to Wall, Perl has two slogans. The first is \"There's more than one way to do it\", commonly known as TMTOWTDI. The second slogan is \"Easy things should be easy and hard things should be possible\".[11]\n\nFeatures[edit]\nThe overall structure of Perl derives broadly from C. Perl is procedural in nature, with variables, expressions, assignment statements, brace-delimited blocks, control structures, and subroutines.\n\nPerl also takes features from shell programming. All variables are marked with leading sigils, which allow variables to be interpolated directly into strings. However, unlike the shell, Perl uses sigils on all accesses to variables, and unlike most other programming languages that use sigils, the sigil doesn't denote the type of the variable but the type of the expression. So for example, to access a list of values in a hash, the sigil for an array (\"@\") is used, not the sigil for a hash (\"%\"). Perl also has many built-in functions that provide tools often used in shell programming (although many of these tools are implemented by programs external to the shell) such as sorting, and calling operating system facilities.\n\nPerl takes lists from Lisp, hashes (\"associative arrays\") from AWK, and regular expressions from sed. These simplify and facilitate many parsing, text-handling, and data-management tasks. Also shared with Lisp are the implicit return of the last value in a block, and the fact that all statements have a value, and thus are also expressions and can be used in larger expressions themselves.\n\nPerl 5 added features that support complex data structures, first-class functions (that is, closures as values), and an object-oriented programming model. These include references, packages, class-based method dispatch, and lexically scoped variables, along with compiler directives (for example, the strict pragma). A major additional feature introduced with Perl 5 was the ability to package code as reusable modules. Wall later stated that \"The whole intent of Perl 5's module system was to encourage the growth of Perl culture rather than the Perl core.\"[60]\n\nAll versions of Perl do automatic data-typing and automatic memory management. The interpreter knows the type and storage requirements of every data object in the program; it allocates and frees storage for them as necessary using reference counting (so it cannot deallocate circular data structures without manual intervention). Legal type conversions — for example, conversions from number to string — are done automatically at run time; illegal type conversions are fatal errors.\n\nDesign[edit]\nThe design of Perl can be understood as a response to three broad trends in the computer industry: falling hardware costs, rising labor costs, and improvements in compiler technology. Many earlier computer languages, such as Fortran and C, aimed to make efficient use of expensive computer hardware. In contrast, Perl was designed so that computer programmers could write programs more quickly and easily.\n\nPerl has many features that ease the task of the programmer at the expense of greater CPU and memory requirements. These include automatic memory management; dynamic typing; strings, lists, and hashes; regular expressions; introspection; and an eval() function. Perl follows the theory of \"no built-in limits\",[56] an idea similar to the Zero One Infinity rule.\n\nWall was trained as a linguist, and the design of Perl is very much informed by linguistic principles. Examples include Huffman coding (common constructions should be short), good end-weighting (the important information should come first), and a large collection of language primitives. Perl favors language constructs that are concise and natural for humans to write, even where they complicate the Perl interpreter.\n\nPerl's syntax reflects the idea that \"things that are different should look different.\"[61] For example, scalars, arrays, and hashes have different leading sigils. Array indices and hash keys use different kinds of braces. Strings and regular expressions have different standard delimiters. This approach can be contrasted with a language such as Lisp, where the same basic syntax, composed of simple and universal Symbolic expressions, is used for all purposes.\n\nPerl does not enforce any particular programming paradigm (procedural, object-oriented, functional, or others) or even require the programmer to choose among them.\n\nThere is a broad practical bent to both the Perl language and the community and culture that surround it. The preface to Programming Perl begins: \"Perl is a language for getting your job done.\"[11] One consequence of this is that Perl is not a tidy language. It includes many features, tolerates exceptions to its rules, and employs heuristics to resolve syntactical ambiguities. Because of the forgiving nature of the compiler, bugs can sometimes be hard to find. Perl's function documentation remarks on the variant behavior of built-in functions in list and scalar contexts by saying, \"In general, they do what you want, unless you want consistency.\"[62]\n\nNo written specification or standard for the Perl language exists for Perl versions through Perl 5, and there are no plans to create one for the current version of Perl. There has been only one implementation of the interpreter, and the language has evolved along with it. That interpreter, together with its functional tests, stands as a de facto specification of the language. Perl 6, however, started with a specification,[63] and several projects[64] aim to implement some or all of the specification.\n\nApplications[edit]\nPerl has many and varied applications, compounded by the availability of many standard and third-party modules.\n\nPerl has chiefly been used to write CGI scripts: large projects written in Perl include cPanel, Slash, Bugzilla, RT, TWiki, and Movable Type; high-traffic websites that use Perl extensively include Priceline.com, Craigslist,[65] IMDb,[66] LiveJournal, DuckDuckGo,[67][68] Slashdot and Ticketmaster. It is also an optional component of the popular LAMP technology stack for Web development, in lieu of PHP or Python.\n\nPerl is often used as a glue language, tying together systems and interfaces that were not specifically designed to interoperate, and for \"data munging\",[69] that is, converting or processing large amounts of data for tasks such as creating reports. In fact, these strengths are intimately linked. The combination makes Perl a popular all-purpose language for system administrators, particularly because short programs, often called \"one-liner programs\", can be entered and run on a single command line.\n\nPerl code can be made portable across Windows and Unix; such code is often used by suppliers of software (both COTS and bespoke) to simplify packaging and maintenance of software build- and deployment-scripts.\n\nGraphical user interfaces (GUIs) may be developed using Perl. For example, Perl/Tk and WxPerl are commonly used to enable user interaction with Perl scripts. Such interaction may be synchronous or asynchronous, using callbacks to update the GUI.\n\nImplementation[edit]\nPerl is implemented as a core interpreter, written in C, together with a large collection of modules, written in Perl and C. As of 2010. The interpreter is 150,000 lines of C code and compiles to a 1 MB executable on typical machine architectures. Alternatively, the interpreter can be compiled to a link library and embedded in other programs. There are nearly 500 modules in the distribution, comprising 200,000 lines of Perl and an additional 350,000 lines of C code (much of the C code in the modules consists of character encoding tables).\n\nThe interpreter has an object-oriented architecture. All of the elements of the Perl language—scalars, arrays, hashes, coderefs, file handles—are represented in the interpreter by C structs. Operations on these structs are defined by a large collection of macros, typedefs, and functions; these constitute the Perl C API. The Perl API can be bewildering to the uninitiated, but its entry points follow a consistent naming scheme, which provides guidance to those who use it.\n\nThe life of a Perl interpreter divides broadly into a compile phase and a run phase.[70] In Perl, the phases are the major stages in the interpreter's life-cycle. Each interpreter goes through each phase only once, and the phases follow in a fixed sequence.\n\nMost of what happens in Perl's compile phase is compilation, and most of what happens in Perl's run phase is execution, but there are significant exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are compile time and run time. Perl is in compile time at most points during the compile phase, but compile time may also be entered during the run phase. The compile time for code in a string argument passed to the eval built-in occurs during the run phase. Perl is often in run time during the compile phase and spends most of the run phase in run time. Code in BEGIN blocks executes at run time but in the compile phase.\n\nAt compile time, the interpreter parses Perl code into a syntax tree. At run time, it executes the program by walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.\n\nPerl has a Turing-complete grammar because parsing can be affected by run-time code executed during the compile phase.[71] Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.\n\nIt is often said that \"Only perl can parse Perl\",[72] meaning that only the Perl interpreter (perl) can parse the Perl language (Perl), but even this is not, in general, true. Because the Perl interpreter can simulate a Turing machine during its compile phase, it would need to decide the halting problem in order to complete parsing in every case. It is a long-standing result that the halting problem is undecidable, and therefore not even perl can always parse Perl. Perl makes the unusual choice of giving the user access to its full programming power in its own compile phase. The cost in terms of theoretical purity is high, but practical inconvenience seems to be rare.\n\nOther programs that undertake to parse Perl, such as source-code analyzers and auto-indenters, have to contend not only with ambiguous syntactic constructs but also with the undecidability of Perl parsing in the general case. Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do). It was Kennedy who first conjectured that \"parsing Perl suffers from the 'halting problem'\",[73] which was later proved.[74]\n\nPerl is distributed with over 250,000 functional tests for core Perl language and over 250,000 functional tests for core modules. These run as part of the normal build process and extensively exercise the interpreter and its core modules. Perl developers rely on the functional tests to ensure that changes to the interpreter do not introduce software bugs; additionally, Perl users who see that the interpreter passes its functional tests on their system can have a high degree of confidence that it is working properly.\n\nAvailability[edit]\nPerl is dual licensed under both the Artistic License 1.0[3][4] and the GNU General Public License.[5] Distributions are available for most operating systems. It is particularly prevalent on Unix and Unix-like systems, but it has been ported to most modern (and many obsolete) platforms. With only six reported exceptions, Perl can be compiled from source code on all POSIX-compliant, or otherwise-Unix-compatible platforms.[75]\n\nBecause of unusual changes required for the classic Mac OS environment, a special port called MacPerl was shipped independently.[76]\n\nThe Comprehensive Perl Archive Network carries a complete list of supported platforms with links to the distributions available on each.[77] CPAN is also the source for publicly available Perl modules that are not part of the core Perl distribution.\n\nWindows[edit]\nUsers of Microsoft Windows typically install one of the native binary distributions of Perl for Win32, most commonly Strawberry Perl or ActivePerl. Compiling Perl from source code under Windows is possible, but most installations lack the requisite C compiler and build tools. This also makes it difficult to install modules from the CPAN, particularly those that are partially written in C.\n\nActivePerl is a closed source distribution from ActiveState that has regular releases that track the core Perl releases.[78] The distribution also includes the Perl package manager (PPM),[79] a popular tool for installing, removing, upgrading, and managing the use of common Perl modules. Included also is PerlScript, a Windows Script Host (WSH) engine implementing the Perl language. Visual Perl is an ActiveState tool that adds Perl to the Visual Studio .NET development suite.\n\nStrawberry Perl is an open source distribution for Windows. It has had regular, quarterly releases since January 2008, including new modules as feedback and requests come in. Strawberry Perl aims to be able to install modules like standard Perl distributions on other platforms, including compiling XS modules.\n\nThe Cygwin emulation layer is another way of running Perl under Windows. Cygwin provides a Unix-like environment on Windows, and both Perl and CPAN are available as standard pre-compiled packages in the Cygwin setup program. Since Cygwin also includes gcc, compiling Perl from source is also possible.\n\nA perl executable is included in several Windows Resource kits in the directory with other scripting tools.\n\nImplementations of Perl come with the MKS Toolkit and UWIN.\n\nDatabase interfaces[edit]\nPerl's text-handling capabilities can be used for generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a Web page using HTML, JavaScript, and SQL in a here document). The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large amounts of data for post-processing by a Perl subprogram.[80] In early versions of Perl, database interfaces were created by relinking the interpreter with a client-side database library. This was sufficiently difficult that it was done for only a few of the most-important and most widely used databases, and it restricted the resulting perl executable to using just one database interface at a time.\n\nIn Perl 5, database interfaces are implemented by Perl DBI modules. The DBI (Database Interface) module presents a single, database-independent interface to Perl applications, while the DBD (Database Driver) modules handle the details of accessing some 50 different databases; there are DBD drivers for most ANSI SQL databases.\n\nDBI provides caching for database handles and queries, which can greatly improve performance in long-lived execution environments such as mod perl,[81] helping high-volume systems avert load spikes as in the Slashdot effect.\n\nIn modern Perl applications, especially those written using web frameworks such as Catalyst, the DBI module is often used indirectly via object-relational mappers such as DBIx::Class, Class::DBI or Rose::DB::Object that generate SQL queries and handle data transparently to the application author.\n\nComparative performance[edit]\nThe Computer Language Benchmarks Game, a project hosted by Alioth, compares the performance of implementations of typical programming problems in several programming languages.[82] The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.[83]\n\nLarge Perl programs start more slowly than similar programs in compiled languages because perl has to compile the source every time it runs. In a talk at the YAPC::Europe 2005 conference and subsequent article \"A Timely Start\", Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path.[84] Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling.[85] Therefore, Perl programs pay this overhead penalty on every execution. The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.\n\nA number of tools have been introduced to improve this situation. The first such tool was Apache's mod perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. ActivePerl, via Microsoft ISAPI, provides similar performance improvements.\n\nOnce Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.\n\nOptimizing[edit]\nBecause Perl is an interpreted language, it can give problems when efficiency is critical; in such situations, the most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex but flexible XS mechanism.[86]\n\nPerl 6[edit]\nMain article: Perl 6\n\nCamelia, the logo for the Perl 6 project.[87]\nAt the 2000 Perl Conference, Jon Orwant made a case for a major new language-initiative.[88] This led to a decision to begin work on a redesign of the language, to be called Perl 6. Proposals for new language features were solicited from the Perl community at large, which submitted more than 300 RFCs.\n\nWall spent the next few years digesting the RFCs and synthesizing them into a coherent framework for Perl 6. He has presented his design for Perl 6 in a series of documents called \"apocalypses\" - numbered to correspond to chapters in Programming Perl. As of January 2011, the developing specification of Perl 6 is encapsulated in design documents called Synopses - numbered to correspond to Apocalypses.[89]\n\nPerl 6 is not intended to be backward compatible, although there will be a compatibility mode. Perl 6 and Perl 5 are distinct languages with a common ancestry.[90]\n\nThesis work by Bradley M. Kuhn, overseen by Wall, considered the possible use of the Java virtual machine as a runtime for Perl.[91] Kuhn's thesis showed this approach to be problematic. In 2001, it was decided that Perl 6 would run on a cross-language virtual machine called Parrot. This will mean that other languages targeting the Parrot will gain native access to CPAN, allowing some level of cross-language development.\n\nIn 2005, Audrey Tang created the pugs project, an implementation of Perl 6 in Haskell. This acted as, and continues to act as, a test platform for the Perl 6 language (separate from the development of the actual implementation) - allowing the language designers to explore. The pugs project spawned an active Perl/Haskell cross-language community centered around the freenode #perl6 IRC channel.\n\nAs of 2012, a number of features in the Perl 6 language show similarities to Haskell.\n\nAs of 2012, Perl 6 development centers primarily around two compilers:[92]\n\nRakudo Perl 6, an implementation running on the Parrot virtual machine and the Java virtual machine.[93] Developers are also working on MoarVM, a C language-based virtual machine designed specifically for Rakudo.[94]\nNiecza, which targets the Common Language Runtime.\nFuture of Perl 5[edit]\nDevelopment of Perl 5 is also continuing. Perl 5.12.0 was released in April 2010 with some new features influenced by the design of Perl 6,[35][95] followed by Perl 5.14.1 (released on June 17, 2011), Perl 5.16.1 (released on August 9, 2012.[96]), and Perl 5.18.0 (released on May 18, 2013). Perl 5 development versions are released on a monthly basis, with major releases coming out once per year.[97]\n\nFuture plans for Perl 5 include making the core language easier to extend from modules, and providing a small, extensible meta-object protocol in core.[98]\n\nThe relative proportion of Internet searches for 'Perl programming', as compared with similar searches for other programming languages, steadily declined from about 10% in 2005 to about 2% in 2011, and has remained around the 2% level since.[99]\n\nPerl community[edit]\nPerl's culture and community has developed alongside the language itself. Usenet was the first public venue in which Perl was introduced, but over the course of its evolution, Perl's community was shaped by the growth of broadening Internet-based services including the introduction of the World Wide Web. The community that surrounds Perl was, in fact, the topic of Wall's first \"State of the Onion\" talk.[100]\n\nState of the Onion[edit]\nState of the Onion is the name for Wall’s yearly keynote-style summaries on the progress of Perl and its community. They are characterized by his hallmark humor, employing references to Perl’s culture, the wider hacker culture, Wall’s linguistic background, sometimes his family life, and occasionally even his Christian background.[101]\n\nEach talk is first given at various Perl conferences and is eventually also published online.\n\nPerl pastimes[edit]\nJAPHs\nIn email, Usenet, and message board postings, \"Just another Perl hacker\" (JAPH) programs are a common trend, originated by Randal L. Schwartz, one of the earliest professional Perl trainers.[102] In the parlance of Perl culture, Perl programmers are known as Perl hackers, and from this derives the practice of writing short programs to print out the phrase \"Just another Perl hacker\". In the spirit of the original concept, these programs are moderately obfuscated and short enough to fit into the signature of an email or Usenet message. The \"canonical\" JAPH as developed by Schwartz includes the comma at the end, although this is often omitted.[103]\nPerl golf\nPerl \"golf\" is the pastime of reducing the number of characters (key \"strokes\") used in a Perl program to the bare minimum, much in the same way that golf players seek to take as few shots as possible in a round. The phrase's first use[104] emphasized the difference between pedestrian code meant to teach a newcomer and terse hacks likely to amuse experienced Perl programmers, an example of the latter being JAPHs that were already used in signatures in Usenet postings and elsewhere. Similar stunts had been an unnamed pastime in the language APL in previous decades. The use of Perl to write a program that performed RSA encryption prompted a widespread and practical interest in this pastime.[105] In subsequent years, the term \"code golf\" has been applied to the pastime in other languages.[106] A Perl Golf Apocalypse was held at Perl Conference 4.0 in Monterey, California in July 2000.\nObfuscation\nAs with C, obfuscated code competitions were a well known pastime in the late 1990s. The Obfuscated Perl Contest was a competition held by The Perl Journal from 1996 to 2000 that made an arch virtue of Perl's syntactic flexibility. Awards were given for categories such as \"most powerful\"—programs that made efficient use of space—and \"best four-line signature\" for programs that fit into four lines of 76 characters in the style of a Usenet signature block.[107]\nPoetry\nPerl poetry is the practice of writing poems that can be compiled as legal Perl code, for example the piece known as Black Perl. Perl poetry is made possible by the large number of English words that are used in the Perl language. New poems are regularly submitted to the community at PerlMonks.[108]\nPerl on IRC[edit]\nThere are a number of IRC channels that offer support for the language and some modules.\n\nIRC Network\tChannels\nirc.freenode.net\t#perl #perl6 #cbstream #perlcafe #poe\nirc.perl.org\t#moose #poe #catalyst #dbix-class #perl-help #distzilla #epo #corehackers #sdl #win32 #toolchain #padre #dancer\nirc.slashnet.org\t#perlmonks\nirc.oftc.net\t#perl\nirc.efnet.net\t#perlhelp\nirc.rizon.net\t#perl\nirc.debian.org\t#debian-perl (packaging Perl modules for Debian)\nCPAN Acme[edit]\nThere are also many examples of code written purely for entertainment on the CPAN. Lingua::Romana::Perligata, for example, allows writing programs in Latin.[109] Upon execution of such a program, the module translates its source code into regular Perl and runs it.\n\nThe Perl community has set aside the \"Acme\" namespace for modules that are fun in nature (but its scope has widened to include exploratory or experimental code or any other module that is not meant to ever be used in production). Some of the Acme modules are deliberately implemented in amusing ways. This includes Acme::Bleach, one of the first modules in the Acme:: namespace,[110] which allows the program's source code to be \"whitened\" (i.e., all characters replaced with whitespace) and yet still work.\n\nExample code[edit]\nIn older versions of Perl, one would write the Hello World program as:\n\nprint \"Hello World!\\n\";\nIn later versions, which support the say statement, one can also write it as:\n\nuse 5.010;\nsay \"Hello World!\";\nGood Perl practices require more complex programs to add the use strict; and use warnings; pragmas, leading into something like:\n\nuse strict;\nuse warnings;\n\nprint \"Hello World!\\n\";\nHere is a more complex Perl program, that counts down the seconds up to a given threshold:\n\n#!/usr/bin/perl\nuse strict;\nuse warnings;\nuse IO::Handle;\n\nmy ( $remaining, $total );\n\n$remaining = $total = shift(@ARGV);\n\nSTDOUT->autoflush(1);\n\nwhile ( $remaining ) {\n    printf ( \"Remaining %s/%s \\r\", $remaining--, $total );\n    sleep 1;\n}\n\nprint \"\\n\";\nThe perl interpreter can also be used for one-off scripts on the command line. The following example (as invoked from an sh-compatible shell, such as Bash) translates the string \"Bob\" in all files ending with .txt in the current directory to \"Robert\":\n\n$ perl -i.bak -lp -e 's/Bob/Robert/g' *.txt\nCriticism[edit]\nPerl has been referred to as \"line noise\" by some programmers who claim its syntax makes it a write-only language. The earliest such mention was in the first edition of the book Learning Perl, a Perl 5 tutorial book written by Randal L. Schwartz,[111] in the first chapter of which he states: \"Yes, sometimes Perl looks like line noise to the uninitiated, but to the seasoned Perl programmer, it looks like checksummed line noise with a mission in life.\"[112] He also stated that the accusation that Perl is a write-only language could be avoided by coding with \"proper care\".[112] The Perl overview document perlintro states that the names of built-in \"magic\" scalar variables \"look like punctuation or line noise\".[113] The perlstyle document states that line noise in regular expressions could be mitigated using the /x modifier to add whitespace.[114]\n\nAccording to the Perl 6 FAQ, Perl 6 was designed to mitigate \"the usual suspects\" that elicit the \"line noise\" claim from Perl 5 critics, including the removal of \"the majority of the punctuation variables\" and the sanitization of the regex syntax.[115] The Perl 6 FAQ also states that what is sometimes referred to as Perl's line noise is \"the actual syntax of the language\" just as gerunds and prepositions are a part of the English language.[115] In a December 2012 blog posting, despite claiming that \"Rakudo Perl 6 has failed and will continue to fail unless it gets some adult supervision\", chromatic stated that the design of Perl 6 has a \"well-defined grammar\" as well as an \"improved type system, a unified object system with an intelligent metamodel, metaoperators, and a clearer system of context that provides for such niceties as pervasive laziness\".[116] He also stated that \"Perl 6 has a coherence and a consistency that Perl 5 lacks.\"[116]",
          "type": "compiled",
          "plid": 26
        },
        {
          "name": "PHP",
          "details": "PHP is a server-side scripting language designed primarily for web development but also used as a general-purpose programming language. Originally created by Rasmus Lerdorf in 1994,[5] the PHP reference implementation is now produced by The PHP Development Team.[6] PHP originally stood for Personal Home Page,[5] but it now stands for the recursive acronym PHP: Hypertext Preprocessor.[7]\n\nPHP code may be embedded into HTML code, or it can be used in combination with various web template systems, web content management systems and web frameworks. PHP code is usually processed by a PHP interpreter implemented as a module in the web server or as a Common Gateway Interface (CGI) executable. The web server combines the results of the interpreted and executed PHP code, which may be any type of data, including images, with the generated web page. PHP code may also be executed with a command-line interface (CLI) and can be used to implement standalone graphical applications.[8]\n\nThe standard PHP interpreter, powered by the Zend Engine, is free software released under the PHP License. PHP has been widely ported and can be deployed on most web servers on almost every operating system and platform, free of charge.[9]\n\nThe PHP language evolved without a written formal specification or standard until 2014, leaving the canonical PHP interpreter as a de facto standard. Since 2014 work has gone on to create a formal PHP specification.[10]\n\nDuring the 2010s there have been increased efforts towards standardisation and code sharing in PHP applications by projects such as PHP-FIG in the form of PSR-initiatives as well as Composer dependency manager and the Packagist repository.\n\nContents  [hide] \n1\tHistory\n1.1\tEarly history\n1.2\tPHP 3 and 4\n1.3\tPHP 5\n1.4\tPHP 6 and Unicode\n1.5\tPHP 7\n1.6\tRelease history\n2\tMascot\n3\tSyntax\n3.1\tData types\n3.2\tFunctions\n3.3\tObject-oriented programming\n4\tImplementations\n5\tLicensing\n6\tDevelopment and community\n7\tInstallation and configuration\n8\tUse\n9\tSecurity\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory[edit]\nEarly history[edit]\n\n\n\nRasmus Lerdorf, who wrote the original Common Gateway Interface (CGI) component, together with Andi Gutmans and Zeev Suraski, who rewrote the parser that formed PHP 3.\nPHP development began in 1995 when Rasmus Lerdorf wrote several Common Gateway Interface (CGI) programs in C,[11][12][13] which he used to maintain his personal homepage. He extended them to work with web forms and to communicate with databases, and called this implementation \"Personal Home Page/Forms Interpreter\" or PHP/FI.\n\nPHP/FI could help to build simple, dynamic web applications. To accelerate bug reporting and to improve the code, Lerdorf initially announced the release of PHP/FI as \"Personal Home Page Tools (PHP Tools) version 1.0\" on the Usenet discussion group comp.infosystems.www.authoring.cgi on June 8, 1995.[14][15] This release already had the basic functionality that PHP has as of 2013. This included Perl-like variables, form handling, and the ability to embed HTML. The syntax resembled that of Perl but was simpler, more limited and less consistent.[6]\n\nLerdorf did not intend the early PHP to become a new programming language, but it grew organically, with Lerdorf noting in retrospect: \"I don’t know how to stop it, there was never any intent to write a programming language […] I have absolutely no idea how to write a programming language, I just kept adding the next logical step on the way.\"[16] A development team began to form and, after months of work and beta testing, officially released PHP/FI 2 in November 1997.\n\nThe fact that PHP lacked an original overall design but instead developed organically has led to inconsistent naming of functions and inconsistent ordering of their parameters.[17] In some cases, the function names were chosen to match the lower-level libraries which PHP was \"wrapping\",[18] while in some very early versions of PHP the length of the function names was used internally as a hash function, so names were chosen to improve the distribution of hash values.[19]\n\nPHP 3 and 4[edit]\n\nThis is an example of custom php code on a computer screen.\nZeev Suraski and Andi Gutmans rewrote the parser in 1997 and formed the base of PHP 3, changing the language's name to the recursive acronym PHP: Hypertext Preprocessor.[6][20] Afterwards, public testing of PHP 3 began, and the official launch came in June 1998. Suraski and Gutmans then started a new rewrite of PHP's core, producing the Zend Engine in 1999.[21] They also founded Zend Technologies in Ramat Gan, Israel.[6]\n\nOn May 22, 2000, PHP 4, powered by the Zend Engine 1.0, was released.[6] As of August 2008 this branch reached version 4.4.9. PHP 4 is no longer under development nor will any security updates be released.[22][23]\n\nPHP 5[edit]\nOn July 13, 2004, PHP 5 was released, powered by the new Zend Engine II.[6] PHP 5 included new features such as improved support for object-oriented programming, the PHP Data Objects (PDO) extension (which defines a lightweight and consistent interface for accessing databases), and numerous performance enhancements.[24] In 2008 PHP 5 became the only stable version under development. Late static binding had been missing from PHP and was added in version 5.3.[25][26]\n\nMany high-profile open-source projects ceased to support PHP 4 in new code as of February 5, 2008, because of the GoPHP5 initiative,[27] provided by a consortium of PHP developers promoting the transition from PHP 4 to PHP 5.[28][29]\n\nOver time, PHP interpreters became available on most existing 32-bit and 64-bit operating systems, either by building them from the PHP source code, or by using pre-built binaries.[30] For the PHP versions 5.3 and 5.4, the only available Microsoft Windows binary distributions were 32-bit x86 builds,[31][32] requiring Windows 32-bit compatibility mode while using Internet Information Services (IIS) on a 64-bit Windows platform. PHP version 5.5 made the 64-bit x86-64 builds available for Microsoft Windows.[33]\n\nPHP 6 and Unicode[edit]\nPHP received mixed reviews due to lacking native Unicode support at the core language level.[34][35] In 2005, a project headed by Andrei Zmievski was initiated to bring native Unicode support throughout PHP, by embedding the International Components for Unicode (ICU) library, and representing text strings as UTF-16 internally.[36] Since this would cause major changes both to the internals of the language and to user code, it was planned to release this as version 6.0 of the language, along with other major features then in development.[37]\n\nHowever, a shortage of developers who understood the necessary changes, and performance problems arising from conversion to and from UTF-16, which is rarely used in a web context, led to delays in the project.[38] As a result, a PHP 5.3 release was created in 2009, with many non-Unicode features back-ported from PHP 6, notably namespaces. In March 2010, the project in its current form was officially abandoned, and a PHP 5.4 release was prepared containing most remaining non-Unicode features from PHP 6, such as traits and closure re-binding.[39] Initial hopes were that a new plan would be formed for Unicode integration, but as of 2014 none have been adopted.\n\nPHP 7[edit]\nDuring 2014 and 2015, a new major PHP version was developed, which was numbered PHP 7. The numbering of this version involved some debate.[40] While the PHP 6 Unicode experiment had never been released, several articles and book titles referenced the PHP 6 name, which might have caused confusion if a new release were to reuse the name.[41] After a vote, the name PHP 7 was chosen.[42]\n\nThe foundation of PHP 7 is a PHP branch that was originally dubbed PHP next generation (phpng). It was authored by Dmitry Stogov, Xinchen Hui and Nikita Popov,[43] and aimed to optimize PHP performance by refactoring the Zend Engine while retaining near-complete language compatibility.[44] As of 14 July 2014, WordPress-based benchmarks, which served as the main benchmark suite for the phpng project, showed an almost 100% increase in performance. Changes from phpng are also expected to make it easier to improve performance in the future, as more compact data structures and other changes are seen as better suited for a successful migration to a just-in-time (JIT) compiler.[45] Because of the significant changes, the reworked Zend Engine is called Zend Engine 3, succeeding Zend Engine 2 used in PHP 5.[46]\n\nBecause of major internal changes in phpng, it must receive a new major version number of PHP, rather than a minor PHP 5 release, according to PHP's release process.[47] Major versions of PHP are allowed to break backward-compatibility of code and therefore PHP 7 presented an opportunity for other improvements beyond phpng that require backward-compatibility breaks. In particular, it involved the following changes:\n\nMany fatal- or recoverable-level legacy PHP error mechanisms were replaced with modern object-oriented exceptions[48]\nThe syntax for variable dereferencing was reworked to be internally more consistent and complete, allowing the use of the operators ->, [], (), {}, and :: with arbitrary meaningful left-hand-side expressions[49]\nSupport for legacy PHP 4-style constructor methods was deprecated[50]\nThe behavior of the foreach statement was changed to be more predictable[51]\nConstructors for the few classes built-in to PHP which returned null upon failure were changed to throw an exception instead, for consistency[52]\nSeveral unmaintained or deprecated server application programming interfaces (SAPIs) and extensions were removed from the PHP core, most notably the legacy mysql extension[53]\nThe behavior of the list() operator was changed to remove support for strings[54]\nSupport for legacy ASP-style PHP code delimiters (<% and %>, <script language=php> and </script>) was removed[55]\nAn oversight allowing a switch statement to have multiple default clauses was fixed[56]\nSupport for hexadecimal number support in some implicit conversions from strings to number types was removed[57]\nThe left-shift and right-shift operators were changed to behave more consistently across platforms[58]\nConversions between integers and floating point numbers were tightened and implemented more consistently across platforms[58][59]\nPHP 7 also included new language features. Most notably, it introduces return type declarations for functions,[60] which complement the existing parameter type declarations, and support for the scalar types (integer, float, string, and boolean) in parameter and return type declarations.[61]\n\nRelease history[edit]\nKey\nColor\tMeaning\tDevelopment\nRed\tOld release\tNo development\nYellow\tStable release\tSecurity fixes\nGreen\tStable release\tBug and security fixes\nBlue\tFuture release\tNew features\nVersion\tRelease date\tSupported until[62]\tNotes\n1.0\t8 June 1995\t\tOfficially called \"Personal Home Page Tools (PHP Tools)\". This is the first use of the name \"PHP\".[6]\n2.0\t1 November 1997\t\tOfficially called \"PHP/FI 2.0\". This is the first release that could actually be characterised as PHP, being a standalone language with many features that have endured to the present day.\n3.0\t6 June 1998\t20 October 2000[62]\tDevelopment moves from one person to multiple developers. Zeev Suraski and Andi Gutmans rewrite the base for this version.[6]\n4.0\t22 May 2000\t23 June 2001[62]\tAdded more advanced two-stage parse/execute tag-parsing system called the Zend engine.[63]\n4.1\t10 December 2001\t12 March 2002[62]\tIntroduced \"superglobals\" ($_GET, $_POST, $_SESSION, etc.)[63]\n4.2\t22 April 2002\t6 September 2002[62]\tDisabled register_globals by default. Data received over the network is not inserted directly into the global namespace anymore, closing possible security holes in applications.[63]\n4.3\t27 December 2002\t31 March 2005[62]\tIntroduced the command-line interface (CLI), to supplement the CGI.[63][64]\n4.4\t11 July 2005\t7 August 2008[62]\tFixed a memory corruption bug, which required breaking binary compatibility with extensions compiled against PHP version 4.3.x.[65]\n5.0\t13 July 2004\t5 September 2005[62]\tZend Engine II with a new object model.[66]\n5.1\t24 November 2005\t24 August 2006[62]\tPerformance improvements with introduction of compiler variables in re-engineered PHP Engine.[66] Added PHP Data Objects (PDO) as a consistent interface for accessing databases.[67]\n5.2\t2 November 2006\t6 January 2011[62]\tEnabled the filter extension by default. Native JSON support.[66]\n5.3\t30 June 2009\t14 August 2014[62]\tNamespace support; late static bindings, jump label (limited goto), closures, PHP archives (phar), garbage collection for circular references, improved Windows support, sqlite3, mysqlnd as a replacement for libmysql as underlying library for the extensions that work with MySQL, fileinfo as a replacement for mime_magic for better MIME support, the Internationalization extension, and deprecation of ereg extension.\n5.4\t1 March 2012\t3 September 2015[62]\tTrait support, short array syntax support. Removed items: register_globals, safe_mode, allow_call_time_pass_reference, session_register(), session_unregister() and session_is_registered(). Built-in web server.[68] Several improvements to existing features, performance and reduced memory requirements.\n5.5\t20 June 2013\t21 July 2016[62]\tSupport for generators, finally blocks for exceptions handling, OpCache (based on Zend Optimizer+) bundled in official distribution.[69]\n5.6\t28 August 2014\t31 December 2018[70]\tConstant scalar expressions, variadic functions, argument unpacking, new exponentiation operator, extensions of the use statement for functions and constants, new phpdbg debugger as a SAPI module, and other smaller improvements.[71]\n6.x\tNot released\tN/A\tAbandoned version of PHP that planned to include native Unicode support.[72][73]\n7.0\t3 December 2015[2]\t3 December 2018[70]\tZend Engine 3 (performance improvements[45] and 64-bit integer support on Windows[74]), uniform variable syntax,[49] AST-based compilation process,[75] added Closure::call(),[76] bitwise shift consistency across platforms,[77] ?? (null coalesce) operator,[78] Unicode codepoint escape syntax,[79] return type declarations,[60] scalar type (integer, float, string and boolean) declarations,[61] <=> \"spaceship\" three-way comparison operator,[80] generator delegation,[81] anonymous classes,[82] simpler and more consistently available CSPRNG API,[83] replacement of many remaining internal PHP \"errors\" with the more modern exceptions,[48] and shorthand syntax for importing multiple items from a namespace.[84]\n7.1\tNovember 2016[85]\t3 years after release[47]\tvoid return type,[86] class constant visibility modifiers,[87] nullable types[88]\nBeginning on June 28, 2011, the PHP Group implemented a timeline for the release of new versions of PHP.[47] Under this system, at least one release should occur every month. Once per year, a minor release should occur which may include new features. Every minor release should at least be supported for two years with security and bug fixes, followed by at least one year of only security fixes, for a total of a three-year release process for every minor release. No new features, unless small and self-contained, are to be introduced into a minor release during the three-year release process..\n\nMascot[edit]\n\nThe elePHPant, PHP mascot.\nThe mascot of the PHP project is the elePHPant, a blue elephant with the PHP logo on its side, designed by Vincent Pontier[89] in 1998.[90] The elePHPant is sometimes differently colored when in plush toy form.\n\nSyntax[edit]\nMain article: PHP syntax and semantics\nThe following \"Hello, World!\" program is written in PHP code embedded in an HTML document:\n\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>PHP Test</title>\n    </head>\n    <body>\n        <?php echo '<p>Hello World</p>'; ?>\n    </body>\n</html>\nHowever, as no requirement exists for PHP code to be embedded in HTML, the simplest version of Hello, World! may be written like this, with the closing tag omitted as preferred in files containing pure PHP code[91]\n\n <?='Hello world';\nThe PHP interpreter only executes PHP code within its delimiters. Anything outside its delimiters is not processed by PHP, although non-PHP text is still subject to control structures described in PHP code. The most common delimiters are <?php to open and ?> to close PHP sections. The shortened form <? also exists. This short delimiter makes script files less portable, since support for them can be disabled in the local PHP configuration and it is therefore discouraged.[92][93] However, there is no recommendation against the use of the echo short tag <?=.[94] Prior to PHP 5.4.0, this short syntax for echo() only works with the short_open_tag configuration setting enabled, while for PHP 5.4.0 and later it is always available.[92][95][96] The purpose of all these delimiters is to separate PHP code from non-PHP content, such as JavaScript code or HTML markup.[97]\n\nThe first form of delimiters, <?php and ?>, in XHTML and other XML documents, creates correctly formed XML processing instructions.[98] This means that the resulting mixture of PHP code and other markup in the server-side file is itself well-formed XML.\n\nVariables are prefixed with a dollar symbol, and a type does not need to be specified in advance. PHP 5 introduced type hinting that allows functions to force their parameters to be objects of a specific class, arrays, interfaces or callback functions. However, before PHP 7.0, type hints could not be used with scalar types such as integer or string.[61]\n\nUnlike function and class names, variable names are case sensitive. Both double-quoted (\"\") and heredoc strings provide the ability to interpolate a variable's value into the string.[99] PHP treats newlines as whitespace in the manner of a free-form language, and statements are terminated by a semicolon.[100] PHP has three types of comment syntax: /* */ marks block and inline comments; // as well as # are used for one-line comments.[101] The echo statement is one of several facilities PHP provides to output text, e.g., to a web browser.\n\nIn terms of keywords and language syntax, PHP is similar to the C style syntax. if conditions, for and while loops, and function returns are similar in syntax to languages such as C, C++, C#, Java and Perl.\n\nThe following is an example of PHP for loop:\n\n<?php \nfor ($x = 0; $x <= 100; $x++) {\n    echo \"The number is: $x <br>\";\n} \n?>\nData types[edit]\nPHP stores integers in a platform-dependent range, either a 64-bit or 32-bit signed integer equivalent to the C-language long type. Unsigned integers are converted to signed values in certain situations; this behavior is different from that of other programming languages.[102] Integer variables can be assigned using decimal (positive and negative), octal, hexadecimal, and binary notations.\n\nFloating point numbers are also stored in a platform-specific range. They can be specified using floating point notation, or two forms of scientific notation.[103] PHP has a native Boolean type that is similar to the native Boolean types in Java and C++. Using the Boolean type conversion rules, non-zero values are interpreted as true and zero as false, as in Perl and C++.[103]\n\nThe null data type represents a variable that has no value; NULL is the only allowed value for this data type.[103]\n\nVariables of the \"resource\" type represent references to resources from external sources. These are typically created by functions from a particular extension, and can only be processed by functions from the same extension; examples include file, image, and database resources.[103]\n\nArrays can contain elements of any type that PHP can handle, including resources, objects, and other arrays. Order is preserved in lists of values and in hashes with both keys and values, and the two can be intermingled.[103] PHP also supports strings, which can be used with single quotes, double quotes, nowdoc or heredoc syntax.[104]\n\nThe Standard PHP Library (SPL) attempts to solve standard problems and implements efficient data access interfaces and classes.[105]\n\nFunctions[edit]\nPHP defines a large array of functions in the core language and many are also available in various extensions; these functions are well documented in the online PHP documentation.[106] However, the built-in library has a wide variety of naming conventions and associated inconsistencies, as described under history above.\n\nCustom functions may be defined by the developer, e.g.:\n\nfunction myAge($birthYear) {                                  // defines a function, this one is named \"myAge\"\n    $yearsOld = date('Y') - $birthYear;                       // calculates the age\n    return $yearsOld . ' year' . ($yearsOld != 1 ? 's' : ''); // returns the age in a descriptive form\n}\n\necho 'I am currently ' . myAge(1981) . ' old.';               // outputs the text concatenated\n                                                              // with the return value of myAge()\n// As the result of this syntax, myAge() is called.\nIn 2016, the output of the above sample program is 'I am currently 35 years old.'\n\nIn lieu of function pointers, functions in PHP can be referenced by a string containing their name. In this manner, normal PHP functions can be used, for example, as callbacks or within function tables.[107] User-defined functions may be created at any time without being prototyped.[106][107] Functions may be defined inside code blocks, permitting a run-time decision as to whether or not a function should be defined. There is a function_exists function that determines whether a function with a given name has already been defined. Function calls must use parentheses, with the exception of zero-argument class constructor functions called with the PHP operator new, in which case parentheses are optional.\n\nUntil PHP 5.3, support for anonymous functions and closures did not exist in PHP. While create_function() exists since PHP 4.0.1, it is merely a thin wrapper around eval() that allows normal PHP functions to be created during program execution.[108] PHP 5.3 added syntax to define an anonymous function or \"closure\"[109] which can capture variables from the surrounding scope:\n\nfunction getAdder($x) {\n    return function($y) use ($x) {\n        return $x + $y;\n    };\n}\n\n$adder = getAdder(8);\necho $adder(2); // prints \"10\"\nIn the example above, getAdder() function creates a closure using passed argument $x (the keyword use imports a variable from the lexical context), which takes an additional argument $y, and returns the created closure to the caller. Such a function is a first-class object, meaning that it can be stored in a variable, passed as a parameter to other functions, etc.[110]\n\nUnusually for a dynamically typed language, PHP supports type declarations on function parameters, which are enforced at runtime. This has been supported for classes and interfaces since PHP 5.0, for arrays since PHP 5.1, for \"callables\" since PHP 5.4, and scalar (integer, float, string and boolean) types since PHP 7.0.[61] PHP 7.0 also has type declarations for function return types, expressed by placing the type name after the list of parameters, preceded by a colon.[60] For example, the getAdder function from the earlier example could be annotated with types like so in PHP 7:\n\nfunction getAdder(int $x): \\Closure {\n    return function(int $y) use ($x) : int {\n        return $x + $y;\n    };\n}\n\n$adder = getAdder(8);\necho $adder(2);        // prints \"10\"\necho $adder(null);     // throws an exception because an incorrect type was passed\n$adder = getAdder([]); // would also throw an exception\nBy default, scalar type declarations follow weak typing principles. So, for example, if a parameter's type is int, PHP would allow not only integers, but also convertible numeric strings, floats or booleans to be passed to that function, and would convert them.[61] However, PHP 7 has a \"strict typing\" mode which, when used, disallows such conversions for function calls and returns within a file.[61]\n\nObject-oriented programming[edit]\nBasic object-oriented programming functionality was added in PHP 3 and improved in PHP 4.[6] This allowed for PHP to gain further abstraction, making creative tasks easier for programmers using the language. Object handling was completely rewritten for PHP 5, expanding the feature set and enhancing performance.[111] In previous versions of PHP, objects were handled like value types.[111] The drawback of this method was that code had to make heavy use of PHP's \"reference\" variables if it wanted to modify an object it was passed rather than creating a copy of it. In the new approach, objects are referenced by handle, and not by value.\n\nPHP 5 introduced private and protected member variables and methods, along with abstract classes, final classes, abstract methods, and final methods. It also introduced a standard way of declaring constructors and destructors, similar to that of other object-oriented languages such as C++, and a standard exception handling model. Furthermore, PHP 5 added interfaces and allowed for multiple interfaces to be implemented. There are special interfaces that allow objects to interact with the runtime system. Objects implementing ArrayAccess can be used with array syntax and objects implementing Iterator or IteratorAggregate can be used with the foreach language construct. There is no virtual table feature in the engine, so static variables are bound with a name instead of a reference at compile time.[112]\n\nIf the developer creates a copy of an object using the reserved word clone, the Zend engine will check whether a __clone() method has been defined. If not, it will call a default __clone() which will copy the object's properties. If a __clone() method is defined, then it will be responsible for setting the necessary properties in the created object. For convenience, the engine will supply a function that imports the properties of the source object, so the programmer can start with a by-value replica of the source object and only override properties that need to be changed.[113]\n\nThe following is a basic example of object-oriented programming in PHP:\n\nclass Person\n{\n    public $firstName;\n    public $lastName;\n\n    public function __construct($firstName, $lastName = '') { // optional second argument\n        $this->firstName = $firstName;\n        $this->lastName  = $lastName;\n    }\n\n    public function greet() {\n        return 'Hello, my name is ' . $this->firstName .\n               (($this->lastName != '') ? (' ' . $this->lastName) : '') . '.';\n    }\n\n    public static function staticGreet($firstName, $lastName) {\n        return 'Hello, my name is ' . $firstName . ' ' . $lastName . '.';\n    }\n}\n\n$he    = new Person('John', 'Smith');\n$she   = new Person('Sally', 'Davis');\n$other = new Person('iAmine');\n\necho $he->greet(); // prints \"Hello, my name is John Smith.\"\necho '<br />';\n\necho $she->greet(); // prints \"Hello, my name is Sally Davis.\"\necho '<br />';\n\necho $other->greet(); // prints \"Hello, my name is iAmine.\"\necho '<br />';\n\necho Person::staticGreet('Jane', 'Doe'); // prints \"Hello, my name is Jane Doe.\"\nThe visibility of PHP properties and methods is defined using the keywords public, private, and protected. The default is public, if only var is used; var is a synonym for public. Items declared public can be accessed everywhere. protected limits access to inherited classes (and to the class that defines the item). private limits visibility only to the class that defines the item.[114] Objects of the same type have access to each other's private and protected members even though they are not the same instance. PHP's member visibility features have sometimes been described as \"highly useful.\"[115] However, they have also sometimes been described as \"at best irrelevant and at worst positively harmful.\"[116]\n\nImplementations[edit]\nThe original, only complete and most widely used PHP implementation is powered by the Zend Engine and known simply as PHP. To disambiguate it from other implementations, it is sometimes unofficially referred to as \"Zend PHP\". The Zend Engine compiles PHP source code on-the-fly into an internal format that it can execute, thus it works as an interpreter.[117][118] It is also the \"reference implementation\" of PHP, as PHP has no formal specification, and so the semantics of Zend PHP define the semantics of PHP itself. Due to the complex and nuanced semantics of PHP, defined by how Zend works, it is difficult for competing implementations to offer complete compatibility.\n\nPHP's single-request-per-script-execution model, and the fact the Zend Engine is an interpreter, leads to inefficiency; as a result, various products have been developed to help improve PHP performance. In order to speed up execution time and not have to compile the PHP source code every time the web page is accessed, PHP scripts can also be deployed in the PHP engine's internal format by using an opcode cache, which works by caching the compiled form of a PHP script (opcodes) in shared memory to avoid the overhead of parsing and compiling the code every time the script runs. An opcode cache, Zend Opcache, is built into PHP since version 5.5.[119] Another example of a widely used opcode cache is the Alternative PHP Cache (APC), which is available as a PECL extension.[120]\n\nWhile Zend PHP is still the most popular implementation, several other implementations have been developed. Some of these are compilers or support JIT compilation, and hence offer performance benefits over Zend PHP at the expense of lacking full PHP compatibility. Alternative implementations include the following:\n\nHipHop Virtual Machine (HHVM) – developed at Facebook and available as open source, it converts PHP code into a high-level bytecode (commonly known as an intermediate language), which is then translated into x86-64 machine code dynamically at runtime by a just-in-time (JIT) compiler, resulting in up to 6× performance improvements.[121]\nParrot – a virtual machine designed to run dynamic languages efficiently; Pipp transforms the PHP source code into the Parrot intermediate representation, which is then translated into the Parrot's bytecode and executed by the virtual machine.\nPhalanger – compiles PHP into Common Intermediate Language (CIL) bytecode\nHipHop – developed at Facebook and available as open source, it transforms the PHP scripts into C++ code and then compiles the resulting code, reducing the server load up to 50%. In early 2013, Facebook deprecated it in favor of HHVM due to multiple reasons, including deployment difficulties and lack of support for the whole PHP language, including the create_function() and eval() constructs.[122]\nLicensing[edit]\nPHP is free software released under the PHP License, which stipulates that:[123]\n\nProducts derived from this software may not be called \"PHP\", nor may \"PHP\" appear in their name, without prior written permission from group@php.net. You may indicate that your software works in conjunction with PHP by saying \"Foo for PHP\" instead of calling it \"PHP Foo\" or \"phpfoo\".\n\nThis restriction on use of \"PHP\" makes the PHP License incompatible with the General Public License (GPL), while the Zend License is incompatible due to an advertising clause similar to that of the original BSD license.[124]\n\nDevelopment and community[edit]\nPHP includes various free and open-source libraries in its source distribution, or uses them in resulting PHP binary builds. PHP is fundamentally an Internet-aware system with built-in modules for accessing File Transfer Protocol (FTP) servers and many database servers, including PostgreSQL, MySQL, Microsoft SQL Server and SQLite (which is an embedded database), LDAP servers, and others. Numerous functions familiar to C programmers, such as those in the stdio family, are available in standard PHP builds.[125]\n\nPHP allows developers to write extensions in C to add functionality to the PHP language. PHP extensions can be compiled statically into PHP or loaded dynamically at runtime. Numerous extensions have been written to add support for the Windows API, process management on Unix-like operating systems, multibyte strings (Unicode), cURL, and several popular compression formats. Other PHP features made available through extensions include integration with IRC, dynamic generation of images and Adobe Flash content, PHP Data Objects (PDO) as an abstraction layer used for accessing databases,[126][127][128][129][130][131][132] and even speech synthesis. Some of the language's core functions, such as those dealing with strings and arrays, are also implemented as extensions.[133] The PHP Extension Community Library (PECL) project is a repository for extensions to the PHP language.[134]\n\nSome other projects, such as Zephir, provide the ability for PHP extensions to be created in a high-level language and compiled into native PHP extensions. Such an approach, instead of writing PHP extensions directly in C, simplifies the development of extensions and reduces the time required for programming and testing.[135]\n\nThe PHP Group consists of ten people (as of 2015): Thies C. Arntzen, Stig Bakken, Shane Caraveo, Andi Gutmans, Rasmus Lerdorf, Sam Ruby, Sascha Schumann, Zeev Suraski, Jim Winstead, Andrei Zmievski.[136]\n\nZend Technologies provides a PHP Certification based on PHP 5.5[137] exam for programmers to become certified PHP developers.\n\nInstallation and configuration[edit]\nThere are two primary ways for adding support for PHP to a web server – as a native web server module, or as a CGI executable. PHP has a direct module interface called Server Application Programming Interface (SAPI), which is supported by many web servers including Apache HTTP Server, Microsoft IIS, Netscape (now defunct) and iPlanet. Some other web servers, such as OmniHTTPd, support the Internet Server Application Programming Interface (ISAPI), which is a Microsoft's web server module interface. If PHP has no module support for a web server, it can always be used as a Common Gateway Interface (CGI) or FastCGI processor; in that case, the web server is configured to use PHP's CGI executable to process all requests to PHP files.[138]\n\nPHP-FPM (FastCGI Process Manager) is an alternative FastCGI implementation for PHP, bundled with the official PHP distribution since version 5.3.3.[139] When compared to the older FastCGI implementation, it contains some additional features, mostly useful for heavily loaded web servers.[140]\n\nWhen using PHP for command-line scripting, a PHP command-line interface (CLI) executable is needed. PHP supports a CLI SAPI as of PHP 4.3.0.[141] The main focus of this SAPI is developing shell applications using PHP. There are quite a few differences between the CLI SAPI and other SAPIs, although they do share many of the same behaviors.[142]\n\nPHP has a direct module interface called SAPI for different web servers;[143] in case of PHP 5 and Apache 2.0 on Windows, it is provided in form of a DLL file called php5apache2.dll,[144] which is a module that, among other functions, provides an interface between PHP and the web server, implemented in a form that the server understands. This form is what is known as a SAPI.\n\nThere are different kinds of SAPIs for various web server extensions. For example, in addition to those listed above, other SAPIs for the PHP language include the Common Gateway Interface (CGI) and command-line interface (CLI).[143][145]\n\nPHP can also be used for writing desktop graphical user interface (GUI) applications, by using the PHP-GTK extension. PHP-GTK is not included in the official PHP distribution,[138] and as an extension it can be used only with PHP versions 5.1.0 and newer. The most common way of installing PHP-GTK is compiling it from the source code.[146]\n\nWhen PHP is installed and used in cloud environments, software development kits (SDKs) are provided for using cloud-specific features. For example:\n\nAmazon Web Services provides the AWS SDK for PHP[147]\nWindows Azure can be used with the Windows Azure SDK for PHP.[148]\nNumerous configuration options are supported, affecting both core PHP features and extensions.[149][150] Configuration file php.ini is searched for in different locations, depending on the way PHP is used.[151] The configuration file is split into various sections,[152] while some of the configuration options can be also set within the web server configuration.[153]\n\nUse[edit]\n\nA broad overview of the LAMP software bundle, displayed here together with Squid.\nPHP is a general-purpose scripting language that is especially suited to server-side web development, in which case PHP generally runs on a web server. Any PHP code in a requested file is executed by the PHP runtime, usually to create dynamic web page content or dynamic images used on websites or elsewhere.[154] It can also be used for command-line scripting and client-side graphical user interface (GUI) applications. PHP can be deployed on most web servers, many operating systems and platforms, and can be used with many relational database management systems (RDBMS). Most web hosting providers support PHP for use by their clients. It is available free of charge, and the PHP Group provides the complete source code for users to build, customize and extend for their own use.[9]\n\n\nDynamic web page: example of server-side scripting (PHP and MySQL).\nPHP acts primarily as a filter,[155] taking input from a file or stream containing text and/or PHP instructions and outputting another stream of data. Most commonly the output will be HTML, although it could be JSON, XML or binary data such as image or audio formats. Since PHP 4, the PHP parser compiles input to produce bytecode for processing by the Zend Engine, giving improved performance over its interpreter predecessor.[156]\n\nOriginally designed to create dynamic web pages, PHP now focuses mainly on server-side scripting,[157] and it is similar to other server-side scripting languages that provide dynamic content from a web server to a client, such as Microsoft's ASP.NET, Sun Microsystems' JavaServer Pages,[158] and mod_perl. PHP has also attracted the development of many software frameworks that provide building blocks and a design structure to promote rapid application development (RAD). Some of these include PRADO, CakePHP, Symfony, CodeIgniter, Laravel, Yii Framework, Phalcon and Zend Framework, offering features similar to other web frameworks.\n\nThe LAMP architecture has become popular in the web industry as a way of deploying web applications.[159] PHP is commonly used as the P in this bundle alongside Linux, Apache and MySQL, although the P may also refer to Python, Perl, or some mix of the three. Similar packages, WAMP and MAMP, are also available for Windows and OS X, with the first letter standing for the respective operating system. Although both PHP and Apache are provided as part of the Mac OS X base install, users of these packages seek a simpler installation mechanism that can be more easily kept up to date.\n\nAs of April 2007, over 20 million Internet domains had web services hosted on servers with PHP installed and mod_php was recorded as the most popular Apache HTTP Server module.[160] As of October 2010, PHP was used as the server-side programming language on 75% of all websites whose server-side programming language was known[161] (as of February 2014, the percentage had reached 82%[162]), and PHP was the most-used open source software within enterprises.[163] Web content management systems written in PHP include MediaWiki,[164] Joomla,[165] eZ Publish, eZ Platform, SilverStripe,[166] WordPress,[167] Drupal,[168] Moodle,[169] the user-facing portion of Facebook,[170] and Digg.[171]\n\nFor specific and more advanced usage scenarios, PHP offers a well defined and documented way for writing custom extensions in C or C++.[172][173][174][175][176][177][178] Besides extending the language itself in form of additional libraries, extensions are providing a way for improving execution speed where it is critical and there is room for improvements by using a true compiled language.[179][180] PHP also offers well defined ways for embedding itself into other software projects. That way PHP can be easily used as an internal scripting language for another project, also providing tight interfacing with the project's specific internal data structures.[181]\n\nPHP received mixed reviews due to lacking support for multithreading at the core language level,[182] though using threads is made possible by the \"pthreads\" PECL extension.[183][184]\n\nAs of January 2013, PHP was used in more than 240 million websites (39% of those sampled) and was installed on 2.1 million web servers.[185]\n\nSecurity[edit]\nIn 2013, 9% of all vulnerabilities listed by the National Vulnerability Database were linked to PHP;[186] historically, about 30% of all vulnerabilities listed since 1996 in this database are linked to PHP. Technical security flaws of the language itself or of its core libraries are not frequent; (these numbered 22 in 2009, which was about 1% of the total, although PHP applies to about 20% of programs listed.)[187] Recognizing that programmers make mistakes, some languages include taint checking to automatically detect the lack of input validation which induces many issues. Such a feature is being developed for PHP,[188] but its inclusion into a release has been rejected several times in the past.[189][190]\n\nThere are advanced protection patches, such as Suhosin and Hardening-Patch, that are especially designed for web hosting environments,[191] primarily due to these environments being seen as places where carelessly written code may run. However, many security features, such as function whitelists, have proven more powerful in application-specific environments. Due to PHP's extensive capabilities and code size, criticism of PHP within security communities can be deflected somewhat by the use of Suhosin. This may cease to be the case if PHP7 moves to using a JIT engine, thereby preventing exploitation mitigation technologies such as W^X from being effective.\n\nThere are certain language features and configuration parameters (primarily the default values for such runtime settings) that make PHP applications prone to security issues. Among these, magic_quotes_gpc and register_globals[192] configuration directives are the best known; the latter made any URL parameters become PHP variables, opening a path for serious security vulnerabilities by allowing an attacker to set the value of any uninitialized global variable and interfere with the execution of a PHP script. Support for \"magic quotes\" and \"register globals\" has been deprecated as of PHP 5.3.0, and removed as of PHP 5.4.0.[193]\n\nAnother example for the runtime settings vulnerability comes from failing to disable PHP execution (via engine configuration directive)[194] for the directory where uploaded images are stored; leaving the default settings can result in execution of malicious PHP code embedded within the uploaded images.[195][196][197] Also, leaving enabled the dynamic loading of PHP extensions (via enable_dl configuration directive)[198] in a shared web hosting environment can lead to security issues.[199][200]\n\nAlso, implied type conversions that result in incompatible values being treated as identical against the programmer's intent can lead to security issues. For example, the result of the comparison 0e1234 == 0 comparison is true because the first compared value is treated as scientific notation having the value (0×101234), i.e. zero. This feature resulted in authentication vulnerabilities in Simple Machines Forum,[201] Typo3[202] and phpBB[203] when MD5 password hashes were compared. Instead, either the function strcmp or the identity operator (===) should be used; 0e1234 === 0 results in false.[204]\n\nIn a 2013 analysis of over 170,000 website defacements, published by Zone-H, the most frequently (53%) used technique was exploitation of file inclusion vulnerability, mostly related to insecure usage of the PHP functions include, require, and allow_url_fopen.[205][206]\n\nSee also[edit]\nicon\tComputer programming portal\n\tFree software portal\nPEAR (PHP Extension and Application Repository)\nPHP Extension Community Library (PECL)\nPHP accelerator\nList of PHP accelerators\nList of AMP packages\nList of PHP editors\nPHP-GTK\nTemplate processor\nXAMPP (Free and open source cross-platform web server solution stack package)\nZend Server\nHack (programming language)\nComparison of programming languages\nComparison of web frameworks",
          "type": "interpreted",
          "plid": 27
        },
        {
          "name": "Python",
          "details": "Python is a widely used high-level, general-purpose, interpreted, dynamic programming language.[24][25] Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than possible in languages such as C++ or Java.[26][27] The language provides constructs intended to enable writing clear programs on both a small and large scale.[28]\n\nPython supports multiple programming paradigms, including object-oriented, imperative and functional programming or procedural styles. It features a dynamic type system and automatic memory management and has a large and comprehensive standard library.[29]\n\nPython interpreters are available for many operating systems, allowing Python code to run on a wide variety of systems. Using third-party tools, such as Py2exe or Pyinstaller,[30] Python code can be packaged into stand-alone executable programs for some of the most popular operating systems, so Python-based software can be distributed to, and used on, those environments with no need to install a Python interpreter.\n\nCPython, the reference implementation of Python, is free and open-source software and has a community-based development model, as do nearly all of its variant implementations. CPython is managed by the non-profit Python Software Foundation.\n\nContents  [hide] \n1\tHistory\n2\tFeatures and philosophy\n3\tSyntax and semantics\n3.1\tIndentation\n3.2\tStatements and control flow\n3.3\tExpressions\n3.4\tMethods\n3.5\tTyping\n3.6\tMathematics\n4\tLibraries\n5\tDevelopment environments\n6\tImplementations\n7\tDevelopment\n8\tNaming\n9\tUses\n10\tLanguages influenced by Python\n11\tSee also\n12\tReferences\n13\tFurther reading\n14\tExternal links\nHistory[edit]\n\nGuido van Rossum, the creator of Python\nMain article: History of Python\nPython was conceived in the late 1980s,[31] and its implementation began in December 1989[32] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC language (itself inspired by SETL)[33] capable of exception handling and interfacing with the operating system Amoeba.[8] Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, benevolent dictator for life (BDFL).\n\nAbout the origin of Python, Van Rossum wrote in 1996:[34]\n\nOver six years ago, in December 1989, I was looking for a \"hobby\" programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).\n\nPython 2.0 was released on 16 October 2000 and had many major new features, including a cycle-detecting garbage collector and support for Unicode. With this release the development process was changed and became more transparent and community-backed.[35]\n\nPython 3.0 (which early in its development was commonly referred to as Python 3000 or py3k), a major, backwards-incompatible release, was released on 3 December 2008[36] after a long period of testing. Many of its major features have been backported to the backwards-compatible Python 2.6.x[37] and 2.7.x version series.\n\nFeatures and philosophy[edit]\nPython is a multi-paradigm programming language: object-oriented programming and structured programming are fully supported, and many language features support functional programming and aspect-oriented programming (including by metaprogramming[38] and metaobjects (magic methods)).[39] Many other paradigms are supported via extensions, including design by contract[40][41] and logic programming.[42]\n\nPython uses dynamic typing and a mix of reference counting and a cycle-detecting garbage collector for memory management. An important feature of Python is dynamic name resolution (late binding), which binds method and variable names during program execution.\n\nThe design of Python offers some support for functional programming in the Lisp tradition. The language has map(), reduce() and filter() functions; list comprehensions, dictionaries, and sets; and generator expressions.[43] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[44]\n\nThe core philosophy of the language is summarized by the document The Zen of Python (PEP 20), which includes aphorisms such as:[45]\n\nBeautiful is better than ugly\nExplicit is better than implicit\nSimple is better than complex\nComplex is better than complicated\nReadability counts\nRather than requiring all desired functionality to be built into the language's core, Python was designed to be highly extensible. Python can also be embedded in existing applications that need a programmable interface. This design of a small core language with a large standard library and an easily extensible interpreter was intended by Van Rossum from the start because of his frustrations with ABC, which espoused the opposite mindset.[31]\n\nWhile offering choice in coding methodology, the Python philosophy rejects exuberant syntax, such as in Perl, in favor of a sparser, less-cluttered grammar. As Alex Martelli put it: \"To describe something as clever is not considered a compliment in the Python culture.\"[46] Python's philosophy rejects the Perl \"there is more than one way to do it\" approach to language design in favor of \"there should be one—and preferably only one—obvious way to do it\".[45]\n\nPython's developers strive to avoid premature optimization, and moreover, reject patches to non-critical parts of CPython that would offer a marginal increase in speed at the cost of clarity.[47] When speed is important, a Python programmer can move time-critical functions to extension modules written in languages such as C, or try using PyPy, a just-in-time compiler. Cython is also available, which translates a Python script into C and makes direct C-level API calls into the Python interpreter.\n\nAn important goal of Python's developers is making it fun to use. This is reflected in the origin of the name, which comes from Monty Python,[48] and in an occasionally playful approach to tutorials and reference materials, such as using examples that refer to spam and eggs instead of the standard foo and bar.[49][50]\n\nA common neologism in the Python community is pythonic, which can have a wide range of meanings related to program style. To say that code is pythonic is to say that it uses Python idioms well, that it is natural or shows fluency in the language, that it conforms with Python's minimalist philosophy and emphasis on readability. In contrast, code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.\n\nUsers and admirers of Python, especially those considered knowledgeable or experienced, are often referred to as Pythonists, Pythonistas, and Pythoneers.[51][52]\n\nSyntax and semantics[edit]\nMain article: Python syntax and semantics\nPython is intended to be a highly readable language. It is designed to have an uncluttered visual layout, often using English keywords where other languages use punctuation. Further, Python has fewer syntactic exceptions and special cases than C or Pascal.[53]\n\nIndentation[edit]\nMain article: Python syntax and semantics § Indentation\nPython uses whitespace indentation to delimit blocks - rather than curly braces or keywords. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[54] This feature is also sometimes termed the off-side rule.\n\nStatements and control flow[edit]\nPython's statements include (among others):\n\nThe assignment statement (token '=', the equals sign). This operates differently than in traditional imperative programming languages, and this fundamental mechanism (including the nature of Python's version of variables) illuminates many other features of the language. Assignment in C, e.g., x = 2, translates to \"typed variable name x receives a copy of numeric value 2\". The (right-hand) value is copied into an allocated storage location for which the (left-hand) variable name is the symbolic address. The memory allocated to the variable is large enough (potentially quite large) for the declared type. In the simplest case of Python assignment, using the same example, x = 2, translates to \"(generic) name x receives a reference to a separate, dynamically allocated object of numeric (int) type of value 2.\" This is termed binding the name to the object. Since the name's storage location doesn't contain the indicated value, it is improper to call it a variable. Names may be subsequently rebound at any time to objects of greatly varying types, including strings, procedures, complex objects with data and methods, etc. Successive assignments of a common value to multiple names, e.g., x = 2; y = 2; z = 2 result in allocating storage to (at most) three names and one numeric object, to which all three names are bound. Since a name is a generic reference holder it is unreasonable to associate a fixed data type with it. However at a given time a name will be bound to some object, which will have a type; thus there is dynamic typing.\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if).\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block.\nThe while statement, which executes a block of code as long as its condition is true.\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses; it also ensures that clean-up code in a finally block will always be run regardless of how the block exits.\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming.\nThe def statement, which defines a function or method.\nThe with statement (from Python 2.5), which encloses a code block within a context manager (for example, acquiring a lock before the block of code is run and releasing the lock afterwards, or opening a file and then closing it), allowing Resource Acquisition Is Initialization (RAII)-like behavior.\nThe pass statement, which serves as a NOP. It is syntactically needed to create an empty code block.\nThe assert statement, used during debugging to check for conditions that ought to apply.\nThe yield statement, which returns a value from a generator function. From Python 2.5, yield is also an operator. This form is used to implement coroutines.\nThe import statement, which is used to import modules whose functions or variables can be used in the current program.\nThe print statement was changed to the print() function in Python 3.[55]\nPython does not support tail call optimization or first-class continuations, and, according to Guido van Rossum, it never will.[56][57] However, better support for coroutine-like functionality is provided in 2.5, by extending Python's generators.[58] Before 2.5, generators were lazy iterators; information was passed unidirectionally out of the generator. As of Python 2.5, it is possible to pass information back into a generator function, and as of Python 3.3, the information can be passed through multiple stack levels.[59]\n\nExpressions[edit]\nSome Python expressions are similar to languages such as C and Java, while some are not:\n\nAddition, subtraction, and multiplication are the same, but the behavior of division differs (see Mathematics for details). Python also added the ** operator for exponentiation.\nAs of Python 3.5, it supports matrix multiplication directly with the @ operator, versus C and Java, which implement these as library functions. Earlier versions of Python also used methods instead of an infix operator.[60][61]\nIn Python, == compares by value, versus Java, which compares numerics by value[62] and objects by reference.[63] (Value comparisons in Java on objects can be performed with the equals() method.) Python's is operator may be used to compare object identities (comparison by reference). In Python, comparisons may be chained, for example a <= b <= c.\nPython uses the words and, or, not for its boolean operators rather than the symbolic &&, ||, ! used in Java and C.\nPython has a type of expression termed a list comprehension. Python 2.4 extended list comprehensions into a more general expression termed a generator expression.[43]\nAnonymous functions are implemented using lambda expressions; however, these are limited in that the body can only be one expression.\nConditional expressions in Python are written as x if c else y[64] (different in order of operands from the c ? x : y operator common to many other languages).\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples are written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided all elements of the tuple are immutable. The parentheses around the tuple are optional in some contexts. Tuples can appear on the left side of an equal sign; hence a statement like x, y = y, x can be used to swap two variables.\nPython has a \"string format\" operator %. This functions analogous to printf format strings in C, e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python 3 and 2.6+, this was supplemented by the format() method of the str class, e.g. \"spam={0} eggs={1}\".format(\"blah\", 2).\nPython has various kinds of string literals:\nStrings delimited by single or double quote marks. Unlike in Unix shells, Perl and Perl-influenced languages, single quote marks and double quote marks function identically. Both kinds of string use the backslash (\\) as an escape character and there is no implicit string interpolation such as \"$spam\".\nTriple-quoted strings, which begin and end with a series of three single or double quote marks. They may span multiple lines and function like here documents in shells, Perl and Ruby.\nRaw string varieties, denoted by prefixing the string literal with an r. No escape sequences are interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. Compare \"@-quoting\" in C#.\nPython has array index and array slicing expressions on lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter, called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted, for example a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.\nIn Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\n\nList comprehensions vs. for-loops\nConditional expressions vs. if blocks\nThe eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements.\nStatements cannot be a part of an expression, so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case of this is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is syntactically valid (but probably unintended) C code but if c = 1: ... causes a syntax error in Python.\n\nMethods[edit]\nMethods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, or Ruby).[65]\n\nTyping[edit]\nPython uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that the given object is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.\n\nPython allows programmers to define their own types using classes, which are most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.\n\nBefore version 3.0, Python had two kinds of classes: old-style and new-style.[66] Old-style classes were eliminated in Python 3.0, making all classes new-style. In versions between 2.2 and 3.0, both kinds of classes could be used. The syntax of both styles is the same, the difference being whether the class object is inherited from, directly or indirectly (all new-style classes inherit from object and are instances of type).\n\nSummary of Python 3's built-in types\nType\tMutable\tDescription\tSyntax example\nstr\tImmutable\tA character string: sequence of Unicode codepoints\t'Wikipedia'\n\"Wikipedia\"\n\"\"\"Spanning\nmultiple\nlines\"\"\"\nbytearray\tMutable\tSequence of bytes\tbytearray(b'Some ASCII')\nbytearray(b\"Some ASCII\")\nbytearray([119, 105, 107, 105])\nbytes\tImmutable\tSequence of bytes\tb'Some ASCII'\nb\"Some ASCII\"\nbytes([119, 105, 107, 105])\nlist\tMutable\tList, can contain mixed types\t[4.0, 'string', True]\ntuple\tImmutable\tCan contain mixed types\t(4.0, 'string', True)\nset\tMutable\tUnordered set, contains no duplicates; can contain mixed types if hashable\t{4.0, 'string', True}\nfrozenset\tImmutable\tUnordered set, contains no duplicates; can contain mixed types if hashable\tfrozenset([4.0, 'string', True])\ndict\tMutable\tAssociative array (or dictionary) of key and value pairs; can contain mixed types (keys and values), keys must be a hashable type\t{'key1': 1.0, 3: False}\nint\tImmutable\tInteger of unlimited magnitude[67]\t42\nfloat\tImmutable\tFloating point number, system-defined precision\t3.1415927\ncomplex\tImmutable\tComplex number with real and imaginary parts\t3+2.7j\nbool\tImmutable\tBoolean value\tTrue\nFalse\nellipsis\t\tAn ellipsis placeholder to be used as an index in NumPy arrays\t...\nMathematics[edit]\nPython has the usual C arithmetic operators (+, -, *, /, %). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a new matrix multiply @ operator is included in version 3.5.[68]\n\nThe behavior of division has changed significantly over time:[69]\n\nPython 2.1 and earlier use the C division behavior. The / operator is integer division if both operands are integers, and floating-point division otherwise. Integer division rounds towards 0, e.g. 7 / 3 == 2 and -7 / 3 == -2.\nPython 2.2 changes integer division to round towards negative infinity, e.g. 7 / 3 == 2 and -7 / 3 == -3. The floor division // operator is introduced. So 7 // 3 == 2, -7 // 3 == -3, 7.5 // 3 == 2.0 and -7.5 // 3 == -3.0. Adding from __future__ import division causes a module to use Python 3.0 rules for division (see next).\nPython 3.0 changes / to be always floating-point division. In Python terms, the pre-3.0 / is classic division, the version-3.0 / is real division, and // is floor division.\nRounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a+b) // b == a // b + 1 is always true. It also means that the equation b * (a // b) + a % b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a % b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.[70]\n\nPython provides a round function for rounding a float to the nearest integer. For tie-breaking, versions before 3 use round-away-from-zero: round(0.5) is 1.0, round(-0.5) is −1.0.[71] Python 3 uses round to even: round(1.5) is 2, round(2.5) is 2.[72]\n\nPython allows boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[73][page needed]\n\nPython has extensive built-in support for arbitrary precision arithmetic. Integers are transparently switched from the machine-supported maximum fixed-precision (usually 32 or 64 bits), belonging to the python type int, to arbitrary precision, belonging to the python type long, where needed. The latter have an \"L\" suffix in their textual representation.[74] The Decimal type/class in module decimal (since version 2.4) provides decimal floating point numbers to arbitrary precision and several rounding modes.[75] The Fraction type in module fractions (since version 2.6) provides arbitrary precision for rational numbers.[76]\n\nDue to Python's extensive mathematics library, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.\n\nLibraries[edit]\nPython has a large standard library, commonly cited as one of Python's greatest strengths,[77] providing tools suited to many tasks. This is deliberate and has been described as a \"batteries included\"[29] Python philosophy. For Internet-facing applications, many standard formats and protocols (such as MIME and HTTP) are supported. Modules for creating graphical user interfaces, connecting to relational databases, pseudorandom number generators, arithmetic with arbitrary precision decimals,[78] manipulating regular expressions, and doing unit testing are also included.\n\nSome parts of the standard library are covered by specifications (for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[79]), but most modules are not. They are specified by their code, internal documentation, and test suite (if supplied). However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.\n\nThe standard library is not needed to run Python or embed it in an application. For example, Blender 2.49 omits most of the standard library.\n\nAs of November, 2016, the Python Package Index, the official repository containing third-party software for Python, contains over 92,000[80] packages offering a wide range of functionality, including:\n\ngraphical user interfaces, web frameworks, multimedia, databases, networking and communications\ntest frameworks, automation and web scraping, documentation tools, system administration\nscientific computing, text processing, image processing\nDevelopment environments[edit]\nSee also: Comparison of integrated development environments § Python\nMost Python implementations (including CPython) can function as a command line interpreter, for which the user enters statements sequentially and receives the results immediately (read–eval–print loop (REPL)). In short, Python acts as a command-line interface or shell.\n\nOther shells add abilities beyond those in the basic interpreter, including IDLE and IPython. While generally following the visual style of the Python shell, they implement features like auto-completion, session state retention, and syntax highlighting.\n\nIn addition to standard desktop integrated development environments (Python IDEs), there are also web browser-based IDEs, SageMath (intended for developing science and math-related Python programs), and a browser-based IDE and hosting environment, PythonAnywhere. Additionally, the Canopy IDE is also an option for creating programs written in Python.[81]\n\nImplementations[edit]\nSee also: List of Python software § Python implementations\nThe main Python implementation, named CPython, is written in C meeting the C89 standard.[82] It compiles Python programs into intermediate bytecode,[83] which is executed by the virtual machine.[84] CPython is distributed with a large standard library written in a mixture of C and Python. It is available in versions for many platforms, including Windows and most modern Unix-like systems. CPython was intended from almost its very conception to be cross-platform.[85]\n\nPyPy is a fast, compliant[86] interpreter of Python 2.7 and 3.2. Its just-in-time compiler brings a significant speed improvement over CPython.[87] A version taking advantage of multi-core processors using software transactional memory is being created.[88]\n\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the C memory stack, thus allowing massively concurrent programs. PyPy also has a stackless version.[89]\n\nMicroPython is a lean, fast Python 3 variant that is optimised to run on microcontrollers.\n\nOther just-in-time compilers have been developed in the past, but are now unsupported:\n\nGoogle began a project named Unladen Swallow in 2009 with the aims of speeding up the Python interpreter by 5 times, by using the LLVM, and of improving its multithreading ability to scale to thousands of cores.[90]\nPsyco is a just-in-time specialising compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialised for certain data types and is faster than standard Python code.\nIn 2005, Nokia released a Python interpreter for the Series 60 mobile phones named PyS60. It includes many of the modules from the CPython implementations and some added modules to integrate with the Symbian operating system. This project has been kept up to date to run on all variants of the S60 platform and there are several third party modules available. The Nokia N900 also supports Python with GTK widget libraries, with the feature that programs can be both written and run on the target device.[91]\n\nThere are several compilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\n\nJython compiles into Java byte code, which can then be executed by every Java virtual machine implementation. This also enables the use of Java class library functions from the Python program.\nIronPython follows a similar approach in order to run Python programs on the .NET Common Language Runtime.\nThe RPython language can be compiled to C, Java bytecode, or Common Intermediate Language, and is used to build the PyPy interpreter of Python.\nPyjamas compiles Python to JavaScript.\nShed Skin compiles Python to C++.\nCython and Pyrex compile to C.\nA performance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13.[92]\n\nDevelopment[edit]\nPython's development is conducted largely through the Python Enhancement Proposal (PEP) process. The PEP process is the primary mechanism for proposing major new features, for collecting community input on an issue, and for documenting the design decisions that have gone into Python.[93] Outstanding PEPs are reviewed and commented upon by the Python community and by Van Rossum, the Python project's benevolent dictator for life.[93]\n\nEnhancement of the language goes along with development of the CPython reference implementation. The mailing list python-dev is the primary forum for discussion about the language's development; specific issues are discussed in the Roundup bug tracker maintained at python.org.[94] Development takes place on a self-hosted source code repository running Mercurial.[95]\n\nCPython's public releases come in three types, distinguished by which part of the version number is incremented:\n\nBackwards-incompatible versions, where code is expected to break and must be manually ported. The first part of the version number is incremented. These releases happen infrequently—for example, version 3.0 was released 8 years after 2.0.\nMajor or \"feature\" releases, which are largely compatible but introduce new features. The second part of the version number is incremented. These releases are scheduled to occur roughly every 18 months, and each major version is supported by bugfixes for several years after its release.[96]\nBugfix releases, which introduce no new features but fix bugs. The third and final part of the version number is incremented. These releases are made whenever a sufficient number of bugs have been fixed upstream since the last release, or roughly every 3 months. Security vulnerabilities are also patched in bugfix releases.[97]\nMany alpha, beta, and release-candidates are also released as previews, and for testing before final releases. Although there is a rough schedule for each release, this is often pushed back if the code is not ready. The development team monitors the state of the code by running the large unit test suite during development, and using the BuildBot continuous integration system.[98]\n\nThe community of Python developers has also contributed over 86,000[99] software modules (as of August 20, 2016) to the Python Package Index (PyPI), the official repository of third-party libraries for Python.\n\nThe major academic conference on Python is named PyCon. There are special mentoring programmes like the Pyladies.\n\nNaming[edit]\nPython's name is derived from the television series Monty Python's Flying Circus,[100] and it is common to use Monty Python references in example code.[101] For example, the metasyntactic variables often used in Python literature are spam and eggs, instead of the traditional foo and bar.[101][102] Also, the official Python documentation often contains various obscure Monty Python references.\n\nThe prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyS60, an implementation for the Symbian S60 operating system; PyQt and PyGTK, which bind Qt and GTK, respectively, to Python; and PyPy, a Python implementation originally written in Python.\n\nUses[edit]\nMain article: List of Python software\nSince 2003, Python has consistently ranked in the top ten most popular programming languages as measured by the TIOBE Programming Community Index. As of August 2016, it is the fifth most popular language.[103] It was ranked as Programming Language of the Year for the year 2007 and 2010.[24] It is the third most popular language whose grammatical syntax is not predominantly based on C, e.g. C++, Objective-C (note, C# and Java only have partial syntactic similarity to C, such as the use of curly braces, and are closer in similarity to each other than C).\n\nAn empirical study found scripting languages (such as Python) more productive than conventional languages (such as C and Java) for a programming problem involving string manipulation and search in a dictionary. Memory consumption was often \"better than Java and not much worse than C or C++\".[104]\n\nLarge organizations that make use of Python include Wikipedia, Google,[105] Yahoo!,[106] CERN,[107] NASA,[108] and some smaller ones like ILM,[109] and ITA.[110] The social news networking site, Reddit, is written entirely in Python.\n\nPython can serve as a scripting language for web applications, e.g., via mod_wsgi for the Apache web server.[111] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle and Zope support developers in the design and maintenance of complex applications. Pyjamas and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.\n\nLibraries like NumPy, SciPy and Matplotlib allow the effective use of Python in scientific computing,[112][113] with specialized libraries such as BioPython and Astropy providing domain-specific functionality. SageMath is a mathematical software with a \"notebook\" programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus. The Python language re-implemented in Java platform is used for numeric and statistical calculations with 2D/3D visualization by the DMelt project. [114] [115]\n\nPython has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modeler like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[116] Inkscape, Scribus and Paint Shop Pro,[117] and musical notation program or scorewriter capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[118] It has also been used in several video games,[119][120] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[121] Python is also used in algorithmic trading and quantitative finance.[122] Python can also be implemented in APIs of online brokerages that run on other languages by using wrappers.[123]\n\nPython has been used in artificial intelligence tasks.[124][125][126][127] As a scripting language with module architecture, simple syntax and rich text processing tools, Python is often used for natural language processing tasks.[128]\n\nMany operating systems include Python as a standard component; the language ships with most Linux distributions, AmigaOS 4, FreeBSD, NetBSD, OpenBSD and OS X, and can be used from the terminal. Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.\n\nPython has also seen extensive use in the information security industry, including in exploit development.[129][130]\n\nMost of the Sugar software for the One Laptop per Child XO, now developed at Sugar Labs, is written in Python.[131]\n\nThe Raspberry Pi single-board computer project has adopted Python as its main user-programming language.\n\nLibreOffice includes Python and intends to replace Java with Python. Python Scripting Provider is a core feature[132] since Version 4.0 from 7 February 2013.\n\nLanguages influenced by Python[edit]\nPython's design and philosophy have influenced several programming languages, including:\n\nBoo uses indentation, a similar syntax, and a similar object model. However, Boo uses static typing (and optional duck typing) and is closely integrated with the .NET Framework.[133]\nCobra uses indentation and a similar syntax. Cobra's \"Acknowledgements\" document lists Python first among languages that influenced it.[134] However, Cobra directly supports design-by-contract, unit tests, and optional static typing.[135]\nECMAScript borrowed iterators, generators, and list comprehensions from Python.[136]\nGo is described as incorporating the \"development speed of working in a dynamic language like Python\".[137]\nGroovy was motivated by the desire to bring the Python design philosophy to Java.[138]\nJulia was designed \"with true macros [.. and to be] as usable for general programming as Python [and] should be as fast as C\".[21] Calling to or from Julia is possible; to with PyCall.jl and a Python package pyjulia allows calling, in the other direction, from Python.\nOCaml has an optional syntax, named twt (The Whitespace Thing), inspired by Python and Haskell.[139]\nRuby's creator, Yukihiro Matsumoto, has said: \"I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That's why I decided to design my own language.\"[140]\nCoffeeScript is a programming language that cross-compiles to JavaScript; it has Python-inspired syntax.\nSwift is a programming language invented by Apple; it has some Python-inspired syntax.[141]\nPython's development practices have also been emulated by other languages. The practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python's case, a PEP) is also used in Tcl[142] and Erlang[143] because of Python's influence.\n\nPython has been awarded a TIOBE Programming Language of the Year award twice (in 2007 and 2010), which is given to the language with the greatest growth in popularity over the course of a year, as measured by the TIOBE index.[144]\n\nSee also[edit]\n",
          "type": "intepreted",
          "plid": 28
        },
        {
          "name": "Ruby",
          "details": "Ruby is a dynamic, reflective, object-oriented, general-purpose programming language. It was designed and developed in the mid-1990s by Yukihiro \"Matz\" Matsumoto in Japan.\n\nAccording to its creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, and Lisp.[11] It supports multiple programming paradigms, including functional, object-oriented, and imperative. It also has a dynamic type system and automatic memory management.\n\nContents  [hide] \n1\tHistory\n1.1\tEarly concept\n1.2\tThe name \"Ruby\"\n1.3\tFirst publication\n1.4\tEarly releases\n1.5\tRuby 1.8\n1.6\tRuby 1.9\n1.7\tRuby 2.0\n1.8\tRuby 2.1\n1.9\tRuby 2.2\n1.10\tRuby 2.3\n2\tTable of versions\n3\tPhilosophy\n4\tFeatures\n5\tSemantics\n6\tSyntax\n7\tDifferences from other languages\n8\tInteraction\n9\tExamples\n9.1\tStrings\n9.2\tCollections\n9.3\tControl structures\n9.4\tBlocks and iterators\n9.5\tClasses\n9.5.1\tOpen classes\n9.6\tExceptions\n9.7\tMetaprogramming\n9.8\tMore examples\n10\tImplementations\n10.1\tMatz's Ruby Interpreter\n10.2\tAlternate implementations\n10.3\tPlatform support\n11\tRepositories and libraries\n12\tSee also\n13\tReferences\n14\tFurther reading\n15\tExternal links\nHistory[edit]\nEarly concept[edit]\nRuby was conceived on February 24, 1993. In a 1999 post to the ruby-talk mailing list, Ruby author Yukihiro Matsumoto describes some of his early ideas about the language:[12]\n\nI was talking with my colleague about the possibility of an object-oriented scripting language. I knew Perl (Perl4, not Perl5), but I didn't like it really, because it had the smell of a toy language (it still has). The object-oriented language seemed very promising. I knew Python then. But I didn't like it, because I didn't think it was a true object-oriented language — OO features appeared to be add-on to the language. As a language maniac and OO fan for 15 years, I really wanted a genuine object-oriented, easy-to-use scripting language. I looked for but couldn't find one. So I decided to make it.\n\nMatsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.[13]\n\nThe name \"Ruby\"[edit]\nThe name \"Ruby\" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language.[14] Initially two names were proposed: \"Coral\" and \"Ruby\". Matsumoto chose the latter in a later e-mail to Ishitsuka.[15] Matsumoto later noted a factor in choosing the name \"Ruby\" – it was the birthstone of one of his colleagues.[16][17]\n\nFirst publication[edit]\nThe first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995.[18][19] Subsequently, three more versions of Ruby were released in two days.[14] The release coincided with the launch of the Japanese-language ruby-list mailing list, which was the first mailing list for the new language.\n\nAlready present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.[20]\n\nEarly releases[edit]\nFollowing the release of Ruby 0.95 in 1995, several stable versions of Ruby were released in the following years:\n\nRuby 1.0: December 25, 1996[14]\nRuby 1.2: December 1998\nRuby 1.4: August 1999\nRuby 1.6: September 2000\nIn 1997, the first article about Ruby was published on the Web. In the same year, Matsumoto was hired by netlab.jp to work on Ruby as a full-time developer.[14]\n\nIn 1998, the Ruby Application Archive was launched by Matsumoto, along with a simple English-language homepage for Ruby.[14]\n\nIn 1999, the first English language mailing list ruby-talk began, which signaled a growing interest in the language outside Japan.[21] In this same year, Matsumoto and Keiju Ishitsuka wrote the first book on Ruby, The Object-oriented Scripting Language Ruby (オブジェクト指向スクリプト言語 Ruby), which was published in Japan in October 1999. It would be followed in the early 2000s by around 20 books on Ruby published in Japanese.[14]\n\nBy 2000, Ruby was more popular than Python in Japan.[22] In September 2000, the first English language book Programming Ruby was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language ruby-talk mailing list was receiving more messages than the Japanese-language ruby-list, demonstrating Ruby's increasing popularity in the English-speaking world.\n\nRuby 1.8[edit]\nRuby 1.8 was initially released in August 2003, was stable for a long time, and was retired June 2013.[23] Although deprecated, there is still code based on it. Ruby 1.8 is only partially compatible with Ruby 1.9.\n\nRuby 1.8 has been the subject of several industry standards. The language specifications for Ruby were developed by the Open Standards Promotion Center of the Information-Technology Promotion Agency (a Japanese government agency) for submission to the Japanese Industrial Standards Committee (JISC) and then to the International Organization for Standardization (ISO). It was accepted as a Japanese Industrial Standard (JIS X 3017) in 2011[24] and an international standard (ISO/IEC 30170) in 2012.[25]\n\nAround 2005, interest in the Ruby language surged in tandem with Ruby on Rails, a web framework written in Ruby. Rails is frequently credited with increasing awareness of Ruby.[26]\n\nRuby 1.9[edit]\nRuby 1.9 was released in December 2007. Effective with Ruby 1.9.3, released October 31, 2011,[27] Ruby switched from being dual-licensed under the Ruby License and the GPL to being dual-licensed under the Ruby License and the two-clause BSD license.[28] Adoption of 1.9 was slowed by changes from 1.8 that required many popular third party gems to be rewritten.\n\nRuby 1.9 introduces many significant changes over the 1.8 series.[29] Examples:\n\nblock local variables (variables that are local to the block in which they are declared)\nan additional lambda syntax: f = ->(a,b) { puts a + b }\nper-string character encodings are supported\nnew socket API (IPv6 support)\nrequire_relative import security\nRuby 1.9 has been obsolete since February 23, 2015,[30] and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.\n\nRuby 2.0[edit]\nRuby 2.0 added several new features, including:\n\nmethod keyword arguments,\na new method, Module#prepend, for extending a class,\na new literal for creating an array of symbols,\nnew API for the lazy evaluation of Enumerables, and\na new convention of using #to_h to convert objects to Hashes.[31]\nRuby 2.0 is intended to be fully backward compatible with Ruby 1.9.3. As of the official 2.0.0 release on February 24, 2013, there were only five known (minor) incompatibilities.[32]\n\nIt has been obsolete since February 22, 2016 [1] and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.\n\nRuby 2.1[edit]\nRuby 2.1.0 was released on Christmas Day in 2013.[33] The release includes speed-ups, bugfixes, and library updates.\n\nStarting with 2.1.0, Ruby's versioning policy is more like semantic versioning.[34] Although similar, Ruby's versioning policy is not compatible with semantic versioning:\n\nRuby\tSemantic versioning\nMAJOR: Increased when incompatible change which can’t be released in MINOR. Reserved for special events.\tMAJOR: Increased when you make incompatible API changes.\nMINOR: increased every Christmas, may be API incompatible.\tMINOR: increased when you add functionality in a backwards-compatible manner.\nTEENY: security or bug fix which maintains API compatibility. May be increased more than 10 (such as 2.1.11), and will be released every 2–3 months.\tPATCH: increased when you make backwards-compatible bug fixes.\nPATCH: number of commits since last MINOR release (will be reset at 0 when releasing MINOR).\t-\nSemantic versioning also provides additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format, not available at Ruby.\n\nRuby 2.2[edit]\nRuby 2.2.0 was released on Christmas Day in 2014.[35] The release includes speed-ups, bugfixes, and library updates and removes some deprecated APIs. Most notably, Ruby 2.2.0 introduces changes to memory handling – an incremental garbage collector, support for garbage collection of symbols and the option to compile directly against jemalloc. It also contains experimental support for using vfork(2) with system() and spawn(), and added support for the Unicode 7.0 specification.\n\nFeatures that were made obsolete or removed include callcc, the DL library, Digest::HMAC, lib/rational.rb, lib/complex.rb, GServer, Logger::Application as well as various C API functions.[36]\n\nPowerPC64 performance\nSince version 2.2.1,[37] Ruby MRI performance on PowerPC64 was improved.[38][39][40]\nRuby 2.3[edit]\nRuby 2.3.0 was released on Christmas Day in 2015. A few notable changes include:\n\nThe ability to mark all strings literals as frozen by default with consequently large performance increase in string operations.[41]\nHash comparison to allow direct checking of key/value pairs instead of just keys.\nA new safe navigation operator &. that can ease nil handling (e.g. instead of if obj && obj.foo && obj.foo.bar, we can use if obj&.foo&.bar).\nThe did_you_mean gem is now bundled by default and required on startup to automatically suggest similar name matches on a NameError or NoMethodError.\nHash#dig and Array#dig to easily extract deeply nested values (e.g. given profile = { social: { wikipedia: { name: 'Foo Baz' } } }, the value Foo Baz can now be retrieved by profile.dig(:social, :wikipedia, :name)).\n.grep_v(regexp) which will match all negative examples of a given regular expression in addition to other new features.\nThe 2.3 branch also includes many performance improvements, updates, and bugfixes including changes to Proc#call, Socket and IO use of exception keywords, Thread#name handling, default passive Net::FTP connections, and Rake being removed from stdlib.[42]\n\nTable of versions[edit]\nVersion\tLatest teeny version\tInitial release date\tEnd of support phase\tEnd of security maintenance phase\n1.8\t1.8.7-p375[43]\t2003-08-04[44]\t2012-06[45]\t2014-07-01[46]\n1.9\t1.9.3-p551[47]\t2007-12-25[48]\t2014-02-23[49]\t2015-02-23[50]\n2.0\t2.0.0-p648[51]\t2013-02-24[52]\t2015-02-24[51]\t2016-02-24[51]\n2.1\t2.1.10[53]\t2013-12-25[54]\t2016-03-30[55][56]\t2017-03-30[57]\n2.2\t2.2.5[58]\t2014-12-25[59]\tTBA\tTBA\n2.3\t2.3.1[60]\t2015-12-25[61]\tTBA\tTBA\n2.4\t\t2016-12-25\t\t\n3.0\t\tTBA[62]\t\t\nLegend:Old versionOlder version, still supportedLatest versionFuture release\nPhilosophy[edit]\n\nYukihiro Matsumoto, the creator of Ruby\nMatsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[63] At a Google Tech Talk in 2008 Matsumoto further stated, \"I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language.\"[64] He stresses that systems design needs to emphasize human, rather than computer, needs:[65]\n\nOften people, especially computer engineers, focus on the machines. They think, \"By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something.\" They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.\n\nRuby is said to follow the principle of least astonishment (POLA), meaning that the language should behave in such a way as to minimize confusion for experienced users. Matsumoto has said his primary design goal was to make a language that he himself enjoyed using, by minimizing programmer work and possible confusion. He has said that he had not applied the principle of least astonishment to the design of Ruby,[65] but nevertheless the phrase has come to be closely associated with the Ruby programming language. The phrase has itself been a source of surprise, as novice users may take it to mean that Ruby's behaviors try to closely match behaviors familiar from other languages. In a May 2005 discussion on the newsgroup comp.lang.ruby, Matsumoto attempted to distance Ruby from POLA, explaining that because any design choice will be surprising to someone, he uses a personal standard in evaluating surprise. If that personal standard remains consistent, there would be few surprises for those familiar with the standard.[66]\n\nMatsumoto defined it this way in an interview:[65]\n\nEveryone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least my surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.\n\nFeatures[edit]\nThoroughly object-oriented with inheritance, mixins and metaclasses[67]\nDynamic typing and duck typing\nEverything is an expression (even statements) and everything is executed imperatively (even declarations)\nSuccinct and flexible syntax[68] that minimizes syntactic noise and serves as a foundation for domain-specific languages[69]\nDynamic reflection and alteration of objects to facilitate metaprogramming[70]\nLexical closures, iterators and generators, with a unique block syntax[71]\nLiteral notation for arrays, hashes, regular expressions and symbols\nEmbedding code in strings (interpolation)\nDefault arguments\nFour levels of variable scope (global, class, instance, and local) denoted by sigils or the lack thereof\nGarbage collection\nFirst-class continuations\nStrict boolean coercion rules (everything is true except false and nil)\nException handling\nOperator overloading\nBuilt-in support for rational numbers, complex numbers and arbitrary-precision arithmetic\nCustom dispatch behavior (through method_missing and const_missing)\nNative threads and cooperative fibers (fibers are a 1.9/YARV feature)\nInitial support for Unicode and multiple character encodings (no ICU support)[72]\nNative plug-in API in C\nInteractive Ruby Shell (a REPL)\nCentralized package management through RubyGems\nImplemented on all major platforms\nLarge standard library, including modules for YAML, JSON, XML, CGI, OpenSSL, HTTP, FTP, RSS, curses, zlib, and Tk[73]\nSemantics[edit]\nRuby is object-oriented: every value is an object, including classes and instances of types that many other languages designate as primitives (such as integers, booleans, and \"null\"). Variables always hold references to objects. Every function is a method and methods are always called on an object. Methods defined at the top level scope become methods of the Object class. Since this class is an ancestor of every other class, such methods can be called on any object. They are also visible in all scopes, effectively serving as \"global\" procedures. Ruby supports inheritance with dynamic dispatch, mixins and singleton methods (belonging to, and defined for, a single instance rather than being defined on the class). Though Ruby does not support multiple inheritance, classes can import modules as mixins.\n\nRuby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflection and metaprogramming, as well as support for interpreter-based[74] threads. Ruby features dynamic typing, and supports parametric polymorphism.\n\nAccording to the Ruby FAQ, the syntax is similar to Perl and the semantics are similar to Smalltalk but it differs greatly from Python.[75]\n\nSyntax[edit]\nThe syntax of Ruby is broadly similar to that of Perl and Python. Class and method definitions are signaled by keywords, whereas code blocks can be both defined by keywords or braces. In contrast to Perl, variables are not obligatorily prefixed with a sigil. When used, the sigil changes the semantics of scope of the variable. For practical purposes there is no distinction between expressions and statements.[76] Line breaks are significant and taken as the end of a statement; a semicolon may be equivalently used. Unlike Python, indentation is not significant.\n\nOne of the differences of Ruby compared to Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (attr_writer, attr_reader, etc.). Unlike the \"getter\" and \"setter\" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of calling code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.\n\nPython's property descriptors are similar, but come with a tradeoff in the development process. If one begins in Python by using a publicly exposed instance variable, and later changes the implementation to use a private instance variable exposed through a property descriptor, code internal to the class may need to be adjusted to use the private variable rather than the public property. Ruby’s design forces all instance variables to be private, but also provides a simple way to declare set and get methods. This is in keeping with the idea that in Ruby, one never directly accesses the internal members of a class from outside the class; rather, one passes a message to the class and receives a response.\n\nSee the Examples section below for samples of code demonstrating Ruby syntax.\n\nDifferences from other languages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2016) (Learn how and when to remove this template message)\nSome features that differ notably from languages such as C or Perl:\n\nThe language syntax is sensitive to the capitalization of identifiers, in all cases treating capitalized variables as constants. Class and module names are constants and refer to objects derived from Class and Module.\nThe sigils $ and @ do not indicate variable data type as in Perl, but rather function as scope resolution operators.\nFloating point literals must have digits on both sides of the decimal point: neither .5 nor 2. are valid floating point literals, but 0.5 and 2.0 are.\n(In Ruby, integer literals are objects that can have methods apply to them, so requiring a digit after a decimal point helps to clarify whether 1.e5 should be parsed analogously to 1.to_f or as the exponential-format floating literal 1.0e5. The reason for requiring a digit before the decimal point is less clear; it might relate either to method invocation again, or perhaps to the .. and ... operators, for example in the fragment 0.1...3.)\nBoolean non-boolean datatypes are permitted in boolean contexts (unlike in e.g. Smalltalk and Java), but their mapping to boolean values differs markedly from some other languages: 0 and \"empty\" (e.g. empty list, string or associative array) all evaluate to true, thus changing the meaning of some common idioms in related or similar languages such as Lisp, Perl and Python.\nA consequence of this rule is that Ruby methods by convention — for example, regular-expression searches — return numbers, strings, lists, or other non-false values on success, but nil on failure.\nVersions prior to 1.9 use plain integers to represent single characters, much like C. This may cause surprises when slicing strings: \"abc\"[0] yields 97 (the ASCII code of the first character in the string); to obtain \"a\" use \"abc\"[0,1] (a substring of length 1) or \"abc\"[0].chr.\nThe notation statement until expression does not run the statement if the expression is already true. (The behavior is like Perl, but unlike other languages' equivalent statements, e.g. do { statement } while (!(expression)); in C/C++/...). This is because statement until expression is actually syntactic sugar over until expression; statement; end, the equivalent of which in C/C++ is while (!(expression)) { statement; }, just as statement if expression is equivalent to if (expression) { statement; }. However, the notation begin statement end until expression in Ruby will in fact run the statement once even if the expression is already true, acting similarly to the do-while of other languages. (Matsumoto has expressed a desire to remove the special behavior of begin statement end until expression,[77] but it still exists as of Ruby 2.0.)\nBecause constants are references to objects, changing what a constant refers to generates a warning, but modifying the object itself does not. For example, Greeting << \" world!\" if Greeting == \"Hello\" does not generate an error or warning. This is similar to final variables in Java or a const pointer to a non-const object in C++.\nRuby provides the functionality to freeze an object.\nThe usual conjunctive and disjunctive operators for conditional expressions have the same precedence, so and does not bind tighter than or in Ruby, a behaviour similar to languages such as APL, Ada, VHDL, Mathematica, zkl and others. However, Ruby also has C-like operators || and && that work as in C-like languages.\nA list of so-called gotchas may be found in Hal Fulton's book The Ruby Way, 2nd ed (ISBN 0-672-32884-4), Section 1.5. A similar list in the 1st edition pertained to an older version of Ruby (version 1.6), some problems of which have been fixed in the meantime. For example, retry now works with while, until, and for, as well as with iterators.\n\nInteraction[edit]\nSee also: Interactive Ruby Shell\nThe Ruby official distribution also includes irb, an interactive command-line interpreter that can be used to test code quickly. The following code fragment represents a sample session using irb:\n\n$ irb\nirb(main):001:0> puts 'Hello, World'\nHello, World\n => nil\nirb(main):002:0> 1+2\n => 3\nExamples[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2014) (Learn how and when to remove this template message)\nThe following examples can be run in a Ruby shell such as Interactive Ruby Shell, or saved in a file and run from the command line by typing ruby <filename>.\n\nClassic Hello world example:\n\nputs 'Hello World!'\nSome basic Ruby code:\n\n# Everything, including a literal, is an object, so this works:\n-199.abs                                                 # => 199\n'ice is nice'.length                                     # => 11\n'ruby is cool.'.index('u')                               # => 1\n\"Nice Day Isn't It?\".downcase.split('').uniq.sort.join   # => \" '?acdeinsty\"\nInput:\n\nprint 'Please type name >'\nname = gets.chomp\nputs \"Hello #{name}.\"\nConversions:\n\nputs 'Give me a number'\nnumber = gets.chomp\nputs number.to_i\noutput_number = number.to_i + 1\nputs output_number.to_s + ' is a bigger number.'\nStrings[edit]\nThere are a variety of ways to define strings in Ruby.\n\nThe following assignments are equivalent:\n\na = \"\\nThis is a double-quoted string\\n\"\na = %Q{\\nThis is a double-quoted string\\n}\na = %{\\nThis is a double-quoted string\\n}\na = %/\\nThis is a double-quoted string\\n/\na = <<-BLOCK\n\nThis is a double-quoted string\nBLOCK\nStrings support variable interpolation:\n\nvar = 3.14159\n\"pi is #{var}\"\n=> \"pi is 3.14159\"\nThe following assignments are equivalent and produce raw strings:\n\na = 'This is a single-quoted string'\na = %q{This is a single-quoted string}\nCollections[edit]\nConstructing and using an array:\n\na = [1, 'hi', 3.14, 1, 2, [4, 5]]\n\na[2]             # => 3.14\na.[](2)          # => 3.14\na.reverse        # => [[4, 5], 2, 1, 3.14, 'hi', 1]\na.flatten.uniq   # => [1, 'hi', 3.14, 2, 4, 5]\nConstructing and using an associative array (in Ruby, called a hash):\n\nhash = Hash.new # equivalent to hash = {}\nhash = { :water => 'wet', :fire => 'hot' } # makes the previous line redundant as we are now\n                                           # assigning hash to a new, separate hash object\nputs hash[:fire] # prints \"hot\"\n\nhash.each_pair do |key, value|   # or: hash.each do |key, value|\n  puts \"#{key} is #{value}\"\nend\n# returns {:water=>\"wet\", :fire=>\"hot\"} and prints:\n# water is wet\n# fire is hot\n\nhash.delete :water                            # deletes the pair :water => 'wet' and returns \"wet\"\nhash.delete_if {|key,value| value == 'hot'}   # deletes the pair :fire => 'hot' and returns {}\nControl structures[edit]\nIf statement:\n\n# Generate a random number and print whether it's even or odd.\nif rand(100) % 2 == 0\n  puts \"It's even\"\nelse\n  puts \"It's odd\"\nend\nBlocks and iterators[edit]\nThe two syntaxes for creating a code block:\n\n{ puts 'Hello, World!' } # note the braces\n# or:\ndo\n  puts 'Hello, World!'\nend\nA code block can be passed to a method as an optional block argument. Many built-in methods have such arguments:\n\nFile.open('file.txt', 'w') do |file| # 'w' denotes \"write mode\"\n  file.puts 'Wrote some text.'\nend                                  # file is automatically closed here\n\nFile.readlines('file.txt').each do |line|\n  puts line\nend\n# => Wrote some text.\nParameter-passing a block to be a closure:\n\n# In an object instance variable (denoted with '@'), remember a block.\ndef remember(&a_block)\n  @block = a_block\nend\n\n# Invoke the preceding method, giving it a block that takes a name.\nremember {|name| puts \"Hello, #{name}!\"}\n\n# Call the closure (note that this happens not to close over any free variables):\n@block.call('Jon')   # => \"Hello, Jon!\"\nCreating an anonymous function:\n\nproc {|arg| puts arg}\nProc.new {|arg| puts arg}\nlambda {|arg| puts arg}\n->(arg) {puts arg}         # introduced in Ruby 1.9\nReturning closures from a method:\n\ndef create_set_and_get(initial_value=0) # note the default value of 0\n  closure_value = initial_value\n  [ Proc.new {|x| closure_value = x}, Proc.new { closure_value } ]\nend\n\nsetter, getter = create_set_and_get  # returns two values\nsetter.call(21)\ngetter.call      # => 21\n\n# Parameter variables can also be used as a binding for the closure,\n# so the preceding can be rewritten as:\n\ndef create_set_and_get(closure_value=0)\n  [ proc {|x| closure_value = x } , proc { closure_value } ]\nend\nYielding the flow of program control to a block that was provided at calling time:\n\ndef use_hello\n  yield \"hello\"\nend\n\n# Invoke the preceding method, passing it a block.\nuse_hello {|string| puts string}  # => 'hello'\nIterating over enumerations and arrays using blocks:\n\narray = [1, 'hi', 3.14]\narray.each {|item| puts item }\n# prints:\n# 1\n# 'hi'\n# 3.14\n\narray.each_index {|index| puts \"#{index}: #{array[index]}\" }\n# prints:\n# 0: 1\n# 1: 'hi'\n# 2: 3.14\n\n# The following uses a (a..b) Range\n(3..6).each {|num| puts num }\n# prints:\n# 3\n# 4\n# 5\n# 6\n\n# The following uses a (a...b) Range\n(3...6).each {|num| puts num }\n# prints:\n# 3\n# 4\n# 5\nA method such as inject can accept both a parameter and a block. The inject method iterates over each member of a list, performing some function on it while retaining an aggregate. This is analogous to the foldl function in functional programming languages. For example:\n\n[1,3,5].inject(10) {|sum, element| sum + element}   # => 19\nOn the first pass, the block receives 10 (the argument to inject) as sum, and 1 (the first element of the array) as element. This returns 11, which then becomes sum on the next pass. It is added to 3 to get 14, which is then added to 5 on the third pass, to finally return 19.\n\nUsing an enumeration and a block to square the numbers 1 to 10 (using a range):\n\n(1..10).collect {|x| x*x}  # => [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nOr invoke a method on each item (map is a synonym for collect):\n\n(1..5).map(&:to_f)  # => [1.0, 2.0, 3.0, 4.0, 5.0]\nClasses[edit]\nThe following code defines a class named Person. In addition to initialize, the usual constructor to create new objects, it has two methods: one to override the <=> comparison operator (so Array#sort can sort by age) and the other to override the to_s method (so Kernel#puts can format its output). Here, attr_reader is an example of metaprogramming in Ruby: attr_accessor defines getter and setter methods of instance variables, but attr_reader only getter methods. The last evaluated statement in a method is its return value, allowing the omission of an explicit return statement.\n\nclass Person\n  attr_reader :name, :age\n  def initialize(name, age)\n    @name, @age = name, age\n  end\n  def <=>(person) # the comparison operator for sorting\n    @age <=> person.age\n  end\n  def to_s\n    \"#{@name} (#{@age})\"\n  end\nend\n\ngroup = [\n  Person.new(\"Bob\", 33),\n  Person.new(\"Chris\", 16),\n  Person.new(\"Ash\", 23)\n]\n\nputs group.sort.reverse\nThe preceding code prints three names in reverse age order:\n\nBob (33)\nAsh (23)\nChris (16)\nPerson is a constant and is a reference to a Class object.\n\nOpen classes[edit]\nIn Ruby, classes are never closed: methods can always be added to an existing class. This applies to all classes, including the standard, built-in classes. All that is needed to do is open up a class definition for an existing class, and the new contents specified will be added to the existing contents. A simple example of adding a new method to the standard library's Time class:\n\n# re-open Ruby's Time class\nclass Time\n  def yesterday\n    self - 86400\n  end\nend\n\ntoday = Time.now               # => 2013-09-03 16:09:37 +0300\nyesterday = today.yesterday    # => 2013-09-02 16:09:37 +0300\nAdding methods to previously defined classes is often called monkey-patching. If performed recklessly, the practice can lead to both behavior collisions with subsequent unexpected results and code scalability problems.\n\nExceptions[edit]\nAn exception is raised with a raise call:\n\nraise\nAn optional message can be added to the exception:\n\nraise \"This is a message\"\nExceptions can also be specified by the programmer:\n\nraise ArgumentError, \"Illegal arguments!\"\nAlternatively, an exception instance can be passed to the raise method:\n\nraise ArgumentError.new(\"Illegal arguments!\")\nThis last construct is useful when raising an instance of a custom exception class featuring a constructor that takes more than one argument:\n\nclass ParseError < Exception\n  def initialize input, line, pos\n    super \"Could not parse '#{input}' at line #{line}, position #{pos}\"\n  end\nend\n\nraise ParseError.new(\"Foo\", 3, 9)\nExceptions are handled by the rescue clause. Such a clause can catch exceptions that inherit from StandardError. Other flow control keywords that can be used when handling exceptions are else and ensure:\n\nbegin\n  # do something\nrescue\n  # handle exception\nelse\n  # do this if no exception was raised\nensure\n  # do this whether or not an exception was raised\nend\nIt is a common mistake to attempt to catch all exceptions with a simple rescue clause. To catch all exceptions one must write:\n\nbegin\n  # do something\nrescue Exception\n  # Exception handling code here.\n  # Don't write only \"rescue\"; that only catches StandardError, a subclass of Exception.\nend\nOr catch particular exceptions:\n\nbegin\n  # do something\nrescue RuntimeError\n  # handle only RuntimeError and its subclasses\nend\nIt is also possible to specify that the exception object be made available to the handler clause:\n\nbegin\n  # do something\nrescue RuntimeError => e\n  # handling, possibly involving e, such as \"puts e.to_s\"\nend\nAlternatively, the most recent exception is stored in the magic global $!.\n\nSeveral exceptions can also be caught:\n\nbegin\n  # do something\nrescue RuntimeError, Timeout::Error => e\n  # handling, possibly involving e\nend\nMetaprogramming[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2014) (Learn how and when to remove this template message)\nRuby code can programmatically modify, at runtime, aspects of its own structure that would be fixed in more rigid languages, such as class and method definitions. This sort of metaprogramming can be used to write more concise code and effectively extend the language.\n\nFor example, the following Ruby code generates new methods for the built-in String class, based on a list of colors. The methods wrap the contents of the string with an HTML tag styled with the respective color.\n\nCOLORS = { black:   \"000\",\n           red:     \"f00\",\n           green:   \"0f0\",\n           yellow:  \"ff0\",\n           blue:    \"00f\",\n           magenta: \"f0f\",\n           cyan:    \"0ff\",\n           white:   \"fff\" }\n\nclass String\n  COLORS.each do |color,code|\n    define_method \"in_#{color}\" do\n      \"<span style=\\\"color: ##{code}\\\">#{self}</span>\"\n    end\n  end\nend\nThe generated methods could then be used like this:\n\n\"Hello, World!\".in_blue\n => \"<span style=\\\"color: #00f\\\">Hello, World!</span>\"\nTo implement the equivalent in many other languages, the programmer would have to write each method (in_black, in_red, in_green, etc.) separately.\n\nSome other possible uses for Ruby metaprogramming include:\n\nintercepting and modifying method calls\nimplementing new inheritance models\ndynamically generating classes from parameters\nautomatic object serialization\ninteractive help and debugging\nMore examples[edit]\nMore sample Ruby code is available as algorithms in the following article:\n\nExponentiating by squaring\nImplementations[edit]\nSee also: Ruby MRI § Operating systems\nMatz's Ruby Interpreter[edit]\nThe official Ruby interpreter often referred to as the Matz's Ruby Interpreter or MRI. This implementation is written in C and uses its own Ruby-specific virtual machine.\n\nThe standardized and retired Ruby 1.8 implementation was written in C, as a single-pass interpreted language.[23]\n\nStarting with Ruby 1.9, and continuing with Ruby 2.x and above, the official Ruby interpreter has been YARV (\"Yet Another Ruby VM\"), and this implementation has superseded the slower virtual machine used in previous releases of MRI.\n\nAlternate implementations[edit]\nAs of 2010, there are a number of alternative implementations of Ruby, including JRuby, Rubinius, MagLev, IronRuby, MacRuby (and its iOS counterpart, RubyMotion), mruby, HotRuby, Topaz and Opal. Each takes a different approach, with IronRuby, JRuby, MacRuby and Rubinius providing just-in-time compilation and MacRuby and mruby also providing ahead-of-time compilation.\n\nRuby has two major alternate implementations:\n\nJRuby, a Java implementation that runs on the Java virtual machine. JRuby currently targets Ruby 2.2,\nRubinius, a C++ bytecode virtual machine that uses LLVM to compile to machine code at runtime. The bytecode compiler and most core classes are written in pure Ruby. Rubinius currently targets Ruby 2.1,\nOther Ruby implementations include:\n\nMagLev, a Smalltalk implementation that runs on GemTalk Systems' GemStone/S VM\nmruby, an implementation designed to be embedded into C code, in a similar vein to Lua. It is currently being developed by Yukihiro Matsumoto and others\nOpal, a web-based interpreter that compiles Ruby to JavaScript\nRGSS, or Ruby Game Scripting System, a proprietary implementation that is used by the RPG Maker series of software for game design and modification of the RPG Maker engine\nA transpiler (partial) from Ruby to Julia, julializer is available. It can be used for a large speedup over e.g. Ruby or JRuby implementations (may only be useful for numerical code).[78]\nOther now defunct Ruby implementations were:\n\nMacRuby, an OS X implementation on the Objective-C runtime\nIronRuby an implementation on the .NET Framework\nCardinal, an implementation for the Parrot virtual machine\nRuby Enterprise Edition, often shortened to ree, an implementation optimized to handle large-scale Ruby on Rails projects\nThe maturity of Ruby implementations tends to be measured by their ability to run the Ruby on Rails (Rails) framework, because it is complex to implement and uses many Ruby-specific features. The point when a particular implementation achieves this goal is called \"the Rails singularity\". The reference implementation (MRI), JRuby, and Rubinius[79] are all able to run Rails unmodified in a production environment.\n\nPlatform support[edit]\nMatsumoto originally did Ruby development on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.[80][81]\n\nBy 1999, Ruby was known to work across many different operating systems, including NEWS-OS, SunOS, AIX, SVR4, Solaris, NEC UP-UX, NeXTSTEP, BSD, Linux, Mac OS, DOS, Windows, and BeOS.[82]\n\nModern Ruby versions and implementations are available on many operating systems, such as Linux, BSD, Solaris, AIX, OS X, Windows, Windows Phone,[83] Windows CE, Symbian OS, BeOS, and IBM i.\n\nRepositories and libraries[edit]\nRubyGems is Ruby's package manager. A Ruby package is called a \"gem\" and can easily be installed via the command line. Most gems are libraries, though a few exist that are applications, such as IDEs.[84] There are over 124,000 Ruby gems hosted on RubyGems.org.\n\nMany new and existing Ruby libraries are hosted on GitHub, a service that offers version control repository hosting for Git.\n\nThe Ruby Application Archive, which hosted applications, documentation, and libraries for Ruby programming, was maintained until 2013, when its function was transferred to RubyGems.[85]\n\nSee also[edit]\n\tFree software portal\nicon\tComputer programming portal\nComparison of programming languages\nWhy's (poignant) Guide to Ruby — an online ruby textbook in graphic novel format\nMetasploit Project — the world's largest Ruby project, with over 700,000 lines of code\nXRuby",
          "type": "intepreted",
          "plid": 29
        },
        {
          "name": "Scala",
          "details": "Scala (/ˈskɑːlɑː/ skah-lah)[9] is a general-purpose programming language. Scala has full support for functional programming and a strong static type system. Designed to be concise,[10] many of Scala's design decisions were inspired by criticism of Java's shortcomings.[8]\n\nScala source code is intended to be compiled to Java bytecode, so that the resulting executable code runs on a Java virtual machine. Java libraries may be used directly in Scala code and vice versa (language interoperability).[11] Like Java, Scala is object-oriented, and uses a curly-brace syntax reminiscent of the C programming language. Unlike Java, Scala has many features of functional programming languages like Scheme, Standard ML and Haskell, including currying, type inference, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), and anonymous types. Other features of Scala not present in Java include operator overloading, optional parameters, named parameters, raw strings, and no checked exceptions.\n\nThe name Scala is a portmanteau of scalable and language, signifying that it is designed to grow with the demands of its users.[12]\n\nContents  [hide] \n1\tHistory\n2\tPlatforms and license\n3\tExamples\n3.1\t\"Hello World\" example\n3.2\tBasic example\n3.3\tExample with classes\n4\tFeatures (with reference to Java)\n4.1\tSyntactic flexibility\n4.2\tUnified type system\n4.3\tFor-expressions\n4.4\tFunctional tendencies\n4.4.1\tEverything is an expression\n4.4.2\tType inference\n4.4.3\tAnonymous functions\n4.4.4\tImmutability\n4.4.5\tLazy (non-strict) evaluation\n4.4.6\tTail recursion\n4.4.7\tCase classes and pattern matching\n4.4.8\tPartial functions\n4.5\tObject-oriented extensions\n4.6\tExpressive type system\n4.7\tType enrichment\n5\tConcurrency\n6\tCluster computing\n7\tTesting\n8\tVersions\n9\tComparison with other JVM languages\n10\tAdoption\n10.1\tLanguage rankings\n10.2\tCompanies\n11\tCriticism\n12\tSee also\n13\tReferences\n14\tFurther reading\n15\tExternal links\nHistory[edit]\nThe design of Scala started in 2001 at the École Polytechnique Fédérale de Lausanne (EPFL) by Martin Odersky. It followed on from work on Funnel, a programming language combining ideas from functional programming and Petri nets.[13] Odersky formerly worked on Generic Java, and javac, Sun's Java compiler.[13]\n\nAfter an internal release in late 2003, Scala was released publicly in early 2004 on the Java platform,[14] and on the .NET Framework in June 2004.[8][13][15] A second version (v2.0) followed in March 2006.[8] The .NET support was officially dropped in 2012.[16]\n\nAlthough Scala had extensive support for functional programming from its inception, Java remained a mostly object oriented language until the inclusion of lambda expressions with Java 8 in 2014.\n\nOn 17 January 2011 the Scala team won a five-year research grant of over €2.3 million from the European Research Council.[17] On 12 May 2011, Odersky and collaborators launched Typesafe Inc. (renamed Lightbend Inc., February 2016; 9 months ago), a company to provide commercial support, training, and services for Scala. Typesafe received a $3 million investment in 2011 from Greylock Partners.[18][19][20][21]\n\nPlatforms and license[edit]\nScala runs on the Java platform (Java virtual machine) and is compatible with existing Java programs.[14] As Android applications are typically written in Java and translated from Java bytecode into Dalvik bytecode (which may be further translated to native machine code during installation) when packaged, Scala's Java compatibility makes it well suited to Android development, more so when a functional approach is preferred.[22] Scala also can compile to JavaScript, making it possible to write Scala programs that can run in web browsers.[23]\n\nThe Scala software distribution, including compiler and libraries, is released under a BSD license.[24]\n\nExamples[edit]\n\"Hello World\" example[edit]\nThe Hello World program written in Scala has this form:\n\n object HelloWorld extends App {\n   println(\"Hello, World!\")\n }\nUnlike the stand-alone Hello World application for Java, there is no class declaration and nothing is declared to be static; a singleton object created with the object keyword is used instead.\n\nWith the program saved in a file named HelloWorld.scala, it can be compiled from the command line:\n\n$ scalac HelloWorld.scala\nTo run it:\n\n$ scala HelloWorld\n(You may need to use the \"-cp\" option to set the classpath as in Java).\n\nThis is analogous to the process for compiling and running Java code. Indeed, Scala's compiling and executing model is identical to that of Java, making it compatible with Java build tools such as Apache Ant.\n\nA shorter version of the \"Hello World\" Scala program is:\n\nprintln(\"Hello, World!\")\nScala includes interactive shell and scripting support.[25] Saved in a file named HelloWorld2.scala, this can be run as a script with no prior compiling using:\n\n$ scala HelloWorld2.scala\nCommands can also be entered directly into the Scala interpreter, using the option -e:\n\n$ scala -e 'println(\"Hello, World!\")'\nFinally, commands can be entered interactively in the REPL:\n\n$ scala\nWelcome to Scala version 2.10.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_51).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> println(\"Hello, World!\")\nHello, World!\n\nscala>\nBasic example[edit]\nThe following example shows the differences between Java and Scala syntax:\n\n// Java:\nint mathFunction(int num) {\n    int numSquare = num*num;\n    return (int) (Math.cbrt(numSquare) +\n      Math.log(numSquare));\n}\n// Scala: Direct conversion from Java\n\n// no import needed; scala.math\n// already imported as `math`\ndef mathFunction(num: Int): Int = {\n  var numSquare: Int = num*num\n  return (math.cbrt(numSquare) + math.log(numSquare)).\n    asInstanceOf[Int]\n}\n// Scala: More idiomatic\n// Uses type inference, omits `return` statement,\n// uses `toInt` method, declares numSquare immutable\n\nimport math._\ndef intRoot23(num: Int) = {\n  val numSquare = num*num\n  (cbrt(numSquare) + log(numSquare)).toInt\n}\nSome syntactic differences in this code are:\n\nScala does not require semicolons to end statements.\nValue types are capitalized: Int, Double, Boolean instead of int, double, boolean.\nParameter and return types follow, as in Pascal, rather than precede as in C.\nMethods must be preceded by def.\nLocal or class variables must be preceded by val (indicates an immutable variable) or var (indicates a mutable variable).\nThe return operator is unnecessary in a function (although allowed); the value of the last executed statement or expression is normally the function's value.\nInstead of the Java cast operator (Type) foo, Scala uses foo.asInstanceOf[Type], or a specialized function such as toDouble or toInt.\nInstead of Java's import foo.*;, Scala uses import foo._.\nFunction or method foo() can also be called as just foo; method thread.send(signo) can also be called as just thread send signo; and method foo.toString() can also be called as just foo toString.\nThese syntactic relaxations are designed to allow support for domain-specific languages.\n\nSome other basic syntactic differences:\n\nArray references are written like function calls, e.g. array(i) rather than array[i]. (Internally in Scala, both arrays and functions are conceptualized as kinds of mathematical mappings from one object to another.)\nGeneric types are written as e.g. List[String] rather than Java's List<String>.\nInstead of the pseudo-type void, Scala has the actual singleton class Unit (see below).\nExample with classes[edit]\nThe following example contrasts the definition of classes in Java and Scala.\n\n// Java:\npublic class Point {\n  private final double x, y;\n\n  public Point(final double x, final double y) {\n    this.x = x;\n    this.y = y;\n  }\n\n  public Point(\n    final double x, final double y,\n    final boolean addToGrid\n  ) {\n    this(x, y);\n  \n    if (addToGrid)\n      grid.add(this);\n  }\n\n  public Point() {\n    this(0.0, 0.0);\n  }\n\n  public double getX() {\n    return x;\n  }\n\n  public double getY() {\n    return y;\n  }\n\n  double distanceToPoint(final Point other) {\n    return distanceBetweenPoints(x, y,\n      other.x, other.y);\n  }\n\n  private static Grid grid = new Grid();\n\n  static double distanceBetweenPoints(\n      final double x1, final double y1,\n      final double x2, final double y2\n  ) {\n    return Math.hypot(x1 - x2, y1 - y2);\n  }\n}\n// Scala\nclass Point(\n    val x: Double, val y: Double,\n    addToGrid: Boolean = false\n) {\n  import Point._\n\n  if (addToGrid)\n    grid.add(this)\n\n  def this() = this(0.0, 0.0)\n\n  def distanceToPoint(other: Point) =\n    distanceBetweenPoints(x, y, other.x, other.y)\n}\n\nobject Point {\n  private val grid = new Grid()\n\n  def distanceBetweenPoints(x1: Double, y1: Double,\n      x2: Double, y2: Double) = {\n    math.hypot(x1 - x2, y1 - y2)\n  }\n}\nThe above code shows some of the conceptual differences between Java and Scala's handling of classes:\n\nScala has no static variables or methods. Instead, it has singleton objects, which are essentially classes with only one object in the class. Singleton objects are declared using object instead of class. It is common to place static variables and methods in a singleton object with the same name as the class name, which is then known as a companion object.[14] (The underlying class for the singleton object has a $ appended. Hence, for class Foo with companion object object Foo, under the hood there's a class Foo$ containing the companion object's code, and one object of this class is created, using the singleton pattern.)\nIn place of constructor parameters, Scala has class parameters, which are placed on the class, similar to parameters to a function. When declared with a val or var modifier, fields are also defined with the same name, and automatically initialized from the class parameters. (Under the hood, external access to public fields always goes through accessor (getter) and mutator (setter) methods, which are automatically created. The accessor function has the same name as the field, which is why it's unnecessary in the above example to explicitly declare accessor methods.) Note that alternative constructors can also be declared, as in Java. Code that would go into the default constructor (other than initializing the member variables) goes directly at class level.\nDefault visibility in Scala is public.\nFeatures (with reference to Java)[edit]\nScala has the same compiling model as Java and C#, namely separate compiling and dynamic class loading, so that Scala code can call Java libraries.\n\nScala's operational characteristics are the same as Java's. The Scala compiler generates byte code that is nearly identical to that generated by the Java compiler.[14] In fact, Scala code can be decompiled to readable Java code, with the exception of certain constructor operations. To the Java virtual machine (JVM), Scala code and Java code are indistinguishable. The only difference is one extra runtime library, scala-library.jar.[26]\n\nScala adds a large number of features compared with Java, and has some fundamental differences in its underlying model of expressions and types, which make the language theoretically cleaner and eliminate several corner cases in Java. From the Scala perspective, this is practically important because several added features in Scala are also available in C#. Examples include:\n\nSyntactic flexibility[edit]\nAs mentioned above, Scala has a good deal of syntactic flexibility, compared with Java. The following are some examples:\n\nSemicolons are unnecessary; lines are automatically joined if they begin or end with a token that cannot normally come in this position, or if there are unclosed parentheses or brackets.\nAny method can be used as an infix operator, e.g. \"%d apples\".format(num) and \"%d apples\" format num are equivalent. In fact, arithmetic operators like + and << are treated just like any other methods, since function names are allowed to consist of sequences of arbitrary symbols (with a few exceptions made for things like parens, brackets and braces that must be handled specially); the only special treatment that such symbol-named methods undergo concerns the handling of precedence.\nMethods apply and update have syntactic short forms. foo()—where foo is a value (singleton object or class instance)—is short for foo.apply(), and foo() = 42 is short for foo.update(42). Similarly, foo(42) is short for foo.apply(42), and foo(4) = 2 is short for foo.update(4, 2). This is used for collection classes and extends to many other cases, such as STM cells.\nScala distinguishes between no-parens (def foo = 42) and empty-parens (def foo() = 42) methods. When calling an empty-parens method, the parentheses may be omitted, which is useful when calling into Java libraries that do not know this distinction, e.g., using foo.toString instead of foo.toString(). By convention, a method should be defined with empty-parens when it performs side effects.\nMethod names ending in colon (:) expect the argument on the left-hand-side and the receiver on the right-hand-side. For example, the 4 :: 2 :: Nil is the same as Nil.::(2).::(4), the first form corresponding visually to the result (a list with first element 4 and second element 2).\nClass body variables can be transparently implemented as separate getter and setter methods. For trait FooLike { var bar: Int }, an implementation may be object Foo extends FooLike { private var x = 0; def bar = x; def bar_=(value: Int) { x = value }} } }. The call site will still be able to use a concise foo.bar = 42.\nThe use of curly braces instead of parentheses is allowed in method calls. This allows pure library implementations of new control structures.[27] For example, breakable { ... if (...) break() ... } looks as if breakable was a language defined keyword, but really is just a method taking a thunk argument. Methods that take thunks or functions often place these in a second parameter list, allowing to mix parentheses and curly braces syntax: Vector.fill(4) { math.random } is the same as Vector.fill(4)(math.random). The curly braces variant allows the expression to span multiple lines.\nFor-expressions (explained further down) can accommodate any type that defines monadic methods such as map, flatMap and filter.\nBy themselves, these may seem like questionable choices, but collectively they serve the purpose of allowing domain-specific languages to be defined in Scala without needing to extend the compiler. For example, Erlang's special syntax for sending a message to an actor, i.e. actor ! message can be (and is) implemented in a Scala library without needing language extensions.\n\nUnified type system[edit]\nJava makes a sharp distinction between primitive types (e.g. int and boolean) and reference types (any class). Only reference types are part of the inheritance scheme, deriving from java.lang.Object. In Scala, however, all types inherit from a top-level class Any, whose immediate children are AnyVal (value types, such as Int and Boolean) and AnyRef (reference types, as in Java). This means that the Java distinction between primitive types and boxed types (e.g. int vs. Integer) is not present in Scala; boxing and unboxing is completely transparent to the user. Scala 2.10 allows for new value types to be defined by the user.\n\nFor-expressions[edit]\nInstead of the Java \"foreach\" loops for looping through an iterator, Scala has a much more powerful concept of for-expressions. These are similar to list comprehensions in languages such as Haskell, or a combination of list comprehensions and generator expressions in Python. For-expressions using the yield keyword allow a new collection to be generated by iterating over an existing one, returning a new collection of the same type. They are translated by the compiler into a series of map, flatMap and filter calls. Where yield is not used, the code approximates to an imperative-style loop, by translating to foreach.\n\nA simple example is:\n\nval s = for (x <- 1 to 25 if x*x > 50) yield 2*x\nThe result of running it is the following vector:\n\nVector(16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50)\n(Note that the expression 1 to 25 is not special syntax. The method to is rather defined in the standard Scala library as an extension method on integers, using a technique known as implicit conversions[28] that allows new methods to be added to existing types.)\n\nA more complex example of iterating over a map is:\n\n// Given a map specifying Twitter users mentioned in a set of tweets,\n// and number of times each user was mentioned, look up the users\n// in a map of known politicians, and return a new map giving only the\n// Democratic politicians (as objects, rather than strings).\nval dem_mentions = for {\n    (mention, times) <- mentions\n    account          <- accounts.get(mention)\n    if account.party == \"Democratic\"\n  } yield (account, times)\nExpression (mention, times) <- mentions is an example of pattern matching (see below). Iterating over a map returns a set of key-value tuples, and pattern-matching allows the tuples to easily be destructured into separate variables for the key and value. Similarly, the result of the comprehension also returns key-value tuples, which are automatically built back up into a map because the source object (from the variable mentions) is a map. Note that if mentions instead held a list, set, array or other collection of tuples, exactly the same code above would yield a new collection of the same type.\n\nFunctional tendencies[edit]\nWhile supporting all of the object-oriented features available in Java (and in fact, augmenting them in various ways), Scala also provides a large number of capabilities that are normally found only in functional programming languages. Together, these features allow Scala programs to be written in an almost completely functional style, and also allow functional and object-oriented styles to be mixed.\n\nExamples are:\n\nNo distinction between statements and expressions\nType inference\nAnonymous functions with capturing semantics (i.e., closures)\nImmutable variables and objects\nLazy evaluation\nDelimited continuations (since 2.8)\nHigher-order functions\nNested functions\nCurrying\nPattern matching\nAlgebraic data types (through case classes)\nTuples\nEverything is an expression[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2013) (Learn how and when to remove this template message)\nUnlike C or Java, but similar to languages such as Lisp, Scala makes no distinction between statements and expressions. All statements are in fact expressions that evaluate to some value. Functions that would be declared as returning void in C or Java, and statements like while that logically do not return a value, are in Scala considered to return the type Unit, which is a singleton type, with only one object of that type. Functions and operators that never return at all (e.g. the throw operator or a function that always exits non-locally using an exception) logically have return type Nothing, a special type containing no objects; that is, a bottom type, i.e. a subclass of every possible type. (This in turn makes type Nothing compatible with every type, allowing type inference to function correctly.)\n\nSimilarly, an if-then-else \"statement\" is actually an expression, which produces a value, i.e. the result of evaluating one of the two branches. This means that such a block of code can be inserted wherever an expression is desired, obviating the need for a ternary operator in Scala:\n\n// Java:\nint hexDigit = x >= 10 ? x + 'A' - 10 : x + '0';\n// Scala:\nval hexDigit = if (x >= 10) x + 'A' - 10 else x + '0'\nFor similar reasons, return statements are unnecessary in Scala, and in fact are discouraged. As in Lisp, the last expression in a block of code is the value of that block of code, and if the block of code is the body of a function, it will be returned by the function.\n\nTo make it clear that all expressions are functions, even methods that return Unit are written with an equals sign\n\ndef printValue(x: String): Unit = {\n  println(\"I ate a %s\".format(x))\n}\nor equivalently (with type inference, and omitting the unnecessary braces):\n\ndef printValue(x: String) = println(\"I ate a %s\" format x)\nType inference[edit]\nDue to type inference, the type of variables, function return values, and many other expressions can typically be omitted, as the compiler can deduce it. Examples are val x = \"foo\" (for an immutable, constant variable or immutable object) or var x = 1.5 (for a variable whose value can later be changed). Type inference in Scala is essentially local, in contrast to the more global Hindley-Milner algorithm used in Haskell, ML and other more purely functional languages. This is done to facilitate object-oriented programming. The result is that certain types still need to be declared (most notably, function parameters, and the return types of recursive functions), e.g.\n\ndef formatApples(x: Int) = \"I ate %d apples\".format(x)\nor (with a return type declared for a recursive function)\n\ndef factorial(x: Int): Int =\n  if (x == 0)\n    1\n  else\n    x*factorial(x - 1)\nAnonymous functions[edit]\nIn Scala, functions are objects, and a convenient syntax exists for specifying anonymous functions. An example is the expression x => x < 2, which specifies a function with one parameter, that compares its argument to see if it is less than 2. It is equivalent to the Lisp form (lambda (x) (< x 2)). Note that neither the type of x nor the return type need be explicitly specified, and can generally be inferred by type inference; but they can be explicitly specified, e.g. as (x: Int) => x < 2 or even (x: Int) => (x < 2): Boolean.\n\nAnonymous functions behave as true closures in that they automatically capture any variables that are lexically available in the environment of the enclosing function. Those variables will be available even after the enclosing function returns, and unlike in the case of Java's anonymous inner classes do not need to be declared as final. (It is even possible to modify such variables if they are mutable, and the modified value will be available the next time the anonymous function is called.)\n\nAn even shorter form of anonymous function uses placeholder variables: For example, the following:\n\nlist map { x => sqrt(x) }\ncan be written more concisely as\n\nlist map { sqrt(_) }\nor even\n\nlist map sqrt\nImmutability[edit]\nScala enforces a distinction between immutable (unmodifiable, read-only) variables, whose value cannot be changed once assigned, and mutable variables, which can be changed. A similar distinction is made between immutable and mutable objects. The distinction must be made when a variable is declared: Immutable variables are declared with val while mutable variables use var. Similarly, all of the collection objects (container types) in Scala, e.g. linked lists, arrays, sets and hash tables, are available in mutable and immutable variants, with the immutable variant considered the more basic and default implementation. The immutable variants are \"persistent\" data types in that they create a new object that encloses the old object and adds the new member(s); this is similar to how linked lists are built up in Lisp, where elements are prepended by creating a new \"cons\" cell with a pointer to the new element (the \"head\") and the old list (the \"tail\"). This allows for very easy concurrency — no locks are needed as no shared objects are ever modified. Immutable structures are also constructed efficiently, in the sense that modified instances reuses most of old instance data and unused/unreferenced parts are collected by GC.[29]\n\nLazy (non-strict) evaluation[edit]\nEvaluation is strict (\"eager\") by default. In other words, Scala evaluates expressions as soon as they are available, rather than as needed. However, you can declare a variable non-strict (\"lazy\") with the lazy keyword, meaning that the code to produce the variable's value will not be evaluated until the first time the variable is referenced. Non-strict collections of various types also exist (such as the type Stream, a non-strict linked list), and any collection can be made non-strict with the view method. Non-strict collections provide a good semantic fit to things like server-produced data, where the evaluation of the code to generate later elements of a list (that in turn triggers a request to a server, possibly located somewhere else on the web) only happens when the elements are actually needed.\n\nTail recursion[edit]\nFunctional programming languages commonly provide tail call optimization to allow for extensive use of recursion without stack overflow problems. Limitations in Java bytecode complicate tail call optimization on the JVM. In general, a function that calls itself with a tail call can be optimized, but mutually recursive functions cannot. Trampolines have been suggested as a workaround.[30] Trampoline support has been provided by the Scala library with the object scala.util.control.TailCalls since Scala 2.8.0 (released July 14, 2010). A function may optionally be annotated with @tailrec, in which case it will not compile unless it is tail recursive.[31]\n\nCase classes and pattern matching[edit]\nScala has built-in support for pattern matching, which can be thought of as a more sophisticated, extensible version of a switch statement, where arbitrary data types can be matched (rather than just simple types like integers, booleans and strings), including arbitrary nesting. A special type of class known as a case class is provided, which includes automatic support for pattern matching and can be used to model the algebraic data types used in many functional programming languages. (From the perspective of Scala, a case class is simply a normal class for which the compiler automatically adds certain behaviors that could also be provided manually, e.g., definitions of methods providing for deep comparisons and hashing, and destructuring a case class on its constructor parameters during pattern matching.)\n\nAn example of a definition of the quicksort algorithm using pattern matching is this:\n\ndef qsort(list: List[Int]): List[Int] = list match {\n  case Nil => Nil\n  case pivot :: tail =>\n    val (smaller, rest) = tail.partition(_ < pivot)\n    qsort(smaller) ::: pivot :: qsort(rest)\n}\nThe idea here is that we partition a list into the elements less than a pivot and the elements not less, recursively sort each part, and paste the results together with the pivot in between. This uses the same divide-and-conquer strategy of mergesort and other fast sorting algorithms.\n\nThe match operator is used to do pattern matching on the object stored in list. Each case expression is tried in turn to see if it will match, and the first match determines the result. In this case, Nil only matches the literal object Nil, but pivot :: tail matches a non-empty list, and simultaneously destructures the list according to the pattern given. In this case, the associated code will have access to a local variable named pivot holding the head of the list, and another variable tail holding the tail of the list. Note that these variables are read-only, and are semantically very similar to variable bindings established using the let operator in Lisp and Scheme.\n\nPattern matching also happens in local variable declarations. In this case, the return value of the call to tail.partition is a tuple — in this case, two lists. (Tuples differ from other types of containers, e.g. lists, in that they are always of fixed size and the elements can be of differing types — although here they are both the same.) Pattern matching is the easiest way of fetching the two parts of the tuple.\n\nThe form _ < pivot is a declaration of an anonymous function with a placeholder variable; see the section above on anonymous functions.\n\nThe list operators :: (which adds an element onto the beginning of a list, similar to cons in Lisp and Scheme) and ::: (which appends two lists together, similar to append in Lisp and Scheme) both appear. Despite appearances, there is nothing \"built-in\" about either of these operators. As specified above, any string of symbols can serve as function name, and a method applied to an object can be written \"infix\"-style without the period or parentheses. The line above as written:\n\nqsort(smaller) ::: pivot :: qsort(rest)\ncould also be written thus:\n\nqsort(rest).::(pivot).:::(qsort(smaller))\nin more standard method-call notation. (Methods that end with a colon are right-associative and bind to the object to the right.)\n\nPartial functions[edit]\nIn the pattern-matching example above, the body of the match operator is a partial function, which consists of a series of case expressions, with the first matching expression prevailing, similar to the body of a switch statement. Partial functions are also used in the exception-handling portion of a try statement:\n\ntry {\n  ...\n} catch {\n  case nfe:NumberFormatException => { println(nfe); List(0) }\n  case _ => Nil\n}\nFinally, a partial function can be used alone, and the result of calling it is equivalent to doing a match over it. For example, the prior code for quicksort can be written thus:\n\nval qsort: List[Int] => List[Int] = {\n  case Nil => Nil\n  case pivot :: tail =>\n    val (smaller, rest) = tail.partition(_ < pivot)\n    qsort(smaller) ::: pivot :: qsort(rest)\n}\nHere a read-only variable is declared whose type is a function from lists of integers to lists of integers, and bind it to a partial function. (Note that the single parameter of the partial function is never explicitly declared or named.) However, we can still call this variable exactly as if it were a normal function:\n\nscala> qsort(List(6,2,5,9))\nres32: List[Int] = List(2, 5, 6, 9)\nObject-oriented extensions[edit]\nScala is a pure object-oriented language in the sense that every value is an object. Data types and behaviors of objects are described by classes and traits. Class abstractions are extended by subclassing and by a flexible mixin-based composition mechanism to avoid the problems of multiple inheritance.\n\nTraits are Scala's replacement for Java's interfaces. Interfaces in Java versions under 8 are highly restricted, able only to contain abstract function declarations. This has led to criticism that providing convenience methods in interfaces is awkward (the same methods must be reimplemented in every implementation), and extending a published interface in a backwards-compatible way is impossible. Traits are similar to mixin classes in that they have nearly all the power of a regular abstract class, lacking only class parameters (Scala's equivalent to Java's constructor parameters), since traits are always mixed in with a class. The super operator behaves specially in traits, allowing traits to be chained using composition in addition to inheritance. The following example is a simple window system:\n\nabstract class Window {\n  // abstract\n  def draw()\n}\n\nclass SimpleWindow extends Window {\n  def draw() {\n    println(\"in SimpleWindow\")\n    // draw a basic window\n  }\n}\n\ntrait WindowDecoration extends Window { }\n\ntrait HorizontalScrollbarDecoration extends WindowDecoration {\n  // \"abstract override\" is needed here in order for \"super()\" to work because the parent\n  // function is abstract. If it were concrete, regular \"override\" would be enough.\n  abstract override def draw() {\n    println(\"in HorizontalScrollbarDecoration\")\n    super.draw()\n    // now draw a horizontal scrollbar\n  }\n}\n\ntrait VerticalScrollbarDecoration extends WindowDecoration {\n  abstract override def draw() {\n    println(\"in VerticalScrollbarDecoration\")\n    super.draw()\n    // now draw a vertical scrollbar\n  }\n}\n\ntrait TitleDecoration extends WindowDecoration {\n  abstract override def draw() {\n    println(\"in TitleDecoration\")\n    super.draw()\n    // now draw the title bar\n  }\n}\nA variable may be declared thus:\n\nval mywin = new SimpleWindow with VerticalScrollbarDecoration with HorizontalScrollbarDecoration with TitleDecoration\nThe result of calling mywin.draw() is\n\nin TitleDecoration\nin HorizontalScrollbarDecoration\nin VerticalScrollbarDecoration\nin SimpleWindow\nIn other words, the call to draw first executed the code in TitleDecoration (the last trait mixed in), then (through the super() calls) threaded back through the other mixed-in traits and eventually to the code in Window, even though none of the traits inherited from one another. This is similar to the decorator pattern, but is more concise and less error-prone, as it doesn't require explicitly encapsulating the parent window, explicitly forwarding functions whose implementation isn't changed, or relying on run-time initialization of entity relationships. In other languages, a similar effect could be achieved at compile-time with a long linear chain of implementation inheritance, but with the disadvantage compared to Scala that one linear inheritance chain would have to be declared for each possible combination of the mix-ins.\n\nExpressive type system[edit]\nScala is equipped with an expressive static type system that enforces the safe and coherent use of abstractions. In particular, the type system supports:\n\nClasses and abstract types as object members\nStructural types\nPath-dependent types\nCompound types\nExplicitly typed self references\nGeneric classes\nPolymorphic methods\nUpper and lower type bounds\nVariance\nAnnotation\nViews\nScala is able to infer types by usage. This makes most static type declarations optional. Static types need not be explicitly declared unless a compiler error indicates the need. In practice, some static type declarations are included for the sake of code clarity.\n\nType enrichment[edit]\nA common technique in Scala, known as \"enrich my library\"[32] (originally termed as \"pimp my library\" by Martin Odersky in 2006;[28] though concerns were raised about this phrasing due to its negative connotation[33] and immaturity[34]), allows new methods to be used as if they were added to existing types. This is similar to the C# concept of extension methods but more powerful, because the technique is not limited to adding methods and can, for instance, be used to implement new interfaces. In Scala, this technique involves declaring an implicit conversion from the type \"receiving\" the method to a new type (typically, a class) that wraps the original type and provides the additional method. If a method cannot be found for a given type, the compiler automatically searches for any applicable implicit conversions to types that provide the method in question.\n\nThis technique allows new methods to be added to an existing class using an add-on library such that only code that imports the add-on library gets the new functionality, and all other code is unaffected.\n\nThe following example shows the enrichment of type Int with methods isEven and isOdd:\n\nobject MyExtensions {\n  implicit class IntPredicates(i: Int) {\n    def isEven = i % 2 == 0\n    def isOdd  = !isEven\n  }\n}\n\nimport MyExtensions._  // bring implicit enrichment into scope\n4.isEven  // -> true\nImporting the members of MyExtensions brings the implicit conversion to extension class IntPredicates into scope.[35]\n\nConcurrency[edit]\nScala standard library includes support for the actor model, in addition to the standard Java concurrency APIs. Lightbend Inc., provides a platform[36] that includes Akka,[37] a separate open source framework that provides actor-based concurrency. Akka actors may be distributed or combined with software transactional memory (transactors). Alternative communicating sequential processes (CSP) implementations for channel-based message passing are Communicating Scala Objects,[38] or simply via JCSP.\n\nAn Actor is like a thread instance with a mailbox. It can be created by system.actorOf, overriding the receive method to receive messages and using the ! (exclamation point) method to send a message.[39] The following example shows an EchoServer that can receive messages and then print them.\n\nval echoServer = actor(new Act {\n  become {\n    case msg => println(\"echo \" + msg)\n  }\n})\nechoServer ! \"hi\"\nScala also comes with built-in support for data-parallel programming in the form of Parallel Collections[40] integrated into its Standard Library since version 2.9.0.\n\nThe following example shows how to use Parallel Collections to improve performance.[41]\n\nval urls = List(\"http://scala-lang.org\",  \"https://github.com/scala/scala\")\n\ndef fromURL(url: String) = scala.io.Source.fromURL(url)\n  .getLines().mkString(\"\\n\")\n\nval t = System.currentTimeMillis()\nurls.par.map(fromURL(_))\nprintln(\"time: \" + (System.currentTimeMillis - t) + \"ms\")\nBesides actor support and data-parallelism, Scala also supports asynchronous programming with Futures and Promises, software transactional memory, and event streams.[42]\n\nCluster computing[edit]\nThe most well-known open source cluster computing solution, written in Scala, is Apache Spark. Additionally, Apache Kafka, the publish-subscribe message queue popular with Spark and other stream processing technologies, is written in Scala.\n\nTesting[edit]\nThere are several ways to test code in Scala:\n\nScalaTest supports multiple testing styles and can integrate with Java-based testing frameworks[43]\nScalaCheck, a library similar to Haskell's QuickCheck[44]\nspecs2, a library for writing executable software specifications[45]\nScalaMock provides support for testing high-order and curried functions[46]\nJUnit or TestNG, two popular testing frameworks written in Java\nVersions[edit]\nVersion\tReleased\tFeatures\tStatus\tNotes\n2.0[47]\t12-Mar-2006\t_\t_\t_\n2.1.8[48]\t23-Aug-2006\t_\t_\t_\n2.3.0[49]\t23-Nov-2006\t_\t_\t_\n2.4.0[50]\t09-Mar-2007\t_\t_\t_\n2.5.0[51]\t02-May-2007\t_\t_\t_\n2.6.0[52]\t27-Jul-2007\t_\t_\t_\n2.7.0[53]\t07-Feb-2008\t_\t_\t_\n2.8.0[54]\t14-Jul-2010\tRevision the common, uniform, and all-encompassing framework for collection types.\t_\t_\n2.9.0[55]\t12-May-2011\t_\t_\t_\n2.10[56]\t04-Jan-2013\t\nValue Classes[57]\nImplicit Classes[58]\nString Interpolation[59]\nFutures and Promises[60]\nDynamic and applyDynamic[61]\nDependent method types:\ndef identity(x: AnyRef): x.type = x // the return type says we return exactly what we got\nNew ByteCode emitter based on ASM:\nCan target JDK 1.5, 1.6 and 1.7\nEmits 1.6 bytecode by default\nOld 1.5 backend is deprecated\nA new Pattern Matcher: rewritten from scratch to generate more robust code (no more exponential blow-up!)\ncode generation and analyses are now independent (the latter can be turned off with -Xno-patmat-analysis)\nScaladoc Improvements\nImplicits (-implicits flag)\nDiagrams (-diagrams flag, requires graphviz)\nGroups (-groups)\nModularized Language features[62]\nParallel Collections[63] are now configurable with custom thread pools\nAkka Actors now part of the distribution\nscala.actors have been deprecated and the akka implementation is now included in the distribution.\nPerformance Improvements\nFaster inliner\nRange#sum is now O(1)\nUpdate of ForkJoin library\nFixes in immutable TreeSet/TreeMap\nImprovements to PartialFunctions\nAddition of ??? and NotImplementedError\nAddition of IsTraversableOnce + IsTraversableLike type classes for extension methods\nDeprecations and cleanup\nFloating point and octal literal syntax deprecation\nRemoved scala.dbc\nExperimental features\n\nScala Reflection[64]\nMacros[65]\n_\t_\n2.10.2[66]\t06-Jun-2013\t_\t_\t_\n2.10.3[67]\t01-Oct-2013\t_\t_\t_\n2.10.4[68]\t18-Mar-2014\t_\t_\t_\n2.10.5[69]\t05-Mar-2015\t_\t_\t_\n2.11.0[70]\t21-Apr-2014\t_\t_\t_\n2.11.1[71]\t20-May-2014\t_\t_\t_\n2.11.2[72]\t22-Jul-2014\t_\t_\t_\n2.11.4[73]\t31-Oct-2014\t_\t_\t_\n2.11.5[74]\t08-Jan-2015\t_\t_\t_\n2.11.6[75]\t05-Mar-2015\t_\t_\t_\n2.11.7[76]\t23-Jun-2015\t_\t_\t_\n2.11.8[77]\t8-Mar-2016\t_\tCurrent\t_\nComparison with other JVM languages[edit]\nScala is often compared with Groovy and Clojure, two other programming languages also using the JVM. Substantial differences between these languages are found in the type system, in the extent to which each language supports object-oriented and functional programming, and in the similarity of their syntax to the syntax of Java.\n\nScala is statically typed, while both Groovy and Clojure are dynamically typed. This makes the type system more complex and difficult to understand but allows almost all type errors to be caught at compile-time and can result in significantly faster execution. By contrast, dynamic typing requires more testing to ensure program correctness and is generally slower in order to allow greater programming flexibility and simplicity. Regarding speed differences, current versions of Groovy and Clojure allow for optional type annotations to help programs avoid the overhead of dynamic typing in cases where types are practically static. This overhead is further reduced when using recent versions of the JVM, which has been enhanced with an invoke dynamic instruction for methods that are defined with dynamically typed arguments. These advances reduce the speed gap between static and dynamic typing, although a statically typed language, like Scala, is still the preferred choice when execution efficiency is very important.\n\nRegarding programming paradigms, Scala inherits the object-oriented model of Java and extends it in various ways. Groovy, while also strongly object-oriented, is more focused in reducing verbosity. In Clojure, object-oriented programming is deemphasised with functional programming being the main strength of the language. Scala also has many functional programming facilities, including features found in advanced functional languages like Haskell, and tries to be agnostic between the two paradigms, letting the developer choose between the two paradigms or, more frequently, some combination thereof.\n\nRegarding syntax similarity with Java, Scala inherits much of Java's syntax, as is the case with Groovy. Clojure on the other hand follows the Lisp syntax, which is different in both appearance and philosophy. However, learning Scala is also considered difficult because of its many advanced features. This is not the case with Groovy, despite its also being a feature-rich language, mainly because it was designed to be mainly a scripting language.[citation needed]\n\nAdoption[edit]\nLanguage rankings[edit]\nScala was voted the most popular JVM scripting language at the 2012 JavaOne conference.[14]\n\nAs of 2013, all JVM-based languages (Scala, Groovy, Clojure) are significantly less popular than the original Java language, which is usually ranked first or second,[78][79][80] and which is also simultaneously evolving over time.\n\n\nIndeed.com job trends: Scala and related technologies\nThe RedMonk Programming Language Rankings, as of June 2016 placed Scala 14th, based on a position in terms of number of GitHub projects and in terms of number of questions tagged on Stack Overflow.[78] (Groovy and Clojure were both in 20th place.)[78] Here, Scala is shown somewhat between a first-tier group of languages (including, C, Python, PHP, Ruby, etc.), and ahead of a second-tier group.\n\nAnother measure, the Popularity of Programming Language Index[81] which tracks searches for language tutorials ranked Scala 15th in July 2016 with a small upward trend, making it the most popular JVM-based language after Java.\n\nAs of January 2016, the TIOBE index[79] of programming language popularity shows Scala in 30th place (as measured by internet search engine rankings and similar publication-counting), but–as mentioned under \"Bugs & Change Requests\"–TIOBE is aware of issues with its methodology of using search terms which might not be commonly used in some programming language communities. In this ranking Scala is ahead of functional languages Haskell (39th), Erlang (35rd) and Clojure (>50), but below Java (1st).\n\nThe ThoughtWorks Technology Radar, which is an opinion based half-yearly report of a group of senior technologists,[82] recommends Scala adoption in its languages and frameworks category.[83]\n\nAccording to Indeed.com Job Trends, Scala demand has been rapidly increasing since 2010, trending ahead of Clojure and Groovy.[84]\n\nCompanies[edit]\nIn April 2009, Twitter announced that it had switched large portions of its backend from Ruby to Scala and intended to convert the rest.[85]\nGilt uses Scala and Play Framework.[86]\nFoursquare uses Scala and Lift.[87]\nSpinGo uses Scala and Akka.[88]\nCoursera uses Scala and Play Framework.[89]\nApple Inc. uses Scala in certain teams, along with Java and the Play framework.[90][91]\nThe Guardian newspaper's high-traffic website guardian.co.uk[92] announced in April 2011 that it was switching from Java to Scala,[93][94]\nThe New York Times revealed in 2014 that its internal content management system Blackbeard is built using Scala, Akka and Play.[95]\nThe Huffington Post newspaper started to employ Scala as part of its contents delivery system Athena in 2013.[96]\nSwiss bank UBS approved Scala for general production usage.[97]\nThe BitGold platform was built entirely on Scala and Play Framework.[98]\nLinkedIn uses the Scalatra microframework to power its Signal API.[99]\nMeetup uses Unfiltered toolkit for real-time APIs.[100]\nRemember the Milk uses Unfiltered toolkit, Scala and Akka for public API and real time updates.[101]\nVerizon seeking to make \"a next generation framework\" using Scala.[102]\nLeadIQ was built entirely on Scala, Akka and Play Framework.[103]\nAirbnb develops open source machine learning software \"Aerosolve\", written in Java and Scala.[104]\nZalando moved its technology stack from Java to Scala and Play.[105]\nSoundCloud uses Scala for its back-end, employing technologies such as Finagle (micro services),[106] Scalding and Spark (data processing).[107]\nDatabricks uses Scala for the Apache Spark Big Data platform.\nMorgan Stanley uses Scala extensively in their finance and asset-related projects.[108]\nThere are teams within Google/Alphabet Inc. that use Scala, mostly due to acquisitions such as Firebase[109] and Nest.[110]\nWalmart Canada Uses Scala for their back end platform.[111]\nx.ai uses Scala for their AI-driven Personal Assistant.[112]\nCriticism[edit]\nIn March 2015, former VP of the Platform Engineering group at Twitter Raffi Krikorian, stated he would not have chosen Scala in 2011 due to its learning curve.[113] The same month, LinkedIn SVP Kevin Scott stated their decision to \"minimize [their] dependence on Scala.\"[114] In November 2011, Yammer moved away from Scala for reasons that included the learning curve for new team members and incompatibility from one version of the Scala compiler to the next.[115]\n\ndotty is an attempt at creating a simpler, faster Scala compiler based on a formal calculus,[1] that will enable faster language development and future language simplification.[116]",
          "type": "compiled",
          "plid": 30
        },
        {
          "name": "Smalltalk",
          "details": "Smalltalk is an object-oriented, dynamically typed, reflective programming language. Smalltalk was created as the language to underpin the \"new world\" of computing exemplified by \"human–computer symbiosis.\"[2] It was designed and created in part for educational use, more so for constructionist learning, at the Learning Research Group (LRG) of Xerox PARC by Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Scott Wallace, and others during the 1970s.\n\nThe language was first generally released as Smalltalk-80. Smalltalk-like languages are in continuing active development and have gathered loyal communities of users around them. ANSI Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.[3]\n\nContents  [hide] \n1\tHistory\n2\tInfluences\n3\tObject-oriented programming\n4\tReflection\n5\tSyntax\n5.1\tLiterals\n5.2\tVariable declarations\n5.3\tAssignment\n5.4\tMessages\n5.5\tExpressions\n5.6\tCode blocks\n6\tControl structures\n7\tClasses\n7.1\tMethods\n7.2\tInstantiating classes\n8\tHello World example\n9\tImage-based persistence\n10\tLevel of access\n11\tJust-in-time compilation\n12\tList of implementations\n13\tSee also\n14\tReferences\n15\tFurther reading\n16\tExternal links\nHistory[edit]\nThere are a large number of Smalltalk variants.[4] The unqualified word Smalltalk is often used to indicate the Smalltalk-80 language, the first version to be made publicly available and created in 1980.\n\nSmalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, which Dan Ingalls implemented. The first version, known as Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in \"a page of code.\"[2] A later variant actually used for research work is now known as Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.\n\nAfter significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created. This system had a development environment featuring most of the now familiar tools, including a class library code browser/editor. Smalltalk-80 added metaclasses, to help maintain the \"everything is an object\" (except private instance variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways of creating instances).\n\nSmalltalk-80 was the first language variant made available outside of PARC, first as Smalltalk-80 Version 1, given to a small number of firms (Hewlett-Packard, Apple Computer, Tektronix, and DEC) and universities (UC Berkeley) for \"peer review\" and implementation on their platforms. Later (in 1983) a general availability implementation, known as Smalltalk-80 Version 2, was released as an image (platform-independent file with object definitions) and a virtual machine specification. ANSI Smalltalk has been the standard language reference since 1998.[5]\n\nTwo of the currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks.[6] (Dan Ingalls later ported Hobbes to Squeak.)\n\nDuring the late 1980s to mid-1990s, Smalltalk environments—including support, training and add-ons—were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers. While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge/Smalltalk. Easel introduced Enfin at this time on Windows and OS/2. Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge). Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.\n\nIn 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom.[7] VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.\n\nCincom, Gemstone and Object Arts, plus other vendors continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java and it is, as of 2006, supported by Instantiations, Inc.[8] which has renamed the product VA Smalltalk and released several new versions. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented towards research and use in commercial environments.\n\nA significant development, that has spread across all current Smalltalk environments, is the increasing usage of two web frameworks, Seaside and AIDA/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone and Instantiations incorporating and extending it.\n\nInfluences[edit]\nSmalltalk was one of many object-oriented programming languages based on Simula.[9] Smalltalk was also one of the most influential programming languages. Virtually all of the object-oriented languages that came after—Flavors,[10] CLOS, Objective-C, Java, Python, Ruby,[11] and many others—were influenced by Smalltalk. Smalltalk was also one of the most popular languages with the Agile Methods, Rapid Prototyping, and Software Patterns[12] communities. The highly productive environment provided by Smalltalk platforms made them ideal for rapid, iterative development.\n\nSmalltalk emerged from a larger program of ARPA funded research that in many ways defined the modern world of computing. In addition to Smalltalk working prototypes of things such as hypertext, GUIs, multimedia, the mouse, telepresence, and the Internet were developed by ARPA researchers in the 1960s.[13][14] Alan Kay (one of the inventors of Smalltalk) also described a tablet computer he called the Dynabook which resembles modern tablet computers like the iPad.[15]\n\nSmalltalk environments were often the first to develop what are now common object-oriented software design patterns. One of the most popular is the Model–view–controller pattern for User Interface design. The MVC pattern enables developers to have multiple consistent views of the same underlying data. It's ideal for software development environments, where there are various views (e.g., entity-relation, dataflow, object model, etc.) of the same underlying specification. Also, for simulations or games where the underlying model may be viewed from various angles and levels of abstraction.[16]\n\nIn addition to the MVC pattern the Smalltalk language and environment were tremendously influential in the history of the Graphical User Interface (GUI) and the What You See Is What You Get (WYSIWYG) user interface, font editors, and desktop metaphors for UI design. The powerful built-in debugging and object inspection tools that came with Smalltalk environments set the standard for all the Integrated Development Environments, starting with Lisp Machine environments, that came after.[17]\n\nObject-oriented programming[edit]\nMain article: Object-oriented programming\nAs in other object-oriented languages, the central concept in Smalltalk-80 (but not in Smalltalk-72) is that of an object. An object is always an instance of a class. Classes are \"blueprints\" that describe the properties and behavior of their instances. For example, a GUI's window class might declare that windows have properties such as the label, the position and whether the window is visible or not. The class might also declare that instances support operations such as opening, closing, moving and hiding. Each particular window object would have its own values of those properties, and each of them would be able to perform operations defined by its class.\n\nA Smalltalk object can do exactly three things:\n\nHold state (references to other objects).\nReceive a message from itself or another object.\nIn the course of processing a message, send messages to itself or another object.\nThe state an object holds is always private to that object. Other objects can query or change that state only by sending requests (messages) to the object to do so. Any message can be sent to any object: when a message is received, the receiver determines whether that message is appropriate. Alan Kay has commented that despite the attention given to objects, messaging is the most important concept in Smalltalk: \"The big idea is 'messaging'—that is what the kernel of Smalltalk/Squeak is all about (and it's something that was never quite completed in our Xerox PARC phase).\"[18]\n\nSmalltalk is a \"pure\" object-oriented programming language, meaning that, unlike Java and C++, there is no difference between values which are objects and values which are primitive types. In Smalltalk, primitive values such as integers, booleans and characters are also objects, in the sense that they are instances of corresponding classes, and operations on them are invoked by sending messages. A programmer can change or extend (through subclassing) the classes that implement primitive values, so that new behavior can be defined for their instances—for example, to implement new control structures—or even so that their existing behavior will be changed. This fact is summarized in the commonly heard phrase \"In Smalltalk everything is an object\", which may be more accurately expressed as \"all values are objects\", as variables are not.\n\nSince all values are objects, classes themselves are also objects. Each class is an instance of the metaclass of that class. Metaclasses in turn are also objects, and are all instances of a class called Metaclass. Code blocks—Smalltalk's way of expressing anonymous functions—are also objects.[19]\n\nReflection[edit]\nReflection is a term that computer scientists apply to software programs that have the capability to inspect their own structure, for example their parse tree or datatypes of input and output parameters. Reflection was first primarily a feature of interpreted languages such as Smalltalk and Lisp. The fact that statements are interpreted means that the programs have access to information created as they were parsed and can often even modify their own structure.\n\nReflection is also a feature of having a meta-model as Smalltalk does. The meta-model is the model that describes the language itself and developers can use the meta-model to do things like walk through, examine, and modify the parse tree of an object. Or find all the instances of a certain kind of structure (e.g., all the instances of the Method class in the meta-model).\n\nSmalltalk-80 is a totally reflective system, implemented in Smalltalk-80 itself. Smalltalk-80 provides both structural and computational reflection. Smalltalk is a structurally reflective system whose structure is defined by Smalltalk-80 objects. The classes and methods that define the system are themselves objects and fully part of the system that they help define. The Smalltalk compiler compiles textual source code into method objects, typically instances of CompiledMethod. These get added to classes by storing them in a class's method dictionary. The part of the class hierarchy that defines classes can add new classes to the system. The system is extended by running Smalltalk-80 code that creates or defines classes and methods. In this way a Smalltalk-80 system is a \"living\" system, carrying around the ability to extend itself at run time.\n\nSince the classes are themselves objects, they can be asked questions such as \"what methods do you implement?\" or \"what fields/slots/instance variables do you define?\". So objects can easily be inspected, copied, (de)serialized and so on with generic code that applies to any object in the system.[20]\n\nSmalltalk-80 also provides computational reflection, the ability to observe the computational state of the system. In languages derived from the original Smalltalk-80 the current activation of a method is accessible as an object named via a pseudo-variable (one of the six reserved words), thisContext. By sending messages to thisContext a method activation can ask questions like \"who sent this message to me\". These facilities make it possible to implement co-routines or Prolog-like back-tracking without modifying the virtual machine. The exception system is implemented using this facility. One of the more interesting uses of this is in the Seaside web framework which relieves the programmer of dealing with the complexity of a Web Browser's back button by storing continuations for each edited page and switching between them as the user navigates a web site. Programming the web server using Seaside can then be done using a more conventional programming style.[21]\n\nAn example of how Smalltalk can use reflection is the mechanism for handling errors. When an object is sent a message that it does not implement, the virtual machine sends the object the doesNotUnderstand: message with a reification of the message as an argument. The message (another object, an instance of Message) contains the selector of the message and an Array of its arguments. In an interactive Smalltalk system the default implementation of doesNotUnderstand: is one that opens an error window (a Notifier) reporting the error to the user. Through this and the reflective facilities the user can examine the context in which the error occurred, redefine the offending code, and continue, all within the system, using Smalltalk-80's reflective facilities.[22][23]\n\nSyntax[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2014) (Learn how and when to remove this template message)\nSmalltalk-80 syntax is rather minimalist, based on only a handful of declarations and reserved words. In fact, only six \"keywords\" are reserved in Smalltalk: true, false, nil, self, super, and thisContext. These are actually called pseudo-variables, identifiers that follow the rules for variable identifiers but denote bindings that the programmer cannot change. The true, false, and nil pseudo-variables are singleton instances. self and super refer to the receiver of a message within a method activated in response to that message, but sends to super are looked up in the superclass of the method's defining class rather than the class of the receiver, which allows methods in subclasses to invoke methods of the same name in superclasses. thisContext refers to the current activation record. The only built-in language constructs are message sends, assignment, method return and literal syntax for some objects. From its origins as a language for children of all ages, standard Smalltalk syntax uses punctuation in a manner more like English than mainstream coding languages. The remainder of the language, including control structures for conditional evaluation and iteration, is implemented on top of the built-in constructs by the standard Smalltalk class library. (For performance reasons, implementations may recognize and treat as special some of those messages; however, this is only an optimization and is not hardwired into the language syntax.)\n\nThe adage that \"Smalltalk syntax fits on a postcard\" refers to a code snippet by Ralph Johnson, demonstrating all the basic standard syntactic elements of methods:[24]\n\nexampleWithNumber: x\n    | y |\n    true & false not & (nil isNil) ifFalse: [self halt].\n    y := self size + super size.\n    #($a #a \"a\" 1 1.0)\n        do: [ :each |\n            Transcript show: (each class name);\n                       show: ' '].\n    ^x < y\nLiterals[edit]\nThe following examples illustrate the most common objects which can be written as literal values in Smalltalk-80 methods.\n\nNumbers. The following list illustrates some of the possibilities.\n\n42\n-42\n123.45\n1.2345e2\n2r10010010\n16rA000\nThe last two entries are a binary and a hexadecimal number, respectively. The number before the 'r' is the radix or base. The base does not have to be a power of two; for example 36rSMALLTALK is a valid number equal to 80738163270632 decimal.\n\nCharacters are written by preceding them with a dollar sign:\n\n$A\nStrings are sequences of characters enclosed in single quotes:\n\n'Hello, world!'\nTo include a quote in a string, escape it using a second quote:\n\n'I said, ''Hello, world!'' to them.'\nDouble quotes do not need escaping, since single quotes delimit a string:\n\n'I said, \"Hello, world!\" to them.'\nTwo equal strings (strings are equal if they contain all the same characters) can be different objects residing in different places in memory. In addition to strings, Smalltalk has a class of character sequence objects called Symbol. Symbols are guaranteed to be unique—there can be no two equal symbols which are different objects. Because of that, symbols are very cheap to compare and are often used for language artifacts such as message selectors (see below).\n\nSymbols are written as # followed by a string literal. For example:\n\n#'foo'\nIf the sequence does not include whitespace or punctuation characters, this can also be written as:\n\n#foo\nArrays:\n\n#(1 2 3 4)\ndefines an array of four integers.\n\nMany implementations support the following literal syntax for ByteArrays:\n\n#[1 2 3 4]\ndefines a ByteArray of four integers.\n\nAnd last but not least, blocks (anonymous function literals)\n\n[... Some smalltalk code...]\nBlocks are explained in detail further in the text.\n\nMany Smalltalk dialects implement additional syntaxes for other objects, but the ones above are the essentials supported by all.\n\nVariable declarations[edit]\nThe two kinds of variables commonly used in Smalltalk are instance variables and temporary variables. Other variables and related terminology depend on the particular implementation. For example, VisualWorks has class shared variables and namespace shared variables, while Squeak and many other implementations have class variables, pool variables and global variables.\n\nTemporary variable declarations in Smalltalk are variables declared inside a method (see below). They are declared at the top of the method as names separated by spaces and enclosed by vertical bars. For example:\n\n| index |\ndeclares a temporary variable named index. Multiple variables may be declared within one set of bars:\n\n| index vowels |\ndeclares two variables: index and vowels.\n\nAssignment[edit]\nA variable is assigned a value via the ':=' syntax. So:\n\nvowels := 'aeiou'\nAssigns the string 'aeiou' to the previously declared vowels variable. The string is an object (a sequence of characters between single quotes is the syntax for literal strings), created by the compiler at compile time.\n\nIn the original Parc Place image, the glyph of the underscore character (_) appeared as a left-facing arrow (like in the 1963 version of the ASCII code). Smalltalk originally accepted this left-arrow as the only assignment operator. Some modern code still contains what appear to be underscores acting as assignments, hearkening back to this original usage. Most modern Smalltalk implementations accept either the underscore or the colon-equals syntax.\n\nMessages[edit]\nThe message is the most fundamental language construct in Smalltalk. Even control structures are implemented as message sends. Smalltalk adopts by default a synchronous, single dynamic message dispatch strategy (as contrasted to the asynchronous, multiple dispatch strategy adopted by some other object-oriented languages).\n\nThe following example sends the message 'factorial' to number 42:\n\n42 factorial\nIn this situation 42 is called the message receiver, while 'factorial' is the message selector. The receiver responds to the message by returning a value (presumably in this case the factorial of 42). Among other things, the result of the message can be assigned to a variable:\n\naRatherBigNumber := 42 factorial\n\"factorial\" above is what is called a unary message because only one object, the receiver, is involved. Messages can carry additional objects as arguments, as follows:\n\n2 raisedTo: 4\nIn this expression two objects are involved: 2 as the receiver and 4 as the message argument. The message result, or in Smalltalk parlance, the answer is supposed to be 16. Such messages are called keyword messages. A message can have more arguments, using the following syntax:\n\n'hello world' indexOf: $o startingAt: 6\nwhich answers the index of character 'o' in the receiver string, starting the search from index 6. The selector of this message is \"indexOf:startingAt:\", consisting of two pieces, or keywords.\n\nSuch interleaving of keywords and arguments is meant to improve readability of code, since arguments are explained by their preceding keywords. For example, an expression to create a rectangle using a C++ or Java-like syntax might be written as:\n\nnew Rectangle(100, 200);\nIt's unclear which argument is which. By contrast, in Smalltalk, this code would be written as:\n\nRectangle width: 100 height: 200\nThe receiver in this case is \"Rectangle\", a class, and the answer will be a new instance of the class with the specified width and height.\n\nFinally, most of the special (non-alphabetic) characters can be used as what are called binary messages. These allow mathematical and logical operators to be written in their traditional form:\n\n3 + 4\nwhich sends the message \"+\" to the receiver 3 with 4 passed as the argument (the answer of which will be 7). Similarly,\n\n3 > 4\nis the message \">\" sent to 3 with argument 4 (the answer of which will be false).\n\nNotice, that the Smalltalk-80 language itself does not imply the meaning of those operators. The outcome of the above is only defined by how the receiver of the message (in this case a Number instance) responds to messages \"+\" and \">\".\n\nA side effect of this mechanism is operator overloading. A message \">\" can also be understood by other objects, allowing the use of expressions of the form \"a > b\" to compare them.\n\nExpressions[edit]\nAn expression can include multiple message sends. In this case expressions are parsed according to a simple order of precedence. Unary messages have the highest precedence, followed by binary messages, followed by keyword messages. For example:\n\n3 factorial + 4 factorial between: 10 and: 100\nis evaluated as follows:\n\n3 receives the message \"factorial\" and answers 6\n4 receives the message \"factorial\" and answers 24\n6 receives the message \"+\" with 24 as the argument and answers 30\n30 receives the message \"between:and:\" with 10 and 100 as arguments and answers true\nThe answer of the last message sent is the result of the entire expression.\n\nParentheses can alter the order of evaluation when needed. For example,\n\n(3 factorial + 4) factorial between: 10 and: 100\nwill change the meaning so that the expression first computes \"3 factorial + 4\" yielding 10. That 10 then receives the second \"factorial\" message, yielding 3628800. 3628800 then receives \"between:and:\", answering false.\n\nNote that because the meaning of binary messages is not hardwired into Smalltalk-80 syntax, all of them are considered to have equal precedence and are evaluated simply from left to right. Because of this, the meaning of Smalltalk expressions using binary messages can be different from their \"traditional\" interpretation:\n\n3 + 4 * 5\nis evaluated as \"(3 + 4) * 5\", producing 35. To obtain the expected answer of 23, parentheses must be used to explicitly define the order of operations:\n\n3 + (4 * 5)\nUnary messages can be chained by writing them one after another:\n\n3 factorial factorial log\nwhich sends \"factorial\" to 3, then \"factorial\" to the result (6), then \"log\" to the result (720), producing the result 2.85733.\n\nA series of expressions can be written as in the following (hypothetical) example, each separated by a period. This example first creates a new instance of class Window, stores it in a variable, and then sends two messages to it.\n\n | window |\n  window := Window new.\n  window label: 'Hello'.\n  window open\nIf a series of messages are sent to the same receiver as in the example above, they can also be written as a cascade with individual messages separated by semicolons:\n\n  Window new\n    label: 'Hello';\n    open\nThis rewrite of the earlier example as a single expression avoids the need to store the new window in a temporary variable. According to the usual precedence rules, the unary message \"new\" is sent first, and then \"label:\" and \"open\" are sent to the answer of \"new\".\n\nCode blocks[edit]\nA block of code (an anonymous function) can be expressed as a literal value (which is an object, since all values are objects.) This is achieved with square brackets:\n\n[ :params | <message-expressions> ]\nWhere :params is the list of parameters the code can take. This means that the Smalltalk code:\n\n[:x | x + 1]\ncan be understood as:\n\n{\\displaystyle f} f : {\\displaystyle f(x)=x+1} f(x)=x+1\nor expressed in lambda terms as:\n\n{\\displaystyle \\lambda x} \\lambda x : {\\displaystyle x+1} x+1\nand\n\n[:x | x + 1] value: 3\ncan be evaluated as\n\n{\\displaystyle f(3)=3+1} f(3)=3+1\nOr in lambda terms as:\n\n{\\displaystyle (\\lambda x:x+1)3_{\\beta }\\rightarrow 4} (\\lambda x:x+1)3_{\\beta }\\rightarrow 4\nThe resulting block object can form a closure: it can access the variables of its enclosing lexical scopes at any time. Blocks are first-class objects.\n\nBlocks can be executed by sending them the value message (compound variations exist in order to provide parameters to the block e.g. 'value:value:' and 'valueWithArguments:').\n\nThe literal representation of blocks was an innovation which on the one hand allowed certain code to be significantly more readable; it allowed algorithms involving iteration to be coded in a clear and concise way. Code that would typically be written with loops in some languages can be written concisely in Smalltalk using blocks, sometimes in a single line. But more importantly blocks allow control structure to be expressed using messages and polymorphism, since blocks defer computation and polymorphism can be used to select alternatives. So if-then-else in Smalltalk is written and implemented as\n\nexpr ifTrue: [statements to evaluate if expr] ifFalse: [statements to evaluate if not expr]\n\nTrue methods for evaluation\nifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock\n     ^trueAlternativeBlock value\n\nFalse methods for evaluation\nifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock\n    ^falseAlternativeBlock value\npositiveAmounts := allAmounts select: [:anAmount | anAmount isPositive]\nNote that this is related to functional programming, wherein patterns of computation (here selection) are abstracted into higher-order functions. For example, the message select: on a Collection is equivalent to the higher-order function filter on an appropriate functor.[25]\n\nControl structures[edit]\nControl structures do not have special syntax in Smalltalk. They are instead implemented as messages sent to objects. For example, conditional execution is implemented by sending the message ifTrue: to a Boolean object, passing as an argument the block of code to be executed if and only if the Boolean receiver is true.\n\nThe following code demonstrates this:\n\nresult := a > b\n    ifTrue:[ 'greater' ]\n    ifFalse:[ 'less or equal' ]\nBlocks are also used to implement user-defined control structures, enumerators, visitors, pluggable behavior and many other patterns. For example:\n\n| aString vowels |\naString := 'This is a string'.\nvowels := aString select: [:aCharacter | aCharacter isVowel].\nIn the last line, the string is sent the message select: with an argument that is a code block literal. The code block literal will be used as a predicate function that should answer true if and only if an element of the String should be included in the Collection of characters that satisfy the test represented by the code block that is the argument to the \"select:\" message.\n\nA String object responds to the \"select:\" message by iterating through its members (by sending itself the message \"do:\"), evaluating the selection block (\"aBlock\") once with each character it contains as the argument. When evaluated (by being sent the message \"value: each\"), the selection block (referenced by the parameter \"aBlock\", and defined by the block literal \"[:aCharacter | aCharacter isVowel]\"), answers a boolean, which is then sent \"ifTrue:\". If the boolean is the object true, the character is added to a string to be returned. Because the \"select:\" method is defined in the abstract class Collection, it can also be used like this:\n\n| rectangles aPoint collisions |\nrectangles := OrderedCollection \n  with: (Rectangle left: 0 right: 10 top: 100 bottom: 200)\n  with: (Rectangle left: 10 right: 10 top: 110 bottom: 210).\naPoint := Point x: 20 y: 20.\ncollisions := rectangles select: [:aRect | aRect containsPoint: aPoint].\nClasses[edit]\nThis is a stock class definition:[26]\n\nObject subclass: #MessagePublisher\n    instanceVariableNames: ''\n    classVariableNames: ''\n    poolDictionaries: ''\n    category: 'Smalltalk Examples'\nOften, most of this definition will be filled in by the environment. Notice that this is actually a message to the \"Object\"-class to create a subclass called \"MessagePublisher\". In other words: classes are first-class objects in Smalltalk which can receive messages just like any other object and can be created dynamically at execution time.\n\nMethods[edit]\nWhen an object receives a message, a method matching the message name is invoked. The following code defines a method publish, and so defines what will happen when this object receives the 'publish' message.\n\npublish\n    Transcript show: 'Hello World!'\nThe following method demonstrates receiving multiple arguments and returning a value:\n\nquadMultiply: i1 and: i2 \n    \"This method multiplies the given numbers by each other and the result by 4.\"\n    | mul |\n    mul := i1 * i2.\n    ^mul * 4\nThe method's name is #quadMultiply:and:. The return value is specified with the ^ operator.\n\nNote that objects are responsible for determining dynamically at runtime which method to execute in response to a message—while in many languages this may be (sometimes, or even always) determined statically at compile time.\n\nInstantiating classes[edit]\nThe following code:\n\nMessagePublisher new\ncreates (and returns) a new instance of the MessagePublisher class. This is typically assigned to a variable:\n\npublisher := MessagePublisher new\nHowever, it is also possible to send a message to a temporary, anonymous object:\n\nMessagePublisher new publish\nHello World example[edit]\nThe Hello world program is used by virtually all texts to new programming languages as the first program learned to show the most basic syntax and environment of the language. For Smalltalk, the program is extremely simple to write. The following code, the message \"show:\" is sent to the object \"Transcript\" with the String literal 'Hello, world!' as its argument. Invocation of the \"show:\" method causes the characters of its argument (the String literal 'Hello, world!') to be displayed in the transcript (\"terminal\") window.\n\nTranscript show: 'Hello, world!'.\nNote that a Transcript window would need to be open in order to see the results of this example.\n\nImage-based persistence[edit]\nMost popular programming systems separate static program code (in the form of class definitions, functions or procedures) from dynamic, or run time, program state (such as objects or other forms of program data). They load program code when a program starts, and any prior program state must be recreated explicitly from configuration files or other data sources. Any settings the program (and programmer) does not explicitly save must be set up again for each restart. A traditional program also loses much useful document information each time a program saves a file, quits, and reloads. This loses details such as undo history or cursor position. Image based systems don't force losing all that just because a computer is turned off, or an OS updates.\n\nMany Smalltalk systems, however, do not differentiate between program data (objects) and code (classes). In fact, classes are objects themselves. Therefore, most Smalltalk systems store the entire program state (including both Class and non-Class objects) in an image file. The image can then be loaded by the Smalltalk virtual machine to restore a Smalltalk-like system to a prior state.[27] This was inspired by FLEX, a language created by Alan Kay and described in his M.Sc. thesis.[28]\n\nSmalltalk images are similar to (restartable) core dumps and can provide the same functionality as core dumps, such as delayed or remote debugging with full access to the program state at the time of error. Other languages that model application code as a form of data, such as Lisp, often use image-based persistence as well. This method of persistence is powerful for rapid development because all the development information (e.g. parse trees of the program) is saved which facilitates debugging. However, it also has serious drawbacks as a true persistence mechanism. For one thing, developers may often want to hide implementation details and not make them available in a run time environment. For legal reasons as well as for maintenance reasons, allowing anyone to modify the program at run time inevitably introduces complexity and potential errors that would not be possible with a compiled system that does not expose source code in the run time environment. Also, while the persistence mechanism is easy to use it lacks the true persistence capabilities needed for most multi-user systems. The most obvious is the ability to do transactions with multiple users accessing the same database in parallel.[29]\n\nLevel of access[edit]\nEverything in Smalltalk-80 is available for modification from within a running program. This means that, for example, the IDE can be changed in a running system without restarting it. In some implementations, the syntax of the language or the garbage collection implementation can also be changed on the fly. Even the statement true become: false is valid in Smalltalk, although executing it is not recommended. When used judiciously, this level of flexibility allows for one of the shortest required times for new code to enter a production system.[citation needed]\n\nJust-in-time compilation[edit]\nMain article: Just-in-time compilation\nSmalltalk programs are usually compiled to bytecode, which is then interpreted by a virtual machine or dynamically translated into machine-native code.\n\nList of implementations[edit]\nAmber Smalltalk Smalltalk running atop JavaScript\nAthena, Smalltalk scripting engine for Java ≥ 1.6\nBistro\nCincom has the following Smalltalk products: ObjectStudio, VisualWorks and WebVelocity.\nVisual Smalltalk Enterprise, and family, including Smalltalk/V\nCuis Smalltalk, open source, modern Smalltalk-80 [3]\nF-Script\nGemTalk Systems, GemStone/s\nGNU Smalltalk\nÉtoilé Pragmatic Smalltalk, Smalltalk for Étoilé, a GNUstep-based user environment\nStepTalk, GNUstep scripting framework uses Smalltalk language on an Objective-C runtime\nGravel Smalltalk, a Smalltalk implementation for the JVM\nInstantiations, VA Smalltalk being the follow-on to IBM VisualAge Smalltalk\nVisualAge Smalltalk\nLittle Smalltalk\nObject Arts, Dolphin Smalltalk\nObject Connect, Smalltalk MT Smalltalk for Windows\nObjective-Smalltalk, Smalltalk on Objective-C runtime with extensions for Software Architecture\nLSW Vision-Smalltalk have partnered with Object Arts\nPanda Smalltalk, open source engine, written in C, has no dependencies except libc\nPharo Smalltalk, Pharo Project's open-source multi-platform Smalltalk\nPocket Smalltalk, runs on Palm Pilot\nRedline Smalltalk, runs on the Java virtual machine[30]\nRefactory, produces #Smalltalk\nSmalltalk YX\nSmalltalk/X[31]\nSqueak, open source Smalltalk\nCog, JIT VM written in Squeak Smalltalk\nCogDroid, port of non-JIT variant of Cog VM to Android\neToys, eToys visual programming system for learning\niSqueak, Squeak interpreter port for iOS devices, iPhone/iPad\nJSqueak, Squeak interpreter written in Java\nPotato, Squeak interpreter written in Java, a direct derivative of JSqueak\nRoarVM, RoarVM is a multi- and manycore interpreter for Squeak and Pharo\nStrongtalk, for Windows, offers optional strong typing\nVista Smalltalk\nCalmoSoft Project for Vista Smalltalk\nSee also[edit]\nObjective-C\nGLASS (software bundle)\nDistributed Data Management Architecture",
          "type": "compiled",
          "plid": 31
        }
      ],
      "havings": [
        {
          "plid": 1,
          "pdid": 3
        },
        {
          "plid": 4,
          "pdid": 3
        },
        {
          "plid": 2,
          "pdid": 3
        },
        {
          "plid": 2,
          "pdid": 67
        },
        {
          "plid": 2,
          "pdid": 34
        },
        {
          "plid": 2,
          "pdid": 64
        },
        {
          "plid": 5,
          "pdid": 49
        },
        {
          "plid": 5,
          "pdid": 16
        },
        {
          "plid": 5,
          "pdid": 67
        },
        {
          "plid": 5,
          "pdid": 66
        },
        {
          "plid": 6,
          "pdid": 64
        },
        {
          "plid": 6,
          "pdid": 53
        },
        {
          "plid": 7,
          "pdid": 67
        },
        {
          "plid": 7,
          "pdid": 34
        },
        {
          "plid": 7,
          "pdid": 33
        },
        {
          "plid": 7,
          "pdid": 64
        },
        {
          "plid": 8,
          "pdid": 16
        },
        {
          "plid": 9,
          "pdid": 16
        },
        {
          "plid": 10,
          "pdid": 16
        },
        {
          "plid": 10,
          "pdid": 21
        },
        {
          "plid": 10,
          "pdid": 64
        },
        {
          "plid": 11,
          "pdid": 21
        },
        {
          "plid": 13,
          "pdid": 16
        },
        {
          "plid": 14,
          "pdid": 16
        },
        {
          "plid": 14,
          "pdid": 6
        },
        {
          "plid": 15,
          "pdid": 6
        },
        {
          "plid": 15,
          "pdid": 34
        },
        {
          "plid": 15,
          "pdid": 67
        },
        {
          "plid": 16,
          "pdid": 64
        },
        {
          "plid": 16,
          "pdid": 62
        },
        {
          "plid": 16,
          "pdid": 67
        },
        {
          "plid": 16,
          "pdid": 34
        },
        {
          "plid": 16,
          "pdid": 45
        },
        {
          "plid": 17,
          "pdid": 34
        },
        {
          "plid": 17,
          "pdid": 33
        },
        {
          "plid": 17,
          "pdid": 67
        },
        {
          "plid": 18,
          "pdid": 33
        },
        {
          "plid": 18,
          "pdid": 16
        },
        {
          "plid": 18,
          "pdid": 64
        },
        {
          "plid": 19,
          "pdid": 64
        },
        {
          "plid": 19,
          "pdid": 34
        },
        {
          "plid": 19,
          "pdid": 45
        },
        {
          "plid": 19,
          "pdid": 16
        },
        {
          "plid": 20,
          "pdid": 16
        },
        {
          "plid": 21,
          "pdid": 64
        },
        {
          "plid": 21,
          "pdid": 34
        },
        {
          "plid": 22,
          "pdid": 64
        },
        {
          "plid": 22,
          "pdid": 60
        },
        {
          "plid": 22,
          "pdid": 34
        },
        {
          "plid": 22,
          "pdid": 16
        },
        {
          "plid": 23,
          "pdid": 34
        },
        {
          "plid": 23,
          "pdid": 16
        },
        {
          "plid": 23,
          "pdid": 64
        },
        {
          "plid": 24,
          "pdid": 64
        },
        {
          "plid": 24,
          "pdid": 45
        },
        {
          "plid": 24,
          "pdid": 62
        },
        {
          "plid": 25,
          "pdid": 45
        },
        {
          "plid": 25,
          "pdid": 64
        },
        {
          "plid": 25,
          "pdid": 16
        },
        {
          "plid": 25,
          "pdid": 34
        },
        {
          "plid": 26,
          "pdid": 34
        },
        {
          "plid": 26,
          "pdid": 16
        },
        {
          "plid": 26,
          "pdid": 64
        },
        {
          "plid": 26,
          "pdid": 62
        },
        {
          "plid": 26,
          "pdid": 45
        },
        {
          "plid": 26,
          "pdid": 33
        },
        {
          "plid": 27,
          "pdid": 33
        },
        {
          "plid": 27,
          "pdid": 34
        },
        {
          "plid": 27,
          "pdid": 16
        },
        {
          "plid": 27,
          "pdid": 64
        },
        {
          "plid": 27,
          "pdid": 45
        },
        {
          "plid": 28,
          "pdid": 45
        },
        {
          "plid": 28,
          "pdid": 64
        },
        {
          "plid": 28,
          "pdid": 34
        },
        {
          "plid": 28,
          "pdid": 16
        },
        {
          "plid": 28,
          "pdid": 33
        },
        {
          "plid": 29,
          "pdid": 64
        },
        {
          "plid": 29,
          "pdid": 34
        },
        {
          "plid": 29,
          "pdid": 16
        },
        {
          "plid": 29,
          "pdid": 45
        },
        {
          "plid": 30,
          "pdid": 16
        },
        {
          "plid": 30,
          "pdid": 64
        },
        {
          "plid": 30,
          "pdid": 34
        },
        {
          "plid": 31,
          "pdid": 64
        }
      ]
    }
  ]
}