{
  "libraries": [
    {
      "id": 1,
      "email": "abc@yahoo.sample.com",
      "name": "Sample library",
      "public": true,
      "paradigms": [
        {
          "pdid": 1,
          "name": "Action",
          "details": "In computer science, an action language is a language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world.[1] Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning. The best known action language is PDDL.[2]\n\nAction languages fall into two classes: action description languages and action query languages. Examples of the former include STRIPS, PDDL, Language A (a generalization of STRIPS; the propositional part of Pednault's ADL), Language B (an extension of A adding indirect effects, distinguishing static and dynamic laws) and Language C (which adds indirect effects also, and does not assume that every fluent is automatically \"inertial\"). There are also the Action Query Languages P, Q and R. Several different algorithms exist for converting action languages, and in particular, action language C, to answer set programs.[3][4] Since modern answer-set solvers make use of boolean SAT algorithms to very rapidly ascertain satisfiability, this implies that action languages can also enjoy the progress being made in the domain of boolean SAT solving.\n\nFormal definition[edit]\nAll action languages supplement the definition of a state transition system with a set F of fluents, a set V of values that fluents may take, and a function mapping S × F to V, where S is the set of states of a state transition system.",
          "subparadigms": []
        },
        {
          "pdid": 2,
          "name": "Agent-oriented",
          "details": "Agent-oriented programming (AOP) is a programming paradigm where the construction of the software is centered on the concept of software agents. In contrast to object-oriented programming which has objects (providing methods with variable parameters) at its core, AOP has externally specified agents (with interfaces and messaging capabilities) at its core. They can be thought of as abstractions of objects. Exchanged messages are interpreted by receiving \"agents\", in a way specific to its class of agents.\nHistory[edit]\nHistorically the concept of Agent-oriented programming and the idea of centering your software around the concept of agent was first used by Yoav Shoham within his Artificial Intelligence studies, in 1990.[1][2] His agents are specific to his own paradigm as they have just one method, with a single parameter. To quote Yoav Shoham from his paper in 1990 for a basic difference between of AOP against OOP:\n\n...agent-oriented programming (AOP), which can be viewed as a specialization of object-oriented programming. ...\nOOP\tAOP\nBasic unit\tobject\tagent\nParameters defining state of basic unit\tunconstrained\tbeliefs, commitments, capabilities, choices....\nProcess of computation\tmessage passing and response methods\tmessage passing and response methods\nTypes of message\tunconstrained\tinform, request, offer, promise, decline....\nFrameworks[edit]\nThere are multiple AOP 'frameworks' also called Agent Platforms that implement Shoham's programming paradigm. The following examples illustrate how a basic agent is programmed as a Hello World Program.\n\nJADE[edit]\nFor the Java-platform one of the frameworks is JADE [3] (http://jade.tilab.com/). Here is a very basic example [1] of an Agent that runs code\n\npackage helloworld;\nimport jade.core.Agent;\n\npublic class Hello extends Agent {\n\t\n\tprotected void setup() { \n\t\tSystem.out.println(\"Hello World. \");\n\t\tSystem.out.println(\"My name is \"+ getLocalName()); \n\t}\n\t\n\tpublic Hello() {\n\t\tSystem.out.println(\"Constructor called\");\n\t}\n\n}\nAt the core of JADE's AOP model is that its API supports the standard FIPA Agent Communication Language\n\nAgent Speak (Jason)[edit]\nFor a literal translation of Agent Oriented concepts into a scheme unobfuscated as is JADE, behind Java and Object Orientedness, Agent Speak [4] (Jason) provides a \"natural\" language for agents.\n\n\t\n\tstarted.\n\n\t+started <- .print(\"Hello World. \").\nSARL Language[edit]\nSARL[5] (SARL website) provides the fundamental abstractions for coding multiagent systems. It uses a script-like syntax (inspired by Scala and Ruby).\n\npackage helloworld\nimport io.sarl.core.Initialize\nagent HelloWorldAgent {\n        on Initialize {\t\n             println(\"Hello World.\")\n        }\n}\nMiddleware[edit]\nOne way to implement modular or extensible AOP support is to define standard AOP APIs to middleware functions that are themselves implemented as software agents. For example, a directory service can be implemented as a FIPA directory facilitator or DF software agent; life-cycle management to start, stop, suspend and resume agents can be implemented as a FIPA Agent Management Service or AMS agent.[6] A benefit of the AOP approach is that it support more dynamic roles between different users and providers of applications, services and networks. For example, traditionally, networks and services were usually managed by the network and service provider on behalf of the customer and offered as a single virtual network service but customers themselves are becoming more empowered to integrate and manage their own services. This can be achieved via AOP and APIs to middleware agents that can flexibly and dynamically manage communication.[7]",
          "subparadigms": []
        },
        {
          "pdid": 3,
          "name": "Array-oriented",
          "details": "In computer science, array programming languages (also known as vector or multidimensional languages) generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.\n\nArray programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon to find array programming language one-liners that require more than a couple of pages of Java code.[1]\n\nModern programming languages that support array programming are commonly used in scientific and engineering settings; these include Fortran 90, Mata, MATLAB, Analytica, TK Solver (as lists), Octave, R, Cilk Plus, Julia, and the NumPy extension to Python. In these languages, an operation that operates on entire arrays can be called a vectorized operation,[2] regardless of whether it is executed on a vector processor or not.\nContents  [hide] \n1\tConcepts\n2\tUses\n3\tLanguages\n3.1\tScalar languages\n3.2\tArray languages\n3.2.1\tAda\n3.2.2\tAnalytica\n3.2.3\tBASIC\n3.2.4\tMata\n3.2.5\tMATLAB\n3.2.6\trasql\n3.2.7\tR\n4\tMathematical reasoning and language notation\n5\tThird-party libraries\n6\tSee also\n7\tReferences\n8\tExternal links\nConcepts[edit]\nThe fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\n\nIverson described the rationale behind array programming (actually referring to APL) as follows:[3]\n\nmost programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician. [...]\n\nThe thesis [...] is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. [...] it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\n[...] Users of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted, since a clear statement of an algorithm can usually be used as a basis from which one may easily derive more efficient algorithm.\n\nThe basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\n\nFunction rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.\n\nUses[edit]\nArray programming is very well suited to implicit parallelization; a topic of much research nowadays. Further, Intel and compatible CPUs developed and produced after 1997 contained various instruction set extensions, starting from MMX and continuing through SSSE3 and 3DNow!, which include rudimentary SIMD array capabilities. Array processing is distinct from parallel processing in that one physical processor performs operations on a group of items simultaneously while parallel processing aims to split a larger problem into smaller ones (MIMD) to be solved piecemeal by numerous processors. Processors with two or more cores are increasingly common today.\n\nLanguages[edit]\nThe canonical examples of array programming languages are APL, J, and Fortran. Others include: D, A+, Analytica, Chapel, IDL, Julia, K, Q, Mata, Mathematica, MATLAB, MOLSF, NumPy, GNU Octave, PDL, R, S-Lang, SAC, Nial and ZPL.\n\nScalar languages[edit]\nIn scalar languages such as C and Pascal, operations apply only to single values, so a+b expresses the addition of two numbers. In such languages, adding one array to another requires indexing and looping, the coding of which is tedious and error-prone[citation needed].\n\nfor (i = 0; i < n; i++)\n    for (j = 0; j < n; j++)\n        a[i][j] += b[i][j];\nArray languages[edit]\nIn array languages, operations are generalized to apply to both scalars and arrays. Thus, a+b expresses the sum of two scalars if a and b are scalars, or the sum of two arrays if they are arrays.\n\nAn array language simplifies programming but possibly at a cost known as the abstraction penalty.[4][5][6] Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).\n\nAda[edit]\nThe previous C code would become the following in the Ada language,[7] which supports array-programming syntax.\n\n A := A + B;\nAnalytica[edit]\nAnalytica provides the same economy of expression as Ada.\n\n A := A + B;\nThis operation works whether operands, A or B, are scalar or arrays with one more dimensions. Each dimension is identified by an index variable, which controls the nature of the operation. The result has the union of the dimensions of the operands. If A and B have the same dimensions (indexes), the result has those same dimensions. If A and B are vectors with different dimensions, the resulting A is 2-dimensional, containing both dimensions, with each element the sum of the corresponding values of A and B. Variable A must be a local variable; Analytica, as a declarative language, avoids side effects by disallowing assignment to global variables.\n\nBASIC[edit]\nDartmouth BASIC had MAT statements for matrix and array manipulation in its third edition (1966).\n\n DIM A(4),B(4),C(4)\n MAT A = 1\n MAT B = 2*A\n MAT C = A + B\n MAT PRINT A,B,C\nMata[edit]\nStata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.\n\n. mata:\n\n: A = (1,2,3) \\(4,5,6)\n\n: A\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n: B = (2..4) \\(1..3)\n\n: B\n       1   2   3\n    +-------------+\n  1 |  2   3   4  |\n  2 |  1   2   3  |\n    +-------------+\n\n: C = J(3,2,1)           // A 3 by 2 matrix of ones\n\n: C\n       1   2\n    +---------+\n  1 |  1   1  |\n  2 |  1   1  |\n  3 |  1   1  |\n    +---------+\n\n: D = A + B\n\n: D\n       1   2   3\n    +-------------+\n  1 |  3   5   7  |\n  2 |  5   7   9  |\n    +-------------+\n\n: E = A*C\n\n: E\n        1    2\n    +-----------+\n  1 |   6    6  |\n  2 |  15   15  |\n    +-----------+\n\n: F = A:*B\n\n: F\n        1    2    3\n    +----------------+\n  1 |   2    6   12  |\n  2 |   4   10   18  |\n    +----------------+\n\n: G = E :+ 3\n\n: G\n        1    2\n    +-----------+\n  1 |   9    9  |\n  2 |  18   18  |\n    +-----------+\n\n: H = F[(2\\1), (1, 2)]    // Subscripting to get a submatrix of F and\n\n:                         // switch row 1 and 2\n: H\n        1    2\n    +-----------+\n  1 |   4   10  |\n  2 |   2    6  |\n    +-----------+\n\n: I = invsym(F'*F)        // Generalized inverse (F*F^(-1)F=F) of a\n\n:                         // symmetric positive semi-definite matrix\n: I\n[symmetric]\n                 1             2             3\n    +-------------------------------------------+\n  1 |            0                              |\n  2 |            0          3.25                |\n  3 |            0         -1.75   .9444444444  |\n    +-------------------------------------------+\n\n: end\nMATLAB[edit]\nThe implementation in MATLAB allows the same economy allowed by using the Ada language.\n\nA = A + B;\nA variant of the MATLAB language is the GNU Octave language, which extends the original language with augmented assignments:\n\nA += B;\nBoth MATLAB and GNU Octave natively support linear algebra operations such as matrix multiplication, matrix inversion, and the numerical solution of system of linear equations, even using the Moore–Penrose pseudoinverse.[8][9]\n\nThe Nial example of the inner product of two arrays can be implemented using the native matrix multiplication operator. If a is a row vector of size [1 n] and b is a corresponding column vector of size [n 1].\n\na * b;\nThe inner product between two matrices having the same number of elements can be implemented with the auxiliary operator (:), which reshapes a given matrix into a column vector, and the transpose operator ':\n\nA(:)' * B(:);\nrasql[edit]\nThe rasdaman query language is a database-oriented array-programming language. For example, two arrays could be added with the following query:\n\nSELECT A + B\nFROM   A, B\nR[edit]\nThe R language supports array paradigm by default. The following example illustrates a process of multiplication of two matrices followed by an addition of a scalar (which is, in fact, a one-element vector) and a vector:\n\n> A <- matrix(1:6, nrow=2)                              !!this has nrow=2 ... and A has 2 rows\n> A\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n> B <- t( matrix(6:1, nrow=2) )  # t() is a transpose operator                           !!this has nrow=2 ... and B has 3 rows --- a clear contradiction to the definition of A\n> B\n     [,1] [,2]\n[1,]    6    5\n[2,]    4    3\n[3,]    2    1\n> C <- A %*% B\n> C\n     [,1] [,2]\n[1,]   28   19\n[2,]   40   28\n> D <- C + 1\n> D\n     [,1] [,2]\n[1,]   29   20\n[2,]   41   29\n> D + c(1, 1)  # c() creates a vector\n     [,1] [,2]\n[1,]   30   21\n[2,]   42   30\nMathematical reasoning and language notation[edit]\nThe matrix left-division operator concisely expresses some semantic properties of matrices. As in the scalar equivalent, if the (determinant of the) coefficient (matrix) A is not null then it is possible to solve the (vectorial) equation A * x = b by left-multiplying both sides by the inverse of A: A−1 (in both MATLAB and GNU Octave languages: A^-1). The following mathematical statements hold when A is a full rank square matrix:\n\nA^-1 *(A * x)==A^-1 * (b)\n(A^-1 * A)* x ==A^-1 * b       (matrix-multiplication associativity)\nx = A^-1 * b\nwhere == is the equivalence relational operator. The previous statements are also valid MATLAB expressions if the third one is executed before the others (numerical comparisons may be false because of round-off errors).\n\nIf the system is overdetermined - so that A has more rows than columns - the pseudoinverse A+ (in MATLAB and GNU Octave languages: pinv(A)) can replace the inverse A−1, as follows:\n\npinv(A) *(A * x)==pinv(A) * (b)\n(pinv(A) * A)* x ==pinv(A) * b       (matrix-multiplication associativity)\nx = pinv(A) * b\nHowever, these solutions are neither the most concise ones (e.g. still remains the need to notationally differentiate overdetermined systems) nor the most computationally efficient. The latter point is easy to understand when considering again the scalar equivalent a * x = b, for which the solution x = a^-1 * b would require two operations instead of the more efficient x = b / a. The problem is that generally matrix multiplications are not commutative as the extension of the scalar solution to the matrix case would require:\n\n(a * x)/ a ==b / a\n(x * a)/ a ==b / a       (commutativity does not hold for matrices!)\nx * (a / a)==b / a       (associativity also holds for matrices)\nx = b / a\nThe MATLAB language introduces the left-division operator \\ to maintain the essential part of the analogy with the scalar case, therefore simplifying the mathematical reasoning and preserving the conciseness:\n\nA \\ (A * x)==A \\ b\n(A \\ A)* x ==A \\ b       (associativity also holds for matrices, commutativity is no more required)\nx = A \\ b\nThis is not only an example of terse array programming from the coding point of view but also from the computational efficiency perspective, which in several array programming languages benefits from quite efficient linear algebra libraries such as ATLAS or LAPACK.[10][11]\n\nReturning to the previous quotation of Iverson, the rationale behind it should now be evident:\n\nit is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\nThird-party libraries[edit]\nThe use of specialized and efficient libraries to provide more terse abstractions is also common in other programming languages. In C++ several linear algebra libraries exploit the language ability to overload operators. In some cases a very terse abstraction in those languages is explicitly influenced by the array programming paradigm, as the Armadillo and Blitz++ libraries do.[12][13]",
          "subparadigms": []
        },
        {
          "pdid": 4,
          "name": "Automata-based",
          "details": "Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). Sometimes a potentially infinite set of possible states is introduced, and such a set can have a complicated structure, not just an enumeration.\n\nFSM-based programming is generally the same, but, formally speaking, doesn't cover all possible variants, as FSM stands for finite state machine, and automata-based programming doesn't necessarily employ FSMs in the strict sense.\n\nThe following properties are key indicators for automata-based programming:\n\nThe time period of the program's execution is clearly separated down to the steps of the automaton. Each of the steps is effectively an execution of a code section (same for all the steps), which has a single entry point. Such a section can be a function or other routine, or just a cycle body. The step section might be divided down to subsections to be executed depending on different states, although this is not necessary.\nAny communication between the steps is only possible via the explicitly noted set of variables named the state. Between any two steps, the program (or its part created using the automata-based technique) can not have implicit components of its state, such as local (stack) variables' values, return addresses, the current instruction pointer, etc. That is, the state of the whole program, taken at any two moments of entering the step of the automaton, can only differ in the values of the variables being considered as the state of the automaton.\nThe whole execution of the automata-based code is a (possibly explicit) cycle of the automaton's steps.\n\nAnother reason for using the notion of automata-based programming is that the programmer's style of thinking about the program in this technique is very similar to the style of thinking used to solve mathematical tasks using Turing machines, Markov algorithms, etc.\n\nContents  [hide] \n1\tExample\n1.1\tTraditional (imperative) program in C\n1.2\tAutomata-based style program\n1.3\tA separate function for the automation step\n1.4\tExplicit state transition table\n1.5\tAutomation and Automata\n1.5.1\tExample Program\n1.5.2\tAutomation & Events\n1.6\tUsing object-oriented capabilities\n2\tApplications\n3\tHistory\n4\tCompared against imperative and procedural programming\n5\tObject-oriented programming relationship\n6\tSee also\n7\tReferences\n8\tExternal links\nExample[edit]\nConsider a program in C that reads a text from standard input stream, line by line, and prints the first word of each line. It is clear we need first to read and skip the leading spaces, if any, then read characters of the first word and print them until the word ends, and then read and skip all the remaining characters until the end-of-line character is encountered. Upon reaching the end of line character (regardless of the stage), we restart the algorithm from the beginning, and upon encountering the end of file condition (regardless of the stage), we terminate the program.\n\nTraditional (imperative) program in C[edit]\nThe program which solves the example task in traditional (imperative) style can look something like this:\n\n#include <stdio.h>\n#include <ctype.h>\nint main(void)\n{\n    int c;\n    do {\n        do\n            c = getchar();\n        while(c == ' ');\n        while(c != EOF && !isspace(c) && c != '\\n') {\n            putchar(c);\n            c = getchar();\n        }\n        putchar('\\n');\n        while(c != EOF && c != '\\n')\n            c = getchar();\n    } while(c != EOF);\n    return 0;\n}\nAutomata-based style program[edit]\nThe same task can be solved by thinking in terms of finite state machines. Note that line parsing has three stages: skipping the leading spaces, printing the word and skipping the trailing characters. Let's call them states before, inside and after. The program may now look like this:\n\n#include <stdio.h>\n#include <ctype.h>\nint main(void)\n{\n    enum states {\n        before, inside, after\n    } state;\n    int c;\n    state = before;\n    while((c = getchar()) != EOF) {\n        switch(state) {\n            case before:\n                if(c != ' ') {\n                    putchar(c);\n                    if(c != '\\n')\n                        state = inside;\n                }\n                break; \n            case inside:\n                if(!isspace(c))\n                    putchar(c);\n                else {\n                    putchar('\\n');\n                    if(c == '\\n')\n                        state = before;\n                    else\n                        state = after;\n                }\n                break;\n            case after:\n                if(c == '\\n')\n                    state = before;\n        }\n    }\n    return 0;\n}\nAlthough the code now looks longer, it has at least one significant advantage: there's only one reading (that is, call to the getchar() function) instruction in the program. Besides that, there's only one loop instead of the four the previous versions had.\n\nIn this program, the body of the while loop is the automaton step, and the loop itself is the cycle of the automaton's work.\n\nAutomaton's diagram\nThe program implements (models) the work of a finite state machine shown on the picture. The N denotes the end of line character, the S denotes spaces, and the A stands for all the other characters. The automaton follows exactly one arrow on each step depending on the current state and the encountered character. Some state switches are accompanied with printing the character; such arrows are marked with asterisks.\n\nIt is not absolutely necessary to divide the code down to separate handlers for each unique state. Furthermore, in some cases the very notion of the state can be composed of several variables' values, so that it could be impossible to handle each possible state explicitly. In the discussed program it is possible to reduce the code length by noticing that the actions taken in response to the end of line character are the same for all the possible states. The following program is equal to the previous one but is a bit shorter:\n\n#include <stdio.h>\n#include <ctype.h>\nint main(void)\n{\n    enum states {\n        before, inside, after\n    } state;\n    int c;\n    state = before;\n    while((c = getchar()) != EOF) {\n        if(c == '\\n') {\n            putchar('\\n');\n            state = before;\n        } else\n        switch(state) {\n            case before:\n                if(c != ' ') {\n                    putchar(c);\n                    state = inside;\n                }\n                break;\n            case inside:\n                if(c == ' ') {\n                    state = after;\n                } else {\n                    putchar(c);\n                }\n                break;\n            case after:\n                break;\n        }\n    }\n    if(state != before)\n        putchar('\\n');\n    return 0;\n}\nA separate function for the automation step[edit]\nThe most important property of the previous program is that the automaton step code section is clearly localized. With a separate function for it, we can better demonstrate this property:\n\n#include <stdio.h>\nenum states { before, inside, after };\nvoid step(enum states *state, int c)\n{\n    if(c == '\\n') {\n        putchar('\\n');\n        *state = before;\n    } else\n    switch(*state) {\n        case before:\n            if(c != ' ') {\n                putchar(c);\n                *state = inside;\n            }\n            break;\n        case inside:\n            if(c == ' ') {\n                *state = after;\n            } else {\n                putchar(c);\n            }\n            break;\n        case after:\n            break;\n    }\n} \nint main(void)\n{\n    int c;\n    enum states state = before;\n    while((c = getchar()) != EOF) {\n        step(&state, c);\n    }\n    if(state != before)\n        putchar('\\n');\n    return 0;\n}\nThis example clearly demonstrates the basic properties of automata-based code:\n\ntime periods of automaton step executions may not overlap\nthe only information passed from the previous step to the next is the explicitly specified automaton state\nExplicit state transition table[edit]\nA finite automaton can be defined by an explicit state transition table. Generally speaking, an automata-based program code can naturally reflect this approach. In the program below there's an array named the_table, which defines the table. The rows of the table stand for three states, while columns reflect the input characters (first for spaces, second for the end of line character, and the last is for all the other characters).\n\nFor every possible combination, the table contains the new state number and the flag, which determines whether the automaton must print the symbol. In a real life task, this could be more complicated; e.g., the table could contain pointers to functions to be called on every possible combination of conditions.\n\n#include <stdio.h>\nenum states { before = 0, inside = 1, after = 2 };\nstruct branch {\n    unsigned char new_state:2;\n    unsigned char should_putchar:1;\n};\nstruct branch the_table[3][3] = {\n                 /* ' '         '\\n'        others */\n    /* before */ { {before,0}, {before,1}, {inside,1} },\n    /* inside */ { {after, 1}, {before,1}, {inside,1} },\n    /* after  */ { {after, 0}, {before,1}, {after, 0} }\n};\nvoid step(enum states *state, int c)\n{\n    int idx2 = (c == ' ') ? 0 : (c == '\\n') ? 1 : 2;\n    struct branch *b = & the_table[*state][idx2];\n    *state = (enum states)(b->new_state);\n    if(b->should_putchar) putchar(c);\n}\nAutomation and Automata[edit]\nAutomata-based programming indeed closely matches the programming needs found in the field of automation.\n\nA production cycle is commonly modelled as:\n\nA sequence of stages stepping according to input data (from captors).\nA set of actions performed depending on the current stage.\nVarious dedicated programming languages allow expressing such a model in more or less sophisticated ways.\n\nExample Program[edit]\nThe example presented above could be expressed according to this view like in the following program. Here pseudo-code uses such conventions:\n\n'set' and 'reset' respectively activate & inactivate a logic variable (here a stage)\n':' is assignment, '=' is equality test\nSPC : ' '\nEOL : '\\n'\n\nstates : (before, inside, after, end, endplusnl)\n\nsetState(c) {\n    if c=EOF then if inside or after then set endplusnl else set end\n    if before and (c!=SPC and c!=EOL) then set inside\n    if inside and (c=SPC or c=EOL) then set after\n    if after and c=EOL then set before\n}\n\ndoAction(c) {\n    if inside then write(c)\n    else if c=EOL or endplusnl then write(EOL)\n}\n\ncycle {\n    set before\n    loop {\n        c : readCharacter\n        setState(c)\n        doAction(c)\n    }\n    until end or endplusnl\n}\nThe separation of routines expressing cycle progression on one side, and actual action on the other (matching input & output) allows clearer and simpler code.\n\nAutomation & Events[edit]\nIn the field of automation, stepping from step to step depends on input data coming from the machine itself. This is represented in the program by reading characters from a text. In reality, those data inform about position, speed, temperature, etc. of critical elements of a machine.\n\nLike in GUI programming, changes in the machine state can thus be considered as events causing the passage from a state to another, until the final one is reached. The combination of possible states can generate a wide variety of events, thus defining a more complex production cycle. As a consequence, cycles are usually far to be simple linear sequences. There are commonly parallel branches running together and alternatives selected according to different events, schematically represented below:\n\n   s:stage   c:condition\n   \n   s1\n   |\n   |-c2\n   |\n   s2\n   |\n   ----------\n   |        |\n   |-c31    |-c32\n   |        |\n  s31       s32\n   |        |\n   |-c41    |-c42\n   |        |\n   ----------\n   |\n   s4\nUsing object-oriented capabilities[edit]\nIf the implementation language supports object-oriented programming, a simple refactoring is to encapsulate the automaton into an object, thus hiding its implementation details. For example, an object-oriented version in C++ of the same program is below. A more sophisticated refactoring could employ the State pattern.\n\n#include <stdio.h>\nclass StateMachine {\n    enum states { before = 0, inside = 1, after = 2 } state;\n    struct branch {\n        unsigned char new_state:2;\n        unsigned char should_putchar:1;\n    };\n    static struct branch the_table[3][3];\npublic:\n    StateMachine() : state(before) {}\n    void FeedChar(int c) {\n        int idx2 = (c == ' ') ? 0 : (c == '\\n') ? 1 : 2;\n        struct branch *b = & the_table[state][idx2];\n        state = (enum states)(b->new_state);\n        if(b->should_putchar) putchar(c);\n    }\n};\nstruct StateMachine::branch StateMachine::the_table[3][3] = {\n                 /* ' '         '\\n'        others */\n    /* before */ { {before,0}, {before,1}, {inside,1} },\n    /* inside */ { {after, 0}, {before,1}, {inside,1} },\n    /* after  */ { {after, 0}, {before,1}, {after, 0} }\n};\nint main(void)\n{\n    int c;\n    StateMachine machine;\n    while((c = getchar()) != EOF)\n        machine.FeedChar(c);\n    return 0;\n}\nNote: To minimize changes not directly related to the subject of the article, the input/output functions from the standard library of C are being used. Note the use of the ternary operator, which could also be implemented as if-else.\n\nApplications[edit]\nAutomata-based programming is widely used in lexical and syntactic analyses.[1]\n\nBesides that, thinking in terms of automata (that is, breaking the execution process down to automaton steps and passing information from step to step through the explicit state) is necessary for event-driven programming as the only alternative to using parallel processes or threads.\n\nThe notions of states and state machines are often used in the field of formal specification. For instance, UML-based software architecture development uses state diagrams to specify the behaviour of the program. Also various communication protocols are often specified using the explicit notion of state (see, e.g., RFC 793[2]).\n\nThinking in terms of automata (steps and states) can also be used to describe semantics of some programming languages. For example, the execution of a program written in the Refal language is described as a sequence of steps of a so-called abstract Refal machine; the state of the machine is a view (an arbitrary Refal expression without variables).\n\nContinuations in the Scheme language require thinking in terms of steps and states, although Scheme itself is in no way automata-related (it is recursive). To make it possible the call/cc feature to work, implementation needs to be able to catch a whole state of the executing program, which is only possible when there's no implicit part in the state. Such a caught state is the very thing called continuation, and it can be considered as the state of a (relatively complicated) automaton. The step of the automaton is deducing the next continuation from the previous one, and the execution process is the cycle of such steps.\n\nAlexander Ollongren in his book[3] explains the so-called Vienna method of programming languages semantics description which is fully based on formal automata.\n\nThe STAT system [1] is a good example of using the automata-based approach; this system, besides other features, includes an embedded language called STATL which is purely automata-oriented.\n\nHistory[edit]\nAutomata-based techniques were used widely in the domains where there are algorithms based on automata theory, such as formal language analyses.[1]\n\nOne of the early papers on this is by Johnson et al., 1968.[4]\n\nOne of the earliest mentions of automata-based programming as a general technique is found in the paper by Peter Naur, 1963.[5] The author calls the technique Turing machine approach, however no real Turing machine is given in the paper; instead, the technique based on states and steps is described.\n\nCompared against imperative and procedural programming[edit]\nThe notion of state is not exclusive property of automata-based programming.[6] Generally speaking, state (or program state) appears during execution of any computer program, as a combination of all information that can change during the execution. For instance, a state of a traditional imperative program consists of\n\nvalues of all variables and the information stored within dynamic memory\nvalues stored in registers\nstack contents (including local variables' values and return addresses)\ncurrent value of the instruction pointer\nThese can be divided to the explicit part (such as values stored in variables) and the implicit part (return addresses and the instruction pointer).\n\nHaving said this, an automata-based program can be considered as a special case of an imperative program, in which implicit part of the state is minimized. The state of the whole program taken at the two distinct moments of entering the step code section can differ in the automaton state only. This simplifies the analysis of the program.\n\nObject-oriented programming relationship[edit]\nIn the theory of object-oriented programming an object is said to have an internal state and is capable of receiving messages, responding to them, sending messages to other objects and changing the internal state during message handling. In more practical terminology, to call an object's method is considered the same as to send a message to the object.\n\nThus, on the one hand, objects from object-oriented programming can be considered as automata (or models of automata) whose state is the combination of internal fields, and one or more methods are considered to be the step. Such methods must not call each other nor themselves, neither directly nor indirectly, otherwise the object can not be considered to be implemented in an automata-based manner.\n\nOn the other hand, it is obvious that object is good for implementing a model of an automaton. When the automata-based approach is used within an object-oriented language, an automaton model is usually implemented by a class, the state is represented with internal (private) fields of the class, and the step is implemented as a method; such a method is usually the only non-constant public method of the class (besides constructors and destructors). Other public methods could query the state but don't change it. All the secondary methods (such as particular state handlers) are usually hidden within the private part of the class.",
          "subparadigms": []
        },
        {
          "pdid": 5,
          "name": "Relativistic programming",
          "details": "Relativistic programming (RP) is a style of concurrent programming where instead of trying to avoid conflicts between readers and writers (or writers and writers in some cases) the algorithm is designed to tolerate them and get a correct result regardless of the order of events. Also, relativistic programming algorithms are designed to work without the presences of a global order of events. That is, there may be some cases where one thread sees two events in a different order than another thread (hence the term relativistic because in Einstein's theory of special relativity[citation needed] the order of events is not always the same to different viewers).\n\nRelativistic programming provides advantages in performance compared to other concurrency paradigms because it does not require one thread to wait for another nearly as often. Because of this, forms of it (Read-Copy-Update for instance) are now used extensively in the Linux kernel (over 9,000 times as of March 2014 and has grown from nothing to 8% of all locking primitives in about a decade).[1]",
          "subparadigms": []
        },
        {
          "pdid": 6,
          "name": "Concurrent computing",
          "details": "Concurrent computing is a form of computing in which several computations are executed during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or \"thread of control\" for each computation (\"process\"). A concurrent system is one where a computation can advance without waiting for all other computations to complete; where more than one computation can advance at the same time.[1]\n\nAs a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.\n\nContents  [hide] \n1\tIntroduction\n1.1\tCoordinating access to shared resources\n1.2\tAdvantages\n2\tModels\n3\tImplementation\n3.1\tInteraction and communication\n4\tHistory\n5\tPrevalence\n6\tLanguages supporting it\n7\tSee also\n8\tNotes\n9\tReferences\n10\tFurther reading\n11\tExternal links\nIntroduction[edit]\nSee also: Parallel computing\nConcurrent computing is related to but distinct from parallel computing, though these concepts are frequently confused,[2][3] and both can be described as \"multiple processes executing during the same period of time\". In parallel computing, execution occurs at the same physical instant, for example on separate processors of a multi-processor machine, with the goal of speeding up computations—parallel computing is impossible on a (one-core) single processor, as only one computation can occur at any instant (during any single clock cycle).[a] By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.[4]:1\n\nFor example, concurrent processes can be executed on one core by interleaving the execution steps of each process via time-sharing slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.\n\nConcurrent computations may be executed in parallel,[2][5] for example by assigning each process to a separate processor or processor core, or distributing a computation across a network, but in general, the languages, tools and techniques for parallel programming may not be suitable for concurrent programming, and vice versa.\n\nThe exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:\n\nT1 may be executed and finished before T2 or vice versa (serial and sequential);\nT1 and T2 may be executed alternately (serial and concurrent);\nT1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent).\nThe word \"sequential\" is used as an antonym for both \"concurrent\" and \"parallel\"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs.[6] A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called a serial schedule. A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control.\n\nCoordinating access to shared resources[edit]\nThe main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.[5] Potential problems include race conditions, deadlocks, and resource starvation. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource balance:\n\n1 bool withdraw(int withdrawal)\n2 {\n3 \n    if (balance >= withdrawal)\n4     {\n5 \n        balance -= withdrawal;\n6         return true;\n7     } \n8     return false;\n9 }\nSuppose balance = 500, and two concurrent threads make the calls withdraw(300) and withdraw(350). If line 3 in both operations executes before line 5 both operations will find that balance >= withdrawal evaluates to true, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources need the use of concurrency control, or non-blocking algorithms.\n\nBecause concurrent systems rely on the use of shared resources (including communication media), concurrent computing in general needs the use of some form of arbiter somewhere in the implementation to mediate access to these resources.\n\nUnfortunately, while many solutions exist to the problem of a conflict over one resource, many of those \"solutions\" have their own concurrency problems such as deadlock when more than one resource is involved.\n\nAdvantages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)\nIncreased program throughput—parallel execution of a concurrent program allows the number of tasks completed in a given time to increase.\nHigh responsiveness for input/output—input/output-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.\nMore appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes.\nModels[edit]\nThere are several models of concurrent computing, which can be used to understand and analyze concurrent systems. These models include:\n\nActor model\nObject-capability model for security\nPetri nets\nProcess calculi such as\nAmbient calculus\nCalculus of communicating systems (CCS)\nCommunicating sequential processes (CSP)\nπ-calculus\nJoin-calculus\nInput/output automaton\nImplementation[edit]\n[icon]\tThis section needs expansion. You can help by adding to it. (February 2014)\nA number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process.\n\nInteraction and communication[edit]\nIn some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. Explicit communication can be divided into two classes:\n\nShared memory communication\nConcurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). This style of concurrent programming usually needs the use of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. A program that properly implements any of these is said to be thread-safe.\nMessage passing communication\nConcurrent components communicate by exchanging messages (exemplified by Scala, Erlang and occam). The exchange of messages may be carried out asynchronously, or may use a synchronous \"rendezvous\" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as \"send and pray\"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.[citation needed] A wide variety of mathematical theories to understand and analyze message-passing systems are available, including the actor model, and various process calculi. Message passing can be efficiently implemented via symmetric multiprocessing, with or without shared memory cache coherence.\nShared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors.\n\nHistory[edit]\nConcurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via time-division multiplexing (1870s).\n\nThe academic study of concurrent algorithms started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.[7]\n\nPrevalence[edit]\nConcurrency is pervasive in computing, occurring from low-level hardware on a single chip to world-wide networks. Examples follow.\n\nAt the programming language level:\n\nChannel\nCoroutine\nFutures and promises\nAt the operating system level:\n\nComputer multitasking, including both cooperative multitasking and preemptive multitasking\nTime-sharing, which replaced sequential batch processing of jobs with concurrent use of a system\nProcess\nThread\nAt the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.\n\nLanguages supporting it[edit]\nConcurrent programming languages are programming languages that use language constructs for concurrency. These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. Such languages are sometimes described as Concurrency Oriented Languages or Concurrency Oriented Programming Languages (COPL).[8]\n\nToday, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.[citation needed]\n\nMany concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include:\n\nAda—general purpose, with native support for message passing and monitor based concurrency\nAlef—concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs\nAlice—extension to Standard ML, adds support for concurrency via futures\nAteji PX—extension to Java with parallel primitives inspired from π-calculus\nAxum—domain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntax\nC++—std::thread\nCω (C omega)—for research, extends C#, uses asynchronous communication\nC#—supports concurrent computing since version 5.0 using lock, yield, async and await keywords\nClojure—modern Lisp for the JVM\nConcurrent Clean—functional programming, similar to Haskell\nConcurrent Collections (CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and control\nConcurrent Haskell—lazy, pure functional language operating concurrent processes on shared memory\nConcurrent ML—concurrent extension of Standard ML\nConcurrent Pascal—by Per Brinch Hansen\nCurry\nD—multi-paradigm system programming language with explicit support for concurrent programming (actor model)\nE—uses promises to preclude deadlocks\nECMAScript—promises available in various libraries, proposed for inclusion in standard in ECMAScript 6\nEiffel—through its SCOOP mechanism based on the concepts of Design by Contract\nElixir—dynamic and functional meta-programming aware language running on the Erlang VM.\nErlang—uses asynchronous message passing with nothing shared\nFAUST—real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler\nFortran—coarrays and do concurrent are part of Fortran 2008 standard\nGo—for system programming, with a concurrent programming model based on CSP\nHume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing\nIo—actor-based concurrency\nJanus—features distinct askers and tellers to logical variables, bag channels; is purely declarative\nJava—Thread class or Runnable interface.\nJavaScript—via web workers, in a browser environment, promises, and callbacks.\nJoCaml—concurrent and distributed channel based, extension of OCaml, implements the Join-calculus of processes\nJoin Java—concurrent, based on Java language\nJoule—dataflow-based, communicates by message passing\nJoyce—concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen\nLabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language\nLimbo—relative of Alef, for system programming in Inferno (operating system)\nMultiLisp—Scheme variant extended to support parallelism\nModula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutines\nModula-3—modern member of Algol family with extensive support for threads, mutexes, condition variables\nNewsqueak—for research, with channels as first-class values; predecessor of Alef\nNode.js—a server-side runtime environment for JavaScript\noccam—influenced heavily by communicating sequential processes (CSP)\noccam-π—a modern variant of occam, which incorporates ideas from Milner's π-calculus\nOrc—heavily concurrent, nondeterministic, based on Kleene algebra\nOz-Mozart—multiparadigm, supports shared-state and message-passing concurrency, and futures\nParaSail—object-oriented, parallel, free of pointers, race conditions\nPict—essentially an executable implementation of Milner's π-calculus\nPerl with AnyEvent and Coro\nPython with Twisted, greenlet and gevent\nReia—uses asynchronous message passing between shared-nothing objects\nRed/System—for system programming, based on Rebol\nRuby with Concurrent Ruby and Celluloid\nRust—for system programming, focus on massive concurrency, using message-passing with move semantics, shared immutable memory, and shared mutable memory that is provably free of race conditions.[9]\nSALSA—actor-based with token-passing, join, and first-class continuations for distributed computing over the Internet\nScala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way\nSequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions\nSR—for research\nStackless Python\nStratifiedJS—combinator-based concurrency, based on JavaScript\nSuperPascal—concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen\nUnicon—for research\nTermite Scheme—adds Erlang-like concurrency to Scheme\nTNSDL—for developing telecommunication exchanges, uses asynchronous message passing\nVHSIC Hardware Description Language (VHDL)—IEEE STD-1076\nXC—concurrency-extended subset of C language developed by XMOS, based on communicating sequential processes, built-in constructs for programmable I/O\nMany other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.",
          "subparadigms": [
            5
          ]
        },
        {
          "pdid": 7,
          "name": "Concurrent constraint logic programming",
          "details": "Concurrent constraint logic programming is a version of constraint logic programming aimed primarily at programming concurrent processes rather than (or in addition to) solving constraint satisfaction problems. Goals in constraint logic programming are evaluated concurrently; a concurrent process is therefore programmed as the evaluation of a goal by the interpreter.\n\nSyntactically, concurrent constraints logic programs are similar to non-concurrent programs, the only exception being that clauses include guards, which are constraints that may block the applicability of the clause under some conditions. Semantically, concurrent constraint logic programming differs from its non-concurrent versions because a goal evaluation is intended to realize a concurrent process rather than finding a solution to a problem. Most notably, this difference affects how the interpreter behaves when more than one clause is applicable: non-concurrent constraint logic programming recursively tries all clauses; concurrent constraint logic programming chooses only one. This is the most evident effect of an intended directionality of the interpreter, which never revise a choice it has previously taken. Other effects of this are the semantical possibility of having a goal that cannot be proved while the whole evaluation does not fail, and a particular way for equating a goal and a clause head.\n\nConstraint handling rules can be seen as a form of concurrent constraint logic programming, but are used for programming a constraint simplifier or solver rather than concurrent processes.\n\nContents  [hide] \n1\tDescription\n2\tHistory\n3\tSee also\n4\tReferences\nDescription[edit]\nIn constraint logic programming, the goals in the current goal are evaluated sequentially, usually proceeding in a LIFO order in which newer goals are evaluated first. The concurrent version of logic programming allows for evaluating goals in parallel: every goal is evaluated by a process, and processes run concurrently. These processes interact via the constraint store: a process can add a constraint to the constraint store while another one checks whether a constraint is entailed by the store.\n\nAdding a constraint to the store is done like in regular constraint logic programming. Checking entailment of a constraint is done via guards to clauses. Guards require a syntactic extension: a clause of concurrent constraint logic programming is written as H :- G | B where G is a constraint called the guard of the clause. Roughly speaking, a fresh variant of this clause can be used to replace a literal in the goal only if the guard is entailed by the constraint store after the equation of the literal and the clause head is added to it. The precise definition of this rule is more complicated, and is given below.\n\nThe main difference between non-concurrent and concurrent constraint logic programming is that the first is aimed at search, while the second is aimed at implementing concurrent processes. This difference affects whether choices can be undone, whether processes are allowed not to terminate, and how goals and clause heads are equated.\n\nThe first semantical difference between regular and concurrent constraint logic programming is about the condition when more than one clause can be used for proving a goal. Non-concurrent logic programming tries all possible clauses when rewriting a goal: if the goal cannot be proved while replacing it with the body of a fresh variant of a clause, another clause is proved, if any. This is because the aim is to prove the goal: all possible ways to prove the goal are tried. On the other hand, concurrent constraint logic programming aims at programming parallel processes. In general concurrent programming, if a process makes a choice, this choice cannot be undone. The concurrent version of constraint logic programming implements processes by allowing them to take choices, but committing to them once they have been taken. Technically, if more than one clause can be used to rewrite a literal in the goal, the non-concurrent version tries in turn all clauses, while the concurrent version chooses a single arbitrary clause: contrary to the non-concurrent version, the other clauses will never be tried. These two different ways for handling multiple choices are often called \"don't know nondeterminism\" and \"don't care nondeterminism\".\n\nWhen rewriting a literal in the goal, the only considered clauses are those whose guard is entailed by the union of the constraint store and the equation of the literal with the clause head. The guards provide a way for telling which clauses are not to be considered at all. This is particularly important given the commitment to a single clause of concurrent constraint logic programming: once a clause has been chosen, this choice will be never reconsidered. Without guards, the interpreter could choose a \"wrong\" clause to rewrite a literal, while other \"good\" clauses exist. In non-concurrent programming, this is less important, as the interpreter always tries all possibilities. In concurrent programming, the interpreter commits to a single possibility without trying the other ones.\n\nA second effect of the difference between the non-concurrent and the concurrent version is that concurrent constraint logic programming is specifically designed to allow processes to run without terminating. Non-terminating processes are common in general in concurrent processing; the concurrent version of constraint logic programming implements them by not using the condition of failure: if no clause is applicable for rewriting a goal, the process evaluating this goal stops instead of making the whole evaluation fail like in non-concurrent constraint logic programming. As a result, the process evaluating a goal may be stopped because no clause is available to proceed, but at the same time the other processes keep running.\n\nSynchronization among processes that are solving different goals is achieved via the use of guards. If a goal cannot be rewritten because all clauses that could be used have a guard that is not entailed by the constraint store, the process solving this goal is blocked until the other processes add the constraints that are necessary to entail the guard of at least one of the applicable clauses. This synchronization is subject to deadlocks: if all goals are blocked, no new constraints will be added and therefore no goal will ever be unblocked.\n\nA third effect of the difference between concurrent and non-concurrent logic programming is in the way a goal is equated to the head of a fresh variant of a clause. Operationally, this is done by checking whether the variables in the head can be equated to terms in such a way the head is equal to the goal. This rule differs from the corresponding rule for constraint logic programming in that it only allows adding constraints in the form variable=term, where the variable is one of the head. This limitation can be seen as a form of directionality, in that the goal and the clause head are treated differently.\n\nPrecisely, the rule telling whether a fresh variant H:-G|B of a clause can be used to rewrite a goal A is as follows. First, it is checked whether A and H have the same predicate. Second, it is checked whether there exists a way for equating {\\displaystyle A} A with {\\displaystyle H} H given the current constraint store; contrary to regular logic programming, this is done under one-sided unification, which only allows a variable of the head to be equal to a term. Third, the guard is checked for entailment from the constraint store and the equations generated in the second step; the guard may contain variables that are not mentioned in the clause head: these variables are interpreted existentially. This method for deciding the applicability of a fresh variant of a clause for replacing a goal can be compactly expressed as follows: the current constraint store entails that there exists an evaluation of the variables of the head and the guard such that the head is equal to the goal and the guard is entailed. In practice, entailment may be checked with an incomplete method.\n\nAn extension to the syntax and semantics of concurrent logic programming is the atomic tell. When the interpreter uses a clause, its guard is added to the constraint store. However, also added are the constraints of the body. Due to commitment to this clause, the interpreter does not backtrack if the constraints of the body are inconsistent with the store. This condition can be avoided by the use of atomic tell, which is a variant in which the clause contain a sort of \"second guard\" that is only checked for consistency. Such a clause is written H :- G:D|B. This clause is used to rewrite a literal only if G is entailed by the constraint store and D is consistent with it. In this case, both G and D are added to the constraint store.\n\nHistory[edit]\nThe study of concurrent constraint logic programming started at the end of the 1980s, when some of the principles of concurrent logic programming were integrated into constraint logic programming by Michael J. Maher. The theoretical properties of concurrent constraint logic programming were later studied by various authors, such as Vijay A. Saraswat.",
          "subparadigms": []
        },
        {
          "pdid": 8,
          "name": "Constraint logic programming",
          "details": "Constraint logic programming is a form of constraint programming, in which logic programming is extended to include concepts from constraint satisfaction. A constraint logic program is a logic program that contains constraints in the body of clauses. An example of a clause including a constraint is A(X,Y) :- X+Y>0, B(X), C(Y). In this clause, X+Y>0 is a constraint; A(X,Y), B(X), and C(Y) are literals as in regular logic programming. This clause states one condition under which the statement A(X,Y) holds: X+Y is greater than zero and both B(X) and C(Y) are true.\n\nAs in regular logic programming, programs are queried about the provability of a goal, which may contain constraints in addition to literals. A proof for a goal is composed of clauses whose bodies are satisfiable constraints and literals that can in turn be proved using other clauses. Execution is performed by an interpreter, which starts from the goal and recursively scans the clauses trying to prove the goal. Constraints encountered during this scan are placed in a set called constraint store. If this set is found out to be unsatisfiable, the interpreter backtracks, trying to use other clauses for proving the goal. In practice, satisfiability of the constraint store may be checked using an incomplete algorithm, which does not always detect inconsistency.\n\nContents  [hide] \n1\tOverview\n2\tSemantics\n3\tTerms and conditions\n3.1\tTree terms\n3.2\tReals\n3.3\tFinite domains\n4\tThe constraint store\n5\tLabeling\n6\tProgram reformulations\n7\tConstraint handling rules\n8\tBottom-up evaluation\n9\tConcurrent constraint logic programming\n10\tApplications\n11\tHistory\n12\tReferences\n13\tSee also\nOverview[edit]\nFormally, constraint logic programs are like regular logic programs, but the body of clauses can contain constraints, in addition to the regular logic programming literals. As an example, X>0 is a constraint, and is included in the last clause of the following constraint logic program.\n\n B(X,1):- X<0.\n B(X,Y):- X=1, Y>0.\n A(X,Y):- X>0, B(X,Y).\nLike in regular logic programming, evaluating a goal such as A(X,1) requires evaluating the body of the last clause with Y=1. Like in regular logic programming, this in turn requires proving the goal B(X,1). Contrary to regular logic programming, this also requires a constraint to be satisfied: X>0, the constraint in the body of the last clause (in regular logic programming, X>0 cannot be proved unless X is bound to a fully-ground term and execution of the program will fail if that is not the case).\n\nWhether a constraint is satisfied cannot always be determined when the constraint is encountered. In this case, for example, the value of X is not determined when the last clause is evaluated. As a result, the constraint X>0 is not satisfied nor violated at this point. Rather than proceeding in the evaluation of B(X,1) and then checking whether the resulting value of X is positive afterwards, the interpreter stores the constraint X>0 and then proceeds in the evaluation of B(X,1); this way, the interpreter can detect violation of the constraint X>0 during the evaluation of B(X,1), and backtrack immediately if this is the case, rather than waiting for the evaluation of B(X,1) to conclude.\n\nIn general, the evaluation of a constraint logic program proceeds like for a regular logic program, but constraints encountered during evaluation are placed in a set called constraint store. As an example, the evaluation of the goal A(X,1) proceeds by evaluating the body of the first clause with Y=1; this evaluation adds X>0 to the constraint store and requires the goal B(X,1) to be proved. While trying to prove this goal, the first clause is applicable, but its evaluation adds X<0 to the constraint store. This addition makes the constraint store unsatisfiable, and the interpreter backtracks, removing the last addition from the constraint store. The evaluation of the second clause adds X=1 and Y>0 to the constraint store. Since the constraint store is satisfiable and no other literal is left to prove, the interpreter stops with the solution X=1, Y=1.\n\nSemantics[edit]\nThe semantics of constraint logic programs can be defined in terms of a virtual interpreter that maintains a pair {\\displaystyle \\langle G,S\\rangle } \\langle G,S\\rangle  during execution. The first element of this pair is called current goal; the second element is called constraint store. The current goal contains the literals the interpreter is trying to prove and may also contain some constraints it is trying to satisfy; the constraint store contains all constraints the interpreter has assumed satisfiable so far.\n\nInitially, the current goal is the goal and the constraint store is empty. The interpreter proceeds by removing the first element from the current goal and analyzing it. The details of this analysis are explained below, but in the end this analysis may produce a successful termination or a failure. This analysis may involve recursive calls and addition of new literals to the current goal and new constraint to the constraint store. The interpreter backtracks if a failure is generated. A successful termination is generated when the current goal is empty and the constraint store is satisfiable.\n\nThe details of the analysis of a literal removed from the goal is as follows. After having removed this literal from the front of the goal, it is checked whether it is a constraint or a literal. If it is a constraint, it is added to the constraint store. If it is a literal, a clause whose head has the same predicate of the literal is chosen; the clause is rewritten by replacing its variables with new variables (variables not occurring in the goal): the result is called a fresh variant of the clause; the body of the fresh variant of the clause is then placed in front of the goal; the equality of each argument of the literal with the corresponding one of the fresh variant head is placed in front of the goal as well.\n\nSome checks are done during these operations. In particular, the constraint store is checked for consistency every time a new constraint is added to it. In principle, whenever the constraint store is unsatisfiable the algorithm could backtrack. However, checking unsatisfiability at each step would be inefficient. For this reason, an incomplete satisfiability checker may be used instead. In practice, satisfiability is checked using methods that simplify the constraint store, that is, rewrite it into an equivalent but simpler-to-solve form. These methods can sometimes but not always prove unsatisfiability of an unsatisfiable constraint store.\n\nThe interpreter has proved the goal when the current goal is empty and the constraint store is not detected unsatisfiable. The result of execution is the current set of (simplified) constraints. This set may include constraints such as {\\displaystyle X=2} X=2 that force variables to a specific value, but may also include constraints like {\\displaystyle X>2} X>2 that only bound variables without giving them a specific value.\n\nFormally, the semantics of constraint logic programming is defined in terms of derivations. A transition is a pair of pairs goal/store, noted {\\displaystyle \\langle G,S\\rangle \\rightarrow \\langle G',S'\\rangle } \\langle G,S\\rangle \\rightarrow \\langle G',S'\\rangle . Such a pair states the possibility of going from state {\\displaystyle \\langle G,S\\rangle } \\langle G,S\\rangle  to state {\\displaystyle \\langle G',S'\\rangle } \\langle G',S'\\rangle . Such a transition is possible in three possible cases:\n\nan element of {\\displaystyle G} G is a constraint {\\displaystyle C} C, {\\displaystyle G'=G\\backslash \\{C\\}} G'=G\\backslash \\{C\\} and {\\displaystyle S'=S\\cup \\{C\\}} S'=S\\cup \\{C\\}; in other words, a constraint can be moved from the goal to the constraint store\nan element of {\\displaystyle G} G is a literal {\\displaystyle L(t_{1},\\ldots ,t_{n})} L(t_{1},\\ldots ,t_{n}), there exists a clause that, rewritten using new variables, is {\\displaystyle L(t_{1}',\\ldots ,t_{n}'):-B} L(t_{1}',\\ldots ,t_{n}'):-B, {\\displaystyle G'} G' is {\\displaystyle G} G with {\\displaystyle L(t_{1},\\ldots ,t_{n})} L(t_{1},\\ldots ,t_{n}) replaced by {\\displaystyle t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}',B} t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}',B, and {\\displaystyle S'=S} S'=S; in other words, a literal can be replaced by the body of a fresh variant of a clause having the same predicate in the head, adding the body of the fresh variant and the above equalities of terms to the goal\n{\\displaystyle S} S and {\\displaystyle S'} S' are equivalent according to the specific constraint semantics\nA sequence of transitions is a derivation. A goal {\\displaystyle G} G can be proved if there exists a derivation from {\\displaystyle \\langle G,\\emptyset \\rangle } \\langle G,\\emptyset \\rangle  to {\\displaystyle \\langle \\emptyset ,S\\rangle } \\langle \\emptyset ,S\\rangle  for some satisfiable constraint store {\\displaystyle S} S. This semantics formalizes the possible evolutions of an interpreter that arbitrarily chooses the literal of the goal to process and the clause to replace literals. In other words, a goal is proved under this semantics if there exists a sequence of choices of literals and clauses, among the possibly many ones, that lead to an empty goal and satisfiable store.\n\nActual interpreters process the goal elements in a LIFO order: elements are added in the front and processed from the front. They also choose the clause of the second rule according to the order in which they are written, and rewrite the constraint store when it is modified.\n\nThe third possible kind of transition is a replacement of the constraint store with an equivalent one. This replacement is limited to those done by specific methods, such as constraint propagation. The semantics of constraint logic programming is parametric not only to the kind of constraints used but also to the method for rewriting the constraint store. The specific methods used in practice replace the constraint store with one that is simpler to solve. If the constraint store is unsatisfiable, this simplification may detect this unsatisfiability sometimes, but not always.\n\nThe result of evaluating a goal against a constraint logic program is defined if the goal is proved. In this case, there exists a derivation from the initial pair to a pair where the goal is empty. The constraint store of this second pair is considered the result of the evaluation. This is because the constraint store contains all constraints assumed satisfiable to prove the goal. In other words, the goal is proved for all variable evaluations that satisfy these constraints.\n\nThe pairwise equality of terms of two literals is often compactly denoted by {\\displaystyle L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}')} L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}'): this is a shorthand for the constraints {\\displaystyle t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}'} t_{1}=t_{1}',\\ldots ,t_{n}=t_{n}'. A common variant of the semantics for constraint logic programming adds {\\displaystyle L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}')} L(t_{1},\\ldots ,t_{n})=L(t_{1}',\\ldots ,t_{n}') directly to the constraint store rather than to the goal.\n\nTerms and conditions[edit]\nDifferent definitions of terms are used, generating different kinds of constraint logic programming: over trees, reals, or finite domains. A kind of constraint that is always present is the equality of terms. Such constraints are necessary because the interpreter adds t1=t2 to the goal whenever a literal P(...t1...) is replaced with the body of a clause fresh variant whose head is P(...t2...).\n\nTree terms[edit]\nConstraint logic programming with tree terms emulates regular logic programming by storing substitutions as constraints in the constraint store. Terms are variables, constants, and function symbols applied to other terms. The only constraints considered are equalities and disequalities between terms. Equality is particularly important, as constraints like t1=t2 are often generated by the interpreter. Equality constraints on terms can be simplified, that is solved, via unification:\n\nA constraint t1=t2 can be simplified if both terms are function symbols applied to other terms. If the two function symbols are the same and the number of subterms is also the same, this constraint can be replaced with the pairwise equality of subterms. If the terms are composed of different function symbols or the same functor but on different number of terms, the constraint is unsatisfiable.\n\nIf one of the two terms is a variable, the only allowed value the variable can take is the other term. As a result, the other term can replace the variable in the current goal and constraint store, thus practically removing the variable from consideration. In the particular case of equality of a variable with itself, the constraint can be removed as always satisfied.\n\nIn this form of constraint satisfaction, variable values are terms.\n\nReals[edit]\nConstraint logic programming with real numbers uses real expressions as terms. When no function symbols are used, terms are expressions over reals, possibly including variables. In this case, each variable can only take a real number as a value.\n\nTo be precise, terms are expressions over variables and real constants. Equality between terms is a kind of constraint that is always present, as the interpreter generates equality of terms during execution. As an example, if the first literal of the current goal is A(X+1) and the interpreter has chosen a clause that is A(Y-1):-Y=1 after rewriting is variables, the constraints added to the current goal are X+1=Y-1 and {\\displaystyle Y=1} Y=1. The rules of simplification used for function symbols are obviously not used: X+1=Y-1 is not unsatisfiable just because the first expression is built using + and the second using -.\n\nReals and function symbols can be combined, leading to terms that are expressions over reals and function symbols applied to other terms. Formally, variables and real constants are expressions, as any arithmetic operator over other expressions. Variables, constants (zero-arity-function symbols), and expressions are terms, as any function symbol applied to terms. In other words, terms are built over expressions, while expressions are built over numbers and variables. In this case, variables ranges over real numbers and terms. In other words, a variable can take a real number as a value, while another takes a term.\n\nEquality of two terms can be simplified using the rules for tree terms if none of the two terms is a real expression. For example, if the two terms have the same function symbol and number of subterms, their equality constraint can be replaced with the equality of subterms.\n\nFinite domains[edit]\nSee also: Constraint satisfaction problem\nThe third class of constraints used in constraint logic programming is that of finite domains. Values of variables are in this case taken from a finite domain, often that of integer numbers. For each variable, a different domain can be specified: X::[1..5] for example means that the value of X is between 1 and 5. The domain of a variable can also be given by enumerating all values a variable can take; therefore, the above domain declaration can be also written X::[1,2,3,4,5]. This second way of specifying a domain allows for domains that are not composed of integers, such as X::[george,mary,john]. If the domain of a variable is not specified, it is assumed to be the set of integers representable in the language. A group of variables can be given the same domain using a declaration like [X,Y,Z]::[1..5].\n\nThe domain of a variable may be reduced during execution. Indeed, as the interpreter adds constraints to the constraint store, it performs constraint propagation to enforce a form of local consistency, and these operations may reduce the domain of variables. If the domain of a variable becomes empty, the constraint store is inconsistent, and the algorithm backtracks. If the domain of a variable becomes a singleton, the variable can be assigned the unique value in its domain. The forms of consistency typically enforced are arc consistency, hyper-arc consistency, and bound consistency. The current domain of a variable can be inspected using specific literals; for example, dom(X,D) finds out the current domain D of a variable X.\n\nAs for domains of reals, functors can be used with domains of integers. In this case, a term can be an expression over integers, a constant, or the application of a functor over other terms. A variable can take an arbitrary term as a value, if its domain has not been specified to be a set of integers or constants.\n\nThe constraint store[edit]\nThe constraint store contains the constraints that are currently assumed satisfiable. It can be considered what the current substitution is for regular logic programming. When only tree terms are allowed, the constraint store contains constraints in the form t1=t2; these constraints are simplified by unification, resulting in constraints of the form variable=term; such constraints are equivalent to a substitution.\n\nHowever, the constraint store may also contain constraints in the form t1!=t2, if the difference != between terms is allowed. When constraints over reals or finite domains are allowed, the constraint store may also contain domain-specific constraints like X+2=Y/2, etc.\n\nThe constraint store extends the concept of current substitution in two ways. First, it does not only contain the constraints derived from equating a literal with the head of a fresh variant of a clause, but also the constraints of the body of clauses. Second, it does not only contain constraints of the form variable=value but also constraints on the considered constraint language. While the result of a successful evaluation of a regular logic program is the final substitution, the result for a constraint logic program is the final constraint store, which may contain constraint of the form variable=value but in general may contain arbitrary constraints.\n\nDomain-specific constraints may come to the constraint store both from the body of a clauses and from equating a literal with a clause head: for example, if the interpreter rewrites the literal A(X+2) with a clause whose fresh variant head is A(Y/2), the constraint X+2=Y/2 is added to the constraint store. If a variable appears in a real or finite domain expression, it can only take a value in the reals or the finite domain. Such a variable cannot take a term made of a functor applied to other terms as a value. The constraint store is unsatisfiable if a variable is bound to take both a value of the specific domain and a functor applied to terms.\n\nAfter a constraint is added to the constraint store, some operations are performed on the constraint store. Which operations are performed depends on the considered domain and constraints. For example, unification is used for finite tree equalities, variable elimination for polynomial equations over reals, constraint propagation to enforce a form of local consistency for finite domains. These operations are aimed at making the constraint store simpler to be checked for satisfiability and solved.\n\nAs a result of these operations, the addition of new constraints may change the old ones. It is essential that the interpreter is able to undo these changes when it backtracks. The simplest case method is for the interpreter to save the complete state of the store every time it makes a choice (it chooses a clause to rewrite a goal). More efficient methods for allowing the constraint store to return to a previous state exist. In particular, one may just save the changes to the constraint store made between two points of choice, including the changes made to the old constraints. This can be done by simply saving the old value of the constraints that have been modified; this method is called trailing. A more advanced method is to save the changes that have been done on the modified constraints. For example, a linear constraint is changed by modifying its coefficient: saving the difference between the old and new coefficient allows reverting a change. This second method is called semantic backtracking, because the semantics of the change is saved rather than the old version of the constraints only.\n\nLabeling[edit]\nThe labeling literals are used on variables over finite domains to check satisfiability or partial satisfiability of the constraint store and to find a satisfying assignment. A labeling literal is of the form labeling([variables]), where the argument is a list of variables over finite domains. Whenever the interpreter evaluates such a literal, it performs a search over the domains of the variables of the list to find an assignment that satisfies all relevant constraints. Typically, this is done by a form of backtracking: variables are evaluated in order, trying all possible values for each of them, and backtracking when inconsistency is detected.\n\nThe first use of the labeling literal is to actual check satisfiability or partial satisfiability of the constraint store. When the interpreter adds a constraint to the constraint store, it only enforces a form of local consistency on it. This operation may not detect inconsistency even if the constraint store is unsatisfiable. A labeling literal over a set of variables enforces a satisfiability check of the constraints over these variables. As a result, using all variables mentioned in the constraint store results in checking satisfiability of the store.\n\nThe second use of the labeling literal is to actually determine an evaluation of the variables that satisfies the constraint store. Without the labeling literal, variables are assigned values only when the constraint store contains a constraint of the form X=value and when local consistency reduces the domain of a variable to a single value. A labeling literal over some variables forces these variables to be evaluated. In other words, after the labeling literal has been considered, all variables are assigned a value.\n\nTypically, constraint logic programs are written in such a way labeling literals are evaluated only after as many constraints as possible have been accumulated in the constraint store. This is because labeling literals enforce search, and search is more efficient if there are more constraints to be satisfied. A constraint satisfaction problem is typical solved by a constraint logic program having the following structure:\n\nsolve(X):-constraints(X), labeling(X)\nconstraints(X):- (all constraints of the CSP)\nWhen the interpreter evaluates the goal solve(args), it places the body of a fresh variant of the first clause in the current goal. Since the first goal is constraints(X'), the second clause is evaluated, and this operation moves all constraints in the current goal and eventually in the constraint store. The literal labeling(X') is then evaluated, forcing a search for a solution of the constraint store. Since the constraint store contains exactly the constraints of the original constraint satisfaction problem, this operation searches for a solution of the original problem.\n\nProgram reformulations[edit]\nA given constraint logic program may be reformulated to improve its efficiency. A first rule is that labeling literals should be placed after as much constraints on the labeled literals are accumulated in the constraint store. While in theory A(X):-labeling(X),X>0 is equivalent to A(X):-X>0,labeling(X), the search that is performed when the interpreter encounters the labeling literal is on a constraint store that does not contain the constraint X>0. As a result, it may generate solutions, such as X=-1, that are later found out not to satisfy this constraint. On the other hand, in the second formulation the search is performed only when the constraint is already in the constraint store. As a result, search only returns solutions that are consistent with it, taking advantage of the fact that additional constraints reduce the search space.\n\nA second reformulation that can increase efficiency is to place constraints before literals in the body of clauses. Again, A(X):-B(X),X>0 and A(X):-X>0,B(X) are in principle equivalent. However, the first may require more computation. For example, if the constraint store contains the constraint X<-2, the interpreter recursively evaluates B(X) in the first case; if it succeeds, it then finds out that the constraint store is inconsistent when adding X>0. In the second case, when evaluating that clause, the interpreter first adds X>0 to the constraint store and then possibly evaluates B(X). Since the constraint store after the addition of X>0 turns out to be inconsistent, the recursive evaluation of B(X) is not performed at all.\n\nA third reformulation that can increase efficiency is the addition of redundant constrains. If the programmer knows (by whatever means) that the solution of a problem satisfies a specific constraint, they can include that constraint to cause inconsistency of the constraint store as soon as possible. For example, if it is known beforehand that the evaluation of B(X) will result in a positive value for X, the programmer may add X>0 before any occurrence of B(X). As an example, A(X,Y):-B(X),C(X) will fail on the goal A(-2,Z), but this is only found out during the evaluation of the subgoal B(X). On the other hand, if the above clause is replaced by A(X,Y):-X>0,A(X),B(X), the interpreter backtracks as soon as the constraint X>0 is added to the constraint store, which happens before the evaluation of B(X) even starts.\n\nConstraint handling rules[edit]\nConstraint handling rules were initially defined as a stand-alone formalism for specifying constraint solvers, and were later embedded in logic programming. There are two kinds of constraint handling rules. The rules of the first kind specify that, under a given condition, a set of constraints is equivalent to another one. The rules of the second kind specify that, under a given condition, a set of constraints implies another one. In a constraint logic programming language supporting constraint handling rules, a programmer can use these rules to specify possible rewritings of the constraint store and possible additions of constraints to it. The following are example rules:\n\nA(X) <=> B(X) | C(X)\nA(X) ==> B(X) | C(X)\nThe first rule tells that, if B(X) is entailed by the store, the constraint A(X) can be rewritten as C(X). As an example, N*X>0 can be rewritten as X>0 if the store implies that N>0. The symbol <=> resembles equivalence in logic, and tells that the first constraint is equivalent to the latter. In practice, this implies that the first constraint can be replaced with the latter.\n\nThe second rule instead specifies that the latter constraint is a consequence of the first, if the constraint in the middle is entailed by the constraint store. As a result, if A(X) is in the constraint store and B(X) is entailed by the constraint store, then C(X) can be added to the store. Differently from the case of equivalence, this is an addition and not a replacement: the new constraint is added but the old one remains.\n\nEquivalence allows for simplifying the constraint store by replacing some constraints with simpler ones; in particular, if the third constraint in an equivalence rule is true, and the second constraint is entailed, the first constraint is removed from the constraint store. Inference allows for the addition of new constraints, which may lead to proving inconsistency of the constraint store, and may generally reduce the amount of search needed to establish its satisfiability.\n\nLogic programming clauses in conjunction with constraint handling rules can be used to specify a method for establishing the satisfiability of the constraint store. Different clauses are used to implement the different choices of the method; the constraint handling rules are used for rewriting the constraint store during execution. As an example, one can implement backtracking with unit propagation this way. Let holds(L) represents a propositional clause, in which the literals in the list L are in the same order as they are evaluated. The algorithm can be implemented using clauses for the choice of assigning a literal to true or false, and constraint handling rules to specify propagation. These rules specify that holds([l|L]) can be removed if l=true follows from the store, and it can be rewritten as holds(L) if l=false follows from the store. Similarly, holds([l]) can be replaced by l=true. In this example, the choice of value for a variable is implemented using clauses of logic programming; however, it can be encoded in constraint handling rules using an extension called disjunctive constraint handling rules or CHR∨.\n\nBottom-up evaluation[edit]\nThe standard strategy of evaluation of logic programs is top-down and depth-first: from the goal, a number of clauses are identified as being possibly able to prove the goal, and recursion over the literals of their bodies is performed. An alternative strategy is to start from the facts and use clauses to derive new facts; this strategy is called bottom-up. It is considered better than the top-down one when the aim is that of producing all consequences of a given program, rather than proving a single goal. In particular, finding all consequences of a program in the standard top-down and depth-first manner may not terminate while the bottom-up evaluation strategy terminates.\n\nThe bottom-up evaluation strategy maintains the set of facts proved so far during evaluation. This set is initially empty. With each step, new facts are derived by applying a program clause to the existing facts, and are added to the set. For example, the bottom up evaluation of the following program requires two steps:\n\nA(q).\nB(X):-A(X).\nThe set of consequences is initially empty. At the first step, A(q) is the only clause whose body can be proved (because it is empty), and A(q) is therefore added to the current set of consequences. At the second step, since A(q) is proved, the second clause can be used and B(q) is added to the consequences. Since no other consequence can be proved from {A(q),B(q)}, execution terminates.\n\nThe advantage of the bottom-up evaluation over the top-down one is that cycles of derivations do not produce an infinite loop. This is because adding a consequence to the current set of consequences that already contains it has no effect. As an example, adding a third clause to the above program generates a cycle of derivations in the top-down evaluation:\n\nA(q).\nB(X):-A(X).\nA(X):-B(X).\nFor example, while evaluating all answers to the goal A(X), the top-down strategy would produce the following derivations:\n\nA(q)\nA(q):-B(q), B(q):-A(q), A(q)\nA(q):-B(q), B(q):-A(q), A(q):-B(q), B(q):-A(q), A(q)\nIn other words, the only consequence A(q) is produced first, but then the algorithm cycles over derivations that do not produce any other answer. More generally, the top-down evaluation strategy may cycle over possible derivations, possibly when other ones exist.\n\nThe bottom-up strategy does not have the same drawback, as consequences that were already derived has no effect. On the above program, the bottom-up strategy starts adding A(q) to the set of consequences; in the second step, B(X):-A(X) is used to derive B(q); in the third step, the only facts that can be derived from the current consequences are A(q) and B(q), which are however already in the set of consequences. As a result, the algorithm stops.\n\nIn the above example, the only used facts were ground literals. In general, every clause that only contains constraints in the body is considered a fact. For example, a clause A(X):-X>0,X<10 is considered a fact as well. For this extended definition of facts, some facts may be equivalent while not syntactically equal. For example, A(q) is equivalent to A(X):-X=q and both are equivalent to A(X):-X=Y, Y=q. To solve this problem, facts are translated into a normal form in which the head contains a tuple of all-different variables; two facts are then equivalent if their bodies are equivalent on the variables of the head, that is, their sets of solutions are the same when restricted to these variables.\n\nAs described, the bottom-up approach has the advantage of not considering consequences that have already been derived. However, it still may derive consequences that are entailed by those already derived while not being equal to any of them. As an example, the bottom up evaluation of the following program is infinite:\n\nA(0).\nA(X):-X>0.\nA(X):-X=Y+1, A(Y).\nThe bottom-up evaluation algorithm first derives that A(X) is true for X=0 and X>0. In the second step, the first fact with the third clause allows for the derivation of A(1). In the third step, A(2) is derived, etc. However, these facts are already entailed by the fact that A(X) is true for any nonnegative X. This drawback can be overcome by checking for entailment facts that are to be added to the current set of consequences. If the new consequence is already entailed by the set, it is not added to it. Since facts are stored as clauses, possibly with \"local variables\", entailment is restricted over the variables of their heads.\n\nConcurrent constraint logic programming[edit]\nMain article: Concurrent constraint logic programming\nThe concurrent versions of constraint logic programming are aimed at programming concurrent processes rather than solving constraint satisfaction problems. Goals in constraint logic programming are evaluated concurrently; a concurrent process is therefore programmed as the evaluation of a goal by the interpreter.\n\nSyntactically, concurrent constraints logic programs are similar to non-concurrent programs, the only exception being that clauses includes guards, which are constraints that may block the applicability of the clause under some conditions. Semantically, concurrent constraint logic programming differs from its non-concurrent versions because a goal evaluation is intended to realize a concurrent process rather than finding a solution to a problem. Most notably, this difference affects how the interpreter behaves when more than one clause is applicable: non-concurrent constraint logic programming recursively tries all clauses; concurrent constraint logic programming chooses only one. This is the most evident effect of an intended directionality of the interpreter, which never revises a choice it has previously taken. Other effects of this are the semantical possibility of having a goal that cannot be proved while the whole evaluation does not fail, and a particular way for equating a goal and a clause head.\n\nApplications[edit]\nConstraint logic programming has been applied to a number of fields, such as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, finance, and others.\n\nHistory[edit]\nConstraint logic programming was introduced by Jaffar and Lassez in 1987. They generalized the observation that the term equations and disequations of Prolog II were a specific form of constraints, and generalized this idea to arbitrary constraint languages. The first implementations of this concept were Prolog III, CLP(R), and CHIP.",
          "subparadigms": [
            7
          ]
        },
        {
          "pdid": 9,
          "name": "Constraint programming",
          "details": "In computer science, constraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints. Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found. This makes constraint programming a form of declarative programming. The constraints used in constraint programming are of various kinds: those used in constraint satisfaction problems (e.g. \"A or B is true\"), those solved by the simplex algorithm (e.g. \"x ≤ 5\"), and others. Constraints are usually embedded within a programming language or provided via separate software libraries.\n\nConstraint programming can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. This variant of logic programming is due to Jaffar and Lassez, who extended in 1987 a specific class of constraints that were introduced in Prolog II. The first implementations of constraint logic programming were Prolog III, CLP(R), and CHIP.\n\nInstead of logic programming, constraints can be mixed with functional programming, term rewriting, and imperative languages. Programming languages with built-in support for constraints include Oz (functional programming) and Kaleidoscope (imperative programming). Mostly, constraints are implemented in imperative languages via constraint solving toolkits, which are separate libraries for an existing imperative language.\n\nContents  [hide] \n1\tConstraint logic programming\n2\tPerturbation vs refinement models\n3\tDomains\n4\tConstraint programming libraries for imperative programming languages\n5\tSome languages that support constraint programming\n5.1\tLogic programming based constraint logic languages\n6\tSee also\n7\tReferences\n8\tExternal links\nConstraint logic programming[edit]\nMain article: Constraint logic programming\nConstraint programming is an embedding of constraints in a host language. The first host languages used were logic programming languages, so the field was initially called constraint logic programming. The two paradigms share many important features, like logical variables and backtracking. Today most Prolog implementations include one or more libraries for constraint logic programming.\n\nThe difference between the two is largely in their styles and approaches to modeling the world. Some problems are more natural (and thus, simpler) to write as logic programs, while some are more natural to write as constraint programs.\n\nThe constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time. A problem is typically stated as a state of the world containing a number of unknown variables. The constraint program searches for values for all the variables.\n\nTemporal concurrent constraint programming (TCC) and non-deterministic temporal concurrent constraint programming (MJV) are variants of constraint programming that can deal with time.\n\nPerturbation vs refinement models[edit]\nLanguages for constraint-based programming follow one of two approaches:[1]\n\nRefinement model: variables in the problem are initially unassigned, and each variable is assumed to be able to contain any value included in its range or domain. As computation progresses, values in the domain of a variable are pruned if they are shown to be incompatible with the possible values of other variables, until a single value is found for each variable.\nPerturbation model: variables in the problem are assigned a single initial value. At different times one or more variables receive perturbations (changes to their old value), and the system propagates the change trying to assign new values to other variables that are consistent with the perturbation.\nConstraint propagation in constraint satisfaction problems is a typical example of a refinement model, and spreadsheets are a typical example of a perturbation model.\n\nThe refinement model is more general, as it does not restrict variables to have a single value, it can lead to several solutions to the same problem. However, the perturbation model is more intuitive for programmers using mixed imperative constraint object-oriented languages.[2]\n\nDomains[edit]\nThe constraints used in constraint programming are typically over some specific domains. Some popular domains for constraint programming are:\n\nboolean domains, where only true/false constraints apply (SAT problem)\ninteger domains, rational domains\nlinear domains, where only linear functions are described and analyzed (although approaches to non-linear problems do exist)\nfinite domains, where constraints are defined over finite sets\nmixed domains, involving two or more of the above\nFinite domains is one of the most successful domains of constraint programming. In some areas (like operations research) constraint programming is often identified with constraint programming over finite domains.\n\nAll of the above examples are commonly solved by satisfiability modulo theories (SMT) solvers.\n\nFinite domain solvers are useful for solving constraint satisfaction problems, and are often based on arc consistency or one of its approximations.\n\nThe syntax for expressing constraints over finite domains depends on the host language. The following is a Prolog program that solves the classical alphametic puzzle SEND+MORE=MONEY in constraint logic programming:\n\n% This code works in both YAP and SWI-Prolog using the environment-supplied\n% CLPFD constraint solver library.  It may require minor modifications to work\n% in other Prolog environments or using other constraint solvers.\n:- use_module(library(clpfd)).\nsendmore(Digits) :-\n   Digits = [S,E,N,D,M,O,R,Y],     % Create variables\n   Digits ins 0..9,                % Associate domains to variables\n   S #\\= 0,                        % Constraint: S must be different from 0\n   M #\\= 0,\n   all_different(Digits),          % all the elements must take different values\n                1000*S + 100*E + 10*N + D     % Other constraints\n              + 1000*M + 100*O + 10*R + E\n   #= 10000*M + 1000*O + 100*N + 10*E + Y,\n   label(Digits).                  % Start the search\nThe interpreter creates a variable for each letter in the puzzle. The operator ins is used to specify the domains of these variables, so that they range over the set of values {0,1,2,3, ..., 9}. The constraints S#\\=0 and M#\\=0 means that these two variables cannot take the value zero. When the interpreter evaluates these constraints, it reduces the domains of these two variables by removing the value 0 from them. Then, the constraint all_different(Digits) is considered; it does not reduce any domain, so it is simply stored. The last constraint specifies that the digits assigned to the letters must be such that \"SEND+MORE=MONEY\" holds when each letter is replaced by its corresponding digit. From this constraint, the solver infers that M=1. All stored constraints involving variable M are awakened: in this case, constraint propagation on the all_different constraint removes value 1 from the domain of all the remaining variables. Constraint propagation may solve the problem by reducing all domains to a single value, it may prove that the problem has no solution by reducing a domain to the empty set, but may also terminate without proving satisfiability or unsatisfiability. The label literals are used to actually perform search for a solution.\n\nConstraint programming libraries for imperative programming languages[edit]\nConstraint programming is often realized in imperative programming via a separate library. Some popular libraries for constraint programming are:\n\nArtelys Kalis (C++, Java, Python library, FICO Xpress module, proprietary)\nCassowary (Smalltalk, C++, Java, Python, JavaScript, Ruby library, free software: LGPL, no longer maintained)\nCHIP V5 C++ and C libraries (proprietary)\nChoco (Java library, free software: X11 style)\nComet (C style language for constraint programming, constraint-based local search and mathematical programming, free binaries available for academic use)\nCream (Java library, free software: LGPL)\nDisolver (C++ library, proprietary)\nGecode (C++ library, Python bindings, free software: X11 style)\nGoogle or-tools (Python, Java, C++ and .NET library, Apache license)\nGurobi\nIBM ILOG CP (C++ library, proprietary) and CP Optimizer (C++, Java, .NET libraries, proprietary) successor[3] of ILOG Solver, which was considered the market leader in commercial constraint programming software as of 2006[4]\nJaCoP (Java library, open source) available here\nJOpt (Java library, free software)\nJSR-331 (Java Constraint Programming API, JCP standard)\nKoalog Constraint Solver (Java library, proprietary)\nNumberjack (Python platform, free software: LGPL)\nMinion (C++ program, GPL)\npython-constraint (Python library, GPL)\nOscaR (Scala library, LGPL)\nScarab (Scala library, BSD license)\nSMOCS (Scala Monadic library, BSD license)\nOptaPlanner (Java library, Apache license)\nZ3 (C++ solver with C, Java, C#, and Python bindings, MIT license)\nSome languages that support constraint programming[edit]\nAIMMS, an algebraic modeling language with support for constraint programming.[5]\nAlma-0 a small, strongly typed, constraint language with a limited number of features inspired by logic programming, supporting imperative programming.\nAMPL, an algebraic modeling language with support for constraint programming.[6]\nBabelsberg a family of object-constraint programming languages for Ruby, JavaScript, Squeak, and Python.[7]\nBertrand a language for building constraint programming systems.\nCommon Lisp via Screamer (a free software library which provides backtracking and CLP(R), CHiP features).\nConstraint Handling Rules\nMiniZinc (a high-level constraint programming system, BSD-style license)\nKaleidoscope, an object-oriented imperative constraint programming language.\nOz\nClaire\nCurry (Haskell based, with free implementations)\nSystemVerilog Computer hardware simulation language has built in constraint solver.\nWolfram Language\nLogic programming based constraint logic languages[edit]\nB-Prolog (Prolog-based, proprietary)\nCHIP V5[8] (Prolog-based, also includes C++ and C libraries, proprietary)\nCiao (Prolog-based, Free software: GPL/LGPL)\nECLiPSe (Prolog-based, open source)\nSICStus (Prolog-based, proprietary)\nGNU Prolog (free software)\nPicat (open C source)\nYAP Prolog[1]\nSWI Prolog a free Prolog system containing several libraries for constraint solving\nJekejeke Minlog (Prolog-based, proprietary)\nF1 Compiler (proprietary no-cost software)",
          "subparadigms": [
            8
          ]
        },
        {
          "pdid": 10,
          "name": "Flow-based programming",
          "details": "In computer programming, flow-based programming (FBP) is a programming paradigm that defines applications as networks of \"black box\" processes, which exchange data across predefined connections by message passing, where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. FBP is thus naturally component-oriented.\n\nFBP is a particular form of dataflow programming based on bounded buffers, information packets with defined lifetimes, named ports, and separate definition of connections.\n\nContents  [hide] \n1\tIntroduction\n2\tHistory\n3\tConcepts\n4\tExamples\n4.1\t\"Telegram Problem\"\n4.2\tBatch update\n4.3\tMultiplexing processes\n4.4\tSimple interactive network\n5\tComparison with other paradigms and methodologies\n5.1\tJackson Structured Programming (JSP) and Jackson System Development (JSD)\n5.2\tApplicative programming\n5.3\tLinda\n5.4\tObject-oriented programming\n6\tSee also\n7\tReferences\n8\tExternal links\nIntroduction[edit]\nFlow-based programming defines applications using the metaphor of a \"data factory\". It views an application not as a single, sequential process, which starts at a point in time, and then does one thing at a time until it is finished, but as a network of asynchronous processes communicating by means of streams of structured data chunks, called \"information packets\" (IPs). In this view, the focus is on the application data and the transformations applied to it to produce the desired outputs. The network is defined externally to the processes, as a list of connections which is interpreted by a piece of software, usually called the \"scheduler\".\n\nThe processes communicate by means of fixed-capacity connections. A connection is attached to a process by means of a port, which has a name agreed upon between the process code and the network definition. More than one process can execute the same piece of code. At any point in time, a given IP can only be \"owned\" by a single process, or be in transit between two processes. Ports may either be simple, or array-type, as used e.g. for the input port of the Collate component described below. It is the combination of ports with asynchronous processes that allows many long-running primitive functions of data processing, such as Sort, Merge, Summarize, etc., to be supported in the form of software black boxes.\n\nBecause FBP processes can continue executing as long they have data to work on and somewhere to put their output, FBP applications generally run in less elapsed time than conventional programs, and make optimal use of all the processors on a machine, with no special programming required to achieve this.[citation needed]\n\nThe network definition is usually diagrammatic, and is converted into a connection list in some lower-level language or notation. FBP is thus a visual programming language at this level. More complex network definitions have a hierarchical structure, being built up from subnets with \"sticky\" connections.\n\nFBP has much in common with the Linda[1] language in that it is, in Gelernter and Carriero's terminology, a \"coordination language\":[2] it is essentially language-independent. Indeed, given a scheduler written in a sufficiently low-level language, components written in different languages can be linked together in a single network. FBP thus lends itself to the concept of domain-specific languages or \"mini-languages\".\n\nFBP exhibits \"data coupling\", described in the article on coupling as the loosest type of coupling between components. The concept of loose coupling is in turn related to that of service-oriented architectures, and FBP fits a number of the criteria for such an architecture, albeit at a more fine-grained level than most examples of this architecture.\n\nFBP promotes high-level, functional style of specifications that simplify reasoning about system behavior. An example of this is the distributed data flow model for constructively specifying and analyzing the semantics of distributed multi-party protocols.\n\nHistory[edit]\nFlow-Based Programming was invented by J. Paul Morrison in the early 1970s, and initially implemented in software for a Canadian bank.[3] FBP at its inception was strongly influenced by some IBM simulation languages of the period, in particular GPSS, but its roots go all the way back to Conway's seminal paper on what he called coroutines.[4]\n\nFBP has undergone a number of name changes over the years: the original implementation was called AMPS (Advanced Modular Processing System). One large application in Canada went live in 1975, and, as of 2013, has been in continuous production use, running daily, for almost 40 years. Because IBM considered the ideas behind FBP \"too much like a law of nature\" to be patentable they instead put the basic concepts of FBP into the public domain, by means of a Technical Disclosure Bulletin, \"Data Responsive Modular, Interleaved Task Programming System\",[5] in 1971.[3] An article describing its concepts and experience using it was published in 1978 in the IBM Research IBM Systems Journal under the name DSLM.[6] A second implementation was done as a joint project of IBM Canada and IBM Japan, under the name \"Data Flow Development Manager\" (DFDM), and was briefly marketed in Japan in the late '80s under the name \"Data Flow Programming Manager\".\n\nGenerally the concepts were referred to within IBM as \"Data Flow\", but this term was felt to be too general, and eventually the name flow-based programming was adopted.\n\nFrom the early '80s to 1993 J. Paul Morrison and IBM architect Wayne Stevens refined and promoted the concepts behind FBP. Stevens wrote several articles describing and supporting the FBP concept, and included material about it in several of his books.[7][8][non-primary source needed][9][non-primary source needed]. In 1994 Morrison published a book describing FBP, and providing empirical evidence that FBP led to reduced development times.[10]\n\nIn 2013, a European FBP-based project called NoFlo was implemented in Javascript. It is being used mainly in the area of web site design incorporating a number of AI concepts.[11]\n\nConcepts[edit]\nThe following diagram shows the major entities of an FBP diagram (apart from the Information Packets). Such a diagram can be converted directly into a list of connections, which can then be executed by an appropriate engine (software or hardware).\n\n\nSimple FBP diagram\nA, B and C are processes executing code components. O1, O2, and the two INs are ports connecting the connections M and N to their respective processes. It is permitted for processes B and C to be executing the same code, so each process must have its own set of working storage, control blocks, etc. Whether or not they do share code, B and C are free to use the same port names, as port names only have meaning within the components referencing them (and at the network level, of course).\n\nM and N are what are often referred to as \"bounded buffers\", and have a fixed capacity in terms of the number of IPs that they can hold at any point in time.\n\nThe concept of ports is what allows the same component to be used at more than one place in the network. In combination with a parametrization ability, called Initial Information Packets (IIPs), ports provide FBP with a component reuse ability, making FBP a component-based architecture. FBP thus exhibits what Raoul de Campo and Nate Edwards of IBM Research have termed configurable modularity.\n\nInformation Packets or IPs are allocated in what might be called \"IP space\" (just as Linda's tuples are allocated in \"tuple space\"), and have a well-defined lifetime until they are disposed of and their space is reclaimed - in FBP this must be an explicit action on the part of an owning process. IPs traveling across a given connection (actually it is their \"handles\" that travel) constitute a \"stream\", which is generated and consumed asynchronously - this concept thus has similarities to the lazy cons concept described in the 1976 article by Friedman and Wise.[12]\n\nIPs are usually structured chunks of data - some IPs, however, may not contain any real data, but are used simply as signals. An example of this is \"bracket IPs\", which can be used to group data IPs into sequential patterns within a stream, called \"substreams\". Substreams may in turn be nested. IPs may also be chained together to form \"IP trees\", which travel through the network as single objects.\n\nThe system of connections and processes described above can be \"ramified\" to any size. During the development of an application, monitoring processes may be added between pairs of processes, processes may be \"exploded\" to subnets, or simulations of processes may be replaced by the real process logic. FBP therefore lends itself to rapid prototyping.\n\nThis is really an assembly line image of data processing: the IPs travelling through a network of processes may be thought of as widgets travelling from station to station in an assembly line. \"Machines\" may easily be reconnected, taken off line for repair, replaced, and so on. Oddly enough, this image is very similar to that of unit record equipment that was used to process data before the days of computers, except that decks of cards had to be hand-carried from one machine to another.\n\nImplementations of FBP may be non-preemptive or preemptive - the earlier implementations tended to be non-preemptive (mainframe and C language), whereas the latest Java implementation (see below) uses Java Thread class and is preemptive.\n\nExamples[edit]\n\"Telegram Problem\"[edit]\nFBP components often form complementary pairs. This example uses two such pairs. The problem described seems very simple as described in words, but in fact is surprisingly difficult to accomplish using conventional procedural logic. The task, called the \"Telegram Problem\", originally described by Peter Naur, is to write a program which accepts lines of text and generates output lines containing as many words as possible, where the number of characters in each line does not exceed a certain length. The words may not be split and we assume no word is longer than the size of the output lines. This is analogous to the word-wrapping problem in text editors.[13]\n\nIn conventional logic, the programmer rapidly discovers that neither the input nor the output structures can be used to drive the call hierarchy of control flow. In FBP, on the other hand, the problem description itself suggests a solution:\n\n\"words\" are mentioned explicitly in the description of the problem, so it is reasonable for the designer to treat words as information packets (IPs)\nin FBP there is no single call hierarchy, so the programmer is not tempted to force a sub-pattern of the solution to be the top level.\nHere is the most natural solution in FBP (there is no single \"correct\" solution in FBP, but this seems like a natural fit):\n\n\nPeter Naur's \"Telegram problem\"\nwhere DC and RC stand for \"DeCompose\" and \"ReCompose\", respectively.\n\nAs mentioned above, Initial Information Packets (IIPs) can be used to specify parametric information such as the desired output record length (required by the rightmost two components), or file names. IIPs are data chunks associated with a port in the network definition which become \"normal\" IPs when a \"receive\" is issued for the relevant port.\n\nBatch update[edit]\nThis type of program involves passing a file of \"details\" (changes, adds and deletes) against a \"master file\", and producing (at least) an updated master file, and one or more reports. Update programs are generally quite hard to code using synchronous, procedural code, as two (sometimes more) input streams have to be kept synchronized, even though there may be masters without corresponding details, or vice versa.\n\n\nCanonical \"batch update\" structure\nIn FBP, a reusable component (Collate), based on the unit record idea of a Collator, makes writing this type of application much easier as Collate merges the two streams and inserts bracket IPs to indicate grouping levels, significantly simplifying the downstream logic. Suppose that one stream (\"masters\" in this case) consists of IPs with key values of 1, 2 and 3, and the second stream IPs (\"details\") have key values of 11, 12, 21, 31, 32, 33 and 41, where the first digit corresponds to the master key values. Using bracket characters to represent \"bracket\" IPs, the collated output stream will be as follows:\n\n( m1 d11 d12 ) ( m2 d21 ) ( m3 d31 d32 d33 ) (d41)\nAs there was no master with a value of 4, the last group consists of a single detail (plus brackets).\n\nThe structure of the above stream can be described succinctly using a BNF-like notation such as\n\n{ ( [m] d* ) }*\nCollate is a reusable black box which only needs to know where the control fields are in its incoming IPs (even this is not strictly necessary as transformer processes can be inserted upstream to place the control fields in standard locations), and can in fact be generalized to any number of input streams, and any depth of bracket nesting. Collate uses an array-type port for input, allowing a variable number of input streams.\n\nMultiplexing processes[edit]\nFlow-based programming supports process multiplexing in a very natural way. Since components are read-only, any number of instances of a given component (\"processes\") can run asynchronously with each other.\n\n\nExample of multiplexing\nWhen computers usually had a single processor, this was useful when a lot of I/O was going on; now that machines usually have multiple processors, this is starting to become useful when processes are CPU-intensive as well. The diagram in this section shows a single \"Load Balancer\" process distributing data between 3 processes, labeled S1, S2 and S3, respectively, which are instances of a single component, which in turn feed into a single process on a \"first-come, first served\" basis.\n\nSimple interactive network[edit]\n\nSchematic of general interactive application\nIn this general schematic, requests (transactions) coming from users enter the diagram at the upper left, and responses are returned at the lower left. The \"back ends\" (on the right side) communicate with systems at other sites, e.g. using CORBA, MQSeries, etc. The cross-connections represent requests that do not need to go to the back ends, or requests that have to cycle through the network more than once before being returned to the user.\n\nAs different requests may use different back-ends, and may require differing amounts of time for the back-ends (if used) to process them, provision must be made to relate returned data to the appropriate requesting transactions, e.g. hash tables or caches.\n\nThe above diagram is schematic in the sense that the final application may contain many more processes: processes may be inserted between other processes to manage caches, display connection traffic, monitor throughput, etc. Also the blocks in the diagram may represent \"subnets\" - small networks with one or more open connections.\n\nComparison with other paradigms and methodologies[edit]\nJackson Structured Programming (JSP) and Jackson System Development (JSD)[edit]\nMain articles: Jackson Structured Programming and Jackson System Development\nThis methodology assumes that a program must be structured as a single procedural hierarchy of subroutines. Its starting point is to describe the application as a set of \"main lines\", based on the input and output data structures. One of these \"main lines\" is then chosen to drive the whole program, and the others are required to be \"inverted\" to turn them into subroutines (hence the name \"Jackson inversion\"). This sometimes results in what is called a \"clash\", requiring the program to be split into multiple programs or coroutines. When using FBP, this inversion process is not required, as every FBP component can be considered a separate \"main line\".\n\nFBP and JSP share the concept of treating a program (or some components) as a parser of an input stream.\n\nIn Jackson's later work, Jackson System Development (JSD), the ideas were developed further.[14][15]\n\nIn JSD the design is maintained as a network design until the final implementation stage. The model is then transformed into a set of sequential processes to the number of available processors. Jackson discusses the possibility of directly executing the network model that exists prior to this step, in section 1.3 of his book (italics added):\n\nThe specification produced at the end of the System Timing step is, in principle, capable of direct execution. The necessary environment would contain a processor for each process, a device equivalent to an unbounded buffer for each data stream, and some input and output devices where the system is connected to the real world. Such an environment could, of course, be provided by suitable software running on a sufficiently powerful machine. Sometimes, such direct execution of the specification will be possible, and may even be a reasonable choice.[15]\nFBP was recognized by M A Jackson as an approach that follows his method of \"Program decomposition into sequential processes communicating by a coroutine-like mechanism\" [16]\n\nApplicative programming[edit]\nMain article: Applicative programming language\nW.B. Ackerman defines an applicative language as one which does all of its processing by means of operators applied to values.[17] The earliest known applicative language was LISP.\n\nAn FBP component can be regarded as a function transforming its input stream(s) into its output stream(s). These functions are then combined to make more complex transformations, as shown here:\n\n\nTwo functions feeding one\nIf we label streams, as shown, with lower case letters, then the above diagram can be represented succinctly as follows:\n\nc = G(F(a),F(b));\nJust as in functional notation F can be used twice because it only works with values, and therefore has no side effects, in FBP two instances of a given component may be running concurrently with each other, and therefore FBP components must not have side-effects either. Functional notation could clearly be used to represent at least a part of an FBP network.\n\nThe question then arises whether FBP components can themselves be expressed using functional notation. W.H. Burge showed how stream expressions can be developed using a recursive, applicative style of programming, but this work was in terms of (streams of) atomic values.[18] In FBP, it is necessary to be able to describe and process structured data chunks (FBP IPs).\n\nFurthermore, most applicative systems assume that all the data is available in memory at the same time, whereas FBP applications need to be able to process long-running streams of data while still using finite resources. Friedman and Wise suggested a way to do this by adding the concept of \"lazy cons\" to Burge's work. This removed the requirement that both of the arguments of \"cons\" be available at the same instant of time. \"Lazy cons\" does not actually build a stream until both of its arguments are realized - before that it simply records a \"promise\" to do this. This allows a stream to be dynamically realized from the front, but with an unrealized back end. The end of the stream stays unrealized until the very end of the process, while the beginning is an ever-lengthening sequence of items.\n\nLinda[edit]\nMain article: Linda\nMany of the concepts in FBP seem to have been discovered independently in different systems over the years. Linda, mentioned above, is one such. The difference between the two techniques is illustrated by the Linda \"school of piranhas\" load balancing technique - in FBP, this requires an extra \"load balancer\" component which routes requests to the component in a list which has the smallest number of IPs waiting to be processed. Clearly FBP and Linda are closely related, and one could easily be used to simulate the other.\n\nObject-oriented programming[edit]\nMain article: Object-oriented programming\nAn object in OOP can be described as a semi-autonomous unit comprising both information and behaviour. Objects communicate by means of \"method calls\", which are essentially subroutine calls, done indirectly via the class to which the receiving object belongs. The object's internal data can only be accessed by means of method calls, so this is a form of information hiding or \"encapsulation\". Encapsulation, however, predates OOP - David Parnas wrote one of the seminal articles on it in the early 70s [19] - and is a basic concept in computing. Encapsulation is the very essence of an FBP component, which may be thought of as a black box, performing some conversion of its input data into its output data. In FBP, part of the specification of a component is the data formats and stream structures that it can accept, and those it will generate. This constitutes a form of design by contract. In addition, the data in an IP can only be accessed directly by the currently owning process. Encapsulation can also be implemented at the network level, by having outer processes protect inner ones.\n\nA paper by C. Ellis and S. Gibbs distinguishes between active objects and passive objects.[20] Passive objects comprise information and behaviour, as stated above, but they cannot determine the timing of this behaviour. Active objects on the other hand can do this. In their article Ellis and Gibbs state that active objects have much more potential for the development of maintainable systems than do passive objects. An FBP application can be viewed as a combination of these two types of object, where FBP processes would correspond to active objects, while IPs would correspond to passive objects.",
          "subparadigms": []
        },
        {
          "pdid": 11,
          "name": "Spreadsheet",
          "details": "A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form.[1][2][3] Spreadsheets are developed as computerized simulations of paper accounting worksheets.[4] The program operates on data entered in cells of a table. Each cell may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells. A spreadsheet may also refer to one such electronic document.[5][6][7]\n\nSpreadsheet users can adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for \"what-if\" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.\n\nBesides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.\n\nSpreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.\n\nLANPAR, available in 1969,[8] was the first electronic spreadsheet on mainframe and time sharing computers. LANPAR was an acronym: LANguage for Programming Arrays at Random.[8] VisiCalc was the first electronic spreadsheet on a microcomputer,[9] and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system.[10] Excel now has the largest market share on the Windows and Macintosh platforms.[11][12][13] A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.\n\nContents  [hide] \n1\tUsage\n2\tHistory\n2.1\tPaper spreadsheets\n2.2\tEarly implementations\n2.2.1\tBatch spreadsheet report generator\n2.2.2\tLANPAR spreadsheet compiler\n2.2.3\tAutoplan/Autotab spreadsheet programming language\n2.2.4\tIBM Financial Planning and Control System\n2.2.5\tAPLDOT modeling language\n2.3\tVisiCalc\n2.4\tSuperCalc\n2.5\tLotus 1-2-3 and other MS-DOS spreadsheets\n2.6\tMicrosoft Excel\n2.7\tOpen source software\n2.8\tWeb based spreadsheets\n2.9\tOther spreadsheets\n2.10\tOther products\n3\tConcepts\n3.1\tCells\n3.1.1\tValues\n3.1.2\tAutomatic recalculation\n3.1.3\tReal-time update\n3.1.4\tLocked cell\n3.1.5\tData format\n3.1.6\tCell formatting\n3.1.7\tNamed cells\n3.1.7.1\tCell reference\n3.1.7.2\tCell ranges\n3.2\tSheets\n3.3\tFormulas\n3.4\tFunctions\n3.5\tSubroutines\n3.6\tRemote spreadsheet\n3.7\tCharts\n3.8\tMulti-dimensional spreadsheets\n3.9\tLogical spreadsheets\n4\tProgramming issues\n4.1\tEnd-user development\n4.2\tSpreadsheet programs\n5\tShortcomings\n6\tSpreadsheet risk\n7\tSee also\n8\tReferences\n9\tExternal links\nUsage[edit]\nbasic spreadsheet with toolbar\nOpenOffice.org Calc spreadsheet\nA spreadsheet consists of a table of cells arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, \"A\", \"B\", \"C\", etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, \"C10\" for instance. This system of cell references was introduced in VisiCalc, and known as \"A1 notation\". Additionally, spreadsheets have the concept of a range, a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range \"A1:A10\".\n\nIn modern spreadsheet applications, several spreadsheets, often known as worksheets or simply sheets, are gathered together to form a workbook. A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, \"Sheet 1!C10\". Some systems extend this syntax to allow cell references to different workbooks.\n\nUsers interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text hello world, the number 5 or the date 16-Dec-91. A formula would begin with the equals sign, =5*3, but this would normally be invisible because the display shows the result of the calculation, 15 in this case, not the formula itself. This may lead to confusion in some cases.\n\nThe key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula =5*C10 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value 3 the result will be 15. But C10 might also hold its own formula referring to other cells, and so on.\n\nThe ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical steps, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the SUM function that adds up all the numbers within a range.\n\nSpreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable—sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.\n\nA spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.\n\nHistory[edit]\nPaper spreadsheets[edit]\nThe word \"spreadsheet\" came from \"spread\" in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word \"spread-sheet\" came to mean the format used to present book-keeping ledgers—with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect—which were, traditionally, a \"spread\" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed \"analysis paper\") ruled into rows and columns in that format and approximately twice as wide as ordinary paper.[14]\n\nEarly implementations[edit]\nBatch spreadsheet report generator[edit]\nA batch \"spreadsheet\" is indistinguishable from a batch compiler with added input data, producing an output report, i.e., a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper \"Budgeting Models and System Simulation\" by Richard Mattessich.[15] The subsequent work by Mattessich (1964a, Chpt. 9, Accounting and Analytical Methods) and its companion volume, Mattessich (1964b, Simulation of the Firm through a Budget Computer Program) applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual cells.\n\nIn 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled Business Computer Language was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed[16]\n\nApplied Data Resources had a FORTRAN preprocessor called Empires.\n\nIn the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.\n\nLANPAR spreadsheet compiler[edit]\nA key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 U.S. Patent 4,398,249 on spreadsheet automatic natural order recalculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the CCPA (Predecessor Court of the Federal Circuit) overturning the Patent Office in 1983—establishing that \"something does not cease to become patentable merely because the point of novelty is in an algorithm.\" However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.[17]\n\nThe actual software was called LANPAR — LANguage for Programming Arrays at Random.[18] This was conceived and entirely developed in the summer of 1969 following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having computer calculating results in the right order (\"Forward Referencing/Natural Order Calculation\"). Pardo and Landau developed and implemented the software in 1969.[19]\n\nLANPAR was used by Bell Canada, AT&T and the 18 operating telcos nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first \"non-procedural\" computer languages) [20] as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, Supercalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward Referencing/Natural Order Calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.\n\nThe LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions including logical comparisons and \"if/then\" statements could be used in any cell, and cells could be presented in any order.\n\nAutoplan/Autotab spreadsheet programming language[edit]\nIn 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. \"AutoPlan\" ran on GE’s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name AutoTab. (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.)\n\nAutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column. In 1975, Autotab-II was advertised as extending the original to a maximum of \"1,500 rows and columns, combined in any proportion the user requires...\"[21]\n\nIBM Financial Planning and Control System[edit]\nThe IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.\n\nAPLDOT modeling language[edit]\nAn example of an early \"industrial weight\" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD.[22] The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a \"spreadsheet\" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.\n\nVisiCalc[edit]\n\nVisiCalc running on an Apple II\nBecause of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time PC World magazine called VisiCalc the first electronic spreadsheet.[23]\n\nBricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.\n\nVisiCalc went on to become the first killer app,[24][25] an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.\n\nSuperCalc[edit]\nSuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M and was ported to MS-DOS in 1982.\n\nLotus 1-2-3 and other MS-DOS spreadsheets[edit]\nThe acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.[26]\n\nLotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.\n\nMicrosoft Excel[edit]\nMicrosoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported[27] it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3,[14] and in 2013, IBM discontinued Lotus-1-2-3 altogether.[28]\n\nOpen source software[edit]\nGnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the very closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.\n\nWeb based spreadsheets[edit]\nMain article: List of online spreadsheets\nWith the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as EditGrid, Google Sheets, Microsoft Excel Online, Smartsheet, ZK Spreadsheet, or Zoho Office Suite also have strong multi-user collaboration features and/or offer real time updates from remote sources such as stock prices and currency exchange rates.\n\nOther spreadsheets[edit]\nNotable current spreadsheet software:\n\nCalligra Sheets (formerly KCalc)\nCorel Quattro Pro (WordPerfect Office)\nKingsoft Spreadsheets\nMariner Calc and Calc XLS are Mariner Software's spreadsheet applications for Mac OS X and iOS.\nNeoOffice\nNumbers is Apple Inc.'s spreadsheet software, part of iWork.\nPyspread\nZCubes-Calc\nDiscontinued spreadsheet software:\n\n3D-Calc for Atari ST computers\nFramework by Forefront Corporation/Ashton-Tate (1983/84)\nGNU Oleo – A traditional terminal mode spreadsheet for UNIX/UNIX-like systems\nIBM Lotus Symphony (2007)\nJavelin Software\nKCells\nLotus Improv[29]\nLotus Jazz for Macintosh\nLotus Symphony (1984)\nMultiPlan\nClaris' Resolve (Macintosh)\nResolver One\nBorland's Quattro Pro\nSIAG\nSuperCalc\nT/Maker\nTarget Planner Calc for CP/M and TRS-DOS[30][31]\nTrapeze for Macintosh[32]\nWingz for Macintosh\nOther products[edit]\nA number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day.\n\nSpreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.\n\nConcepts[edit]\nThe main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.\n\nCells[edit]\nA \"cell\" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).\n\nMy Spreadsheet\nA\tB\tC\tD\n01\tvalue1\tvalue2\tadded\tmultiplied\n02\t10\t20\t30\t200\nAn array of cells is called a sheet or worksheet. It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).\n\nA cell may contain a value or a formula, or it may simply be left empty. By convention, formulas usually begin with = sign.\n\nValues[edit]\nA value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.\n\nThe Spreadsheet Value Rule\n\nComputer scientist Alan Kay used the term value rule to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.[33] The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.[34]\n\nAutomatic recalculation[edit]\nA standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.\n\nRecalculation generally requires that there are no circular dependencies in a spreadsheet. A dependency graph is a graph that has a vertex for each object to be updated, and an edge connecting two objects whenever one of them needs to be updated earlier than the other. Dependency graphs without circular dependencies form directed acyclic graphs, representations of partial orderings (in this case, across a spreadsheet) that can be relied upon to give a definite result.[35]\n\nReal-time update[edit]\nThis feature refers to updating a cell's contents periodically with a value from an external source—such as a cell in a \"remote\" spreadsheet. For shared, Web-based spreadsheets, it applies to \"immediately\" updating cells another user has updated. All dependent cells must be updated also.\n\nLocked cell[edit]\nOnce entered, selected cells (or the entire spreadsheet) can optionally be \"locked\" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing \"constants\" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.\n\nData format[edit]\nA cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example \"31/12/2007\" or \"31 Dec 2007\" would default to the cell format of date. Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.\n\nSome cell formats such as \"numeric\" or \"currency\" can also specify the number of decimal places.\n\nThis can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.\n\nCell formatting[edit]\nDepending on the capability of the spreadsheet application, each cell (like its counterpart the \"style\" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.\n\nA cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.\n\nNamed cells[edit]\n\nUse of named column variables x & y in Microsoft Excel. Formula for y=x2 resembles Fortran, and Name Manager shows the definitions of x & y.\nIn most implementations, a cell, or group of cells in a column or row, can be \"named\" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.\n\nCell reference[edit]\nIn place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.\n\nA typical cell reference in \"A1\" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A–Z and AA–IV) followed by a row number (e.g., in the range 1–65536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative \"R1C1\" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.\n\nWhen the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).\n\nA cell on the same \"sheet\" is usually addressed as:\n\n=A1\nA cell on a different sheet of the same spreadsheet is usually addressed as:\n\n=SHEET2!A1             (that is; the first cell in sheet 2 of same spreadsheet).\nSome spreadsheet implementations in Excel allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:\n\n='C:\\Documents and Settings\\Username\\My spreadsheets\\[main sheet]Sheet1!A1\nIn a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values—which they often do not.\n\nA circular reference occurs when the formula in one cell refers—directly, or indirectly through a chain of cell references—to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.\n\nCell ranges[edit]\nLikewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as \"=SUM(A1:A6)\" would add all the cells specified and put the result in the cell containing the formula itself.\n\nSheets[edit]\nIn the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.\n\nFormulas[edit]\n\nAnimation of a simple spreadsheet that multiplies values in the left column by 2, then sums the calculated values from the right column to the bottom-most cell. In this example, only the values in the A column are entered (10, 20, 30), and the remainder of cells are formulas. Formulas in the B column multiply values from the A column using relative references, and the formula in B4 uses the SUM() function to find the sum of values in the B1:B3 range.\nA formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by \"clicking\" the mouse over a particular cell; otherwise it contains the result of the calculation.\n\nA formula assigns values to a cell or range of cells, and typically has the format:\n\n=expression\nwhere the expression consists of:\n\nvalues, such as 2, 9.14 or 6.67E-11;\nreferences to other cells, such as, e.g., A1 for a single cell or B1:B3 for a range;\narithmetic operators, such as +, -, *, /, and others;\nrelational operators, such as >=, <, and others; and,\nfunctions, such as SUM(), TAN(), and many others.\nWhen a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., A1, or B1:B3), absolute (e.g., $A$1, or $B$1:$B$3) or mixed row– or column-wise absolute/relative (e.g., $A1 is column-wise absolute and A$1 is row-wise absolute).\n\nThe available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.\n\nA formula may contain a condition (or nested conditions)—with or without an actual calculation—and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.\n\n=IF(SUM(A1:A6) > 100, \"More than 100%\", SUM(A1:A6))\nFurther examples:\n\n=IF(AND(A1<>\"\",B1<>\"\"),A1/B1,\"\") means that if both cells A1 and B1 are not <> empty \"\", then divide A1 by B1 and display, other do not display anything.\n=IF(AND(A1<>\"\",B1<>\"\"),IF(B1<>0,A1/B1,\"Division by zero\"),\"\") means that if cells A1 and B1 are not empty, and B1 is not zero, then divide A1 by B1, if B1 is zero, then display \"Division by zero, and do not display anything if either A1 and B1 are empty.\n=IF(OR(A1<>\"\",B1<>\"\"),\"Either A1 or B1 show text\",\"\") means to display the text if either cells A1 or B1 are not empty.\nThe best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.\n\nA spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.\n\nFunctions[edit]\n\nUse of user-defined function sq(x) in Microsoft Excel.\nSpreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for user-defined functions. In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name sq is user-assigned, and function sq is introduced using the Visual Basic editor supplied with Excel. Name Manager displays the spreadsheet definitions of named variables x & y.\n\nSubroutines[edit]\n\nSubroutine in Microsoft Excel writes values calculated using x into y.\nFunctions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable x, calculates its square, and writes this value into the corresponding element of named column variable y. The y column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.\n\nRemote spreadsheet[edit]\nWhenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a \"remote\" spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.\n\nCharts[edit]\n\nGraph made using Microsoft Excel\nMany spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.\n\nMulti-dimensional spreadsheets[edit]\nIn the late 1980s and early 1990s, first Javelin Software and Lotus Improv appeared. Unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin—the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation.\n\nIn these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets—variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, the program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.\n\nTrapeze,[32] a spreadsheet on the Mac, went further and explicitly supported not just table columns, but also matrix operators.\n\nLogical spreadsheets[edit]\nSpreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.\n\nProgramming issues[edit]\nJust as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.\n\nEnd-user development[edit]\nSpreadsheets are a popular End-user development tool.[36] EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.\n\nThey use spatial relationships to define program relationships. Humans have highly developed intuitions about spaces, and of dependencies between items. Sequential programming usually requires typing line after line of text, which must be read slowly and carefully to be understood and changed.\nThey are forgiving, allowing partial results and functions to work. One or more parts of a program can work correctly, even if other parts are unfinished or broken. This makes writing and debugging programs easier, and faster. Sequential programming usually needs every program line and character to be correct for a program to run. One error usually stops the whole program and prevents any result.\nModern spreadsheets allow for secondary notation. The program can be annotated with colors, typefaces, lines, etc. to provide visual cues about the meaning of elements in the program.\nExtensions that allow users to create new functions can provide the capabilities of a functional language.[37]\nExtensions that allow users to build and apply models from the domain of machine learning.[38][39]\nSpreadsheets are versatile. With their boolean logic and graphics capabilities, even electronic circuit design is possible.[40]\nSpreadsheets can store relational data and spreadsheet formulas can express all queries of SQL. There exists a query translator, which automatically generates the spreadsheet implementation from the SQL code.[41]\nSpreadsheet programs[edit]\nA \"spreadsheet program\" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.\n\nIt is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.\n\nSpreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.\n\nMany of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).\n\nSpreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.\n\nShortcomings[edit]\nWhile spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.[42]\n\nResearch by ClusterSeven has shown huge discrepancies in the way financial institutions and corporate entities understand, manage and police their often vast estates of spreadsheets and unstructured financial data (including comma separated variable (CSV) files and Microsoft Access Databases). One study in early 2011 of nearly 1,500 people in the UK found that 57% of spreadsheet users have never received formal training on the spreadsheet package they use. 72% said that no internal department checks their spreadsheets for accuracy. Only 13% said that Internal Audit reviews their spreadsheets, while a mere 1% receive checks from their risk department.[43]\nSpreadsheets have significant reliability problems. Research studies estimate that roughly 94% of spreadsheets deployed in the field contain errors, and 5.2% of cells in unaudited spreadsheets contain errors.[44]\nDespite the high error risks often associated with spreadsheet authorship and use, specific steps can be taken to significantly enhance control and reliability by structurally reducing the likelihood of error occurrence at their source.[45]\nThe practical expressiveness of spreadsheets can be limited unless their modern features are used. Several factors contribute to this limitation. Implementing a complex model on a cell-at-a-time basis requires tedious attention to detail. Authors have difficulty remembering the meanings of hundreds or thousands of cell addresses that appear in formulas.\nThese drawbacks are mitigated by the use of named variables for cell designations, and employing variables in formulas rather than cell locations and cell-by-cell manipulations. Graphs can be used to show instantly how results are changed by changes in parameter values. In fact, the spreadsheet can be made invisible except for a transparent user interface that requests pertinent input from the user, displays results requested by the user, creates reports, and has built-in error traps to prompt correct input.[46]\nSimilarly, formulas expressed in terms of cell addresses are hard to keep straight and hard to audit. Research shows that spreadsheet auditors who check numerical results and cell formulas find no more errors than auditors who only check numerical results.[44] That is another reason to use named variables and formulas employing named variables.\nThe alteration of a dimension demands major surgery. When rows (or columns) are added to or deleted from a table, one has to adjust the size of many downstream tables that depend on the table being changed. In the process, it is often necessary to move other cells around to make room for the new columns or rows, and to adjust graph data sources. In large spreadsheets, this can be extremely time consuming.[47][48]\nAdding or removing a dimension is so difficult, one generally has to start over. The spreadsheet as a paradigm really forces one to decide on dimensionality right of the beginning of one's spreadsheet creation, even though it is often most natural to make these choices after one's spreadsheet model has matured. The desire to add and remove dimensions also arises in parametric and sensitivity analyses.[47][48]\nMulti-dimensional spreadsheets and tools such as Analytica avoid this important pitfall by generalizing the 2-D paradigm of the classical spreadsheet to a multi-dimensional representation.\nCollaboration in authoring spreadsheet formulas can be difficult when such collaboration occurs at the level of cells and cell addresses.\nHowever, like programming languages, spreadsheets are capable of using aggregate cells with similar meaning and indexed variables with names that indicate meaning. Some spreadsheets have good collaboration features, and it is inadvisable to author at the level of cells and cell formulas to avoid obstacles to collaboration, where many people cooperate on data entry and many people use the same spreadsheet. In collaborative authoring, it is advisable to use the range-protection feature of spreadsheets that prevents the contents of specific parts of a worksheet from being inadvertently altered.\nOther problems associated with spreadsheets include:[49][50]\n\nSome sources advocate the use of specialized software instead of spreadsheets for some applications (budgeting, statistics)[51][52][53]\nMany spreadsheet software products, such as Microsoft Excel[54] (versions prior to 2007) and OpenOffice.org Calc[55] (versions prior to 2008), have a capacity limit of 65,536 rows by 256 columns (216 and 28 respectively). This can present a problem for people using very large datasets, and may result in lost data.\nLack of auditing and revision control. This makes it difficult to determine who changed what and when. This can cause problems with regulatory compliance. Lack of revision control greatly increases the risk of errors due the inability to track, isolate and test changes made to a document.\nLack of security. Spreadsheets lack controls on who can see and modify particular data. This, combined with the lack of auditing above, can make it easy for someone to commit fraud.[56]\nBecause they are loosely structured, it is easy for someone to introduce an error, either accidentally or intentionally, by entering information in the wrong place or expressing dependencies among cells (such as in a formula) incorrectly.[47][57][58]\nThe results of a formula (example \"=A1*B1\") applies only to a single cell (that is, the cell the formula is actually located in—in this case perhaps C1), even though it can \"extract\" data from many other cells, and even real time dates and actual times. This means that to cause a similar calculation on an array of cells, an almost identical formula (but residing in its own \"output\" cell) must be repeated for each row of the \"input\" array. This differs from a \"formula\" in a conventional computer program, which typically makes one calculation that it applies to all the input in turn. With current spreadsheets, this forced repetition of near identical formulas can have detrimental consequences from a quality assurance standpoint and is often the cause of many spreadsheet errors. Some spreadsheets have array formulas to address this issue.\nTrying to manage the sheer volume of spreadsheets that may exist in an organization without proper security, audit trails, unintentional introduction of errors, and other items listed above can become overwhelming.\nWhile there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals \"don't know\" how their spreadsheets are audited; only 6% invest in a third-party solution[59]\n\nSpreadsheet risk[edit]\nFurther information: Financial modeling § Accounting\nSpreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g., out of date exchange rates). Some single-instance errors have exceeded US$1 billion.[60][61] Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.\n\nIn the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.\n\nDespite this, research[62] carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over £50m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.[62][63]\n\nIn 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010–13 European austerity programs. [64]",
          "subparadigms": []
        },
        {
          "pdid": 12,
          "name": "Reactive programming",
          "details": "In computing, reactive programming is a programming paradigm oriented around data flows and the propagation of change. This means that it should be possible to express static or dynamic data flows with ease in the programming languages used, and that the underlying execution model will automatically propagate changes through the data flow.\n\nFor example, in an imperative programming setting, {\\displaystyle a:=b+c} a:=b+c would mean that {\\displaystyle a} a is being assigned the result of {\\displaystyle b+c} b+c in the instant the expression is evaluated, and later, the values of {\\displaystyle b} b and {\\displaystyle c} c can be changed with no effect on the value of {\\displaystyle a} a. However, in reactive programming, the value of {\\displaystyle a} a would be automatically updated whenever the values of {\\displaystyle b} b and {\\displaystyle c} c change, without the program executing the sentence {\\displaystyle a:=b+c} a:=b+c again.\n\nAnother example is a hardware description language such as Verilog. In this case, reactive programming allows changes to be modeled as they propagate through a circuit.\n\nReactive programming has foremost been proposed as a way to simplify the creation of interactive user interfaces, animations in real time systems, but is essentially a general programming paradigm.\n\nFor example, in a model–view–controller architecture, reactive programming can allow changes in the underlying model to automatically be reflected in the view, and vice versa.[1]\n\nContents  [hide] \n1\tDefinition of Reactive Programming\n2\tApproaches to Creating Reactive Programming Languages\n3\tProgramming Models and Semantics\n4\tImplementation Techniques and Challenges\n4.1\tEssence of Implementations\n4.1.1\tChange Propagation Algorithms\n4.1.2\tWhat to push?\n4.2\tImplementation Challenges in Reactive Programming\n4.2.1\tGlitches\n4.2.2\tCyclic Dependencies\n4.2.3\tInteraction with Mutable State\n4.2.4\tDynamic Updating of the Graph of Dependencies\n5\tConcepts\n5.1\tDegrees of explicitness\n5.2\tStatic or Dynamic\n5.3\tHigher-order reactive programming\n5.4\tData flow differentiation\n5.5\tEvaluation models of reactive programming\n5.5.1\tSimilarities with observer pattern\n6\tApproaches\n6.1\tImperative\n6.2\tObject-oriented\n6.3\tFunctional\n7\tExamples\n7.1\tSpreadsheets\n8\tSee also\n9\tReferences\n10\tExternal links\nDefinition of Reactive Programming[edit]\nQuoting Gérard Berry:[2]\n\nIt is convenient to distinguish roughly between three kinds of computer programs. Transformational programs compute results from a given set of inputs; typical examples are compilers or numerical computation programs. Interactive programs interact at their own speed with users or with other programs; from a user point of view, a time-sharing system is interactive. Reactive programs also maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not the program itself. Interactive programs work at their own pace and mostly deal with communication, while reactive programs only work in respond to external demands and mostly deal with accurate interrupt handling. Real-time programs are usually reactive. However, there are reactive programs that are not usually considered as being real-time, such as protocols, system drivers, or man-machine interface handlers.\n\nApproaches to Creating Reactive Programming Languages[edit]\nThere are several popular approaches to creating reactive programming languages. Some are dedicated languages that are specific to some domain constraints (such as real-time or embedded computing or hardware description). Some are general-purpose languages that support reactivity. Finally, some are libraries or embedded domain-specific languages that enable reactivity alongside or on top of an existing general-purpose programming language. These different approaches result in trade-offs in the languages; in general, the more restricted the language, the more compilers and analysis tools can inform programmers (e.g., in performing analysis for whether programs can be executed in real time), while trading off general applicability.\n\nProgramming Models and Semantics[edit]\nA variety of models and semantics govern the family of reactive programming. We can loosely split them along the following dimensions:\n\nSynchrony: is the underlying model of time synchronous versus asynchronous?\nDeterminism: Deterministic versus non-deterministic in both evaluation process and results (the former does not necessarily imply the latter)\nUpdate process: callbacks versus dataflow versus actors\nImplementation Techniques and Challenges[edit]\nEssence of Implementations[edit]\nThe runtime of reactive programming languages usually relies on a graph that captures the dependencies among the reactive values. In the graph, nodes represent computations and edges model dependency relationships. The language runtime uses the graph to keep track of which computations must be executed again when one of the inputs changes.\n\nChange Propagation Algorithms[edit]\nThere are numerous implementation techniques used by reactive programming systems that represent the data flow graph explicitly. The most common algorithms are:\n\npull\npush\nhybrid push-pull\nWhat to push?[edit]\nAt the implementation level, reacting to an event consists of propagating across the graph the information that a change has happened. As a consequence, computations that are affected by the change and may be outdated are re-executed. These computations are usually in the transitive closure of the changed source. Change propagation may lead to an update of the sinks of the graph.\n\nThe information propagated in the graph can consist of the complete state of a node, i.e., the result of the computation of that node. In this case the previous output of the node is ignored. Another option is that changes are propagated incrementally. In this case, the information propagated along edges consists only of a delta that describes how the previous node has changed. The latter approach is especially important when nodes hold a large amount of state which would be expensive to recompute from scratch. Propagating deltas is essentially an optimization and has been extensively studied in incremental computing. This approach requires a solution to the view-update problem, which is well-known from databases maintaining views of changing data. Another common optimization is to accumulate changes and propagate a batch of them instead of a single one. This solution can be faster because it reduces communication among nodes and optimization strategies can reason about a batch of changes - for example two changes in the batch can cancel each other and can be simply ignored. Finally, it is also possible to propagate notifications of invalidity, causing nodes with invalid inputs to pull updates in order to update their own outputs.\n\nThere are two principal ways in which the dependency graph is built:\n\nThe graph of dependencies is maintained implicitly by an event loop. In this case, the registration of explicit callbacks creates implicit dependencies. This means that the inversion of control induced by callbacks is left in place; however, by making the callbacks functional (returning a state value instead of a unit value) callbacks become compositional.\nThe graph of dependencies is program-specific and given by the programmer. This approach enables addressing the inversion of control of callbacks in two ways: either the graph is specified explicitly (typically using a DSL which may be embedded), or the graph is implicitly defined by expressions and generated by \"the language\".\nImplementation Challenges in Reactive Programming[edit]\nGlitches[edit]\nWhen propagating changes, it is possible to pick propagation orders such that the value of an expression is not a natural consequence of the source program. We can illustrate this easily with an example. Suppose seconds is a reactive value that changes every second to represent the current time (in seconds). Consider this expression:\n\nt = seconds + 1\ng = (t > seconds)\nBecause t should always be greater than seconds, this expression should always evaluate to a true value. Unfortunately, this can depend on the order of evaluation. When seconds changes, two expressions have to update: seconds + 1 and the conditional. If the first evaluates before the second, then this invariant will hold. If, however, the conditional updates first, using the old value of t and the new value of seconds, then the expression will evaluate to a false value. This is called a glitch.\n\nSome reactive languages are glitch-free, and prove this property[citation needed]. This is usually achieved by topologically sorting expressions and updating values in topological order. This can, however, have performance implications, such as delaying the delivery of values (due to the order of propagation). In some cases, therefore, reactive languages permit glitches, and developers must be aware of the possibility that values may temporarily fail to correspond to the program source, and that some expressions may evaluate multiple times (for instance, t > seconds may evaluate twice: once when the new value of seconds arrives, and once more when t updates).\n\nCyclic Dependencies[edit]\nTopological sorting of dependencies depends on the dependency graph being a directed acyclic graph (DAG). In practice, a program may define a dependency graph that has cycles. Usually, reactive programming languages expect such cycles to be \"broken\" by placing some element along a \"back edge\" to permit reactive updating to terminate. Typically, languages provide an operator like delay that is used by the update mechanism for this purpose, since a delay implies that what follows must be evaluated in the \"next time step\" (allowing the current evaluation to terminate).\n\nInteraction with Mutable State[edit]\nReactive languages typically assume that their expressions are purely functional. This allows an update mechanism to choose different orders in which to perform updates, and leave the specific order unspecified (thereby enabling optimizations). When a reactive language is embedded in a programming language with state, however, it may be possible for programmers to perform mutable operations. How to make this interaction smooth remains an open problem.\n\nIn some cases, it is possible to have principled partial solutions. Two such solutions include:\n\nA language might offer a notion of \"mutable cell\". A mutable cell is one that the reactive update system is aware of, so that changes made to the cell propagate to the rest of the reactive program. This enables the non-reactive part of the program to perform a traditional mutation while enabling reactive code to be aware of and respond to this update, thus maintaining the consistency of the relationship between values in the program. An example of a reactive language that provides such a cell is FrTime.[3]\nProperly encapsulated object-oriented libraries offer an encapsulated notion of state. In principle, it is therefore possible for such a library to interact smoothly with the reactive portion of a language. For instance, callbacks can be installed in the getters of the object-oriented library to notify the reactive update engine about state changes, and changes in the reactive component can be pushed to the object-oriented library through getters. FrTime employs such a strategy.[4]\nDynamic Updating of the Graph of Dependencies[edit]\nIn some reactive languages, the graph of dependencies is static, i.e., the graph is fixed throughout the program's execution. In other languages, the graph can be \"dynamic\", i.e., it can change as the program executes. For a simple example, consider this illustrative example (where seconds is a reactive value):\n\nt =\n  if ((seconds mod 2) == 0):\n    seconds + 1\n  else:\n    seconds - 1\n  end\nt + 1\nEvery second, the value of this expression changes to a different reactive expression, which t + 1 then depends on. Therefore, the graph of dependencies updates every second.\n\nPermitting dynamic updating of dependencies provides significant expressive power (for instance, dynamic dependencies routinely occur in graphical user interface (GUI) programs). However, the reactive update engine must decide whether to reconstruct expressions each time, or to keep an expression's node constructed but inactive; in the latter case, ensure that they do not participate in the computation when they are not supposed to be active.\n\nConcepts[edit]\nDegrees of explicitness[edit]\nReactive programming languages can range from very explicit ones where data flows are set up by using arrows, to implicit where the data flows are derived from language constructs that look similar to those of imperative or functional programming. For example, in implicitly lifted functional reactive programming (FRP) a function call might implicitly cause a node in a data flow graph to be constructed. Reactive programming libraries for dynamic languages (such as the Lisp \"Cells\" and Python \"Trellis\" libraries) can construct a dependency graph from runtime analysis of the values read during a function's execution, allowing data flow specifications to be both implicit and dynamic.\n\nSometimes the term reactive programming refers to the architectural level of software engineering, where individual nodes in the data flow graph are ordinary programs that communicate with each other.\n\nStatic or Dynamic[edit]\nReactive programming can be purely static where the data flows are set up statically, or be dynamic where the data flows can change during the execution of a program.\n\nThe use of data switches in the data flow graph could to some extent make a static data flow graph appear as dynamic, and blur the distinction slightly. True dynamic reactive programming however could use imperative programming to reconstruct the data flow graph.\n\nHigher-order reactive programming[edit]\nReactive programming could be said to be of higher order if it supports the idea that data flows could be used to construct other data flows. That is, the resulting value out of a data flow is another data flow graph that is executed using the same evaluation model as the first.\n\nData flow differentiation[edit]\nIdeally all data changes are propagated instantly, but this cannot be assured in practice. Instead it might be necessary to give different parts of the data flow graph different evaluation priorities. This can be called differentiated reactive programming.[5]\n\nFor example, in a word processor the marking of spelling errors need not be totally in sync with the inserting of characters. Here differentiated reactive programming could potentially be used to give the spell checker lower priority, allowing it to be delayed while keeping other data-flows instantaneous.\n\nHowever, such differentiation introduces additional design complexity. For example, deciding how to define the different data flow areas, and how to handle event passing between different data flow areas.\n\nEvaluation models of reactive programming[edit]\nEvaluation of reactive programs is not necessarily based on how stack based programming languages are evaluated. Instead, when some data is changed, the change is propagated to all data that is derived partially or completely from the data that was changed. This change propagation could be achieved in a number of ways, where perhaps the most natural way is an invalidate/lazy-revalidate scheme.\n\nIt could be problematic simply to naively propagate a change using a stack, because of potential exponential update complexity if the data structure has a certain shape. One such shape can be described as \"repeated diamonds shape\", and has the following structure: An→Bn→An+1, An→Cn→An+1, where n=1,2... This problem could be overcome by propagating invalidation only when some data is not already invalidated, and later re-validate the data when needed using lazy evaluation.\n\nOne inherent problem for reactive programming is that most computations that would be evaluated and forgotten in a normal programming language, needs to be represented in the memory as data-structures.[citation needed] This could potentially make RP highly memory consuming. However, research on what is called lowering could potentially overcome this problem.[6]\n\nOn the other side, reactive programming is a form of what could be described as \"explicit parallelism\", and could therefore be beneficial for utilizing the power of parallel hardware.\n\nSimilarities with observer pattern[edit]\nReactive programming has principal similarities with the observer pattern commonly used in object-oriented programming. However, integrating the data flow concepts into the programming language would make it easier to express them and could therefore increase the granularity of the data flow graph. For example, the observer pattern commonly describes data-flows between whole objects/classes, whereas object-oriented reactive programming could target the members of objects/classes.\n\nThe stack-based evaluation model of common object orientation is also not entirely suitable for data-flow propagation, as occurrences of \"tree feedback edges\" in the data structures could make the program face exponential complexities. But because of its relatively limited use and low granularity, this is rarely a problem for the observer pattern in practice.\n\nApproaches[edit]\nImperative[edit]\nIt is possible to fuse reactive programming with ordinary imperative programming. In such a paradigm, imperative programs operate upon reactive data structures.[7] Such a set-up is analogous to constraint imperative programming; however, while constraint imperative programming manages bidirectional constraints, reactive imperative programming manages one-way dataflow constraints.\n\nObject-oriented[edit]\nObject-oriented reactive programming (OORP) is a combination of object oriented programming and reactive programming. Perhaps the most natural way to make such a combination is as follows: Instead of methods and fields, objects have reactions that automatically re-evaluate when the other reactions they depend on have been modified.[citation needed]\n\nBelow is an illustration of the A=X+Y introductory example using JavaScript and jQuery:[8]\n\n    X: <input type=\"text\" id=\"X\" /> <br />\n    Y: <input type=\"text\" id=\"Y\" /> <br />\n    A: <span id=\"A\"></span>\nfunction setA() {  // A=X+Y as integers\n   var A = parseInt($('#X').val()) + parseInt($('#Y').val());\n   $('#A').text(A);\n}\nsetA();  // for initial value of A\n$('#X,#Y').css('cursor', 'pointer').click(function () {\n   // by reaction to a click at X or at Y...\n   var obj = $(this);\n   obj.val(parseInt(obj.val()) + 1);  // updates X or Y \n   setA();  // updates A\n});\nIf an OORP programming language maintains its imperative methods, it would also fall under the category of imperative reactive programming.\n\nFunctional[edit]\nFunctional reactive programming (FRP) is a programming paradigm for reactive programming on functional programming.\n\nExamples[edit]\nSpreadsheets[edit]\nA modern spreadsheet is often cited as an example of reactive programming. This is problematic because the unqualified term \"spreadsheet\" may refer to either:\n\nThe underlying collection of cells, where each cell contains either a literal value or a formula that refers to other cells such as \"=B1+C1\". This table of cells is effectively a computer program that determines how a set of output cells are computed from a set of input cells. This may be saved to a file, and this is often referred to as a \"spreadsheet\" (e.g. \"the budget spreadsheet\").\nThe interactive application program with a graphical user interface that is used to edit and evaluate the underlying table of cells from (1). In virtually all spreadsheet applications, interactively changing any one cell on the sheet will result in immediately re-evaluating all formulas that directly or indirectly depend on that cell and updating the display to reflect these re-evaluations.\nConfusion arises because the spreadsheet application (2) is an example of a reactive program, while the program effectively defined by the underlying spreadsheet (1) is typically not itself a reactive program.[citation needed] Semantically, the underlying spreadsheet (1) simply determines a calculation from a set of input cells to a set of output cells, and thus could be directly translated to a simple transformational calculation (i.e. function) in a traditional programming language.",
          "subparadigms": []
        },
        {
          "pdid": 13,
          "name": "Dataflow programming",
          "details": "In computer programming, dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Dataflow programming languages share some features of functional languages, and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing. Some authors use the term Datastream instead of Dataflow to avoid confusion with Dataflow Computing or Dataflow architecture, based on an indeterministic machine paradigm. Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.\n\nContents  [hide] \n1\tProperties of dataflow programming languages\n1.1\tState\n1.2\tRepresentation\n2\tHistory\n3\tLanguages\n4\tApplication programming interfaces\n5\tSee also\n6\tReferences\n7\tExternal links\nProperties of dataflow programming languages[edit]\nTraditionally, a program is modeled as a series of operations happening in a specific order; this may be referred to as sequential,[1]:p.3 procedural,[2] Control flow[2] (indicating that the program chooses a specific path), or imperative programming. The program focuses on commands, in line with the von Neumann[1]:p.3 vision of sequential programming, where data is normally \"at rest\"[2]:p.7\n\nIn contrast, dataflow programming emphasizes the movement of data and models programs as a series of connections. Explicitly defined inputs and outputs connect operations, which function like black boxes.[2]:p.2 An operation runs as soon as all of its inputs become valid.[3] Thus, dataflow languages are inherently parallel and can work well in large, decentralized systems.[1]:p.3[4] [5]\n\nState[edit]\nOne of the key concepts in computer programming is the idea of state, essentially a snapshot of various conditions in the system. Most programming languages require a considerable amount of state information, which is generally hidden from the programmer. Often, the computer itself has no idea which piece of information encodes the enduring state. This is a serious problem, as the state information needs to be shared across multiple processors in parallel processing machines. Most languages force the programmer to add extra code to indicate which data and parts of the code are important to the state. This code tends to be both expensive in terms of performance, as well as difficult to read or debug. Explicit parallelism is one of the main reasons for the poor performance of Enterprise Java Beans when building data-intensive, non-OLTP applications.\n\nWhere a linear program can be imagined as a single worker moving between tasks (operations), a dataflow program is more like a series of workers on an assembly line, each doing a specific task whenever materials are available. Since the operations are only concerned with the availability of data inputs, they have no hidden state to track, and are all \"ready\" at the same time.\n\nRepresentation[edit]\nDataflow programs are represented in different ways. A traditional program is usually represented as a series of text instructions, which is reasonable for describing a serial system which pipes data between small, single-purpose tools that receive, process, and return. Dataflow programs start with an input, perhaps the command line parameters, and illustrate how that data is used and modified. The flow of data is explicit, often visually illustrated as a line or pipe.\n\nIn terms of encoding, a dataflow program might be implemented as a hash table, with uniquely identified inputs as the keys, used to look up pointers to the instructions. When any operation completes, the program scans down the list of operations until it finds the first operation where all inputs are currently valid, and runs it. When that operation finishes, it will typically output data, thereby making another operation become valid.\n\nFor parallel operation, only the list needs to be shared; it is the state of the entire program. Thus the task of maintaining state is removed from the programmer and given to the language's runtime. On machines with a single processor core where an implementation designed for parallel operation would simply introduce overhead, this overhead can be removed completely by using a different runtime.\n\nHistory[edit]\nA pioneer dataflow language was BLODI (BLOck DIagram), developed by John Larry Kelly, Jr., Carol Lochbaum and Victor A. Vyssotsky for specifying sampled data systems.[6] A BLODI specification of functional units (amplifiers, adders, delay lines, etc.) and their interconnections was compiled into a single loop that updated the entire system for one clock tick.\n\nMore conventional dataflow languages were originally developed in order to make parallel programming easier. In Bert Sutherland's 1966 Ph.D. thesis, The On-line Graphical Specification of Computer Procedures,[7] Sutherland created one of the first graphical dataflow programming frameworks. Subsequent dataflow languages were often developed at the large supercomputer labs. One of the most popular was SISAL, developed at Lawrence Livermore National Laboratory. SISAL looks like most statement-driven languages, but variables should be assigned once. This allows the compiler to easily identify the inputs and outputs. A number of offshoots of SISAL have been developed, including SAC, Single Assignment C, which tries to remain as close to the popular C programming language as possible.\n\nThe United States Navy funded development of ACOS and SPGN (signal processing graph notation) starting in early 1980's. This is in use on a number of platforms in the field today.[8]\n\nA more radical concept is Prograph, in which programs are constructed as graphs onscreen, and variables are replaced entirely with lines linking inputs to outputs. Incidentally, Prograph was originally written on the Macintosh, which remained single-processor until the introduction of the DayStar Genesis MP in 1996.\n\nThere are many hardware architectures oriented toward the efficient implementation of dataflow programming models. MIT's tagged token dataflow architecture was designed by Greg Papadopoulos.\n\nData flow has been proposed as an abstraction for specifying the global behavior of distributed system components: in the live distributed objects programming model, distributed data flows are used to store and communicate state, and as such, they play the role analogous to variables, fields, and parameters in Java-like programming languages.\n\nLanguages[edit]\n\nIt has been suggested that List of dataflow programming languages be merged into this section. (Discuss) Proposed since April 2015.\nAgilent VEE\nAlteryx\nANI\nDARTS\nASCET\nAviSynth scripting language, for video processing\nBLODI\nBMDFM Binary Modular Dataflow Machine\nCAL\nSPGN - Signal Processing Graph Notation\nCOStream\nCassandra-vision - A Visual programming language with OpenCV support and C++ extension API\nСorezoid - cloud OS with visual development environment for business process management within any company\nCuneiform, a functional workflow language.\nCurin\nCMS Pipelines\nHume\nJoule (programming language)\nJuttle\nKNIME\nKorduene\nLabVIEW, G[3]\nLinda\nLucid[2]\nLustre\nM, used as the backend of Microsoft Excel's ETL plugin Power Query.\nMaxCompiler - Designed by Maxeler Technologies and is compatible with OpenSPL\nMax/MSP\nMicrosoft Visual Programming Language - A component of Microsoft Robotics Studio designed for Robotics programming\nNextflow - A data-driven toolkit for computational pipelines based on the dataflow programming model\nOpenWire - adds visual dataflow programming capabilities to Delphi via VCL or FireMonkey components and a graphical editor (homonymous binary protocol is unrelated)\nOpenWire Studio - A visual development environment which allows the development of software prototypes by non developers.\nOrange - An open-source, visual programming tool for data mining, statistical data analysis, and machine learning.\nOz now also distributed since 1.4.0\nPifagor\nPipeline Pilot\nPOGOL\nPrograph\nPure Data\nPythonect\nQuartz Composer - Designed by Apple; used for graphic animations and effects\nRaftLib - Massive multi-threading with C++ IOStream-like Operators\nSAC Single Assignment C\nScala (with a library SynapseGrid)\nSIGNAL (a dataflow-oriented synchronous language enabling multi-clock specifications)\nSimulink\nSIMPL\nSISAL\nStreamIt - A programming language and a compilation infrastructure, specifically engineered for modern streaming systems.\nstromx - A visual programming environment focused on industrial vision (open source)\nSPL - Data Flow description language of IBM InfoSphere Streams streaming engine\nSWARM\nSystemVerilog - A hardware description language\nTersus - Visual programming platform (open source)\nVerilog - A hardware description language absorbed into the SystemVerilog standard in 2009\nVHDL - A hardware description language\nVignette's VBIS language for business processes integration\nvvvv\nVSXu\nWidget Workshop, a \"game\" designed for children which is technically a simplified dataflow programming language.\nXEE (Starlight) XML Engineering Environment\nXProc\nAnandamideScript, flexible data-flow scripting language for C++ and QT [9]\nApplication programming interfaces[edit]\nApache Airflow: Library for pything aimed at coordinating large computational workflows.\nDC: Library that allows the embedding of one-way dataflow constraints in a C/C++ program.\nSystemC: Library for C++, mainly aimed at hardware design.\nMDF: Library for python aimed at financial models",
          "subparadigms": [
            10,
            11,
            12
          ]
        },
        {
          "pdid": 14,
          "name": "Functional logic programming",
          "details": "Functional logic programming is the combination, in a single programming language, of the paradigms of functional programming (including higher-order programming) and logic programming (Nondeterministic programming, unification). This style of programming is embodied by various programming languages, including Curry and Mercury.\n\nA journal devoted to the integration of functional and logic programming was published by MIT Press and the European Association for Programming Languages and Systems between 1995 and 2008.[1]",
          "subparadigms": []
        },
        {
          "pdid": 15,
          "name": "Purely functional programming",
          "details": "In computer science, purely functional programming usually designates a programming paradigm—a style of building the structure and elements of computer programs—that treats all computation as the evaluation of mathematical functions. Purely functional programing may also be defined by forbidding changing-state and mutable data.\n\nPurely functional programing consists in restricting programming to its functional paradigm.\n\nContents  [hide] \n1\tDifference between pure and not-pure functional programming\n2\tProperties of purely functional program\n2.1\tStrict versus non-strict evaluation\n2.2\tParallel computing\n2.3\tData structures\n3\tPurely functional language\n4\tReferences\nDifference between pure and not-pure functional programming[edit]\nThe exact difference between pure and impure functional programing is a matter of controversy.[1]\n\nA program is usually said to be functional when it uses some concepts of functional programming, such as first-class functions and higher-order functions.[2] However, a first-class function may use techniques from the imperative paradigm, such as arrays or input/output methods are not purely functional programs, therefore the first-class function needs not be purely functional. In fact, the earliest programming languages cited as being functional, IPL and Lisp,[3][4] were both \"impure\" functional languages by the current definition.\n\nPurely functional data structures are persistent. Persistency is required for functional programming; without it, the same computation could return different results. Functional programming may use persistent non-purely functional data structures, while those data structures may not be used in purely functional programs.\n\nProperties of purely functional program[edit]\nStrict versus non-strict evaluation[edit]\nMain article: Evaluation strategy\nAll evaluation strategy which ends on a purely functional programs returns the same result. In particular, it ensures that the programmer does not have to consider in which order programs are evaluated, since eager evaluation will return the same result than lazy evaluation. However, it is still possible that an eager evaluation may not terminate while the lazy evaluation of the same program halts.\n\nParallel computing[edit]\nPurely functional programing simplifies parallel computing[5] since two purely functional parts of the evaluation never interact.\n\nData structures[edit]\nMain article: Purely functional data structure\nPurely functional data structures are often represented in a different way than their imperative counterparts.[6] For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nPurely functional language[edit]\nMain article: List of programming languages by type § Pure\nA purely functional language is a language which only admits purely functional programming. Purely functional programs can however be written in languages which are not purely functional.",
          "subparadigms": []
        },
        {
          "pdid": 16,
          "name": "Functional programming",
          "details": "In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions[1] or declarations[2] instead of statements. In functional code, the output value of a function depends only on the arguments that are input to the function, so calling a function f twice with the same value for an argument x will produce the same result f(x) each time. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.\n\nFunctional programming has its roots in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, logic programming, is based on relations.[3]\n\nIn contrast, imperative programming changes state with commands in the source language, the most simple example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.[3]\n\nFunctional programming languages, especially purely functional ones such as Hope, have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme,[4][5][6][7] Clojure,[8][9] Wolfram Language[10] (also known as Mathematica), Racket,[11] Erlang,[12][13][14] OCaml,[15][16] Haskell,[17][18] and F#[19][20] have been used in industrial and commercial applications by a wide variety of organizations. Functional programming is also supported in some domain-specific programming languages like R (statistics),[21] J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML),[22][23] and Opal.[24] Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.[25]\n\nProgramming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts.[26] This is also true of the PHP programming language.[27] C++11, Java 8, and C# 3.0 all added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala[28] – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.\n\nContents  [hide] \n1\tHistory\n2\tConcepts\n2.1\tFirst-class and higher-order functions\n2.2\tPure functions\n2.3\tRecursion\n2.4\tStrict versus non-strict evaluation\n2.5\tType systems\n2.6\tReferential transparency\n2.7\tFunctional programming in non-functional languages\n2.8\tData structures\n3\tComparison to imperative programming\n3.1\tSimulating state\n3.2\tEfficiency issues\n3.3\tCoding styles\n3.3.1\tPython\n3.3.2\tHaskell\n3.3.3\tPerl 6\n3.3.4\tErlang\n3.3.5\tElixir\n3.3.6\tLisp\n3.3.7\tClojure\n3.3.8\tD\n3.3.9\tR\n3.3.10\tSequenceL\n4\tUse in industry\n5\tIn education\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory[edit]\nLambda calculus provides a theoretical framework for describing functions and their evaluation. Although it is a mathematical abstraction rather than a programming language, it forms the basis of almost all functional programming languages today. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.[29]\n\nAn early functional-flavored language was Lisp, developed in the late 1950s for the IBM 700/7000 series scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT).[30] Lisp introduced many features now found in functional languages, though Lisp is technically a multi-paradigm language. Scheme and Dylan were later attempts to simplify and improve Lisp.\n\nInformation Processing Language (IPL) is sometimes cited as the first computer-based functional programming language.[31] It is an assembly-style language for manipulating lists of symbols. It does have a notion of \"generator\", which amounts to a function accepting a function as an argument, and, since it is an assembly-level language, code can be used as data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.\n\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\n\nJohn Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\".[32] He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style which has come to be associated with functional programming.\n\nIn the 1970s, ML was created by Robin Milner at the University of Edinburgh, and David Turner initially developed the language SASL at the University of St Andrews and later the language Miranda at the University of Kent. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL.[33] NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation.[34] Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope.[35] ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML. Meanwhile, the development of Scheme (a partly functional dialect of Lisp), as described in the influential Lambda Papers and the 1985 textbook Structure and Interpretation of Computer Programs, brought awareness of the power of functional programming to the wider programming-languages community.\n\nIn the 1980s, Per Martin-Löf developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs of arbitrarily complex mathematical propositions expressed as dependent types. This led to powerful new approaches to interactive theorem proving and has influenced the development of many subsequent functional programming languages.\n\nThe Haskell language began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\n\nConcepts[edit]\nA number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages are often hybrids of several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.[36]\n\nFirst-class and higher-order functions[edit]\nMain articles: First-class function and Higher-order function\nHigher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator {\\displaystyle d/dx} d/dx, which returns the derivative of a function {\\displaystyle f} f.\n\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term that describes programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\n\nHigher-order functions enable partial application or currying, a technique in which a function is applied to its arguments one at a time, with each application returning a new function that accepts the next argument. This allows one to succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\n\nPure functions[edit]\nPure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\n\nIf the result of a pure expression is not used, it can be removed without affecting other expressions.\nIf a pure function is called with arguments that cause no side-effects, the result is constant with respect to that argument list (sometimes called referential transparency), i.e. if the pure function is again called with the same arguments, the same result will be returned (this can enable caching optimizations such as memoization).\nIf there is no data dependency between two pure expressions, then their order can be reversed, or they can be performed in parallel and they cannot interfere with one another (in other terms, the evaluation of any pure expression is thread-safe).\nIf the entire language does not allow side-effects, then any evaluation strategy can be used; this gives the compiler freedom to reorder or combine the evaluation of expressions in a program (for example, using deforestation).\nWhile most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also allows functions to be designated \"pure\".\n\nRecursion[edit]\nMain article: Recursion (computer science)\nIteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Though some recursion requires maintaining a stack, tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. The Scheme language standard requires implementations to recognize and optimize tail recursion. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\n\nCommon patterns of recursion can be factored out using higher order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples. Such higher order functions play a role analogous to built-in control structures such as loops in imperative languages.\n\nMost general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.[37]\n\nStrict versus non-strict evaluation[edit]\nMain article: Evaluation strategy\nFunctional languages can be categorized by whether they use strict (eager) or non-strict (lazy) evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm will itself fail. For example, the expression:\n\nprint length([2+1, 3*2, 1/0, 5-4])\nwill fail under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function will return the value 4 (i.e., the number of items in the list), since evaluating it will not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\n\nThe usual implementation strategy for lazy evaluation in functional languages is graph reduction.[38] Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.\n\nHughes 1984 argues for lazy evaluation as a mechanism for improving program modularity through separation of concerns, by easing independent implementation of producers and consumers of data streams.[39] Launchbury 1993 describes some difficulties that lazy evaluation introduces, particularly in analyzing a program's storage requirements, and proposes an operational semantics to aid in such analysis.[40] Harper 2009 proposes including both strict and lazy evaluation in the same language, using the language's type system to distinguish them.[41]\n\nType systems[edit]\nEspecially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), although they reject all invalid programs at runtime, when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.\n\nSome research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which allows types to depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with[citation needed]. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.[42]\n\nA limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience.[43] GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as \"case classes\"), and have been proposed as additions to other languages including Java and C#.[44]\n\nReferential transparency[edit]\nMain article: Referential transparency\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.[45]\n\nConsider C assignment statement x = x * 10, this changes the value assigned to the variable x. Let us say that the initial value of x was 1, then two consecutive evaluations of the variable x will yield 10 and 100 respectively. Clearly, replacing x = x * 10 with either 10 or 100 gives a program with different meaning, and so the expression is not referentially transparent. In fact, assignment statements are never referentially transparent.\n\nNow, consider another function such as int plusone(int x) {return x+1;} is transparent, as it will not implicitly change the input x and thus has no such side effects. Functional programs exclusively use this type of function and are therefore referentially transparent.\n\nFunctional programming in non-functional languages[edit]\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages.[46] For example, both D and Fortran 95 explicitly support pure functions.[47]\n\nJavaScript, Lua[48] and Python had first class functions from their inception.[49] Amrit Prem added support to Python for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2,[50] though Python 3 relegated \"reduce\" to the functools standard library module.[51] First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, and C++11.[citation needed]\n\nIn Java, anonymous classes can sometimes be used to simulate closures;[52] however, anonymous classes are not always proper replacements to closures because they have more limited capabilities.[53] Java 8 supports lambda expressions as a replacement for some anonymous classes.[54] However, the presence of checked exceptions in Java can make functional programming inconvenient, because it can be necessary to catch checked exceptions and then rethrow them—a problem that does not occur in other JVM languages that do not have checked exceptions, such as Scala.[citation needed]\n\nIn C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\n\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\n\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages,[55] for example the tuple in Python, which is an immutable array.\n\nData structures[edit]\nMain article: Purely functional data structure\nPurely functional data structures are often represented in a different way than their imperative counterparts.[56] For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nComparison to imperative programming[edit]\nFunctional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\n\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.\n\nSimulating state[edit]\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\n\nThe pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).[57]\n\nAnother way in which functional languages can simulate state is by passing around a data structure that represents the current state as a parameter to function calls. On each function call, a copy of this data structure is created with whatever differences are the result of the function. This is referred to as 'state-passing style'.\n\nImpure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.\n\nAlternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.\n\nEfficiency issues[edit]\nFunctional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal.[58] This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware (which is a highly evolved Turing machine). Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree).[59] However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C.[60] For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\n\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.[61]\n\nLazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993[40] discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan et al. 2008[62] give some practical advice for analyzing and fixing them. However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles)[citation needed].\n\nCoding styles[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2013) (Learn how and when to remove this template message)\nImperative programs have the environment and a sequence of steps manipulating the environment. Functional programs have an expression that is successively substituted until it reaches normal form. An example illustrates this with different solutions to the same programming goal (calculating Fibonacci numbers).\n\nPython[edit]\nPrinting first 10 Fibonacci numbers, iterative\n\ndef fibonacci(n, first=0, second=1):\n    while n != 0:\n        print(first, end=\"\\n\") # side-effect\n        n, first, second = n - 1, second, first + second # assignment\nfibonacci(10)\nPrinting first 10 Fibonacci numbers, functional expression style\n\nfibonacci = (lambda n, first=0, second=1:\n    \"\" if n == 0 else\n    str(first) + \"\\n\" + fibonacci(n - 1, second, first + second))\nprint(fibonacci(10), end=\"\")\nPrinting a list with first 10 Fibonacci numbers, with generators\n\ndef fibonacci(n, first=0, second=1):\n    while n != 0:\n        yield first\n        n, first, second = n - 1, second, first + second # assignment\nprint(list(fibonacci(10)))\nPrinting a list with first 10 Fibonacci numbers, functional expression style\n\nfibonacci = (lambda n, first=0, second=1:\n    [] if n == 0 else\n    [first] + fibonacci(n - 1, second, first + second))\nprint(fibonacci(10))\nHaskell[edit]\nPrinting first 10 fibonacci numbers, functional expression style[1]\n\nfibonacci_aux = \\n first second->\n    if n == 0 then \"\" else\n    show first ++ \"\\n\" ++ fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStr (fibonacci 10)\nPrinting a list with first 10 fibonacci numbers, functional expression style[1]\n\nfibonacci_aux = \\n first second->\n    if n == 0 then [] else\n    [first] ++ fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1]\n\nfibonacci = \\n-> if n == 0 then 0\n                 else if n == 1 then 1\n                      else fibonacci(n - 1) + fibonacci(n - 2)\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style,[1] tail recursive\n\nfibonacci_aux = \\n first second->\n    if n == 0 then first else\n    fibonacci_aux (n - 1) second (first + second)\nfibonacci = \\n-> fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with recursive lists\n\nfibonacci_aux = \\first second-> first : fibonacci_aux second (first + second)\nselect = \\n zs-> if n==0 then head zs\n                 else select (n - 1) (tail zs)\nfibonacci = \\n-> select n (fibonacci_aux 0 1)\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with primitives for recursive lists\n\nfibonacci_aux = \\first second-> first : fibonacci_aux second (first + second)\nfibonacci = \\n-> (fibonacci_aux 0 1) !! n\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional expression style[1] with primitives for recursive lists, more concisely\n\nfibonacci_aux = 0:1:zipWith (+) fibonacci_aux (tail fibonacci_aux)\nfibonacci = \\n-> fibonacci_aux !! n\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional declaration style,[2] tail recursive\n\nfibonacci_aux 0 first _ = first\nfibonacci_aux n first second = fibonacci_aux (n - 1) second (first + second)\nfibonacci n = fibonacci_aux n 0 1\nmain = putStrLn (show (fibonacci 10))\nPrinting the 11th fibonacci number, functional declaration style, using lazy infinite lists and primitives\n\nfibs = 1 : 1 : zipWith (+) fibs (tail fibs) \n-- an infinite list of the fibonacci numbers\n-- fibs is defined in terms of fibs\nfibonacci = (fibs !!)\nmain = putStrLn $ show $ fibonacci 11\nPerl 6[edit]\nAs influenced by Haskell and others, Perl 6 has several functional and declarative approaches to problems. For example, you can declaratively build up a well-typed recursive version (the type constraints are optional) through signature pattern matching:\n\nsubset NonNegativeInt of Int where * >= 0;\n\nproto fib (|) is cached returns NonNegativeInt {*}\nmulti fib (0) { 0 }\nmulti fib (1) { 1 }\nmulti fib (NonNegativeInt $n) { fib($n - 1) + fib($n - 2) }\n\nfor ^10 -> $n { say fib($n) }\nAn alternative to this is to construct a lazy iterative sequence, which appears as an almost direct illustration of the sequence:\n\nmy @fib = 0, 1, *+* ... *; # Each additional entry is the sum of the previous two\n                           # and this sequence extends lazily indefinitely\nsay @fib[^10];             # Display the first 10 entries\nErlang[edit]\nErlang is a functional, concurrent, general-purpose programming language. A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. Use other algorithms for fast performance[63]):\n\n-module(fib).    % This is the file 'fib.erl', the module and the filename must match\n-export([fib/1]). % This exports the function 'fib' of arity 1\n\nfib(1) -> 1; % If 1, then return 1, otherwise (note the semicolon ; meaning 'else')\nfib(2) -> 1; % If 2, then return 1, otherwise\nfib(N) -> fib(N - 2) + fib(N - 1).\nElixir[edit]\nElixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM).\n\nThe Fibonacci function can be written in Elixir as follows:\n\ndefmodule Fibonacci do\n   def fib(0), do: 0\n   def fib(1), do: 1\n   def fib(n), do: fib(n-1) + fib(n-2)\nend\nLisp[edit]\nThe Fibonacci function can be written in Common Lisp as follows:\n\n(defun fib (n &optional (a 0) (b 1))\n  (if (= n 0)\n      a\n      (fib (- n 1) b (+ a b))))\nThe program can then be called as\n\n(fib 10)\nClojure[edit]\nThe Fibonacci function can be written in Clojure as follows:\n\n(defn fib\n  [n]\n  (loop [a 0 b 1 i n]\n    (if (zero? i)\n      a\n      (recur b (+ a b) (dec i)))))\nThe program can then be called as\n\n(fib 7)\nD[edit]\nD has support for functional programming[clarification needed][citation needed]:\n\nimport std.stdio;\nimport std.range;\n\nvoid main()\n{\n    /* 'f' is a range representing the first 10 Fibonacci numbers */\n    auto f = recurrence!((seq, i) => seq[0] + seq[1])(0, 1)\n             .take(10);\n\n    writeln(f);\n}\nR[edit]\nR is an environment for statistical computing and graphics. It is also a functional programming language.\n\nThe Fibonacci function can be written in R as a recursive function as follows:\n\nfib <- function(n) {\n if (n <= 2) 1\n else fib(n - 1) + fib(n - 2)\n}\nOr it can be written as a singly recursive function:\n\nfib <- function(n,a=1,b=1) { \n if (n == 1) a \n else fib(n-1,b,a+b) \n}\nOr it can be written as an iterative function:\n\nfib <- function(n) {\n if (n == 1) 1\n else if (n == 2) 1\n else {\n  fib<-c(1,1)\n  for (i in 3:n) fib<-c(0,fib[1])+fib[2]\n  fib[2]\n }\n}\nThe function can then be called as\n\nfib(10)\nSequenceL[edit]\nSequenceL is a functional, concurrent, general-purpose programming language. The Fibonacci function can be written in SequenceL as follows:\n\nfib(n) := n when n < 2 else\n          fib(n - 1) + fib(n - 2);\nThe function can then be called as\n\nfib(10)\nTo reduce the memory consumed by the call stack when computing a large Fibonacci term, a tail-recursive version can be used. A tail-recursive function is implemented by the SequenceL compiler as a memory-efficient looping structure:\n\nfib(n) := fib_Helper(0, 1, n);\n\nfib_Helper(prev, next, n) :=\n    prev when n < 1 else\n    next when n = 1 else\n    fib_Helper(next, next + prev, n - 1);\nUse in industry[edit]\nFunctional programming has long been popular in academia, but with few industrial applications.[64]:page 11 However, recently several prominent functional programming languages have been used in commercial or industrial systems. For example, the Erlang programming language, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems.[13] It has since become popular for building a range of applications at companies such as T-Mobile, Nortel, Facebook, Électricité de France and WhatsApp.[12][14][65][66][67] The Scheme dialect of Lisp was used as the basis for several applications on early Apple Macintosh computers,[4][5] and has more recently been applied to problems such as training simulation software[6] and telescope control.[7] OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis,[15] driver verification, industrial robot programming, and static analysis of embedded software.[16] Haskell, although initially intended as a research language,[18] has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.[17][18]\n\nOther functional programming languages that have seen use in industry include Scala,[68] F#,[19][20] (both being functional-OO hybrids with support for both purely functional and imperative programming) Wolfram Language,[10] Lisp,[69] Standard ML[70][71] and Clojure[72]\n\nIn education[edit]\nFunctional programming is being used as a method to teach problem solving, algebra and geometric concepts.[73] It has also been used as a tool to teach classical mechanics in Structure and Interpretation of Classical Mechanics.\n",
          "subparadigms": [
            14,
            15
          ]
        },
        {
          "pdid": 17,
          "name": "Abductive logic programming",
          "details": "Abductive logic programming (ALP) is a high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates. Problem solving is effected by deriving hypotheses on these abducible predicates (abductive hypotheses) as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abduction) or goals to be achieved (as in normal logic programming). It can be used to solve problems in diagnosis, planning, natural language and machine learning. It has also been used to interpret negation as failure as a form of abductive reasoning.\n\nContents  [hide] \n1\tSyntax\n2\tInformal meaning and problem solving\n2.1\tExample 1\n2.2\tExample 2\n2.3\tExample 3\n3\tFormal semantics\n4\tImplementation and systems\n5\tSee also\n6\tNotes\n7\tReferences\n8\tExternal links\nSyntax[edit]\nAbductive logic programs have three components, {\\displaystyle \\langle P,A,IC\\rangle ,} \\langle P,A,IC\\rangle, where:\n\nP is a logic program of exactly the same form as in logic programming\nA is a set of predicate names, called the abducible predicates\nIC is a set of first-order classical formulae.\nNormally, the logic program P does not contain any clauses whose head (or conclusion) refers to an abducible predicate. (This restriction can be made without loss of generality.) Also in practice, many times, the integrity constraints in IC are often restricted to the form of denials, i.e. clauses of the form:\n\n   false:- A1,...,An, not B1, ..., not Bm.\nSuch a constraint means that it is not possible for all A1,...,An to be true and at the same time all of B1,...,Bm to be false.\n\nInformal meaning and problem solving[edit]\nThe clauses in P define a set of non-abducible predicates and through this they provide a description (or model) of the problem domain. The integrity constraints in IC specify general properties of the problem domain that need to be respected in any solution of a problem.\n\nA problem, G, which expresses either an observation that needs to be explained or a goal that is desired, is represented by a conjunction of positive and negative (NAF) literals. Such problems are solved by computing \"abductive explanations\" of G.\n\nAn abductive explanation of a problem G is a set of positive (and sometimes also negative) ground instances of the abducible predicates, such that, when these are added to the logic program P, the problem G and the integrity constraints IC both hold. Thus abductive explanations extend the logic program P by the addition of full or partial definitions of the abducible predicates. In this way, abductive explanations form solutions of the problem according to the description of the problem domain in P and IC. The extension or completion of the problem description given by the abductive explanations provides new information, hitherto not contained in the solution to the problem. Quality criteria to prefer one solution over another, often expressed via integrity constraints, can be applied to select specific abductive explanations of the problem G.\n\nComputation in ALP combines the backwards reasoning of normal logic programming (to reduce problems to sub-problems) with a kind of integrity checking to show that the abductive explanations satisfy the integrity constraints.\n\nThe following two examples, written in simple structured English rather than in the strict syntax of ALP, illustrate the notion of abductive explanation in ALP and its relation to problem solving.\n\nExample 1[edit]\nThe abductive logic program, {\\displaystyle \\langle P,A,{\\mathit {IC}}\\rangle } \\langle P,A,\\mathit{IC} \\rangle, has in {\\displaystyle P} P the following sentences:\n\n  Grass is wet if it rained.\n  Grass is wet if the sprinkler was on.\n  The sun was shining.\nThe abducible predicates in {\\displaystyle A} A are \"it rained\" and \"the sprinkler was on\" and the only integrity constraint in {\\displaystyle {\\mathit {IC}}} \\mathit{IC} is:\n\n  false if it rained and the sun was shining.\nThe observation that the grass is wet has two potential explanations, \"it rained\" and \"the sprinkler was on\", which entail the observation. However, only the second potential explanation, \"the sprinkler was on\", satisfies the integrity constraint.\n\nExample 2[edit]\nConsider the abductive logic program consisting of the following (simplified) clauses:\n\n  X is a citizen if X is born in the USA.\n  X is a citizen if X is born outside the USA and X is a resident of the USA and X is naturalized.\n  X is a citizen if X is born outside the USA and Y is the mother of X and Y is a citizen and X is registered.\n  Mary is the mother of John.\n  Mary is a citizen.\ntogether with the five abducible predicates, \"is born in the USA\", \"is born outside the USA\", \"is a resident of the USA\", \"is naturalized\" and \"is registered\" and the integrity constraint:\n\n  false if John is a resident of the USA.\nThe goal \"John is citizen\" has two abductive solutions, one of which is \"John is born in the USA\", the other of which is \"John is born outside the USA\" and \"John is registered\". The potential solution of becoming a citizen by residence and naturalization fails because it violates the integrity constraint.\n\nA more complex example that is also written in the more formal syntax of ALP is the following.\n\nExample 3[edit]\nThe abductive logic program below describes a simple model of the lactose metabolism of the bacterium E. coli. The program, P, describes (in its first rule) that E. coli can feed on the sugar lactose if it makes two enzymes permease and galactosidase. Like all enzymes, these are made if they are coded by a gene (Gene) that is expressed (described by the second rule). The two enzymes of permease and galactosidase are coded by two genes, lac(y) and lac(z) respectively (stated in the fifth and sixth rule of the program), in a cluster of genes (lac(X)) – called an operon – that is expressed when the amounts (amt) of glucose are low and lactose are high or when they are both at medium level (see the fourth and fifth rule). The abducibles, A, declare all ground instances of the predicates \"amount\" as assumable. This reflects that in the model the amounts at any time of the various substances are unknown. This is incomplete information that is to be determined in each problem case. The integrity constraints, IC, state that the amount of any substance (S) can only take one value.\n\nDomain knowledge (P)\n\n   feed(lactose):-make(permease),make(galactosidase).\n   make(Enzyme):-code(Gene,Enzyme),express(Gene).\n   express(lac(X)):-amount(glucose,low),amount(lactose,hi).\n   express(lac(X)):-amount(glucose,medium),amount(lactose,medium).\n   code(lac(y),permease).\n   code(lac(z),galactosidase).\n   temperature(low):-amount(glucose,low).\nIntegrity constraints (IC)\n\n  false :- amount(S,V1), amount(S,V2), V1 ≠ V2.\nAbducibles (A)\n\n  abducible_predicate(amount).\nThe problem goal is {\\displaystyle G={\\text{feed(lactose)}}} G=\\text{feed(lactose)}. This can arise either as an observation to be explained or as a state of affairs to be achieved by finding a plan. This goal has two abductive explanations:\n\n{\\displaystyle \\Delta _{1}=\\{{\\text{amount(lactose, hi), amount(glucose, low)}}\\}} \\Delta_1=\\{\\text{amount(lactose, hi), amount(glucose, low)}\\}\n{\\displaystyle \\Delta _{2}=\\{{\\text{amount(lactose, medium), amount(glucose, medium)}}\\}} \\Delta_2=\\{\\text{amount(lactose, medium), amount(glucose, medium)}\\}\nThe decision which of the two to adopt could depend on addition information that is available, e.g. it may be known that when the level of glucose is low then the organism exhibits a certain behaviour – in the model such additional information is that the temperature of the organism is low – and by observing the truth or falsity of this it is possible to choose the first or second explanation respectively.\n\nOnce an explanation has been chosen, then this becomes part of the theory, which can be used to draw new conclusions. The explanation and more generally these new conclusions form the solution of the problem.\n\nFormal semantics[edit]\nThe formal semantics of the central notion of an abductive explanation in ALP can be defined in the following way.\n\nGiven an abductive logic program, {\\displaystyle \\langle P,A,{\\mathit {IC}}\\rangle } \\langle P,A,\\mathit{IC}\\rangle, an abductive explanation for a problem {\\displaystyle G} G is a set {\\displaystyle \\Delta } \\Delta  of ground atoms on abducible predicates such that:\n\n{\\displaystyle P\\cup \\Delta \\models G} P \\cup \\Delta \\models G\n{\\displaystyle P\\cup \\Delta \\models IC} P \\cup \\Delta \\models IC\n{\\displaystyle P\\cup \\Delta } P \\cup \\Delta is consistent\nThis definition leaves open the choice of the underlying semantics of logic programming through which we give the exact meaning of the entailment relation {\\displaystyle \\models } \\models  and the notion of consistency of the (extended) logic programs. Any of the different semantics of logic programming such as the completion, stable or well-founded semantics can (and have been used in practice) to give different notions of abductive explanations and thus different forms of ALP frameworks.\n\nThe above definition takes a particular view on the formalization of the role of the integrity constraints {\\displaystyle {\\mathit {IC}}} \\mathit{IC} as restrictions on the possible abductive solutions. It requires that these are entailed by the logic program extended with an abductive solution, thus meaning that in any model of the extended logic program (which one can think of as an ensuing world given {\\displaystyle \\Delta } \\Delta ) the requirements of the integrity constraints are met. In some cases this may be unnecessarily strong and the weaker requirement of consistency, namely that {\\displaystyle P\\cup {\\mathit {IC}}\\cup \\Delta } P \\cup \\mathit{IC} \\cup \\Delta is consistent, can be sufficient, meaning that there exists at least one model (possible ensuing world) of the extended program where the integrity constraints hold. In practice, in many cases these two ways of formalizing the role of the integrity constraints coincide as the logic program and its extensions always have a unique model. Many of the ALP systems use the entailment view of the integrity constraints as this can be easily implemented without the need for any extra specialized procedures for the satisfaction of the integrity constraints since this view treats the constraints in the same way as the problem goal. Note also that in many practical cases the third condition in this formal definition of an abductive explanation in ALP is either trivially satisfied or it is contained in the second condition via the use of specific integrity constraints that capture consistency.\n\nImplementation and systems[edit]\nMost of the implementations of ALP extend the SLD resolution-based computational model of logic programming. ALP can also be implemented by means of its link with Answer Set Programming (ASP), where the ASP systems can be employed. Examples of systems of the former approach are ACLP, A-system, CIFF, SCIFF, ABDUAL and ProLogICA.\n\nSee also[edit]\nAbductive reasoning\nAnswer set programming\nInductive logic programming\nNegation as failure\nArgumentation",
          "subparadigms": []
        },
        {
          "pdid": 18,
          "name": "Answer set programming",
          "details": "Answer set programming (ASP) is a form of declarative programming oriented towards difficult (primarily NP-hard) search problems. It is based on the stable model (answer set) semantics of logic programming. In ASP, search problems are reduced to computing stable models, and answer set solvers — programs for generating stable models—are used to perform search. The computational process employed in the design of many answer set solvers is an enhancement of the DPLL algorithm and, in principle, it always terminates (unlike Prolog query evaluation, which may lead to an infinite loop).\n\nIn a more general sense, ASP includes all applications of answer sets to knowledge representation[1][2] and the use of Prolog-style query evaluation for solving problems arising in these applications.\n\nContents  [hide] \n1\tHistory\n2\tAnswer set programming language AnsProlog\n3\tGenerating stable models\n4\tExamples of ASP programs\n4.1\tGraph coloring\n4.2\tLarge clique\n4.3\tHamiltonian cycle\n4.4\tDependency parsing\n5\tComparison of implementations\n6\tSee also\n7\tReferences\n8\tExternal links\nHistory[edit]\nThe planning method proposed in 1993 by Dimopoulos, Nebel and Köhler[3] is an early example of answer set programming. Their approach is based on the relationship between plans and stable models.[4] Soininen and Niemelä[5] applied what is now known as answer set programming to the problem of product configuration. The use of answer set solvers for search was identified as a new programming paradigm by Marek and Truszczyński in a paper that appeared in a 25-year perspective on the logic programming paradigm published in 1999 [6] and in [Niemelä 1999].[7] Indeed, the new terminology of \"answer set\" instead of \"stable model\" was first proposed by Lifschitz[8] in a paper appearing in the same retrospective volume as the Marek-Truszczynski paper.\n\nAnswer set programming language AnsProlog[edit]\nLparse is the name of the program that was originally created as a grounding tool (front-end) for the answer set solver smodels. The language that Lparse accepts is now commonly called AnsProlog*,[9] short for Answer Set Programming in Logic.[10] It is now used in the same way in many other answer set solvers, including assat, clasp, cmodels, gNt, nomore++ and pbmodels. (dlv is an exception; the syntax of ASP programs written for dlv is somewhat different.)\n\nAn AnsProlog program consists of rules of the form\n\n<head> :- <body> .\nThe symbol :- (\"if\") is dropped if <body> is empty; such rules are called facts. The simplest kind of Lparse rules are rules with constraints.\n\nOne other useful construct included in this language is choice. For instance, the choice rule\n\n{p,q,r}.\nsays: choose arbitrarily which of the atoms {\\displaystyle p,q,r} p,q,r to include in the stable model. The lparse program that contains this choice rule and no other rules has 8 stable models—arbitrary subsets of {\\displaystyle \\{p,q,r\\}} \\{p,q,r\\}. The definition of a stable model was generalized to programs with choice rules.[11] Choice rules can be treated also as abbreviations for propositional formulas under the stable model semantics.[12] For instance, the choice rule above can be viewed as shorthand for the conjunction of three \"excluded middle\" formulas:\n\n{\\displaystyle (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r).} (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r).\nThe language of lparse allows us also to write \"constrained\" choice rules, such as\n\n1{p,q,r}2.\nThis rule says: choose at least 1 of the atoms {\\displaystyle p,q,r} p,q,r, but not more than 2. The meaning of this rule under the stable model semantics is represented by the propositional formula\n\n{\\displaystyle (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r)} (p\\lor \\neg p)\\land (q\\lor \\neg q)\\land (r\\lor \\neg r)\n{\\displaystyle \\land \\,(p\\lor q\\lor r)\\land \\neg (p\\land q\\land r).} \\land \\,(p\\lor q\\lor r)\\land \\neg (p\\land q\\land r).\nCardinality bounds can be used in the body of a rule as well, for instance:\n\n:- 2{p,q,r}.\nAdding this constraint to an Lparse program eliminates the stable models that contain at least 2 of the atoms {\\displaystyle p,q,r} p,q,r. The meaning of this rule can be represented by the propositional formula\n\n{\\displaystyle \\neg ((p\\land q)\\lor (p\\land r)\\lor (q\\land r)).} \\neg ((p\\land q)\\lor (p\\land r)\\lor (q\\land r)).\nVariables (capitalized, as in Prolog) are used in Lparse to abbreviate collections of rules that follow the same pattern, and also to abbreviate collections of atoms within the same rule. For instance, the Lparse program\n\np(a). p(b). p(c).\nq(X) :- p(X), X!=a.\nhas the same meaning as\n\np(a). p(b). p(c).\nq(b). q(c).\nThe program\n\np(a). p(b). p(c).\n{q(X):-p(X)}2.\nis shorthand for\n\np(a). p(b). p(c).\n{q(a),q(b),q(c)}2.\nA range is of the form:\n\n<Predicate>(start..end)\nwhere start and end are constant valued arithmetic expressions. A range is a notational shortcut that is mainly used to define numerical domains in a compatible way. For example, the fact\n\na(1..3).\nis a shortcut for\n\na(1). a(2). a(3).\nRanges can also be used in rule bodies with the same semantics.\n\nA conditional literal is of the form:\n\np(X):q(X)\nIf the extension of q is {q(a1); q(a2); ... ; q(aN)}, the above condition is semantically equivalent to writing p(a1), p(a2), ... , p(aN) in the place of the condition. For example\n\nq(1..2).\na :- 1 {p(X):q(X)}.\nis a shorthand for\n\nq(1). q(2).\na :- 1 {p(1), p(2)}.\nGenerating stable models[edit]\nTo find a stable model of the Lparse program stored in file ${filename} we use the command\n\n% lparse ${filename} | smodels\nOption 0 instructs smodels to find all stable models of the program. For instance, if file test contains the rules\n\n1{p,q,r}2.\ns :- not p.\nthen the command produces the output\n\n% lparse test | smodels 0\nAnswer: 1\nStable Model: q p \nAnswer: 2\nStable Model: p \nAnswer: 3\nStable Model: r p \nAnswer: 4\nStable Model: q s \nAnswer: 5\nStable Model: r s \nAnswer: 6\nStable Model: r q s\nExamples of ASP programs[edit]\nGraph coloring[edit]\nAn {\\displaystyle n} n-coloring of a graph {\\displaystyle G=\\left\\langle V,E\\right\\rangle } {\\displaystyle G=\\left\\langle V,E\\right\\rangle } is a function {\\displaystyle color:V\\to \\{1,\\dots ,n\\}} {\\displaystyle color:V\\to \\{1,\\dots ,n\\}} such that {\\displaystyle color(x)\\neq color(y)} color(x)\\neq color(y) for every pair of adjacent vertices {\\displaystyle (x,y)\\in E} {\\displaystyle (x,y)\\in E}. We would like to use ASP to find an {\\displaystyle n} n-coloring of a given graph (or determine that it does not exist).\n\nThis can be accomplished using the following Lparse program:\n\nc(1..n).                                           \n1 {color(X,I) : c(I)} 1 :- v(X).             \n:- color(X,I), color(Y,I), e(X,Y), c(I).\nLine 1 defines the numbers {\\displaystyle 1,\\dots ,n} 1,\\dots ,n to be colors. According to the choice rule in Line 2, a unique color {\\displaystyle i} i should be assigned to each vertex {\\displaystyle x} x. The constraint in Line 3 prohibits assigning the same color to vertices {\\displaystyle x} x and {\\displaystyle y} y if there is an edge connecting them.\n\nIf we combine this file with a definition of {\\displaystyle G} G, such as\n\nv(1..100). % 1,...,100 are vertices\ne(1,55). % there is an edge from 1 to 55\n. . .\nand run smodels on it, with the numeric value of {\\displaystyle n} n specified on the command line, then the atoms of the form {\\displaystyle color(\\dots ,\\dots )} color(\\dots ,\\dots ) in the output of smodels will represent an {\\displaystyle n} n-coloring of {\\displaystyle G} G.\n\nThe program in this example illustrates the \"generate-and-test\" organization that is often found in simple ASP programs. The choice rule describes a set of \"potential solutions\" — a simple superset of the set of solutions to the given search problem. It is followed by a constraint, which eliminates all potential solutions that are not acceptable. However, the search process employed by smodels and other answer set solvers is not based on trial and error.\n\nLarge clique[edit]\nA clique in a graph is a set of pairwise adjacent vertices. The following lparse program finds a clique of size {\\displaystyle \\geq n} \\geq n in a given graph, or determines that it does not exist:\n\nn {in(X) : v(X)}.\n:- in(X), in(Y), v(X), v(Y), X!=Y, not e(X,Y), not e(Y,X).\nThis is another example of the generate-and-test organization. The choice rule in Line 1 \"generates\" all sets consisting of {\\displaystyle \\geq n} \\geq n vertices. The constraint in Line 2 \"weeds out\" the sets that are not cliques.\n\nHamiltonian cycle[edit]\nA Hamiltonian cycle in a directed graph is a cycle that passes through each vertex of the graph exactly once. The following Lparse program can be used to find a Hamiltonian cycle in a given directed graph if it exists; we assume that 0 is one of the vertices.\n\n{in(X,Y)} :- e(X,Y).\n\n:- 2 {in(X,Y) : e(X,Y)}, v(X).\n:- 2 {in(X,Y) : e(X,Y)}, v(Y).\n\nr(X) :- in(0,X), v(X).\nr(Y) :- r(X), in(X,Y), e(X,Y).\n\n:- not r(X), v(X).\nThe choice rule in Line 1 \"generates\" all subsets of the set of edges. The three constraints \"weed out\" the subsets that are not Hamiltonian cycles. The last of them uses the auxiliary predicate {\\displaystyle r(x)} r(x) (\" {\\displaystyle x} x is reachable from 0\") to prohibit the vertices that do not satisfy this condition. This predicate is defined recursively in Lines 4 and 5.\n\nThis program is an example of the more general \"generate, define and test\" organization: it includes the definition of an auxiliary predicate that helps us eliminate all \"bad\" potential solutions.\n\nDependency parsing[edit]\nIn natural language processing, dependency-based parsing can be formulated as an ASP problem.[13] The following code parses the Latin sentence Puella pulchra in villa linguam latinam discit \"the pretty girl is learning Latin in the villa\". The syntax tree is expressed by the arc predicates which represent the dependencies between the words of the sentence. The computed structure is a linearly ordered rooted tree.\n\n% ********** input sentence **********\nword(1, puella). word(2, pulchra). word(3, in). word(4, villa). word(5, linguam). word(6, latinam). word(7, discit).\n% ********** lexicon **********\n1{ node(X, attr(pulcher, a, fem, nom, sg));\n   node(X, attr(pulcher, a, fem, nom, sg)) }1 :- word(X, pulchra).\nnode(X, attr(latinus, a, fem, acc, sg)) :- word(X, latinam).\n1{ node(X, attr(puella, n, fem, nom, sg));\n   node(X, attr(puella, n, fem, abl, sg)) }1 :- word(X, puella).\n1{ node(X, attr(villa, n, fem, nom, sg));\n   node(X, attr(villa, n, fem, abl, sg)) }1 :- word(X, villa).\nnode(X, attr(linguam, n, fem, acc, sg)) :- word(X, linguam).\nnode(X, attr(discere, v, pres, 3, sg)) :- word(X, discit).\nnode(X, attr(in, p)) :- word(X, in).\n% ********** syntactic rules **********\n0{ arc(X, Y, subj) }1 :- node(X, attr(_, v, _, 3, sg)), node(Y, attr(_, n, _, nom, sg)).\n0{ arc(X, Y, dobj) }1 :- node(X, attr(_, v, _, 3, sg)), node(Y, attr(_, n, _, acc, sg)).\n0{ arc(X, Y, attr) }1 :- node(X, attr(_, n, Gender, Case, Number)), node(Y, attr(_, a, Gender, Case, Number)).\n0{ arc(X, Y, prep) }1 :- node(X, attr(_, p)), node(Y, attr(_, n, _, abl, _)), X < Y.\n0{ arc(X, Y, adv) }1 :- node(X, attr(_, v, _, _, _)), node(Y, attr(_, p)), not leaf(Y).\n% ********** guaranteeing the treeness of the graph **********\n1{ root(X):node(X, _) }1.\n:- arc(X, Z, _), arc(Y, Z, _), X != Y.\n:- arc(X, Y, L1), arc(X, Y, L2), L1 != L2.\npath(X, Y) :- arc(X, Y, _).\npath(X, Z) :- arc(X, Y, _), path(Y, Z).\n:- path(X, X).\n:- root(X), node(Y, _), X != Y, not path(X, Y).\nleaf(X) :- node(X, _), not arc(X, _, _).\nComparison of implementations[edit]\nEarly systems, such as Smodels, used backtracking to find solutions. As the theory and practice of Boolean SAT solvers evolved, a number of ASP solvers were built on top of SAT solvers, including ASSAT and Cmodels. These converted ASP formula into SAT propositions, applied the SAT solver, and then converted the solutions back to ASP form. More recent systems, such as Clasp, use a hybrid approach, using conflict-driven algorithms inspired by SAT, without full converting into a boolean-logic form. These approaches allow for significant improvements of performance, often by an order of magnitude, over earlier backtracking algorithms.\n\nThe Potassco project acts as an umbrella for many of the systems below, including clasp, grounding systems (gringo), incremental systems (iclingo), constraint solvers (clingcon), action language to ASP compilers (coala), distributed MPI implementations (claspar), and many others.\n\nMost systems support variables, but only indirectly, by forcing grounding, by using a grounding system such as Lparse or gringo as a front end. The need for grounding can cause a combinatorial explosion of clauses; thus, systems that perform on-the-fly grounding might have an advantage.\n\nPlatform\tFeatures\tMechanics\nName\tOS\tLicence\tVariables\tFunction symbols\tExplicit sets\tExplicit lists\tDisjunctive (choice rules) support\t\nASPeRiX\tLinux\tGPL\tYes\t\t\t\tNo\ton-the-fly grounding\nASSAT\tSolaris\tFreeware\t\t\t\t\t\tSAT-solver based\nClasp Answer Set Solver\tLinux, macOS, Windows\tGPL\tYes, in Clingo\tYes\tNo\tNo\tYes\tincremental, SAT-solver inspired (nogood, conflict-driven)\nCmodels\tLinux, Solaris\tGPL\tRequires grounding\t\t\t\tYes\tincremental, SAT-solver inspired (nogood, conflict-driven)\nDLV\tLinux, macOS, Windows[14]\tfree for academic and non-commercial educational use, and for non-profit organizations[14]\tYes\tYes\tNo\tNo\tYes\tnot Lparse compatible\nDLV-Complex\tLinux, macOS, Windows\tGPL\t\tYes\tYes\tYes\tYes\tbuilt on top of DLV — not Lparse compatible\nGnT\tLinux\tGPL\tRequires grounding\t\t\t\tYes\tbuilt on top of smodels\nnomore++\tLinux\tGPL\t\t\t\t\t\tcombined literal+rule-based\nPlatypus\tLinux, Solaris, Windows\tGPL\t\t\t\t\t\tdistributed, multi-threaded nomore++, smodels\nPbmodels\tLinux\t?\t\t\t\t\t\tpseudo-boolean solver based\nSmodels\tLinux, macOS, Windows\tGPL\tRequires grounding\tNo\tNo\tNo\tNo\t\nSmodels-cc\tLinux\t?\tRequires grounding\t\t\t\t\tSAT-solver based; smodels w/conflict clauses\nSup\tLinux\t?\t\t\t\t\t\tSAT-solver based\nSee also[edit]\nDefault logic\nLogic programming\nNon-monotonic logic\nProlog\nStable model semantics",
          "subparadigms": []
        },
        {
          "pdid": 19,
          "name": "Concurrent logic programming",
          "details": "Concurrent logic programming is a variant of logic programming in which programs are sets of guarded Horn clauses of the form:\n\nH :- G1, …, Gn | B1, …, Bn.\nThe conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator.\n\nDeclaratively, guarded Horn clauses are read as ordinary logical implications:\n\nH if G1 and … and Gn and B1 and … and Bn.\nHowever, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of \"don't care nondeterminism\", rather than \"don't know nondeterminism\".\n\nHistory[edit]\nThe first concurrent logic programming language was the Relational Language of Clark and Gregory, which was an offshoot of IC-Prolog. Later versions of concurrent logic programming include Shapiro's Concurrent Prolog and Ueda's Guarded Horn Clause language .\n\nThe development of concurrent logic programming was given an impetus when GHC was used to implement KL1, the systems programming language of the Japanese Fifth Generation Project (FGCS). The FGCS Project was a $400M initiative by Japan's Ministry of International Trade and Industry, begun in 1982, to use massively parallel computing/processing for artificial intelligence applications. The choice of concurrent logic programming as the “missing link” between the hardware and the applications was influenced by a visit to the FGCS Project in 1982 by Ehud Shapiro, who invented Concurrent Prolog.",
          "subparadigms": []
        },
        {
          "pdid": 20,
          "name": "Inductive logic programming",
          "details": "Inductive logic programming (ILP) is a subfield of machine learning which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\n\nSchema: positive examples + negative examples + background knowledge ⇒ hypothesis.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[1][2][3] Shapiro built its first implementation (Model Inference System) in 1981:[4] a Prolog program that inductively inferred logic programs from positive and negative examples. The term Inductive Logic Programming was first introduced[5] in a paper by Stephen Muggleton in 1991.[6] Muggleton also founded the annual international conference on Inductive Logic Programming, introduced the theoretical ideas of Predicate Invention, Inverse resolution,[7] and Inverse entailment,.[8] Muggleton implemented Inverse entailment first in the PROGOL system. The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction.\n\nContents  [hide] \n1\tFormal definition\n2\tExample\n3\tInductive Logic Programming system\n3.1\tHypothesis search\n3.2\tImplementations\n4\tSee also\n5\tReferences\n6\tFurther reading\nFormal definition[edit]\nThe background knowledge is given as a logic theory B, commonly in the form of Horn clauses used in logic programming. The positive and negative examples are given as a conjunction {\\displaystyle E^{+}} E^{+} and {\\displaystyle E^{-}} E^{-} of unnegated and negated ground literals, respectively. A correct hypothesis h is a logic proposition satisfying the following requirements.[9]\n\n{\\displaystyle {\\begin{array}{llll}{\\text{Necessity:}}&B&\\not \\models &E^{+}\\\\{\\text{Sufficiency:}}&B\\land h&\\color {blue}{\\models }&E^{+}\\\\{\\text{Weak consistency:}}&B\\land h&\\not \\models &{\\textit {false}}\\\\{\\text{Strong consistency:}}&B\\land h\\land E^{-}&\\not \\models &{\\textit {false}}\\end{array}}} {\\displaystyle {\\begin{array}{llll}{\\text{Necessity:}}&B&\\not \\models &E^{+}\\\\{\\text{Sufficiency:}}&B\\land h&\\color {blue}{\\models }&E^{+}\\\\{\\text{Weak consistency:}}&B\\land h&\\not \\models &{\\textit {false}}\\\\{\\text{Strong consistency:}}&B\\land h\\land E^{-}&\\not \\models &{\\textit {false}}\\end{array}}}\n\"Necessity\" does not impose a restriction on h, but forbids any generation of a hypothesis as long as the positive facts are explainable without it. \"Sufficiency\" requires any generated hypothesis h to explain all positive examples {\\displaystyle E^{+}} E^{+}. \"Weak consistency\" forbids generation of any hypothesis h that contradicts the background knowledge B. \"Strong consistency\" also forbids generation of any hypothesis h that is inconsistent with the negative examples {\\displaystyle E^{-}} E^{-}, given the background knowledge B; it implies \"Weak consistency\"; if no negative examples are given, both requirements coincide. Džeroski [10] requires only \"Sufficiency\" (called \"Completeness\" there) and \"Strong consistency\".\n\nExample[edit]\n\nAssumed family relations in section \"Example\"\nThe following well-known example about learning definitions of family relations uses the abbreviations\n\n{\\displaystyle {\\textit {par}}:{\\textit {parent}}} {\\textit  {par}}:{\\textit  {parent}}, {\\displaystyle {\\textit {fem}}:{\\textit {female}}} {\\textit  {fem}}:{\\textit  {female}}, {\\displaystyle {\\textit {dau}}:{\\textit {daughter}}} {\\textit  {dau}}:{\\textit  {daughter}}, {\\displaystyle g:{\\textit {George}}} g:{\\textit  {George}}, {\\displaystyle h:{\\textit {Helen}}} h:{\\textit  {Helen}}, {\\displaystyle m:{\\textit {Mary}}} m:{\\textit  {Mary}}, {\\displaystyle t:{\\textit {Tom}}} t:{\\textit  {Tom}}, {\\displaystyle n:{\\textit {Nancy}}} n:{\\textit  {Nancy}}, and {\\displaystyle e:{\\textit {Eve}}} e:{\\textit  {Eve}}.\nIt starts from the background knowledge (cf. picture)\n\n{\\displaystyle {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)} {\\textit  {par}}(h,m)\\land {\\textit  {par}}(h,t)\\land {\\textit  {par}}(g,m)\\land {\\textit  {par}}(t,e)\\land {\\textit  {par}}(n,e)\\land {\\textit  {fem}}(h)\\land {\\textit  {fem}}(m)\\land {\\textit  {fem}}(n)\\land {\\textit  {fem}}(e),\nthe positive examples\n\n{\\displaystyle {\\textit {dau}}(m,h)\\land {\\textit {dau}}(e,t)} {\\textit  {dau}}(m,h)\\land {\\textit  {dau}}(e,t),\nand the trivial proposition {\\displaystyle {\\textit {true}}} {\\textit  {true}} to denote the absence of negative examples.\n\nPlotkin's [11][12] \"relative least general generalization (rlgg)\" approach to inductive logic programming shall be used to obtain a suggestion about how to formally define the daughter relation {\\displaystyle {\\textit {dau}}} {\\textit  {dau}}.\n\nThis approach uses the following steps.\n\nRelativize each positive example literal with the complete background knowledge:\n{\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\end{aligned}}} {\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\leftarrow {\\textit {par}}(h,m)\\land {\\textit {par}}(h,t)\\land {\\textit {par}}(g,m)\\land {\\textit {par}}(t,e)\\land {\\textit {par}}(n,e)\\land {\\textit {fem}}(h)\\land {\\textit {fem}}(m)\\land {\\textit {fem}}(n)\\land {\\textit {fem}}(e)\\end{aligned}}},\nConvert into clause normal form:\n{\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\end{aligned}}} {\\displaystyle {\\begin{aligned}{\\textit {dau}}(m,h)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\\\{\\textit {dau}}(e,t)\\lor \\lnot {\\textit {par}}(h,m)\\lor \\lnot {\\textit {par}}(h,t)\\lor \\lnot {\\textit {par}}(g,m)\\lor \\lnot {\\textit {par}}(t,e)\\lor \\lnot {\\textit {par}}(n,e)\\lor \\lnot {\\textit {fem}}(h)\\lor \\lnot {\\textit {fem}}(m)\\lor \\lnot {\\textit {fem}}(n)\\lor \\lnot {\\textit {fem}}(e)\\end{aligned}}},\nAnti-unify each compatible [13] pair [14] of literals:\n{\\displaystyle {\\textit {dau}}(x_{me},x_{ht})} {\\textit  {dau}}(x_{{me}},x_{{ht}}) from {\\displaystyle {\\textit {dau}}(m,h)} {\\textit  {dau}}(m,h) and {\\displaystyle {\\textit {dau}}(e,t)} {\\textit  {dau}}(e,t),\n{\\displaystyle \\lnot {\\textit {par}}(x_{ht},x_{me})} \\lnot {\\textit  {par}}(x_{{ht}},x_{{me}}) from {\\displaystyle \\lnot {\\textit {par}}(h,m)} \\lnot {\\textit  {par}}(h,m) and {\\displaystyle \\lnot {\\textit {par}}(t,e)} \\lnot {\\textit  {par}}(t,e),\n{\\displaystyle \\lnot {\\textit {fem}}(x_{me})} \\lnot {\\textit  {fem}}(x_{{me}}) from {\\displaystyle \\lnot {\\textit {fem}}(m)} \\lnot {\\textit  {fem}}(m) and {\\displaystyle \\lnot {\\textit {fem}}(e)} \\lnot {\\textit  {fem}}(e),\n{\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m) from {\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m) and {\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m), similar for all other background-knowledge literals\n{\\displaystyle \\lnot {\\textit {par}}(x_{gt},x_{me})} \\lnot {\\textit  {par}}(x_{{gt}},x_{{me}}) from {\\displaystyle \\lnot {\\textit {par}}(g,m)} \\lnot {\\textit  {par}}(g,m) and {\\displaystyle \\lnot {\\textit {par}}(t,e)} \\lnot {\\textit  {par}}(t,e), and many more negated literals\nDelete all negated literals containing variables that don't occur in a positive literal:\nafter deleting all negated literals containing other variables than {\\displaystyle x_{me},x_{ht}} x_{{me}},x_{{ht}}, only {\\displaystyle {\\textit {dau}}(x_{me},x_{ht})\\lor \\lnot {\\textit {par}}(x_{ht},x_{me})\\lor \\lnot {\\textit {fem}}(x_{me})} {\\textit  {dau}}(x_{{me}},x_{{ht}})\\lor \\lnot {\\textit  {par}}(x_{{ht}},x_{{me}})\\lor \\lnot {\\textit  {fem}}(x_{{me}}) remains, together with all ground literals from the background knowledge\nConvert clauses back to Horn form:\n{\\displaystyle {\\textit {dau}}(x_{me},x_{ht})\\leftarrow {\\textit {par}}(x_{ht},x_{me})\\land {\\textit {fem}}(x_{me})\\land ({\\text{all background knowledge facts}})} {\\textit  {dau}}(x_{{me}},x_{{ht}})\\leftarrow {\\textit  {par}}(x_{{ht}},x_{{me}})\\land {\\textit  {fem}}(x_{{me}})\\land ({\\text{all background knowledge facts}})\nThe resulting Horn clause is the hypothesis h obtained by the rlgg approach. Ignoring the background knowledge facts, the clause informally reads \" {\\displaystyle x_{me}} x_{{me}} is called a daughter of {\\displaystyle x_{ht}} x_{{ht}} if {\\displaystyle x_{ht}} x_{{ht}} is the parent of {\\displaystyle x_{me}} x_{{me}} and {\\displaystyle x_{me}} x_{{me}} is female\", which is a commonly accepted definition.\n\nConcerning the above requirements, \"Necessity\" was satisfied because the predicate {\\displaystyle {\\textit {dau}}} {\\textit  {dau}} doesn't appear in the background knowledge, which hence cannot imply any property containing this predicate, such as the positive examples are. \"Sufficiency\" is satisfied by the computed hypothesis h, since it, together with {\\displaystyle {\\textit {par}}(h,m)\\land {\\textit {fem}}(m)} {\\textit  {par}}(h,m)\\land {\\textit  {fem}}(m) from the background knowledge, implies the first positive example {\\displaystyle {\\textit {dau}}(m,h)} {\\textit  {dau}}(m,h), and similarly h and {\\displaystyle {\\textit {par}}(t,e)\\land {\\textit {fem}}(e)} {\\textit  {par}}(t,e)\\land {\\textit  {fem}}(e) from the background knowledge implies the second positive example {\\displaystyle {\\textit {dau}}(e,t)} {\\textit  {dau}}(e,t). \"Weak consistency\" is satisfied by h, since h holds in the (finite) Herbrand structure described by the background knowledge; similar for \"Strong consistency\".\n\nThe common definition of the grandmother relation, viz. {\\displaystyle {\\textit {gra}}(x,z)\\leftarrow {\\textit {fem}}(x)\\land {\\textit {par}}(x,y)\\land {\\textit {par}}(y,z)} {\\textit  {gra}}(x,z)\\leftarrow {\\textit  {fem}}(x)\\land {\\textit  {par}}(x,y)\\land {\\textit  {par}}(y,z), cannot be learned using the above approach, since the variable y occurs in the clause body only; the corresponding literals would have been deleted in the 4th step of the approach. To overcome this flaw, that step has to be modified such that it can be parametrized with different literal post-selection heuristics. Historically, the GOLEM implementation is based on the rlgg approach.\n\nInductive Logic Programming system[edit]\nInductive Logic Programming system is a program that takes as an input logic theories {\\displaystyle B,E^{+},E^{-}} B,E^{+},E^{-} and outputs a correct hypothesis H wrt theories {\\displaystyle B,E^{+},E^{-}} B,E^{+},E^{-} An algorithm of an ILP system consists of two parts: hypothesis search and hypothesis selection. First a hypothesis is searched with an inductive logic programming procedure, then a subset of the found hypotheses (in most systems one hypothesis) is chosen by a selection algorithm. A selection algorithm scores each of the found hypotheses and returns the ones with the highest score. An example of score function include minimal compression length where a hypothesis with a lowest Kolmogorov complexity has the highest score and is returned. An ILP system is complete iff for any input logic theories {\\displaystyle B,E^{+},E^{-}} B,E^{+},E^{-} any correct hypothesis H wrt to these input theories can be found with its hypothesis search procedure.\n\nHypothesis search[edit]\nModern ILP systems like Progol,[6] Hail [15] and Imparo [16] find a hypothesis H using the principle of the inverse entailment[6] for theories B, E, H: {\\displaystyle B\\land H\\models E\\iff B\\land \\neg E\\models \\neg H} B\\land H\\models E\\iff B\\land \\neg E\\models \\neg H. First they construct an intermediate theory F called a bridge theory satisfying the conditions {\\displaystyle B\\land \\neg E\\models F} B\\land \\neg E\\models F and {\\displaystyle F\\models \\neg H} F\\models \\neg H. Then as {\\displaystyle H\\models \\neg F} H\\models \\neg F, they generalize the negation of the bridge theory F with the anti-entailment.[17] However, the operation of the anti-entailment since being highly non-deterministic is computationally more expensive. Therefore, an alternative hypothesis search can be conducted using the operation of the inverse subsumption (anti-subsumption) instead which is less non-deterministic than anti-entailment.\n\nQuestions of completeness of a hypothesis search procedure of specific ILP system arise. For example, Progol's hypothesis search procedure based on the inverse entailment inference rule is not complete by Yamamoto's example.[18] On the other hand, Imparo is complete by both anti-entailment procedure [19] and its extended inverse subsumption [20] procedure.\n\nImplementations[edit]\n1BC and 1BC2: first-order naive Bayesian classifiers: (http://www.cs.bris.ac.uk/Research/MachineLearning/1BC/)\nACE (A Combined Engine) (http://dtai.cs.kuleuven.be/ACE/)\nAleph (http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/)\nAtom (http://www.ahlgren.info/research/atom/)\nClaudien (http://dtai.cs.kuleuven.be/claudien/)\nDL-Learner (http://dl-learner.org)\nDMax (http://dtai.cs.kuleuven.be/dmax/)\nFOIL (ftp://ftp.cs.su.oz.au/pub/foil6.sh)\nGolem (ILP) (http://www.doc.ic.ac.uk/~shm/Software/golem)\nImparo[19]\nInthelex (INcremental THEory Learner from EXamples) (http://lacam.di.uniba.it:8000/systems/inthelex/)\nLime (http://cs.anu.edu.au/people/Eric.McCreath/lime.html)\nMetagol (http://github.com/metagol/metagol)\nMio (http://libra.msra.cn/Publication/3392493/mio-user-s-manual)\nMIS (Model Inference System) by Ehud Shapiro\nPROGOL (http://www.doc.ic.ac.uk/~shm/Software/progol5.0)\nRSD (http://labe.felk.cvut.cz/~zelezny/rsd/)\nTertius (http://www.cs.bris.ac.uk/publications/Papers/1000545.pdf)\nWarmr (now included in ACE)\nProGolem (http://ilp.doc.ic.ac.uk/ProGolem/) [21][22]\nSee also[edit]\nCommonsense reasoning\nFormal concept analysis\nInductive inference\nInductive reasoning\nInductive programming\nInductive probability\nStatistical relational learning\nVersion space learning",
          "subparadigms": []
        },
        {
          "pdid": 21,
          "name": "Logic programming",
          "details": "Logic programming is a programming paradigm based on formal logic. A program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n\nH :- B1, …, Bn.\nand are read declaratively as logical implications:\n\nH if B1 and … and Bn.\nH is called the head of the rule and B1, …, Bn is called the body. Facts are rules that have no body, and are written in the simplified form:\n\nH.\nIn the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.\n\nIn ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:\n\nto solve H, solve B1, and ... and solve Bn.\nConsider, for example, the following clause:\n\nfallible(X) :- human(X).\nbased on an example used by Terry Winograd [1] to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:\n\nhuman(socrates).\ncan be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by \"assigning\" socrates to X.\n\nThe declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.\n\nContents  [hide] \n1\tHistory\n2\tConcepts\n2.1\tLogic and control\n2.2\tProblem solving\n2.3\tNegation as failure\n2.4\tKnowledge representation\n3\tVariants and extensions\n3.1\tProlog\n3.2\tAbductive logic programming\n3.3\tMetalogic programming\n3.4\tConstraint logic programming\n3.5\tConcurrent logic programming\n3.6\tConcurrent constraint logic programming\n3.7\tInductive logic programming\n3.8\tHigher-order logic programming\n3.9\tLinear logic programming\n3.10\tObject-oriented logic programming\n3.11\tTransaction logic programming\n4\tSee also\n5\tReferences\n5.1\tGeneral introductions\n5.2\tOther sources\n6\tFurther reading\n7\tExternal links\nHistory[edit]\nThe use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.[2] This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.[3]\n\nLogic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.[citation needed]\n\nAlthough it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm.[4] Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time.[1] To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.[citation needed]\n\nHayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover.[5] Kowalski, on the other hand, developed SLD resolution,[6] a variant of SL-resolution,[7] and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.\n\nThe Association for Logic Programming was founded to promote Logic Programming in 1986.\n\nProlog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages,[8] constraint logic programming languages and datalog.[citation needed]\n\nConcepts[edit]\nLogic and control[edit]\nMain article: Declarative programming\nLogic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan\n\nAlgorithm = Logic + Control\nwhere \"Logic\" represents a logic program and \"Control\" represents different theorem-proving strategies.[9]\n\nProblem solving[edit]\nIn the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an \"and\". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an \"or\".\n\nAny search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.\n\nIn the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.\n\nNegation as failure[edit]\nMain article: Negation as failure\nFor most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:\n\nH :- A1, …, An, not B1, …, not Bn.\nand is read declaratively as a logical implication:\n\nH if A1 and … and An and not B1 and … and not Bn.\nwhere H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as \"negation as failure\", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:\n\ncanfly(X) :- bird(X), not abnormal(X).\nabnormal(X) :-  wounded(X).\nbird(john).\nbird(mary).\nwounded(john).\nGiven the goal of finding something that can fly:\n\n:- canfly(X).\nthere are two candidate solutions, which solve the first subgoal bird(X), namely X = john and X = mary. The second subgoal not abnormal(john) of the first candidate solution fails, because wounded(john) succeeds and therefore abnormal(john) succeeds. However, The second subgoal not abnormal(mary) of the second candidate solution succeeds, because wounded(mary) fails and therefore abnormal(mary) fails. Therefore, X = mary is the only solution of the goal.\n\nMicro-Planner had a construct, called \"thnot\", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as not(Goal) or \\+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \\+ X == 1 fails when the variable X has been bound to the atom 1, but it succeeds in all other cases, including when X is unbound. This makes Prolog's reasoning non-monotonic: X = 1, \\+ X == 1 always fails, while \\+ X == 1, X = 1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).\n\nThe logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say\n\nH :- Body1.\n…\nH :- Bodyk.\nas a definition of the predicate\n\nH iff (Body1 or … or Bodyk)\nwhere \"iff\" means \"if and only if\". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.\n\nFor example, the completion of the program above is:\n\ncanfly(X) iff bird(X), not abnormal(X).\nabnormal(X) iff wounded(X).\nbird(X) iff X = john or X = mary.\nX = X.\nnot john = mary.\nnot mary = john.\nThe notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.\n\nAs an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in \"extended logic programming\", to formalise such phrases as \"the contrary can not be shown\", where \"contrary\" is classical negation and \"can not be shown\" is the epistemic interpretation of negation as failure.\n\nKnowledge representation[edit]\nThe fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.\n\nDespite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor [10] credit the representation of the British Nationality Act as a logic program [11] with being \"hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences\".\n\nVariants and extensions[edit]\nProlog[edit]\nMain article: Prolog\nThe programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.\n\nIt was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation\n\nH :- B1, …, Bn.\nwhich can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.[6]\n\nColmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.\n\nAbductive logic programming[edit]\nAbductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be \"open\" or undefined. A clause in an abductive logic program has the form:\n\nH :- B1, …, Bn, A1, …, An.\nwhere H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:\n\nfalse :- B1, …, Bn.\nwhere the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:\n\ncanfly(X) :- bird(X), normal(X).\nfalse :-  normal(X), wounded(X).\nbird(john).\nbird(mary).\nwounded(john).\nwhere the predicate normal is abducible.\n\nProblem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:\n\n:- canfly(X).\nAbductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.\n\nMetalogic programming[edit]\nBecause mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called \"vanilla\" meta-interpreter:\n\n    solve(true).\n    solve((A,B)):- solve(A),solve(B).\n    solve(A):- clause(A,B),solve(B).\nwhere true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.\n\nMetalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.\n\nConstraint logic programming[edit]\nMain article: Constraint logic programming\nConstraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:\n\nH :- C1, …, Cn {\\displaystyle \\Diamond } \\Diamond  B1, …, Bn.\nwhere H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:\n\nH if C1 and … and Cn and B1 and … and Bn.\nHowever, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.\n\nProcedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.\n\nThe following constraint logic program represents a toy temporal database of john's history as a teacher:\n\nteaches(john, hardware, T) :- 1990 ≤ T, T < 1999.\nteaches(john, software, T) :- 1999 ≤ T, T < 2005.\nteaches(john, logic, T) :- 2005 ≤ T, T ≤ 2012.\nrank(john, instructor, T) :- 1990 ≤ T, T < 2010.\nrank(john, professor, T) :- 2010 ≤ T, T < 2014.\nHere ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:\n\n:- teaches(john, logic, T), rank(john, professor, T).\nThe solution is 2010 ≤ T, T ≤ 2012.\n\nConstraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.\n\nConcurrent logic programming[edit]\nMain article: Concurrent logic programming\nConcurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).[12]\n\nA concurrent logic program is a set of guarded Horn clauses of the form:\n\nH :- G1, …, Gn | B1, …, Bn.\nThe conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:\n\nH if G1 and … and Gn and B1 and … and Bn.\nHowever, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of \"don't care nondeterminism\", rather than \"don't know nondeterminism\".\n\nFor example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:\n\nshuffle([], [], []).\nshuffle(Left, Right, Merge) :-\n    Left = [First | Rest] |\n    Merge = [First | ShortMerge],\n    shuffle(Rest, Right, ShortMerge).\nshuffle(Left, Right, Merge) :-\n    Right = [First | Rest] |\n    Merge = [First | ShortMerge],\n    shuffle(Left, Rest, ShortMerge).\nHere, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:\n\nshuffle([ace, queen, king], [1, 4, 2], Merge).\nThe program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].\n\nArguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced.[13] However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.[neutrality is disputed]\n\nConcurrent constraint logic programming[edit]\nMain article: Concurrent constraint logic programming\nConcurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.\n\nInductive logic programming[edit]\nMain article: Inductive logic programming\nInductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.\n\nHigher-order logic programming[edit]\nSeveral researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.\n\nLinear logic programming[edit]\nBasing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli,[14] ACL,[15] and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.\n\nObject-oriented logic programming[edit]\nF-logic extends logic programming with objects and the frame syntax. A number of systems are based on F-logic, including Flora-2, FLORID, and a highly scalable commercial system Ontobroker.\n\nLogtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. Highly portable, it supports most standard-complaint Prolog systems as backend compilers.\n\nTransaction logic programming[edit]\nTransaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.\n\nSee also[edit]\nBoolean satisfiability problem\nConstraint logic programming\nDatalog\nFril\nFunctional programming\nFuzzy logic\nInductive logic programming\nLogic in computer science (includes Formal methods)\nLogic programming languages\nProgramming paradigm\nR++\nReasoning system\nRule-based machine learning\nSatisfiability",
          "subparadigms": [
            16,
            17,
            18,
            19
          ]
        },
        {
          "pdid": 22,
          "name": "Declarative programming",
          "details": "In computer science, declarative programming is a programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.[1]\n\nMany languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives[2] (the how being left up to the language's implementation). This is in contrast with imperative programming, which implements algorithms in explicit steps.\n\nDeclarative programming often considers programs as theories of a formal logic, and computations as deductions in that logic space. Declarative programming may greatly simplify writing parallel programs.[3]\n\nCommon declarative languages include those of database query languages (e.g., SQL, XQuery), regular expressions, logic programming, functional programming, and configuration management systems.\n\nContents  [hide] \n1\tDefinition\n2\tSubparadigms\n2.1\tConstraint programming\n2.2\tDomain-specific languages\n2.3\tFunctional programming\n2.4\tHybrid languages\n2.5\tLogic programming\n2.6\tModeling\n3\tSee also\n4\tReferences\n5\tExternal links\nDefinition[edit]\nDeclarative programming is often defined as any style of programming that is not imperative. A number of other common definitions exist that attempt to give the term a definition other than simply contrasting it with imperative programming. For example:\n\nA program that describes what computation should be performed and not how to compute it\nAny programming language that lacks side effects (or more specifically, is referentially transparent)\nA language with a clear correspondence to mathematical logic.[4]\nThese definitions overlap substantially.\n\nDeclarative programming contrasts with imperative and procedural programming. Declarative programming is a non-imperative style of programming in which programs describe their desired results without explicitly listing commands or steps that must be performed. Functional and logical programming languages are characterized by a declarative programming style. In logical programming languages, programs consist of logical statements, and the program executes by searching for proofs of the statements.\n\nIn a pure functional language, such as Haskell, all functions are without side effects, and state changes are only represented as functions that transform the state, which is explicitly represented as a first class object in the program. Although pure functional languages are non-imperative, they often provide a facility for describing the effect of a function as a series of steps. Other functional languages, such as Lisp, OCaml and Erlang, support a mixture of procedural and functional programming.\n\nSome logical programming languages, such as Prolog, and database query languages, such as SQL, while declarative in principle, also support a procedural style of programming.\n\nSubparadigms[edit]\nDeclarative programming is an umbrella term that includes a number of better-known programming paradigms.\n\nConstraint programming[edit]\nConstraint programming states relations between variables in the form of constraints that specify the properties of the target solution. The set of constraints is solved by giving a value to each variable so that the solution is consistent with the maximum number of constraints. Constraint programming often complements other paradigms: functional, logical, or even imperative programming.\n\nDomain-specific languages[edit]\nWell-known examples of declarative domain-specific languages (DSLs) include the yacc parser generator input language, the Make build specification language, Puppet's configuration management language, regular expressions, and a subset of SQL (SELECT queries, for example). DSLs have the advantage of being useful while not necessarily needing to be Turing-complete, which makes it easier for a language to be purely declarative.\n\nMany markup languages such as HTML, MXML, XAML, XSLT or other user-interface markup languages are often declarative. HTML, for example, only describes what should appear on a webpage - it specifies neither control flow rendering a page nor its possible interactions with a user.\n\nAs of 2013 some software systems combine traditional user-interface markup languages (such as HTML) with declarative markup that defines what (but not how) the back-end server systems should do to support the declared interface. Such systems, typically using a domain-specific XML namespace, may include abstractions of SQL database syntax or parameterised calls to web services using representational state transfer (REST) and SOAP.\n\nFunctional programming[edit]\nFunctional programming, and in particular purely functional programming, attempts to minimize or eliminate side effects, and is therefore considered declarative. Most[citation needed] functional languages, such as Scheme, Clojure, Erlang, Haskell, OCaml, Standard ML and Unlambda, however, do permit side effects in practice.\n\nWhile functional languages typically do appear to specify \"how\", a compiler for a purely functional programming language is free to extensively rewrite the operational behavior of a function, so long as the same result is returned for the same inputs. This can be used to, for example, make a function compute its result in parallel, or to perform substantial optimizations (such as deforestation) that a compiler may not be able to safely apply to a language with side effects.\n\nHybrid languages[edit]\nSee also: Multi-paradigm programming language\nMakefiles, for example, specify dependencies in a declarative fashion,[5] but include an imperative list of actions to take as well. Similarly, yacc specifies a context free grammar declaratively, but includes code snippets from a host language, which is usually imperative (such as C).\n\nLogic programming[edit]\nLogic programming languages such as Prolog state and query relations. The specifics of how these queries are answered is up to the implementation and its theorem prover, but typically take the form of some sort of unification. Like functional programming, many logic programming languages permit side effects, and as a result are not strictly declarative.\n\nModeling[edit]\nMain article: Mathematical model\nModels, or mathematical representations, of physical systems may be implemented in computer code that is declarative. The code contains a number of equations, not imperative assignments, that describe (\"declare\") the behavioral relationships. When a model is expressed in this formalism, a computer is able to perform algebraic manipulations to best formulate the solution algorithm. The mathematical causality is typically imposed at the boundaries of the physical system, while the behavioral description of the system itself is declarative or acausal. Declarative modeling languages and environments include Analytica, Modelica and Simile.[6]\n\nSee also[edit]\nList of declarative programming languages\nComparison of programming paradigms\nInductive programming",
          "subparadigms": [
            9,
            13,
            16,
            21
          ]
        },
        {
          "pdid": 23,
          "name": "Dynamic programming language",
          "details": "Dynamic programming language, in computer science, is a class of high-level programming languages which, at runtime, execute many common programming behaviors that static programming languages perform during compilation. These behaviors could include extension of the program, by adding new code, by extending objects and definitions, or by modifying the type system. Although similar behaviours can be emulated in nearly any language, with varying degrees of difficulty, complexity and performance costs, dynamic languages provide direct tools to make use of them. Many of these features were first implemented as native features in the Lisp programming language.\n\nMost dynamic languages are also dynamically typed, but not all are. Dynamic languages are frequently (but not always) referred to as \"scripting languages\", although the term \"scripting language\" in its narrowest sense refers to languages specific to a given run-time environment.\n\n\n\nContents  [hide] \n1\tImplementation\n1.1\tEval\n1.2\tObject runtime alteration\n1.3\tFunctional programming\n1.3.1\tClosures\n1.3.2\tContinuations\n1.4\tReflection\n1.5\tMacros\n2\tExamples\n2.1\tComputation of code at runtime and late binding\n2.2\tObject runtime alteration\n2.3\tAssembling of code at runtime based on the class of instances\n3\tExamples\n4\tSee also\n5\tReferences\n6\tFurther reading\n7\tExternal links\nImplementation[edit]\n[icon]\tThis section needs expansion. You can help by adding to it. (October 2009)\nEval[edit]\nSome dynamic languages offer an eval function. This function takes a string parameter containing code in the language, and executes it. If this code stands for an expression, the resulting value is returned. However, Erik Meijer and Peter Drayton suggest that programmers \"use eval as a poor man's substitute for higher-order functions.\"[1]\n\nObject runtime alteration[edit]\nA type or object system can typically be modified during runtime in a dynamic language. This can mean generating new objects from a runtime definition or based on mixins of existing types or objects. This can also refer to changing the inheritance or type tree, and thus altering the way that existing types behave (especially with respect to the invocation of methods).\n\nFunctional programming[edit]\nFunctional programming concepts are a feature of many dynamic languages, and also derive from Lisp.\n\nClosures[edit]\nOne of the most widely used aspects of functional programming in dynamic languages is the closure, which allows creating a new instance of a function which retains access to the context in which it was created. A simple example of this is generating a function for scanning text for a word:\n\nfunction new_scanner (word)\n  temp_function = function (input)\n    scan_for_text (input, word)\n  end function\n  return temp_function\nend function\nNote that the inner function has no name, and is instead stored in the variable temp_function. Each time new_scanner is executed, it will return a new function which remembers the value of the word parameter that was passed in when it was defined.\n\nClosures[2] are one of the core tools of functional programming, and many languages support at least this degree of functional programming.\n\nContinuations[edit]\nAnother feature of some dynamic languages is the continuation. Continuations represent execution states that can be re-invoked. For example, a parser might return an intermediate result and a continuation that, when invoked, will continue to parse the input. Continuations interact in very complex ways with scoping, especially with respect to closures. For this reason, many dynamic languages do not provide continuations.\n\nReflection[edit]\nReflection is common in many dynamic languages, and typically involves analysis of the types and metadata of generic or polymorphic data. It can, however, also include full evaluation and modification of a program's code as data, such as the features that Lisp provides in analyzing S-expressions.\n\nMacros[edit]\nA limited number of dynamic programming languages provide features which combine code introspection (the ability to examine classes, functions and keywords to know what they are, what they do and what they know) and eval in a feature called macros. Most programmers today who are aware of the term macro have encountered them in C or C++, where they are a static feature which are built in a small subset of the language, and are capable only of string substitutions on the text of the program. In dynamic languages, however, they provide access to the inner workings of the compiler, and full access to the interpreter, virtual machine, or runtime, allowing the definition of language-like constructs which can optimize code or modify the syntax or grammar of the language.\n\nAssembly, C, C++, early Java, and FORTRAN do not generally fit into this category.[clarification needed]\n\nExamples[edit]\nThe following examples show dynamic features using the language Common Lisp and its Common Lisp Object System.\n\nComputation of code at runtime and late binding[edit]\nThe example shows how a function can be modified at runtime from computed source code\n\n; the source code is stored as data in a variable\nCL-USER > (defparameter *best-guess-formula* '(lambda (x) (* x x 2.5)))\n*BEST-GUESS-FORMULA*\n\n; a function is created from the code and compiled at runtime, the function is available under the name best-guess\nCL-USER >  (compile 'best-guess *best-guess-formula*)\n#<Function 15 40600152F4>\n\n; the function can be called\nCL-USER > (best-guess 10.3)\n265.225\n\n; the source code might be improved at runtime\nCL-USER > (setf *best-guess-formula* `(lambda (x) ,(list 'sqrt (third *best-guess-formula*))))\n(LAMBDA (X) (SQRT (* X X 2.5)))\n\n; a new version of the function is being compiled\nCL-USER > (compile 'best-guess *best-guess-formula*)\n#<Function 16 406000085C>\n\n; the next call will call the new function, a feature of late binding\nCL-USER > (best-guess 10.3)\n16.28573\nObject runtime alteration[edit]\nThis example shows how an existing instance can be changed to include a new slot when its class changes and that an existing method can be replaced with a new version.\n\n; a person class. The person has a name.\nCL-USER > (defclass person () ((name :initarg :name)))\n#<STANDARD-CLASS PERSON 4020081FB3>\n\n; a custom printing method for the objects of class person\nCL-USER > (defmethod print-object ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (format stream \"~a\" (slot-value p 'name))))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 4020066E5B>\n\n; one example person instance\nCL-USER > (setf *person-1* (make-instance 'person :name \"Eva Luator\"))\n#<PERSON Eva Luator>\n\n; the class person gets a second slot. It then has the slots name and age.\nCL-USER > (defclass person () ((name :initarg :name) (age :initarg :age :initform :unknown)))\n#<STANDARD-CLASS PERSON 4220333E23>\n\n; updating the method to print the object\nCL-USER > (defmethod print-object ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (format stream \"~a age: ~\" (slot-value p 'name) (slot-value p 'age))))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 402022ADE3>\n\n; the existing object has now changed, it has an additional slot and a new print method\nCL-USER > *person-1*\n#<PERSON Eva Luator age: UNKNOWN>\n\n; we can set the new age slot of instance\nCL-USER > (setf (slot-value *person-1* 'age) 25)\n25\n\n; the object has been updated\nCL-USER > *person-1*\n#<PERSON Eva Luator age: 25>\nAssembling of code at runtime based on the class of instances[edit]\nIn the next example the class person gets a new superclass. The print method gets redefined such that it assembles several methods into the effective method. The effective method gets assembled based on the class of the argument and the at runtime available and applicable methods.\n\n; the class person\nCL-USER > (defclass person () ((name :initarg :name)))\n#<STANDARD-CLASS PERSON 4220333E23>\n\n; a person just prints its name\nCL-USER > (defmethod print-object ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (format stream \"~a\" (slot-value p 'name))))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 40200605AB>\n\n; a person instance\nCL-USER > (defparameter *person-1* (make-instance 'person :name \"Eva Luator\"))\n*PERSON-1*\n\n; displaying a person instance\nCL-USER > *person-1*\n#<PERSON Eva Luator>\n\n; now redefining the print method to be extensible\n; the around method creates the context for the print method and it calls the next method\nCL-USER > (defmethod print-object :around ((p person) stream)\n            (print-unreadable-object (p stream :type t)\n              (call-next-method)))\n#<STANDARD-METHOD PRINT-OBJECT (:AROUND) (PERSON T) 4020263743>\n\n; the primary method prints the name\nCL-USER > (defmethod print-object ((p person) stream)\n            (format stream \"~a\" (slot-value p 'name)))\n#<STANDARD-METHOD PRINT-OBJECT NIL (PERSON T) 40202646BB>\n\n; a new class id-mixin provides an id\nCL-USER > (defclass id-mixin () ((id :initarg :id)))\n#<STANDARD-CLASS ID-MIXIN 422034A7AB>\n\n; the print method just prints the value of the id slot\nCL-USER > (defmethod print-object :after ((object id-mixin) stream)\n          (format stream \" ID: ~a\" (slot-value object 'id)))\n#<STANDARD-METHOD PRINT-OBJECT (:AFTER) (ID-MIXIN T) 4020278E33>\n\n; now we redefine the class person to include the mixin id-mixin\nCL-USER 241 > (defclass person (id-mixin) ((name :initarg :name)))\n#<STANDARD-CLASS PERSON 4220333E23>\n\n; the existing instance *person-1* now has a new slot and we set it to 42\nCL-USER 242 > (setf (slot-value *person-1* 'id) 42)\n42\n\n; displaying the object again. The print-object function now has an effective method, which calls three methods: an around method, the primary method and the after method.\nCL-USER 243 > *person-1*\n#<PERSON Eva Luator ID: 42>\nExamples[edit]\nPopular dynamic programming languages include JavaScript, Python, Ruby, PHP, Lua and Perl. The following are generally considered dynamic languages:\n\nActionScript\nBeanShell[3]\nC# (using Reflection)\nCobolscript\nClojure\nColdFusion Markup Language\nCommon Lisp and most other Lisps\nDylan\nE\nGambas\nGroovy[4]\nJava (using Reflection)\nJavaScript\nJulia\nLua\nMATLAB / Octave\nObjective-C\nPerl\nPHP\nPowershell\nProlog\nPython\nR\nRuby\nScala\nSmalltalk\nSuperCollider\nTcl\nVBScript\nWolfram Language\nGDScript",
          "subparadigms": []
        },
        {
          "pdid": 24,
          "name": "End-user development",
          "details": "End-user development (EUD) or end-user programming (EUP) refers to activities and tools that allow end-users – people who are not professional software developers – to program computers. People who are not professional developers can use EUD tools to create or modify software artifacts (descriptions of automated behavior) and complex data objects without significant knowledge of a programming language. Various approaches exist, and it is an active research topic within the field of computer science and human-computer interaction. Examples include spreadsheets, scripting languages (particularly in an office suite or art application), and programming by example.\n\nThe most popular EUD tool is the spreadsheet.[1] [2] Due to their unrestricted nature, spreadsheets allow relatively un-sophisticated computer users to write programs that represent complex data models, while shielding them from the need to learn lower-level programming languages.[3] Because of their common use in business, spreadsheet skills are among the most beneficial skills for a graduate employee to have, and are therefore the most commonly sought after[4] In the United States of America alone, there are an estimated 13 million end user developers programming with spreadsheets[5]\n\nEarly attempts in End-user development were centered on adding simple scripting programming languages to extend and adapt an existing application, such as an office suite.\n\nMore recent research tries to bring programming closer to the needs of end users. The Programming by example (PbE) approach reduces the need for the user to learn the abstractions of a classic programming language. The user instead introduces some examples of the desired results or operations that should be performed on the data, and the PbE system infers some abstractions corresponding to a program that produces this output, which the user can refine. New data may then be introduced to the automatically created program, and the user can correct any mistakes made by the program in order to improve its definition.\n\nThere are two basic reasons why EUD has become popular.[citation needed] One is because organizations are facing delays on projects and using EUD can effectively cut the time of completion on a project. The second reason is that software tools are more powerful and easier to use.\n\nLessons learned from EUD solutions can significantly influence the software life cycles for commercial software products, in-house intranet/extranet developments and enterprise application deployments.\n\nContents  [hide] \n1\tDefinition\n2\tExamples\n3\tCost-benefit modeling of end-user development\n4\tCriticism\n5\tSee also\n6\tReferences\n7\tFurther reading\n8\tExternal links\nDefinition[edit]\nLieberman et al. propose the following definition:[6]\n\nEnd-User Development can be defined as a set of methods, techniques, and tools that allow users of software systems, who are acting as non-professional software developers, at some point to create, modify or extend a software artifact.\n\nArtifacts defined by end users may be objects describing some automated behavior or control sequence, such as database requests or grammar rules,[7] which can be described with programming paradigms such as programming by demonstration, programming with examples, visual programming, or macro generation.[8] They can also be parameters that choose between alternative predefined behaviors of an application.[9] Other artifacts of end-user development may also refer to the creation of user-generated content such as annotations, which may be or not computationally interpretable (i.e. can be processed by associated automated functions).[10]\n\nExamples[edit]\nExamples of end-user development include the creation and modification of:\n\n3D models created with end-user oriented tools and approaches such as Sketchup\nAnimation scripts used by graphic artists to describe characters, environments and how characters move to produce an intended animation\nConfiguration files that blur the line between programs and data (e.g., email filters are sequenced lists of criteria and actions to take)\nExample-Centric Programming tools[11]\nArguably, contributions to open source projects where users of a software package contribute their own code for inclusion in the main package — in some cases, end-users participate as full-fledged developers\nGame modifications to introduce users' own characters, environments, etc. — many recent games are distributed with modification in mind\nInteraction scripts used in CRM call centres\nProcess models used in workflow applications\nPrototypes and domain-specific programs written by businesspeople, engineers, and scientists to demonstrate or test specific theories\nScientific models used in computer simulation\nScripts and macros added to extend or automate office productivity suites and graphics applications.\nSimulations created using application definition software\nSimultaneous editing of many related items either through a batch process specified by the end user or by direct manipulation, like those available in the Lapis text editor and multi edit.\nSpreadsheet models, e.g., used for budgeting, risk analysis, interactive machine learning,[12] or electronic circuit design[13]\nVisual programming in the form of visual languages such as AgentSheets, LabVIEW, Scratch (programming language) or LEGO Mindstorms.\nEnd-user mobile app development tools such as App Inventor\nWeb pages - plain HTML or HTML and scripting\nWikis - a collaborative end-user development process[citation needed]\nWeb Mashups in the form of visual languages such as Yahoo! Pipes.[14]\nVisual query systems such as OptiqueVQS.[15]\nCost-benefit modeling of end-user development[edit]\nAccording to Sutcliffe,[16] EUD essentially outsources development effort to the end user. Because there is always some effort to learn an EUD tool, the users' motivation depends on their confidence that it will empower their work, save time on the job or raise productivity. In this model, the benefits to users are initially based on marketing, demonstrations and word-of-mouth. Once the technology is put into use, experience of actual benefits becomes the key motivator.\n\nThis study defines costs as the sum of:\n\nTechnical cost: the price of the technology and the effort to install it\nLearning cost: the time taken to understand the technology\nDevelopment cost: the effort to develop applications using the technology\nTest and debugging cost: the time taken to verify the system\nThe first and second costs are incurred once during acquisition, whereas the third and fourth are incurred every time an application is developed. Benefits (which may be perceived or actual) are seen as:\n\nFunctionality delivered by the technology\nFlexibility to respond to new requirements\nUsability of applications produced\nOverall quality of the applications produced\nCriticism[edit]\nCommentators have been concerned that end users do not understand how to test and secure their applications. Warren Harrison, a professor of computer science at Portland State University, wrote:[17]\n\nIt’s simply unfathomable that we could expect security... from the vast majority of software applications out there when they’re written with little, if any, knowledge of generally accepted good practices such as specifying before coding, systematic testing, and so on.... How many X for Complete Idiots (where \"X\" is your favorite programming language) books are out there? I was initially amused by this trend, but recently I’ve become uneasy thinking about where these dabblers are applying their newfound knowledge.\n\nThis viewpoint assumes that all end users are equally naive when it comes to understanding software, although Pliskin and Shoval argue this is not the case, that sophisticated end users are capable of end-user development.[18]\n\nIn response to this, the study of end-user software engineering has emerged. It is concerned with issues beyond end-user development, whereby end users become motivated to consider issues such as reusability, security and verifiability when developing their solutions.[19]\n\nAn alternative scenario is that end users or their consultants employ declarative tools that support rigorous business and security rules at the expense of performance and scalability; tools created using EUD will typically have worse efficiency than those created with professional programming environments. Though separating functionality from efficiency is a valid separation of concerns, it can lead to a situation where end users will complete and document the requirements analysis and prototyping of the tool, without the involvement of business analysts. Thus, users will define the functions required before these experts have a chance to consider the limitations of a specific application or software framework. Senior management support for such end-user initiatives depends on their attitude to existing or potential vendor lock-in.\n\nSee also[edit]\nEnd-user computing\nSituational application\nSoftware engineering\nNatural language programming",
          "subparadigms": []
        },
        {
          "pdid": 25,
          "name": "Service-oriented",
          "details": "A service-oriented architecture (SOA) is a style of software design where services are provided to the other components by application components, through a communication protocol over a network. The basic principles of service oriented architecture are independent of vendors, products and technologies.[1] A service is a discrete unit of functionality that can be accessed remotely and acted upon and updated independently, such as retrieving a credit card statement online.\n\nA service has four properties according to one of many definitions of SOA:[2]\n\nIt logically represents a business activity with a specified outcome.\nIt is self-contained.\nIt is a black box for its consumers.\nIt may consist of other underlying services.[3]\nDifferent services can be used in conjunction to provide the functionality of a large software application.[4] Service-oriented architecture makes it easier for software components to communicate and cooperate over the network, without requiring any human interaction or changes in the underlying program, so that service candidates can be redesigned before their implementation.\n\nContents  [hide] \n1\tOverview\n2\tDefining concepts\n3\tPrinciples\n4\tPatterns\n5\tImplementation approaches\n6\tOrganizational benefits\n7\tCriticisms\n8\tExtensions and variants\n8.1\tEvent-driven architectures\n8.2\tWeb 2.0\n8.3\tMicroservices\n9\tSee also\n10\tReferences\n11\tExternal links\nOverview[edit]\nIn SOA, services use protocols which describe how they pass and parse messages using description metadata, this metadata describes both the functional characteristics of the service and quality-of-service characteristics. Service-oriented architecture aims to allow users to combine large chunks of functionality to form applications which are built purely from existing services and combining them in an ad hoc manner. A service presents a simple interface to the requester that abstracts away the underlying complexity acting as a black box, Further users can also access these independent services without any knowledge of their internal implementation.[5]\n\nDefining concepts[edit]\nThe related buzzword service-orientation promotes loose coupling between services. SOA separates functions into distinct units, or services,[6] which developers make accessible over a network in order to allow users to combine and reuse them in the production of applications. These services and their corresponding consumers communicate with each other by passing data in a well-defined, shared format, or by coordinating an activity between two or more services.[7]\n\nA manifesto was published for service-oriented architecture in October, 2009. This came up with six core values which are listed as follows[8]\n\nBusiness value is given more importance than technical strategy.\nStrategic goals is given more importance than project-specific benefits.\nIntrinsic inter-operability is given more importance than custom integration.\nShared services is given more importance than specific-purpose implementations.\nFlexibility is given more importance than optimization.\nEvolutionary refinement is given more importance than pursuit of initial perfection.\nSOA can be seen as part of the continuum which ranges from the older concept of distributed computing[6][9] and modular programming, through SOA, and on to current practices of mashups, SaaS, and cloud computing (which some see as the offspring of SOA).[10]\n\nPrinciples[edit]\nThere are no industry standards relating to the exact composition of a service-oriented architecture, although many industry sources have published their own principles. Some of these[11][12][13][14] include the following:\n\nStandardized service contract\nServices adhere to a standard communications agreements, as defined collectively by one or more service-description documents within a given set of services.\nService reference autonomy (an aspect of loose coupling)\nThe relationship between services is minimized to the level that they are only aware of their existence.\nService location transparency (an aspect of loose coupling)\nServices can be called from anywhere within the network that it is located no matter where it is present.\nService abstraction\nThe services act as black boxes, that is their inner logic is hidden from the consumers.\nService autonomy\nServices are independent and control the functionality they encapsulate, from a Design-time and a run-time perspective.\nService statelessness\nServices are stateless that is either return the requested value or a give an exception hence minimizing resource use.\nService granularity\nA principle to ensure services have an adequate size and scope. The functionality provided by the service to the user must be relevant.\nService normalization\nServices are decomposed or consolidated (normalized) to minimize redundancy. In some, this may not be done, These are the cases where performance optimization, access, and aggregation are required.[15]\nService composability\nServices can be used to compose other services.\nService discovery\nServices are supplemented with communicative meta data by which they can be effectively discovered and interpreted.\nService re usability\nLogic is divided into various services, to promote re use of code.\nService encapsulation\nMany services which were not initially planned under SOA, may get encapsulated or become a part of SOA.\nPatterns[edit]\nEach SOA building block can play any of the three roles:\n\nService provider\nIt creates a web service and provides its information to the service registry. Each provider debates upon a lot of hows and whys likes which service to expose, whom to give more importance: security or easy availability, what price to offer the service for and many more. The provider also has to decide what category the service should be listed in for a given broker service and what sort of trading partner agreements are required to use the service.\nService broker, service registry or service repository\nIts main functionality is to make the information regarding the web service available to any potential requester. Whoever implements the broker decides the scope of the broker. Public brokers are available anywhere and everywhere but private brokers are only available to a limited amount of public. UDDI was an early, no longer actively supported attempt to provide Web services discovery.\nService requester/consumer\nIt locates entries in the broker registry using various find operations and then binds to the service provider in order to invoke one of its web services. Whichever service the service-consumers need, they have to take it into the brokers, bind it with respective service and then use it. They can access multiple services if the service provides multiple services.\nThe service consumer-provider relationship is governed by a service contract, which has a business part, a functional part and a technical part.\n\nPossible first-class service composition patterns are two sides of the same coin. These patterns are generally event-driven:\n\nOrchestration is usually implemented and executed centrally through a Enterprise service bus\nChoreography is enacted by all participants and could be implemented with workflow management system [16]\nLower level Enterprise Integration Patterns that are not bound to a particular architectural style continue to be relevant and eligible in SOA design.[17][18][19]\n\nImplementation approaches[edit]\nService-oriented architecture can be implemented with Web services.[20] This is done to make the functional building-blocks accessible over standard Internet protocols that are independent of platforms and programming languages. These services can represent either new applications or just wrappers around existing legacy systems to make them network-enabled.[21]\n\nImplementers commonly build SOAs using web services standards (for example, SOAP) that have gained broad industry acceptance after recommendation of Version 1.2 from the W3C[22] (World Wide Web Consortium) in 2003. These standards (also referred to as web service specifications) also provide greater interoperability and some protection from lock-in to proprietary vendor software. One can, however, implement SOA using any service-based technology, such as Jini, CORBA or REST.\n\nArchitectures can operate independently of specific technologies and can therefore be implemented using a wide range of technologies, including:\n\nWeb services based on WSDL and SOAP\nMessaging, e.g., with ActiveMQ, JMS, RabbitMQ\nRESTful HTTP, with Representational state transfer (REST) constituting its own constraints-based architectural style\nOPC-UA\nWCF (Microsoft's implementation of Web services, forming a part of WCF)\nApache Thrift\nSORCER\nImplementations can use one or more of these protocols and, for example, might use a file-system mechanism to communicate data following a defined interface specification between processes conforming to the SOA concept. The key is independent services with defined interfaces that can be called to perform their tasks in a standard way, without a service having foreknowledge of the calling application, and without the application having or needing knowledge of how the service actually performs its tasks. SOA enables the development of applications that are built by combining loosely coupled and interoperable services.\n\nThese services inter-operate based on a formal definition (or contract, e.g., WSDL) that is independent of the underlying platform and programming language. The interface definition hides the implementation of the language-specific service. SOA-based systems can therefore function independently of development technologies and platforms (such as Java, .NET, etc.). Services written in C# running on .NET platforms and services written in Java running on Java EE platforms, for example, can both be consumed by a common composite application (or client). Applications running on either platform can also consume services running on the other as web services that facilitate reuse. Managed environments can also wrap COBOL legacy systems and present them as software services..[23]\n\nHigh-level programming languages such as BPEL and specifications such as WS-CDL and WS-Coordination extend the service concept by providing a method of defining and supporting orchestration of fine-grained services into more coarse-grained business services, which architects can in turn incorporate into workflows and business processes implemented in composite applications or portals[24]\n\nService-oriented modeling is an SOA framework that identifies the various disciplines that guide SOA practitioners to conceptualize, analyze, design, and architect their service-oriented assets. The Service-oriented modeling framework (SOMF) offers a modeling language and a work structure or \"map\" depicting the various components that contribute to a successful service-oriented modeling approach. It illustrates the major elements that identify the \"what to do\" aspects of a service development scheme. The model enables practitioners to craft a project plan and to identify the milestones of a service-oriented initiative. SOMF also provides a common modeling notation to address alignment between business and IT organizations.\n\n\nElements of SOA, by Dirk Krafzig, Karl Banke, and Dirk Slama[25]\n\nSOA meta-model, The Linthicum Group, 2007\nOrganizational benefits[edit]\nSome enterprise architects believe that SOA can help businesses respond more quickly and more cost-effectively to changing market conditions.[26] This style of architecture promotes reuse at the macro (service) level rather than micro (classes) level. It can also simplify interconnection to—and usage of—existing IT (legacy) assets.\n\nWith SOA, the idea is that an organization can look at a problem holistically. A business has more overall control. Theoretically there would not be a mass of developers using whatever tool sets might please them. But rather they would be coding to a standard that is set within the business. They can also develop enterprise-wide SOA that encapsulates a business-oriented infrastructure. SOA has also been illustrated as a highway system providing efficiency for car drivers. The point being that if everyone had a car, but there was no highway anywhere, things would be limited and disorganized, in any attempt to get anywhere quickly or efficiently. IBM Vice President of Web Services Michael Liebow says that SOA \"builds highways\".[27]\n\nIn some respects, SOA could be regarded as an architectural evolution rather than as a revolution. It captures many of the best practices of previous software architectures. In communications systems, for example, little development of solutions that use truly static bindings to talk to other equipment in the network has taken place. By embracing a SOA approach, such systems can position themselves to stress the importance of well-defined, highly inter-operable interfaces. Other predecessors of SOA include Component-based software engineering and Object-Oriented Analysis and Design (OOAD) of remote objects, for instance, in CORBA.\n\nA service comprises a stand-alone unit of functionality available only via a formally defined interface. Services can be some kind of \"nano-enterprises\" that are easy to produce and improve. Also services can be \"mega-corporations\" constructed as the coordinated work of subordinate services. A mature rollout of SOA effectively defines the API of an organization.\n\nReasons for treating the implementation of services as separate projects from larger projects include:\n\nSeparation promotes the concept to the business that services can be delivered quickly and independently from the larger and slower-moving projects common in the organization. The business starts understanding systems and simplified user interfaces calling on services. This advocates agility. That is to say, it fosters business innovations and speeds up time-to-market.[28]\nSeparation promotes the decoupling of services from consuming projects. This encourages good design insofar as the service is designed without knowing who its consumers are.\nDocumentation and test artifacts of the service are not embedded within the detail of the larger project. This is important when the service needs to be reused later.\nSOA promises to simplify testing indirectly. Services are autonomous, stateless, with fully documented interfaces, and separate from the cross-cutting concerns of the implementation. If an organization possesses appropriately defined test data, then a corresponding stub is built that reacts to the test data when a service is being built. A full set of regression tests, scripts, data, and responses is also captured for the service. The service can be tested as a 'black box' using existing stubs corresponding to the services it calls. Test environments can be constructed where the primitive and out-of-scope services are stubs, while the remainder of the mesh is test deployments of full services. As each interface is fully documented with its own full set of regression test documentation, it becomes simple to identify problems in test services. Testing evolves to merely validate that the test service operates according to its documentation, and finds gaps in documentation and test cases of all services within the environment. Managing the data state of idempotent services is the only complexity.\n\nExamples may prove useful to aid in documenting a service to the level where it becomes useful. The documentation of some APIs within the Java Community Process provide good examples. As these are exhaustive, staff would typically use only important subsets. The 'ossjsa.pdf' file within JSR-89 exemplifies such a file.[29]\n\nCriticisms[edit]\nSOA has been conflated with Web services.;[30] however, Web services are only one option to implement the patterns that comprise the SOA style. In the absence of native or binary forms of remote procedure call (RPC), applications could run more slowly and require more processing power, increasing costs. Most implementations do incur these overheads, but SOA can be implemented using technologies (for example, Java Business Integration (JBI), Windows Communication Foundation (WCF) and data distribution service (DDS)) that do not depend on remote procedure calls or translation through XML. At the same time, emerging open-source XML parsing technologies (such as VTD-XML) and various XML-compatible binary formats promise to significantly improve SOA performance. Services implemented using JSON instead of XML do not suffer from this performance concern.[31][32][33]\n\nStateful services require both the consumer and the provider to share the same consumer-specific context, which is either included in or referenced by messages exchanged between the provider and the consumer. This constraint has the drawback that it could reduce the overall scalability of the service provider if the service-provider needs to retain the shared context for each consumer. It also increases the coupling between a service provider and a consumer and makes switching service providers more difficult.[34] Ultimately, some critics feel that SOA services are still too constrained by applications they represent.[35]\n\nA primary challenge faced by service-oriented architecture is managing of metadata. Environments based on SOA include many services which communicate among each other to perform tasks. Due to the fact that the design may involve multiple services working in conjunction, an Application may generate millions of messages. Further services may belong to different organizations or even competing firms creating a huge trust issue. Thus SOA governance comes into the scheme of things.[36]\n\nAnother major problem faced by SOA is the lack of a uniform testing framework. There are no tools that provide the required features for testing these services in a Service Oriented Architecture. The major causes of difficulty are:[37]\n\nHeterogeneity and complexity of solution.\nHuge set of testing combinations due to integration of autonomous services.\nInclusion of services from different and competing vendors.\nPlatform is continuously changing due to availability of new features and services.\nSee [38] for additional challenges, partial solutions and research roadmap ionput regarding software service engineering\n\nExtensions and variants[edit]\nEvent-driven architectures[edit]\nMain article: Event-driven architecture\nWeb 2.0[edit]\nTim O'Reilly coined the term \"Web 2.0\" to describe a perceived, quickly growing set of web-based applications.[39] A topic that has experienced extensive coverage involves the relationship between Web 2.0 and service-oriented architectures.[which?]\n\nSOA is the philosophy of encapsulating application logic in services with a uniformly defined interface and making these publicly available via discovery mechanisms. The notion of complexity-hiding and reuse, but also the concept of loosely coupling services has inspired researchers to elaborate on similarities between the two philosophies, SOA and Web 2.0, and their respective applications. Some argue Web 2.0 and SOA have significantly different elements and thus can not be regarded \"parallel philosophies\", whereas others consider the two concepts as complementary and regard Web 2.0 as the global SOA.[40]\n\nThe philosophies of Web 2.0 and SOA serve different user needs and thus expose differences with respect to the design and also the technologies used in real-world applications. However, as of 2008, use-cases demonstrated the potential of combining technologies and principles of both Web 2.0 and SOA.[40]\n\nMicroservices[edit]\nMicroservices are a modern interpretation of service-oriented architectures used to build distributed software systems. Services in a microservice architecture[41] are processes that communicate with each other over the network in order to fulfill a goal. These services use technology agnostic protocols,[42] which aid in encapsulating choice of language and frameworks, making their choice a concern internal to the service. Microservices is a new realisation and implementation approach to SOA, which has become popular since 2014 (and after the introduction of DevOps), which also emphasizes continuous deployment and other agile practices.[43]\n\nSee also[edit]\nOASIS SOA Reference Model\nService (systems architecture)\nService granularity principle\nSOA governance\nSoftware architecture",
          "subparadigms": []
        },
        {
          "pdid": 26,
          "name": "Time-driven",
          "details": "Time-driven programming is a computer programming paradigm, where the control flow of the computer program is driven by a clock and is often used in Real-time computing. A program is divided into a set of tasks (i.e., processes or threads), each of which has a periodic activation pattern. The activation patterns are stored in a dispatch table ordered by time. The Least-Common-Multiple (LCM) of all period-times determines the length of the dispatch table. The scheduler of the program dispatches tasks by consulting the next entry in the dispatch table. After processing all entries, it continues by looping back to the beginning of the table.\n\nThe programming paradigm is mostly used for safety critical programs, since the behaviour of the program is highly deterministic. No external events are allowed to affect the control-flow of the program, the same pattern (i.e., described by the dispatch table) will be repeated time after time. However, idle time of the processor is also highly deterministic, allowing for the scheduling of other non-critical tasks through slack stealing techniques during these idle periods.\n\nThe drawback with the method is that the program becomes static (in the sense that small changes may recompile into large effects on execution structure), and unsuitable for applications requiring a large amount of flexibility. For example, the execution time of a task may change if its program code is altered. As a consequence, a new dispatch table must be regenerated for the entire task set. Such a change may require expensive retesting as is often required in safety critical systems.",
          "subparadigms": []
        },
        {
          "pdid": 27,
          "name": "Event-driven programming",
          "details": "In computer programming, event-driven programming is a programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs/threads. Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g. JavaScript web applications) that are centered on performing certain actions in response to user input.\n\nIn an event-driven application, there is generally a main loop that listens for events, and then triggers a callback function when one of those events is detected. In embedded systems the same may be achieved using hardware interrupts instead of a constantly running main loop. Event-driven programs can be written in any programming language, although the task is easier in languages that provide high-level abstractions, such as closures. In October 2016 Microsoft open-sourced the P programming language, which was designed specifically for safe event-driven programming.[1]\n\nContents  [hide] \n1\tEvent handlers\n1.1\tA trivial event handler\n1.2\tException handlers\n1.3\tCreating event handlers\n2\tCommon uses\n3\tCriticism\n4\tStackless threading\n5\tSee also\n6\tReferences\n7\tExternal links\nEvent handlers[edit]\nMain article: Event handler\nA trivial event handler[edit]\nBecause the code for checking for events and the main loop do not depend on the application, many programming frameworks take care of their implementation and expect the user to provide only the code for the event handlers. In this simple example there may be a call to an event handler called OnKeyEnter() that includes an argument with a string of characters, corresponding to what the user typed before hitting the ENTER key. To add two numbers, storage outside the event handler must be used. The implementation might look like below.\n\nglobally declare the counter K and the integer T.\nOnKeyEnter(character C)\n{\n   convert C to a number N\n   if K is zero store N in T and increment K\n   otherwise add N to T, print the result and reset K to zero\n}\nWhile keeping track of history is straightforward in a batch program, it requires special attention and planning in an event-driven program.\n\nException handlers[edit]\nIn PL/1, even though a program itself may not be predominantly event-driven, certain abnormal events such as a hardware error, overflow or \"program checks\" may occur that possibly prevent further processing. Exception handlers may be provided by \"ON statements\" in (unseen) callers to provide housekeeping routines to clean up afterwards before termination.\n\nCreating event handlers[edit]\nThe first step in developing an event-driven program is to write a series of subroutines, or methods, called event-handler routines. These routines handle the events to which the main program will respond. For example, a single left-button mouse-click on a command button in a GUI program may trigger a routine that will open another window, save data to a database or exit the application. Many modern-day programming environments provide the programmer with event templates, allowing the programmer to focus on writing the event code.\n\nThe second step is to bind event handlers to events so that the correct function is called when the event takes place. Graphical editors combine the first two steps: double-click on a button, and the editor creates an (empty) event handler associated with the user clicking the button and opens a text window so you can edit the event handler.\n\nThe third step in developing an event-driven program is to write the main loop. This is a function that checks for the occurrence of events, and then calls the matching event handler to process it. Most event-driven programming environments already provide this main loop, so it need not be specifically provided by the application programmer. RPG, an early programming language from IBM, whose 1960s design concept was similar to event-driven programming discussed above, provided a built-in main I/O loop (known as the \"program cycle\") where the calculations responded in accordance to 'indicators' (flags) that were set earlier in the cycle.\n\nCommon uses[edit]\nMost of existing GUI development tools and architectures rely on event-driven programming.[2]\n\nIn addition, systems such as Node.js are also event-driven[3]\n\nCriticism[edit]\nThe design of those programs which rely on event-action model has been criticised, and it has been suggested that event-action model leads programmers to create error prone, difficult to extend and excessively complex application code.[2] Table-driven state machines have been advocated as a viable alternative.[4] On the other hand, table-driven state machines themselves suffer from significant weaknesses including \"state explosion\" phenomenon[5]\n\nStackless threading[edit]\nAn event-driven approach is used in hardware description languages. A thread context only needs a CPU stack while actively processing an event, once done the CPU can move on to process other event-driven threads, which allows an extremely large number of threads to be handled. This is essentially a finite-state machine approach.\n\nSee also[edit]\nTime-triggered system (an alternative architecture for computer systems)\nInterrupt\nComparison of programming paradigms\nDataflow programming (a similar concept)\nDOM events\nEvent-driven architecture\nEvent stream processing (a similar concept)\nHardware description language\nInversion of control\nMessage-oriented middleware\nProgramming paradigm\nPublish–subscribe pattern\nSignal programming (a similar concept)\nStaged event-driven architecture (SEDA)\nVirtual synchrony, a distributed execution model for event-driven programming",
          "subparadigms": [
            25,
            26
          ]
        },
        {
          "pdid": 28,
          "name": "Expression-oriented programming language",
          "details": "An expression-oriented programming language is a programming language where every (or nearly every) construction is an expression and thus yields a value. The typical exceptions are macro definitions, preprocessor commands, and declarations, which expression-oriented languages often treat as statements rather than expressions. Some expression-oriented languages introduce a void return type to be yielded by expressions that merely cause side-effects.\n\nALGOL 68 and Lisp are examples of expression-oriented languages. Pascal is not an expression-oriented language. All functional programming languages are expression-oriented.\n\nContents  [hide] \n1\tCriticism\n2\tSee also\n3\tNotes\n4\tReferences\nCriticism[edit]\nCritics, including language designers,[1] blame expression-orientation for an entire class of programming mistakes wherein a programmer introduces an assignment expression where they meant to test for equality. For example, the designers of Ada and Java were so worried about this type of mistake, they restricted control expressions to those that evaluate strictly to the boolean data type.[2][3] The designers of Python had similar worries but took the alternative strategy of implementing assignment as a statement rather than an expression, thus prohibiting assignment from nesting inside of any other statement or expression.[4]",
          "subparadigms": []
        },
        {
          "pdid": 29,
          "name": "Feature-oriented programming",
          "details": "Feature Oriented Programming (FOP) or Feature Oriented Software Development (FOSD) is a paradigm for program generation in software product lines and for incremental development of programs.\n\nvertical stacking of layers\nConnection Between Layer Stacks and Transformation Compositions\nFOSD arose out of layer-based designs and levels of abstraction in network protocols and extensible database systems in the late-1980s.[1] A program was a stack of layers. Each layer added functionality to previously composed layers and different compositions of layers produced different programs. Not surprisingly, there was a need for a compact language to express such designs. Elementary algebra fit the bill: each layer was function (program transformation) that added new code to an existing program to produce a new program, and a program's design was modeled by an expression, i.e., a composition of transformations (layers). The figure to the left illustrates the stacking of layers i, j, and h (where h is on the bottom and i is on the top). The algebraic notations i(j(h)), i•j•h, and i+j+h have been used to express these designs.\n\n\nOver time, layers were equated to features, where a feature is an increment in program functionality. The paradigm for program design and generation was recognized to be an outgrowth of relational query optimization, where query evaluation programs were defined as relational algebra expressions, and query optimization was expression optimization.[2] A software product line (SPL) is a family of programs where each program is defined by a unique composition of features. FOSD has since evolved into the study of feature modularity, tools, analyses, and design techniques to support feature-based program generation.\n\nThe second generation of FOSD research was on feature interactions, which originated in telecommunications. Later, the term feature-oriented programming was coined;[3] this work exposed interactions between layers. Interactions require features to be adapted when composed with other features.\n\nA third generation of research focussed on the fact that every program has multiple representations (e.g., source, makefiles, documentation, etc.) and adding a feature to a program should elaborate each of its representations so that all are consistent. Additionally, some of representations could be generated (or derived) from others. In the sections below, the mathematics of the three most recent generations of FOSD, namely GenVoca,[1] AHEAD,[4] and FOMDD[5][6] are described, and links to product lines that have been developed using FOSD tools are provided. Also, four additional results that apply to all generations of FOSD are presented elsewhere: MetaModels, Program Cubes, and Feature Interactions.\n\nContents  [hide] \n1\tGenVoca\n2\tAHEAD\n3\tFOMDD\n4\tApplications\n5\tSee also\n6\tReferences\nGenVoca[edit]\nGenVoca (a portmanteau of the names Genesis and Avoca)[1] is a compositional paradigm for defining programs of product lines. Base programs are 0-ary functions or transformations called values:\n\n  f      -- base program with feature f\n  h      -- base program with feature h\nand features are unary functions/transformations that elaborate (modify, extend, refine) a program:\n\n  i + x  -- adds feature i to program x\n  j + x  -- adds feature j to program x\nwhere + denotes function composition. The design of a program is a named expression, e.g.:\n\n  p1 = j + f       -- program p1 has features j and f\n  p2 = j + h       -- program p2 has features j and h\n  p3 = i + j + h   -- program p3 has features i, j, and h\nA GenVoca model of a domain or software product line is a collection of base programs and features (see MetaModels and Program Cubes). The programs (expressions) that can be created defines a product line. Expression optimization is program design optimization, and expression evaluation is program generation.\n\nNote: GenVoca is based on the stepwise development of programs: a process that emphasizes design simplicity and understandability, which are key to program comprehension and automated program construction. Consider program p3 above: it begins with base program h, then feature j is added (read: the functionality of feature j is added to the codebase of h), and finally feature i is added (read: the functionality of feature i is added to the codebase of j•h).\nNote: not all combinations of features are meaningful. Feature models (which can be translated into propositional formulas) are graphical representations that define legal combinations of features.[7]\nNote: A more recent formulation of GenVoca is symmetric: there is only one base program, 0 (the empty program), and all features are unary functions. This suggests the interpretation that GenVoca composes program structures by superposition, the idea that complex structures are composed by superimposing simpler structures.[8][9] Yet another reformulation of GenVoca is as a monoid: a GenVoca model is a set of features with a composition operation (•); composition is associative and there is an identity element (namely 1, the identity function). Although all compositions are possible, not all are meaningful. That's the reason for feature models.\nGenVoca features were originally implemented using C preprocessor (#ifdef feature ... #endif) techniques. A more advanced technique, called mixin layers, showed the connection of features to object-oriented collaboration-based designs.\n\nAHEAD[edit]\nAlgebraic Hierarchical Equations for Application Design (AHEAD) [4] generalized GenVoca in two ways. First it revealed the internal structure of GenVoca values as tuples. Every program has multiple representations, such as source, documentation, bytecode, and makefiles. A GenVoca value is a tuple of program representations. In a product line of parsers, for example, a base parser f is defined by its grammar gf, Java source sf, and documentation df. Parser f is modeled by the tuple f=[gf, sf, df]. Each program representation may have subrepresentations, and they too may have subrepresentations, recursively. In general, a GenVoca value is a tuple of nested tuples that define a hierarchy of representations for a particular program.\n\n\nHierarchical Relationships among Program Artifacts\nExample. Suppose terminal representations are files. In AHEAD, grammar gf corresponds to a single BNF file, source sf corresponds to a tuple of Java files [c1…cn], and documentation df is a tuple of HTML files [h1…hk]. A GenVoca value (nested tuples) can be depicted as a directed graph: the graph for parser f is shown in the figure to the right. Arrows denote projections, i.e., mappings from a tuple to one of its components. AHEAD implements tuples as file directories, so f is a directory containing file gf and subdirectories sf and df. Similarly, directory sf contains files c1…cn, and directory df contains files h1…hk.\nNote: Files can be hierarchically decomposed further. Each Java class can be decomposed into a tuple of members and other class declarations (e.g., initialization blocks, etc.). The important idea here is that the mathematics of AHEAD are recursive.\nSecond, AHEAD expresses features as nested tuples of unary functions called deltas. Deltas can be program refinements (semantics-preserving transformations), extensions (semantics-extending transformations), or interactions (semantics-altering transformations). We use the neutral term “delta” to represent all of these possibilities, as each occurs in FOSD.\n\nTo illustrate, suppose feature j extends a grammar by {\\displaystyle \\Delta } \\Delta gj (new rules and tokens are added), extends source code by {\\displaystyle \\Delta } \\Delta sj (new classes and members are added and existing methods are modified), and extends documentation by {\\displaystyle \\Delta } \\Delta dj. The tuple of deltas for feature j is modeled by j=[ {\\displaystyle \\Delta } \\Delta gj, {\\displaystyle \\Delta } \\Delta sj, {\\displaystyle \\Delta } \\Delta dj], which we call a delta tuple. Elements of delta tuples can themselves be delta tuples. Example: {\\displaystyle \\Delta } \\Delta sj represents the changes that are made to each class in sf by feature j, i.e., {\\displaystyle \\Delta } \\Delta sj=[ {\\displaystyle \\Delta } \\Delta c1… {\\displaystyle \\Delta } \\Delta cn]. The representations of a program are computed recursively by nested vector addition. The representations for parser p2 (whose GenVoca expression is j+f) are:\n\n  p2 = j + f                              -- GenVoca expression\n     = [\n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gj, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta sj, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta dj] + [gf, sf, df]   -- substitution\n     = [\n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gj+gf, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta sj+sf, \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta dj+df]         -- compose tuples element-wise\nThat is, the grammar of p2 is the base grammar composed with its extension ( {\\displaystyle \\Delta } \\Delta gj+gf), the source of p2 is the base source composed with its extension ( {\\displaystyle \\Delta } \\Delta sj+sf), and so on. As elements of delta tuples can themselves be delta tuples, composition recurses, e.g., {\\displaystyle \\Delta } \\Delta sj+sf= [ {\\displaystyle \\Delta } \\Delta c1… {\\displaystyle \\Delta } \\Delta cn]+[c1…cn]=[ {\\displaystyle \\Delta } \\Delta c1+c1… {\\displaystyle \\Delta } \\Delta cn+cn]. Summarizing, GenVoca values are nested tuples of program artifacts, and features are nested delta tuples, where + recursively composes them by vector addition. This is the essence of AHEAD.\n\nThe ideas presented above concretely expose two FOSD principles. The Principle of Uniformity states that all program artifacts are treated and modified in the same way. (This is evidenced by deltas for different artifact types above). The Principle of Scalability states all levels of abstractions are treated uniformly. (This gives rise to the hierarchical nesting of tuples above).\n\nThe original implementation of AHEAD is the AHEAD Tool Suite and Jak language, which exhibits both the Principles of Uniformity and Scalability. Next-generation tools include CIDE [10] and FeatureHouse.[11]\n\nFOMDD[edit]\n\nDerivational and Refinement Relationships among Program Artifacts\nFeature Oriented Model Driven Design (FOMDD) [5][6] combines the ideas of AHEAD with Model Driven Design (MDD) (a.k.a. Model-Driven Architecture (MDA)). AHEAD functions capture the lockstep update of program artifacts when a feature is added to a program. But there are other functional relationships among program artifacts that express derivations. For example, the relationship between a grammar gf and its parser source sf is defined by a compiler-compiler tool, e.g., javacc. Similarly, the relationship between Java source sf and its bytecode bf is defined by the javac compiler. A commuting diagram expresses these relationships. Objects are program representations, downward arrows are derivations, and horizontal arrows are deltas. The figure to the right shows the commuting diagram for program p3 = i+j+h = [g3,s3,b3].\n\nA fundamental property of a commuting diagram is that all paths between two objects are equivalent. For example, one way to derive the bytecode b3 of parser p3 (lower right object in the figure to the right) from grammar gh of parser h (upper left object) is to derive the bytecode bh and refine to b3, while another way refines gh to g3, and then derive b3, where + represents delta composition and () is function or tool application:\n\n  b3 = \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta bj + \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta bi + javacc( javac( gh ) ) = javac( javacc( \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gi + \n  \n    \n    {\\displaystyle \\Delta }\n  \n\\Delta gj + gh ) )\nThere are {\\displaystyle {\\tbinom {4}{2}}} {\\tbinom  {4}{2}} possible paths to derive the bytecode b3 of parser p3 from the grammar gh of parser h. Each path represents a metaprogram whose execution generates the target object (b3) from the starting object (gf). There is a potential optimization: traversing each arrow of a commuting diagram has a cost. The cheapest (i.e., shortest) path between two objects in a commuting diagram is a geodesic, which represents the most efficient metaprogram that produces the target object from a given object.\n\nNote: A “cost metric” need not be a monetary value; cost may be measured in production time, peak or total memory requirements, power consumption, or some informal metric like “ease of explanation”, or a combination of the above (e.g., multi-objective optimization). The idea of a geodesic is general, and should be understood and appreciated from this more general context.\nNote: It is possible for there to be m starting objects and n ending objects in a geodesic; when m=1 and n>1, this is the Directed Steiner Tree Problem, which is NP-hard.\nCommuting diagrams are important for at least two reasons: (1) there is the possibility of optimizing the generation of artifacts (e.g., geodesics) and (2) they specify different ways of constructing a target object from a starting object.[5][12] A path through a diagram corresponds to a tool chain: for an FOMDD model to be consistent, it should be proven (or demonstrated through testing) that all tool chains that map one object to another in fact yield equivalent results. If this is not the case, then either there is a bug in one or more of the tools or the FOMDD model is wrong.\n\nNote: the above ideas were inspired by category theory.[5][6]\nApplications[edit]\nNetwork Protocols\nExtensible Database Systems\nData Structures\nDistributed Army Fire Support Simulator\nProduction System Compiler\nGraph Product Line\nExtensible Java Preprocessors\nWeb Portlets\nSVG Applications",
          "subparadigms": []
        },
        {
          "pdid": 30,
          "name": "Function-level programming",
          "details": "In computer science, function-level programming refers to one of the two contrasting programming paradigms identified by John Backus in his work on programs as mathematical objects, the other being value-level programming.\n\nIn his 1977 Turing award lecture, Backus set forth what he considered to be the need to switch to a different philosophy in programming language design:[1]\n\nProgramming languages appear to be in trouble. Each successive language incorporates, with a little cleaning up, all the features of its predecessors plus a few more. [...] Each new language claims new and fashionable features... but the plain fact is that few languages make programming sufficiently cheaper or more reliable to justify the cost of producing and learning to use them.\n\nHe designed FP to be the first programming language to specifically support the function-level programming style.\n\nA function-level program is variable-free (cf. point-free programming), since program variables, which are essential in value-level definitions, are not needed in function-level programs.\n\nContents  [hide] \n1\tIntroduction\n2\tContrast to functional programming\n3\tExample languages\n4\tSee also\n5\tReferences\nIntroduction[edit]\nIn the function-level style of programming, a program is built directly from programs that are given at the outset, by combining them with program-forming operations or functionals. Thus, in contrast with the value-level approach that applies the given programs to values to form a succession of values culminating in the desired result value, the function-level approach applies program-forming operations to the given programs to form a succession of programs culminating in the desired result program.\n\nAs a result, the function-level approach to programming invites study of the space of programs under program-forming operations, looking to derive useful algebraic properties of these program-forming operations. The function-level approach offers the possibility of making the set of programs a mathematical space by emphasizing the algebraic properties of the program-forming operations over the space of programs.\n\nAnother potential advantage of the function-level view is the ability to use only strict functions and thereby have bottom-up semantics, which are the simplest kind of all. Yet another is the existence of function level definitions that are not the lifted (that is, lifted from a lower value-level to a higher function-level) image of any existing value-level one: these (often terse) function-level definitions represent a more powerful style of programming not available at the value-level and, arguably, are often easier to understand and reason about.\n\nContrast to functional programming[edit]\nWhen Backus studied and publicized his function-level style of programming, his message was mostly misunderstood,[2] giving boost to the traditional functional programming style languages instead of his own FP and its successor FL.\n\nBackus calls functional programming applicative programming; his function-level programming is a particular, constrained type of applicative programming.\n\nA key distinction from functional languages is that Backus' language has the following hierarchy of types:\n\natoms\nfunctions, which take atoms to atoms\nHigher-order functions (which he calls \"functional forms\"), which take one or two functions to functions\n...and the only way to generate new functions is to use one of the functional forms, which are fixed: you cannot build your own functional form (at least not within FP; you can within FFP (Formal FP)).\n\nThis restriction means that functions in FP are a module (generated by the built-in functions) over the algebra of functional forms, and are thus algebraically tractable. For instance, the general question of equality of two functions is equivalent to the halting problem, and is undecidable, but equality of two functions in FP is just equality in the algebra, and thus (Backus imagines) easier.\n\nEven today, many users of lambda style languages often misinterpret Backus' function-level approach as a restrictive variant of the lambda style, which is a de facto value-level style. In fact, Backus would not have disagreed with the 'restrictive' accusation: he argued that it was precisely due to such restrictions that a well-formed mathematical space could arise, in a manner analogous to the way structured programming limits programming to a restricted version of all the control-flow possibilities available in plain, unrestricted unstructured programs.\n\nThe value-free style of FP is closely related to the equational logic of a cartesian-closed category.\n\nExample languages[edit]\nMain category: Function-level languages\nThe canonical function-level programming language is FP. Others include FL, FPr and J.\n\nSee also[edit]\nValue-level programming, imperative programming (contrast)\nFunctional programming, declarative programming (compare)\nTacit programming\nConcatenative programming language",
          "subparadigms": []
        },
        {
          "pdid": 31,
          "name": "Generic programming",
          "details": "Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. This approach, pioneered by ML in 1973,[1][2] permits writing common functions or types that differ only in the set of types on which they operate when used, thus reducing duplication. Such software entities are known as generics in Ada, Delphi, Eiffel, Java, C#, F#, Objective-C, Swift, and Visual Basic .NET; parametric polymorphism in ML, Scala, Haskell (the Haskell community also uses the term \"generic\" for a related but somewhat different concept) and Julia; templates in C++ and D; and parameterized types in the influential 1994 book Design Patterns.[3] The authors of Design Patterns note that this technique, especially when combined with delegation, is very powerful but also quote the following\n\nDynamic, highly parameterized software is harder to understand than more static software.\n\n— Gang of Four, Design Patterns[3] (Chapter 1)\nThe term generic programming was originally coined by David Musser and Alexander Stepanov[4] in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalised as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above.\n\nContents  [hide] \n1\tStepanov–Musser and other generic programming paradigms\n2\tProgramming language support for genericity\n2.1\tIn object-oriented languages\n2.1.1\tGenerics in Ada\n2.1.1.1\tExample\n2.1.1.2\tAdvantages and limitations\n2.1.2\tTemplates in C++\n2.1.2.1\tTechnical overview\n2.1.2.2\tTemplate specialization\n2.1.2.3\tAdvantages and disadvantages\n2.1.3\tTemplates in D\n2.1.3.1\tCode generation\n2.1.4\tGenericity in Eiffel\n2.1.4.1\tBasic/Unconstrained genericity\n2.1.4.2\tConstrained genericity\n2.1.5\tGenerics in Java\n2.1.6\tGenericity in .NET [C#, VB.NET]\n2.1.7\tGenericity in Delphi\n2.1.8\tGenericity in Free Pascal\n2.2\tFunctional languages\n2.2.1\tGenericity in Haskell\n2.2.1.1\tPolyP\n2.2.1.2\tGeneric Haskell\n2.2.2\tClean\n2.3\tOther languages\n3\tSee also\n4\tReferences\n5\tCitations\n6\tFurther reading\n7\tExternal links\nStepanov–Musser and other generic programming paradigms[edit]\nGeneric programming is defined in Musser & Stepanov (1989) as follows,\n\nGeneric programming centers around the idea of abstracting from concrete, efficient algorithms to obtain generic algorithms that can be combined with different data representations to produce a wide variety of useful software.\n\n— Musser, David R.; Stepanov, Alexander A., Generic Programming[5]\nGeneric programming paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalised as concepts, analogously to the abstraction of algebraic theories in abstract algebra.[6] Early examples of this programming approach were implemented in Scheme and Ada,[7] although the best known example is the Standard Template Library (STL),[8][9] which developed a theory of iterators that is used to decouple sequence data structures and the algorithms operating on them.\n\nFor example, given N sequence data structures, e.g. singly linked list, vector etc., and M algorithms to operate on them, e.g. find, sort etc., a direct approach would implement each algorithm specifically for each data structure, giving N × M combinations to implement. However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type which can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e.g. a pair of iterators pointing to the beginning and end of the subsequence to process. Thus, only N + M data structure-algorithm combinations need be implemented. Several iterator concepts are specified in the STL, each a refinement of more restrictive concepts e.g. forward iterators only provide movement to the next value in a sequence (e.g. suitable for a singly linked list or a stream of input data), whereas a random-access iterator also provides direct constant-time access to any element of the sequence (e.g. suitable for a vector). An important point is that a data structure will return a model of the most general concept that can be implemented efficiently—computational complexity requirements are explicitly part of the concept definition. This limits which data structures a given algorithm can be applied to and such complexity requirements are a major determinant of data structure choice. Generic programming similarly has been applied in other domains, e.g. graph algorithms.[10]\n\nNote that although this approach often utilizes language features of compile-time genericity/templates, it is in fact independent of particular language-technical details. Generic programming pioneer Alexander Stepanov wrote,\n\nGeneric programming is about abstracting and classifying algorithms and data structures. It gets its inspiration from Knuth and not from type theory. Its goal is the incremental construction of systematic catalogs of useful, efficient and abstract algorithms and data structures. Such an undertaking is still a dream.\n\n— Alexander Stepanov, Short History of STL [11][12]\nI believe that iterator theories are as central to Computer Science as theories of rings or Banach spaces are central to Mathematics.\n\n— Alexander Stepanov, An Interview with A. Stepanov[13]\nBjarne Stroustrup noted,\n\nFollowing Stepanov, we can define generic programming without mentioning language features: Lift algorithms and data structures from concrete examples to their most general and abstract form.\n\n— Bjarne Stroustrup, Evolving a language in and for the real world: C++ 1991-2006[12]\nOther programming paradigms that have been described as generic programming include Datatype generic programming as described in “Generic Programming — an Introduction”.[14] The Scrap your boilerplate approach is a lightweight generic programming approach for Haskell.[15]\n\nIn this article we distinguish the high-level programming paradigms of generic programming, above, from the lower-level programming language genericity mechanisms used to implement them (see Programming language support for genericity). For further discussion and comparison of generic programming paradigms, see.[16]\n\nProgramming language support for genericity[edit]\nGenericity facilities have existed in high-level languages since at least the 1970s in languages such as ML, CLU and Ada, and were subsequently adopted by many object-based and object-oriented languages, including BETA, C++, D, Eiffel, Java, and DEC's now defunct Trellis-Owl language.\n\nGenericity is implemented and supported differently in various programming languages; the term \"generic\" has also been used differently in various programming contexts. For example, in Forth the compiler can execute code while compiling and one can create new compiler keywords and new implementations for those words on the fly. It has few words that expose the compiler behaviour and therefore naturally offers genericity capacities which, however, are not referred to as such in most Forth texts. Similarly, dynamically typed languages, especially interpreted ones, usually offer genericity by default as both passing values to functions and value assignment are type-indifferent and such behavior is often utilized for abstraction or code terseness, however this is not typically labeled genericity as it's a direct consequence of dynamic typing system employed by the language[citation needed]. The term has been used in functional programming, specifically in Haskell-like languages, which use a structural type system where types are always parametric and the actual code on those types is generic. These usages still serve a similar purpose of code-saving and the rendering of an abstraction.\n\nArrays and structs can be viewed as predefined generic types. Every usage of an array or struct type instantiates a new concrete type, or reuses a previous instantiated type. Array element types and struct element types are parameterized types, which are used to instantiate the corresponding generic type. All this is usually built-in in the compiler and the syntax differs from other generic constructs. Some extensible programming languages try to unify built-in and user defined generic types.\n\nA broad survey of genericity mechanisms in programming languages follows. For a specific survey comparing suitability of mechanisms for generic programming, see.[17]\n\nIn object-oriented languages[edit]\nWhen creating container classes in statically typed languages, it is inconvenient to write specific implementations for each datatype contained, especially if the code for each datatype is virtually identical. For example, in C++, this duplication of code can be circumvented by defining a class template:\n\ntemplate<typename T> \nclass List \n{ \n   /* class contents */ \n};\n\nList<Animal> list_of_animals;\nList<Car> list_of_cars;\nAbove, T is a placeholder for whatever type is specified when the list is created. These \"containers-of-type-T\", commonly called templates, allow a class to be reused with different datatypes as long as certain contracts such as subtypes and signature are kept. This genericity mechanism should not be confused with inclusion polymorphism, which is the algorithmic usage of exchangeable sub-classes: for instance, a list of objects of type Moving_Object containing objects of type Animal and Car. Templates can also be used for type-independent functions as in the Swap example below:\n\ntemplate<typename T>\nvoid Swap(T & a, T & b) //\"&\" passes parameters by reference\n{\n   T temp = b;\n   b = a;\n   a = temp;\n}\n\nstring hello = \"world!\", world = \"Hello, \";\nSwap( world, hello );\ncout << hello << world << endl; //Output is \"Hello, world!\"\nThe C++ template construct used above is widely cited[citation needed] as the genericity construct that popularized the notion among programmers and language designers and supports many generic programming idioms. The D programming language also offers fully generic-capable templates based on the C++ precedent but with a simplified syntax. The Java programming language has provided genericity facilities syntactically based on C++'s since the introduction of J2SE 5.0.\n\nC# 2.0, Oxygene 1.5 (also known as Chrome) and Visual Basic .NET 2005 have constructs that take advantage of the support for generics present in the Microsoft .NET Framework since version 2.0.\n\nGenerics in Ada[edit]\nAda has had generics since it was first designed in 1977–1980. The standard library uses generics to provide many services. Ada 2005 adds a comprehensive generic container library to the standard library, which was inspired by C++'s standard template library.\n\nA generic unit is a package or a subprogram that takes one or more generic formal parameters.\n\nA generic formal parameter is a value, a variable, a constant, a type, a subprogram, or even an instance of another, designated, generic unit. For generic formal types, the syntax distinguishes between discrete, floating-point, fixed-point, access (pointer) types, etc. Some formal parameters can have default values.\n\nTo instantiate a generic unit, the programmer passes actual parameters for each formal. The generic instance then behaves just like any other unit. It is possible to instantiate generic units at run-time, for example inside a loop.\n\nExample[edit]\nThe specification of a generic package:\n\n generic\n    Max_Size : Natural; -- a generic formal value\n    type Element_Type is private; -- a generic formal type; accepts any nonlimited type\n package Stacks is\n    type Size_Type is range 0 .. Max_Size;\n    type Stack is limited private;\n    procedure Create (S : out Stack;\n                      Initial_Size : in Size_Type := Max_Size);\n    procedure Push (Into : in out Stack; Element : in Element_Type);\n    procedure Pop (From : in out Stack; Element : out Element_Type);\n    Overflow : exception;\n    Underflow : exception;\n private\n    subtype Index_Type is Size_Type range 1 .. Max_Size;\n    type Vector is array (Index_Type range <>) of Element_Type;\n    type Stack (Allocated_Size : Size_Type := 0) is record\n       Top : Index_Type;\n       Storage : Vector (1 .. Allocated_Size);\n    end record;\n end Stacks;\nInstantiating the generic package:\n\n type Bookmark_Type is new Natural;\n -- records a location in the text document we are editing\n \n package Bookmark_Stacks is new Stacks (Max_Size => 20,\n                                        Element_Type => Bookmark_Type);\n -- Allows the user to jump between recorded locations in a document\nUsing an instance of a generic package:\n\n type Document_Type is record\n    Contents : Ada.Strings.Unbounded.Unbounded_String;\n    Bookmarks : Bookmark_Stacks.Stack;\n end record;\n \n procedure Edit (Document_Name : in String) is\n   Document : Document_Type;\n begin\n   -- Initialise the stack of bookmarks:\n   Bookmark_Stacks.Create (S => Document.Bookmarks, Initial_Size => 10);\n   -- Now, open the file Document_Name and read it in...\n end Edit;\nAdvantages and limitations[edit]\nThe language syntax allows precise specification of constraints on generic formal parameters. For example, it is possible to specify that a generic formal type will only accept a modular type as the actual. It is also possible to express constraints between generic formal parameters; for example:\n\n generic\n    type Index_Type is (<>); -- must be a discrete type\n    type Element_Type is private; -- can be any nonlimited type\n    type Array_Type is array (Index_Type range <>) of Element_Type;\nIn this example, Array_Type is constrained by both Index_Type and Element_Type. When instantiating the unit, the programmer must pass an actual array type that satisfies these constraints.\n\nThe disadvantage of this fine-grained control is a complicated syntax, but, because all generic formal parameters are completely defined in the specification, the compiler can instantiate generics without looking at the body of the generic.\n\nUnlike C++, Ada does not allow specialised generic instances, and requires that all generics be instantiated explicitly. These rules have several consequences:\n\nthe compiler can implement shared generics: the object code for a generic unit can be shared between all instances (unless the programmer requests inlining of subprograms, of course). As further consequences:\nthere is no possibility of code bloat (code bloat is common in C++ and requires special care, as explained below).\nit is possible to instantiate generics at run-time, as well as at compile time, since no new object code is required for a new instance.\nactual objects corresponding to a generic formal object are always considered to be nonstatic inside the generic; see Generic formal objects in the Wikibook for details and consequences.\nall instances of a generic being exactly the same, it is easier to review and understand programs written by others; there are no \"special cases\" to take into account.\nall instantiations being explicit, there are no hidden instantiations that might make it difficult to understand the program.\nAda does not permit \"template metaprogramming\", because it does not allow specialisations.\nTemplates in C++[edit]\nMain article: Template (C++)\nC++ uses templates to enable generic programming techniques. The C++ Standard Library includes the Standard Template Library or STL that provides a framework of templates for common data structures and algorithms. Templates in C++ may also be used for template metaprogramming, which is a way of pre-evaluating some of the code at compile-time rather than run-time. Using template specialization, C++ Templates are considered Turing complete.\n\nTechnical overview[edit]\nThere are two kinds of templates: function templates and class templates. A function template is a pattern for creating ordinary functions based upon the parameterizing types supplied when instantiated. For example, the C++ Standard Template Library contains the function template max(x, y) which creates functions that return either x or y, whichever is larger. max() could be defined like this:\n\ntemplate <typename T>\nT max(T x, T y) \n{\n    return x < y ? y : x;\n}\nSpecializations of this function template, instantiations with specific types, can be called just like an ordinary function:\n\ncout << max(3, 7);   // outputs 7\nThe compiler examines the arguments used to call max and determines that this is a call to max(int, int). It then instantiates a version of the function where the parameterizing type T is int, making the equivalent of the following function:\n\nint max(int x, int y) \n{\n    return x < y ? y : x;\n}\nThis works whether the arguments x and y are integers, strings, or any other type for which the expression x < y is sensible, or more specifically, for any type for which operator< is defined. Common inheritance is not needed for the set of types that can be used, and so it is very similar to duck typing. A program defining a custom data type can use operator overloading to define the meaning of < for that type, thus allowing its use with the max() function template. While this may seem a minor benefit in this isolated example, in the context of a comprehensive library like the STL it allows the programmer to get extensive functionality for a new data type, just by defining a few operators for it. Merely defining < allows a type to be used with the standard sort(), stable_sort(), and binary_search() algorithms or to be put inside data structures such as sets, heaps, and associative arrays.\n\nC++ templates are completely type safe at compile time. As a demonstration, the standard type complex does not define the < operator, because there is no strict order on complex numbers. Therefore, max(x, y) will fail with a compile error if x and y are complex values. Likewise, other templates that rely on < cannot be applied to complex data unless a comparison (in the form of a functor or function) is provided. E.g.: A complex cannot be used as key for a map unless a comparison is provided. Unfortunately, compilers historically generate somewhat esoteric, long, and unhelpful error messages for this sort of error. Ensuring that a certain object adheres to a method protocol can alleviate this issue. Languages which use compare instead of < can also use complex values as keys.\n\nThe second kind of template, a class template, extends the same concept to classes. A class template specialization is a class. Class templates are often used to make generic containers. For example, the STL has a linked list container. To make a linked list of integers, one writes list<int>. A list of strings is denoted list<string>. A list has a set of standard functions associated with it, which work for any compatible parameterizing types.\n\nTemplate specialization[edit]\nA powerful feature of C++'s templates is template specialization. This allows alternative implementations to be provided based on certain characteristics of the parameterized type that is being instantiated. Template specialization has two purposes: to allow certain forms of optimization, and to reduce code bloat.\n\nFor example, consider a sort() template function. One of the primary activities that such a function does is to swap or exchange the values in two of the container's positions. If the values are large (in terms of the number of bytes it takes to store each of them), then it is often quicker to first build a separate list of pointers to the objects, sort those pointers, and then build the final sorted sequence. If the values are quite small however it is usually fastest to just swap the values in-place as needed. Furthermore, if the parameterized type is already of some pointer-type, then there is no need to build a separate pointer array. Template specialization allows the template creator to write different implementations and to specify the characteristics that the parameterized type(s) must have for each implementation to be used.\n\nUnlike function templates, class templates can be partially specialized. That means that an alternate version of the class template code can be provided when some of the template parameters are known, while leaving other template parameters generic. This can be used, for example, to create a default implementation (the primary specialization) that assumes that copying a parameterizing type is expensive and then create partial specializations for types that are cheap to copy, thus increasing overall efficiency. Clients of such a class template just use specializations of it without needing to know whether the compiler used the primary specialization or some partial specialization in each case. Class templates can also be fully specialized, which means that an alternate implementation can be provided when all of the parameterizing types are known.\n\nAdvantages and disadvantages[edit]\nSome uses of templates, such as the max() function, were previously filled by function-like preprocessor macros (a legacy of the C programming language). For example, here is a possible max() macro:\n\n#define max(a,b) ((a) < (b) ? (b) : (a))\nMacros are expanded by preprocessor, before compilation proper; templates are expanded at compile time. Macros are always expanded inline; templates can also be expanded as inline functions when the compiler deems it appropriate. Thus both function-like macros and function templates have no run-time overhead.\n\nHowever, templates are generally considered an improvement over macros for these purposes. Templates are type-safe. Templates avoid some of the common errors found in code that makes heavy use of function-like macros, such as evaluating parameters with side effects twice. Perhaps most importantly, templates were designed to be applicable to much larger problems than macros.\n\nThere are three primary drawbacks to the use of templates: compiler support, poor error messages, and code bloat. Many compilers historically have poor support for templates, thus the use of templates can make code somewhat less portable. Support may also be poor when a C++ compiler is being used with a linker which is not C++-aware, or when attempting to use templates across shared library boundaries. Most modern compilers however now have fairly robust and standard template support, and the new C++ standard, C++11, further addresses these issues.\n\nAlmost all compilers produce confusing, long, or sometimes unhelpful error messages when errors are detected in code that uses templates.[18] This can make templates difficult to develop.\n\nFinally, the use of templates requires the compiler to generate a separate instance of the templated class or function for every permutation of type parameters used with it. (This is necessary because types in C++ are not all the same size, and the sizes of data fields are important to how classes work.) So the indiscriminate use of templates can lead to code bloat, resulting in excessively large executables. However, judicious use of template specialization and derivation can dramatically reduce such code bloat in some cases:\n\nSo, can derivation be used to reduce the problem of code replicated because templates are used? This would involve deriving a template from an ordinary class. This technique proved successful in curbing code bloat in real use. People who do not use a technique like this have found that replicated code can cost megabytes of code space even in moderate size programs.\n\n— Bjarne Stroustrup, The Design and Evolution of C++, 1994[19]\nIn simple cases templates can be transformed into generics (not causing code bloat) by creating a class getting a parameter derived from a type in compile time and wrapping a template around this class. It is a nice approach for creating generic heap-based containers.\n\nThe extra instantiations generated by templates can also cause debuggers to have difficulty working gracefully with templates. For example, setting a debug breakpoint within a template from a source file may either miss setting the breakpoint in the actual instantiation desired or may set a breakpoint in every place the template is instantiated.\n\nAlso, because the compiler needs to perform macro-like expansions of templates and generate different instances of them at compile time, the implementation source code for the templated class or function must be available (e.g. included in a header) to the code using it. Templated classes or functions, including much of the Standard Template Library (STL), if not included in header files, cannot be compiled. (This is in contrast to non-templated code, which may be compiled to binary, providing only a declarations header file for code using it.) This may be a disadvantage by exposing the implementing code, which removes some abstractions, and could restrict its use in closed-source projects.[citation needed]\n\nTemplates in D[edit]\nThe D programming language supports templates based in design on C++. Most C++ template idioms will carry over to D without alteration, but D adds some additional functionality:\n\nTemplate parameters in D are not restricted to just types and primitive values, but also allow arbitrary compile-time values (such as strings and struct literals), and aliases to arbitrary identifiers, including other templates or template instantiations.\nTemplate constraints and the static if statement provide an alternative to C++'s substitution failure is not an error (SFINAE) mechanism, similar to C++ concepts.\nThe is(...) expression allows speculative instantiation to verify an object's traits at compile time.\nThe auto keyword and the typeof expression allow type inference for variable declarations and function return values, which in turn allows \"Voldemort types\" (types which do not have a global name).[20]\nTemplates in D use a different syntax as in C++: whereas in C++ template parameters are wrapped in angular brackets (Template<param1, param2>), D uses an exclamation sign and parentheses: Template!(param1, param2). This avoids the C++ parsing difficulties due to ambiguity with comparison operators. If there is only one parameter, the parentheses can be omitted.\n\nConventionally, D combines the above features to provide compile-time polymorphism using trait-based generic programming. For example, an input range is defined as any type which satisfies the checks performed by isInputRange, which is defined as follows:\n\ntemplate isInputRange(R)\n{\n    enum bool isInputRange = is(typeof(\n    (inout int = 0)\n    {\n        R r = R.init;     // can define a range object\n        if (r.empty) {}   // can test for empty\n        r.popFront();     // can invoke popFront()\n        auto h = r.front; // can get the front of the range\n    }));\n}\nA function which accepts only input ranges can then use the above template in a template constraint:\n\nauto fun(Range)(Range range)\n    if (isInputRange!Range)\n{\n    // ...\n}\nCode generation[edit]\nIn addition to template metaprogramming, D also provides several features to enable compile-time code generation:\n\nThe import expression allows reading a file from disk and using its contents as a string expression.\nCompile-time reflection allows enumerating and inspecting declarations and their members during compilation.\nUser-defined attributes allow users to attach arbitrary identifiers to declarations, which can then be enumerated using compile-time reflection.\nCompile-Time Function Execution (CTFE) allows a subset of D (restricted to safe operations) to be interpreted during compilation.\nString mixins allow evaluating and compiling the contents of a string expression as D code which becomes part of the program.\nCombining the above allows generating code based on existing declarations. For example, D serialization frameworks can enumerate a type's members and generate specialized functions for each serialized type to perform serialization and deserialization. User-defined attributes could further indicate serialization rules.\n\nThe import expression and compile-time function execution also allow efficiently implementing domain-specific languages. For example, given a function which takes a string containing an HTML template and returns equivalent D source code, it is possible to use it in the following way:\n\n// Import the contents of example.htt as a string manifest constant.\nenum htmlTemplate = import(\"example.htt\");\n\n// Transpile the HTML template to D code.\nenum htmlDCode = htmlTemplateToD(htmlTemplate);\n\n// Paste the contents of htmlDCode as D code.\nmixin(htmlDCode);\nGenericity in Eiffel[edit]\nGeneric classes have been a part of Eiffel since the original method and language design. The foundation publications of Eiffel,[21][22] use the term genericity to describe the creation and use of generic classes.\n\nBasic/Unconstrained genericity[edit]\nGeneric classes are declared with their class name and a list of one or more formal generic parameters. In the following code, class LIST has one formal generic parameter G\n\nclass\n    LIST [G]\n            ...\nfeature   -- Access\n    item: G\n            -- The item currently pointed to by cursor\n            ...\nfeature   -- Element change\n    put (new_item: G)\n            -- Add `new_item' at the end of the list\n            ...\nThe formal generic parameters are placeholders for arbitrary class names which will be supplied when a declaration of the generic class is made, as shown in the two generic derivations below, where ACCOUNT and DEPOSIT are other class names. ACCOUNT and DEPOSIT are considered actual generic parameters as they provide real class names to substitute for G in actual use.\n\n    list_of_accounts: LIST [ACCOUNT]\n            -- Account list\n\n    list_of_deposits: LIST [DEPOSIT]\n            -- Deposit list\nWithin the Eiffel type system, although class LIST [G] is considered a class, it is not considered a type. However, a generic derivation of LIST [G] such as LIST [ACCOUNT] is considered a type.\n\nConstrained genericity[edit]\nFor the list class shown above, an actual generic parameter substituting for G can be any other available class. To constrain the set of classes from which valid actual generic parameters can be chosen, a generic constraint can be specified. In the declaration of class SORTED_LIST below, the generic constraint dictates that any valid actual generic parameter will be a class which inherits from class COMPARABLE. The generic constraint ensures that elements of a SORTED_LIST can in fact be sorted.\n\nclass\n    SORTED_LIST [G -> COMPARABLE]\nGenerics in Java[edit]\nMain article: Generics in Java\nSupport for the generics, or \"containers-of-type-T\" was added to the Java programming language in 2004 as part of J2SE 5.0. In Java, generics are only checked at compile time for type correctness. The generic type information is then removed via a process called type erasure, to maintain compatibility with old JVM implementations, making it unavailable at runtime. For example, a List<String> is converted to the raw type List. The compiler inserts type casts to convert the elements to the String type when they are retrieved from the list, reducing performance compared to other implementations such as C++ templates.\n\nGenericity in .NET [C#, VB.NET][edit]\nGenerics were added as part of .NET Framework 2.0 in November 2005, based on a research prototype from Microsoft Research started in 1999.[23] Although similar to generics in Java, .NET generics do not apply type erasure, but implement generics as a first class mechanism in the runtime using reification. This design choice provides additional functionality, such as allowing reflection with preservation of generic types, as well as alleviating some of the limitations of erasure (such as being unable to create generic arrays).[24][25] This also means that there is no performance hit from runtime casts and normally expensive boxing conversions. When primitive and value types are used as generic arguments, they get specialized implementations, allowing for efficient generic collections and methods. As in C++ and Java, nested generic types such as Dictionary<string, List<int>> are valid types, however are advised against for member signatures in code analysis design rules.[26]\n\n.NET allows six varieties of generic type constraints using the where keyword including restricting generic types to be value types, to be classes, to have constructors, and to implement interfaces.[27] Below is an example with an interface constraint:\n\nusing System;\n\nclass Sample\n{\n    static void Main()\n    {\n        int[] array = { 0, 1, 2, 3 };\n        MakeAtLeast<int>(array, 2); // Change array to { 2, 2, 2, 3 }\n        foreach (int i in array)\n            Console.WriteLine(i); // Print results.\n        Console.ReadKey(true);\n    }\n\n    static void MakeAtLeast<T>(T[] list, T lowest) where T : IComparable<T>\n    {\n        for (int i = 0; i < list.Length; i++)\n            if (list[i].CompareTo(lowest) < 0)\n                list[i] = lowest;\n    }\n}\nThe MakeAtLeast() method allows operation on arrays, with elements of generic type T. The method's type constraint indicates that the method is applicable to any type T that implements the generic IComparable<T> interface. This ensures a compile time error if the method is called if the type does not support comparison. The interface provides the generic method CompareTo(T).\n\nThe above method could also be written without generic types, simply using the non-generic Array type. However, since arrays are contravariant, the casting would not be type safe, and compiler may miss errors that would otherwise be caught while making use of the generic types. In addition, the method would need to access the array items as objects instead, and would require casting to compare two elements. (For value types like types such as int this requires a boxing conversion, although this can be worked around using the Comparer<T> class, as is done in the standard collection classes.)\n\nA notable behavior of static members in a generic .NET class is static member instantiation per run-time type (see example below).\n\n    //A generic class\n    public class GenTest<T>\n    {\n        //A static variable - will be created for each type on refraction\n        static CountedInstances OnePerType = new CountedInstances();\n\n        //a data member\n        private T mT;\n\n        //simple constructor\n        public GenTest(T pT)\n        {\n            mT = pT;\n        }\n    }\n\n    //a class\n    public class CountedInstances\n    {\n        //Static variable - this will be incremented once per instance\n        public static int Counter;\n\n        //simple constructor\n        public CountedInstances()\n        {\n            //increase counter by one during object instantiation\n            CountedInstances.Counter++;\n        }\n    }\n\n  //main code entry point\n  //at the end of execution, CountedInstances.Counter = 2\n  GenTest<int> g1 = new GenTest<int>(1);\n  GenTest<int> g11 = new GenTest<int>(11);\n  GenTest<int> g111 = new GenTest<int>(111);\n  GenTest<double> g2 = new GenTest<double>(1.0);\nGenericity in Delphi[edit]\nDelphi's Object Pascal dialect acquired generics in the Delphi 2007 release, initially only with the (now discontinued) .NET compiler before being added to the native code one in the Delphi 2009 release. The semantics and capabilities of Delphi generics are largely modelled on those had by generics in .NET 2.0, though the implementation is by necessity quite different. Here's a more or less direct translation of the first C# example shown above:\n\nprogram Sample;\n\n{$APPTYPE CONSOLE}\n\nuses\n  Generics.Defaults; //for IComparer<>\n\ntype\n  TUtils = class\n    class procedure MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T;\n      Comparer: IComparer<T>); overload;\n    class procedure MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T); overload;\n  end;\n\nclass procedure TUtils.MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T;\n  Comparer: IComparer<T>);\nvar\n  I: Integer;\nbegin\n  if Comparer = nil then Comparer := TComparer<T>.Default;\n  for I := Low(Arr) to High(Arr) do\n    if Comparer.Compare(Arr[I], Lowest) < 0 then\n      Arr[I] := Lowest;\nend;\n\nclass procedure TUtils.MakeAtLeast<T>(Arr: TArray<T>; const Lowest: T);\nbegin\n  MakeAtLeast<T>(Arr, Lowest, nil);\nend;\n\nvar\n  Ints: TArray<Integer>;\n  Value: Integer;\nbegin\n  Ints := TArray<Integer>.Create(0, 1, 2, 3);\n  TUtils.MakeAtLeast<Integer>(Ints, 2);\n  for Value in Ints do\n    WriteLn(Value);\n  ReadLn;\nend.\nAs with C#, methods as well as whole types can have one or more type parameters. In the example, TArray is a generic type (defined by the language) and MakeAtLeast a generic method. The available constraints are very similar to the available constraints in C#: any value type, any class, a specific class or interface, and a class with a parameterless constructor. Multiple constraints act as an additive union.\n\nGenericity in Free Pascal[edit]\nFree Pascal implemented generics before Delphi, and with different syntax and semantics. However, work is now underway to implement Delphi generics alongside native FPC ones (see Wiki). This allows Free Pascal programmers to use generics in whatever style they prefer.\n\nDelphi and Free Pascal example:\n\n// Delphi style\nunit A;\n\n{$ifdef fpc}\n  {$mode delphi}\n{$endif}\n\ninterface\n\ntype\n  TGenericClass<T> = class\n    function Foo(const AValue: T): T;\n  end;\n\nimplementation\n\nfunction TGenericClass<T>.Foo(const AValue: T): T;\nbegin\n  Result := AValue + AValue;\nend;\n\nend.\n\n// Free Pascal's ObjFPC style\nunit B;\n\n{$ifdef fpc}\n  {$mode objfpc}\n{$endif}\n\ninterface\n\ntype\n  generic TGenericClass<T> = class\n    function Foo(const AValue: T): T;\n  end;\n\nimplementation\n\nfunction TGenericClass.Foo(const AValue: T): T;\nbegin\n  Result := AValue + AValue;\nend;\n\nend.\n\n// example usage, Delphi style\nprogram TestGenDelphi;\n\n{$ifdef fpc}\n  {$mode delphi}\n{$endif}\n\nuses\n  A,B;\n\nvar\n  GC1: A.TGenericClass<Integer>;\n  GC2: B.TGenericClass<String>;\nbegin\n  GC1 := A.TGenericClass<Integer>.Create;\n  GC2 := B.TGenericClass<String>.Create;\n  WriteLn(GC1.Foo(100)); // 200\n  WriteLn(GC2.Foo('hello')); // hellohello\n  GC1.Free;\n  GC2.Free;\nend.\n\n// example usage, ObjFPC style\nprogram TestGenDelphi;\n\n{$ifdef fpc}\n  {$mode objfpc}\n{$endif}\n\nuses\n  A,B;\n\n// required in ObjFPC\ntype\n  TAGenericClassInt = specialize A.TGenericClass<Integer>;\n  TBGenericClassString = specialize B.TGenericClass<String>;\nvar\n  GC1: TAGenericClassInt;\n  GC2: TBGenericClassString;\nbegin\n  GC1 := TAGenericClassInt.Create;\n  GC2 := TBGenericClassString.Create;\n  WriteLn(GC1.Foo(100)); // 200\n  WriteLn(GC2.Foo('hello')); // hellohello\n  GC1.Free;\n  GC2.Free;\nend.\nFunctional languages[edit]\nGenericity in Haskell[edit]\nThe type class mechanism of Haskell supports generic programming. Six of the predefined type classes in Haskell (including Eq, the types that can be compared for equality, and Show, the types whose values can be rendered as strings) have the special property of supporting derived instances. This means that a programmer defining a new type can state that this type is to be an instance of one of these special type classes, without providing implementations of the class methods as is usually necessary when declaring class instances. All the necessary methods will be \"derived\" – that is, constructed automatically – based on the structure of the type. For instance, the following declaration of a type of binary trees states that it is to be an instance of the classes Eq and Show:\n\ndata BinTree a = Leaf a | Node (BinTree a) a (BinTree a)\n      deriving (Eq, Show)\nThis results in an equality function (==) and a string representation function (show) being automatically defined for any type of the form BinTree T provided that T itself supports those operations.\n\nThe support for derived instances of Eq and Show makes their methods == and show generic in a qualitatively different way from parametrically polymorphic functions: these \"functions\" (more accurately, type-indexed families of functions) can be applied to values of various types, and although they behave differently for every argument type, little work is needed to add support for a new type. Ralf Hinze (2004) has shown that a similar effect can be achieved for user-defined type classes by certain programming techniques. Other researchers have proposed approaches to this and other kinds of genericity in the context of Haskell and extensions to Haskell (discussed below).\n\nPolyP[edit]\nPolyP was the first generic programming language extension to Haskell. In PolyP, generic functions are called polytypic. The language introduces a special construct in which such polytypic functions can be defined via structural induction over the structure of the pattern functor of a regular datatype. Regular datatypes in PolyP are a subset of Haskell datatypes. A regular datatype t must be of kind * → *, and if a is the formal type argument in the definition, then all recursive calls to t must have the form t a. These restrictions rule out higher-kinded datatypes as well as nested datatypes, where the recursive calls are of a different form. The flatten function in PolyP is here provided as an example:\n\n   flatten :: Regular d => d a -> [a]\n   flatten = cata fl\n   \n   polytypic fl :: f a [a] -> [a]\n     case f of\n       g+h -> either fl fl\n       g*h -> \\(x,y) -> fl x ++ fl y\n       () -> \\x -> []\n       Par -> \\x -> [x]\n       Rec -> \\x -> x\n       d@g -> concat . flatten . pmap fl\n       Con t -> \\x -> []\n   \n   cata :: Regular d => (FunctorOf d a b -> b) -> d a -> b\nGeneric Haskell[edit]\nGeneric Haskell is another extension to Haskell, developed at Utrecht University in the Netherlands. The extensions it provides are:\n\nType-indexed values are defined as a value indexed over the various Haskell type constructors (unit, primitive types, sums, products, and user-defined type constructors). In addition, we can also specify the behaviour of a type-indexed values for a specific constructor using constructor cases, and reuse one generic definition in another using default cases.\nThe resulting type-indexed value can be specialised to any type.\n\nKind-indexed types are types indexed over kinds, defined by giving a case for both * and k → k'. Instances are obtained by applying the kind-indexed type to a kind.\nGeneric definitions can be used by applying them to a type or kind. This is called generic application. The result is a type or value, depending on which sort of generic definition is applied.\nGeneric abstraction enables generic definitions be defined by abstracting a type parameter (of a given kind).\nType-indexed types are types which are indexed over the type constructors. These can be used to give types to more involved generic values. The resulting type-indexed types can be specialised to any type.\nAs an example, the equality function in Generic Haskell:[28]\n\n   type Eq {[ * ]} t1 t2 = t1 -> t2 -> Bool\n   type Eq {[ k -> l ]} t1 t2 = forall u1 u2. Eq {[ k ]} u1 u2 -> Eq {[ l ]} (t1 u1) (t2 u2)\n   \n   eq {| t :: k |} :: Eq {[ k ]} t t\n   eq {| Unit |} _ _ = True\n   eq {| :+: |} eqA eqB (Inl a1) (Inl a2) = eqA a1 a2\n   eq {| :+: |} eqA eqB (Inr b1) (Inr b2) = eqB b1 b2\n   eq {| :+: |} eqA eqB _ _ = False\n   eq {| :*: |} eqA eqB (a1 :*: b1) (a2 :*: b2) = eqA a1 a2 && eqB b1 b2\n   eq {| Int |} = (==)\n   eq {| Char |} = (==)\n   eq {| Bool |} = (==)\nClean[edit]\nClean offers generic programming based PolyP and the generic Haskell as supported by the GHC>=6.0. It parametrizes by kind as those but offers overloading.\n\nOther languages[edit]\nThe ML family of programming languages support generic programming through parametric polymorphism and generic modules called functors. Both Standard ML and OCaml provide functors, which are similar to class templates and to Ada's generic packages. Scheme syntactic abstractions also have a connection to genericity – these are in fact a superset of templating à la C++.\n\nA Verilog module may take one or more parameters, to which their actual values are assigned upon the instantiation of the module. One example is a generic register array where the array width is given via a parameter. Such the array, combined with a generic wire vector, can make a generic buffer or memory module with an arbitrary bit width out of a single module implementation.[29]\n\nVHDL, being derived from Ada, also have generic ability.\n\nSee also[edit]\nPartial evaluation\nConcept (generic programming)\nType polymorphism\nTemplate metaprogramming",
          "subparadigms": []
        },
        {
          "pdid": 32,
          "name": "Literate programming",
          "details": "Literate programming is an approach to programming introduced by Donald Knuth in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated.[1]\n\nThe literate programming paradigm, as conceived by Knuth, represents a move away from writing programs in the manner and order imposed by the computer, and instead enables programmers to develop programs in the order demanded by the logic and flow of their thoughts.[2] Literate programs are written as an uninterrupted exposition of logic in an ordinary human language, much like the text of an essay, in which macros are included to hide abstractions and traditional source code.\n\nLiterate programming (LP) tools are used to obtain two representations from a literate source file: one suitable for further compilation or execution by a computer, the \"tangled\" code, and another for viewing as formatted documentation, which is said to be \"woven\" from the literate source.[3] While the first generation of literate programming tools were computer language-specific, the later ones are language-agnostic and exist above the programming languages.\n\nContents  [hide] \n1\tHistory and philosophy\n2\tConcept\n2.1\tAdvantages\n2.2\tContrast with documentation generation\n3\tWorkflow\n4\tExample\n5\tTools\n6\tSee also\n7\tReferences\n8\tFurther reading\nHistory and philosophy[edit]\nLiterate programming was first introduced by Donald E. Knuth. The main intention behind this approach was to treat program as a literature understandable to human beings. This approach was implemented at Stanford university as a part of research on algorithms and digital typography. This implementation was further called as “WEB” by Donald Knuth since he believed that it was one of the few three-letter words of English that hadn’t already been applied to computer. However, it correctly resembles the complicated nature of software delicately pieced together from simple materials.[1]\n\n\n\nConcept[edit]\nLiterate programming is writing out the program logic in a human language with included (separated by a primitive markup) code snippets and macros. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a meta-language on top of the underlying programming language.\n\nA preprocessor is used to substitute arbitrary hierarchies, or rather \"interconnected 'webs' of macros\",[4] to produce the compilable source code with one command (\"tangle\"), and documentation with another (\"weave\"). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.\n\n\n\nAdvantages[edit]\nAccording to Knuth,[5][6] literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one's thoughts during a program's creation.[7] The resulting documentation allows the author to restart his own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher \"bird's eye view\" of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.[5]\n\nContrast with documentation generation[edit]\nLiterate programming is very often misunderstood[8] to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is backwards: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; in literate programming code is embedded in documentation, with the code following the structure of the documentation.\n\nThis misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are \"literate programming tools\". However, because these tools do not implement the \"web of abstract concepts\" hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.[8][9]\n\n\n\nWorkflow[edit]\nImplementing literate programming consists of two steps:\n\n1) Weaving: Generating comprehensive document about program and its maintenance.\n\n2) Tangling: Generating machine executable code\n\nWeaving and Tangling are done on the same source so that they are consistent with each other.\n\nExample[edit]\nA classic example of literate programming is the literate implementation of the standard Unix wc word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his Literate Programming book. The same example was later rewritten for the noweb literate programming tool.[10] This example provides a good illustration of the basic elements of literate programming.\n\nCreation of macros\nThe following snippet of the wc literate program[10] shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new \"operators\" in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets (\"<<...>>\") that indicate macros, the \"@\" symbol which indicates the end of the code section in a noweb file. The \"<<*>>\" symbol stands for the \"root\", topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as \"<<name of the chunk>>=\", with the equal sign), so one literate program file can contain several files with machine source code.\n\nThe purpose of wc is to count lines, words, and/or characters in a list of files. The\nnumber of lines in a file is ......../more explanations/\n\nHere, then, is an overview of the file wc.c that is defined by the noweb program wc.nw:\n    <<*>>=\n    <<Header files to include>>\n    <<Definitions>>\n    <<Global variables>>\n    <<Functions>>\n    <<The main program>>\n    @\n\nWe must include the standard I/O definitions, since we want to send formatted output\nto stdout and stderr.\n    <<Header files to include>>=\n    #include <stdio.h>\n    @\nThe unraveling of the chunks can be done in any place in the literate program text file, not necessarily in the order they are sequenced in the enclosing chunk, but as is demanded by the logic reflected in the explanatory text that envelops the whole program.\n\nProgram as a web—macros are not just section names\nMacros are not the same as \"section names\" in standard documentation. Literate programming macros can hide any chunk of code behind themselves, and be used inside any low-level machine language operators, often inside logical operators such as \"if\", \"while\" or \"case\". This is illustrated by the following snippet of the wc literate program.[10]\n\nThe present chunk, which does the counting, was actually one of\nthe simplest to write. We look at each character and change state if it begins or ends\na word.\n\n    <<Scan file>>=\n    while (1) {\n      <<Fill buffer if it is empty; break at end of file>>\n      c = *ptr++;\n      if (c > ' ' && c < 0177) {\n        /* visible ASCII codes */\n        if (!in_word) {\n          word_count++;\n          in_word = 1;\n        }\n        continue;\n      }\n      if (c == '\\n') line_count++;\n      else if (c != ' ' && c != '\\t') continue;\n      in_word = 0;\n        /* c is newline, space, or tab */\n    }\n    @\nIn fact, macros can stand for any arbitrary chunk of code or other macros, and are thus more general than top-down or bottom-up \"chunking\", or than subsectioning. Knuth says that when he realized this, he began to think of a program as a web of various parts.[1]\n\nOrder of human logic, not that of the compiler\nIn a noweb literate program besides the free order of their exposition, the chunks behind macros, once introduced with \"<<...>>=\", can be grown later in any place in the file by simply writing \"<<name of the chunk>>=\" and adding more content to it, as the following snippet illustrates (\"plus\" is added by the document formatter for readability, and is not in the code).[10]\n\n The grand totals must be initialized to zero at the beginning of the program.\nIf we made these variables local to main, we would have to do this  initialization\nexplicitly; however, C globals are automatically zeroed. (Or rather,``statically\nzeroed.'' (Get it?)\n\n    <<Global variables>>+=\n    long tot_word_count, tot_line_count,\n         tot_char_count;\n      /* total number of words, lines, chars */\n    @\nRecord of the train of thought\nThe documentation for a literate program is produced as part of writing the program. Instead of comments provided as side notes to source code a literate program contains the explanation of concepts on each level, with lower level concepts deferred to their appropriate place, which allows for better communication of thought. The snippets of the literate wc above show how an explanation of the program and its source code are interwoven. Such exposition of ideas creates the flow of thought that is like a literary work. Knuth wrote a \"novel\" which explains the code of the computer strategy game Colossal Cave Adventure.[11]\n\nTools[edit]\nThe first published literate programming environment was WEB, introduced by Donald Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe.[12] The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems and can produce TeX and PDF documentation.\n\nThere are various other implementations of the literate programming concept:\n\nAxiom, which is evolved from scratchpad, a computer algebra system developed by IBM. It is now being developed by Tim Daly, one of the developer of scratchpad, Axiom is totally written as a literate program.\nnoweb is independent of the programming language of the source code. It is well known for its simplicity, given the need of using only two text markup conventions and two tool invocations, and it allows for text formatting in HTML rather than going through the TeX system.\nLiterate is a \"modern literate programming system.\" Like noweb, it works with any programming language, but it produces pretty-printed and syntax-highlighted HTML, and it tries to retain all the advantages of CWEB, including output formatted like CWEB. Other notable advantages compared with older tools include being based on Markdown and generating well-formatted \"tangled\" code. See External Links.\nFunnelWeb is another LP tool that can produce HTML documentation output. It has more complicated markup (with \"@\" escaping any FunnelWeb command), but has many more flexible options. Like noweb, it is independent of the programming language of the source code.\nNuweb can translate a single LP source into any number of code files in any mix of languages together with documentation in LaTeX. It does it in a single invocation; it does not have separate weave and tangle commands. It does not have the extensibility of noweb, but it can use the listings package of LaTeX to provide pretty-printing and the hyperref package to provide hyperlinks in PDF output. It also has extensive indexing and cross-referencing facilities including cross-references from the generated code back to the documentation, both as automatically generated comments and as strings that the code can use to report its behaviour. Vimes is a type-checker for Z notation which shows the use of nuweb in a practical application. Around 15,000 lines of nuweb source are translated into nearly 15,000 lines of C/C++ code and over 460 pages of documentation. See External links.\nMolly is a LP tool written in Perl, which aims to modernize and scale it with \"folding HTML\" and \"virtual views\" on code. It uses \"noweb\" markup for the literate source files. See External links.\nCodnar is an inverse literate programming tool available as a Ruby Gem (see External links). Instead of the machine-readable source code being extracted out of the literate documentation sources, the literate documentation is extracted out of the normal machine-readable source code files. This allows these source code files to be edited and maintained as usual. The approach is similar to that used by popular API documentation tools, such as JavaDoc. Such tools, however, generate API reference documentation, while Codnar generates a linear narrative describing the code, similar to that created by classical LP tools. Codnar can co-exist with API documentation tools, allowing both a reference manual and a linear narrative to be generated from the same set of source code files.\nThe Leo text editor is an outlining editor which supports optional noweb and CWEB markup. The author of Leo mixes two different approaches: first, Leo is an outlining editor, which helps with management of large texts; second, Leo incorporates some of the ideas of literate programming, which in its pure form (i.e., the way it is used by Knuth Web tool or tools like \"noweb\") is possible only with some degree of inventiveness and the use of the editor in a way not exactly envisioned by its author (in modified @root nodes). However, this and other extensions (@file nodes) make outline programming and text management successful and easy and in some ways similar to literate programming.[13]\nThe Haskell programming language has native support for semi-literate programming, generally inspired by CWEB but with a significantly reduced functionality and simpler implementation. When aiming for TeX output, one writes a plain LaTeX file where source code is marked by a given surrounding environment; LaTeX can be set up to handle that environment, while the Haskell compiler looks for the right markers to identify Haskell statements to compile, removing the TeX documentation as if they were comments. However, as described above, this is not literate programming in the sense intended by Knuth. Haskell's functional, modular nature[14] makes literate programming directly in the language somewhat easier, but it is not nearly as powerful as one of the WEB tools where \"tangle\" can reorganize in arbitrary ways.\nThe Web 68 Literate Programming system uses Algol 68 as the underlying programming language, although there is nothing in the pre-processor 'tang' to force the use of that language.[15]\nEmacs org-mode for literate programming through Babel,[16] which allows embedding blocks of source code from multiple programming languages[17] within a single text document. Blocks of code can share data with each other, display images inline, or be parsed into pure source code using the noweb reference syntax.[18]\nCoffeeScript supports a \"literate\" mode, which enables programs to be compiled from a source document written in Markdown with indented blocks of code.[19]\nWolfram Language, formerly known as Mathematica, is written in notebooks which combine text with code.[20]\nSwift (programming language), created by Apple Inc. can be edited in Playgrounds which provide an interactive programming environment that evaluates each statement and displays live results as the code is edited. Playgrounds also allow the user to add Markup language along with the code that provide headers, inline formatting and images.[21]\nJulia (programming language) supports the iJulia mode of development which - inspired by iPython - works in the format of notebooks, which combine text, graphs, etc. with the written code.\nSee also[edit]\nSweave and Knitr – examples of use of the \"noweb\"-like Literate Programming tool inside the R language for creation of dynamic statistical reports\nSelf-documenting code – source code that can be easily understood without documentation",
          "subparadigms": []
        },
        {
          "pdid": 33,
          "name": "Procedural",
          "details": "Procedural programming is a programming paradigm, derived from structured programming, based upon the concept of the procedure call. Procedures, also known as routines, subroutines, or functions (not to be confused with mathematical functions, but similar to those used in functional programming), simply contain a series of computational steps to be carried out. Any given procedure might be called at any point during a program's execution, including by other procedures or itself. Procedural programming languages include C, Go, Fortran, Pascal, Ada, and BASIC.[1]\n\nComputer processors provide hardware support for procedural programming through a stack register and instructions for calling procedures and returning from them. Hardware support for other types of programming is possible, but no attempt was commercially successful (for example Lisp machines or Java processors).\n\nContents  [hide] \n1\tProcedures and modularity\n2\tComparison with imperative programming\n3\tComparison with object-oriented programming\n4\tComparison with functional programming\n5\tComparison with logic programming\n6\tSee also\n7\tReferences\n8\tExternal links\nProcedures and modularity[edit]\nMain article: Modular programming\nModularity is generally desirable, especially in large, complicated programs. Inputs are usually specified syntactically in the form of arguments and the outputs delivered as return values.\n\nScoping is another technique that helps keep procedures modular. It prevents the procedure from accessing the variables of other procedures (and vice versa), including previous instances of itself, without explicit authorization.\n\nLess modular procedures, often used in small or quickly written programs, tend to interact with a large number of variables in the execution environment, which other procedures might also modify.\n\nBecause of the ability to specify a simple interface, to be self-contained, and to be reused, procedures are a convenient vehicle for making pieces of code written by different people or different groups, including through programming libraries.\n\nComparison with imperative programming[edit]\nProcedural programming languages are also imperative languages, because they make explicit references to the state of the execution environment. This could be anything from variables (which may correspond to processor registers) to something like the position of the \"turtle\" in the Logo programming language.\n\nOften, the terms \"procedural programming\" and \"imperative programming\" are used synonymously. However, procedural programming relies heavily on blocks and scope, whereas imperative programming as a whole may or may not have such features. As such, procedural languages generally use reserved words that act on blocks, such as if, while, and for, to implement control flow, whereas non-structured imperative languages use goto statements and branch tables for the same purpose.\n\nComparison with object-oriented programming[edit]\nThe focus of procedural programming is to break down a programming task into a collection of variables, data structures, and subroutines, whereas in object-oriented programming it is to break down a programming task into objects that expose behavior (methods) and data (members or attributes) using interfaces. The most important distinction is that while procedural programming uses procedures to operate on data structures, object-oriented programming bundles the two together, so an \"object\", which is an instance of a class, operates on its \"own\" data structure.[2]\n\nNomenclature varies between the two, although they have similar semantics:\n\nProcedural\tObject-oriented\nprocedure\tmethod\nrecord\tobject\nmodule\tclass\nprocedure call\tmessage\nComparison with functional programming[edit]\nThe principles of modularity and code reuse in practical functional languages are fundamentally the same as in procedural languages, since they both stem from structured programming. So for example:\n\nProcedures correspond to functions. Both allow the reuse of the same code in various parts of the programs, and at various points of its execution.\nBy the same token, procedure calls correspond to function application.\nFunctions and their invocations are modularly separated from each other in the same manner, by the use of function arguments, return values and variable scopes.\nThe main difference between the styles is that functional programming languages remove or at least deemphasize the imperative elements of procedural programming. The feature set of functional languages is therefore designed to support writing programs as much as possible in terms of pure functions:\n\nWhereas procedural languages model execution of the program as a sequence of imperative commands that may implicitly alter shared state, functional programming languages model execution as the evaluation of complex expressions that only depend on each other in terms of arguments and return values. For this reason, functional programs can have a freer order of code execution, and the languages may offer little control over the order in which various parts of the program are executed. (For example, the arguments to a procedure invocation in Scheme are executed in an arbitrary order.)\nFunctional programming languages support (and heavily use) first-class functions, anonymous functions and closures, although these concepts are being included in newer procedural languages.\nFunctional programming languages tend to rely on tail call optimization and higher-order functions instead of imperative looping constructs.\nMany functional languages, however, are in fact impurely functional and offer imperative/procedural constructs that allow the programmer to write programs in procedural style, or in a combination of both styles. It is common for input/output code in functional languages to be written in a procedural style.\n\nThere do exist a few esoteric functional languages (like Unlambda) that eschew structured programming precepts for the sake of being difficult to program in (and therefore challenging). These languages are the exception to the common ground between procedural and functional languages.\n\nComparison with logic programming[edit]\nIn logic programming, a program is a set of premises, and computation is performed by attempting to prove candidate theorems. From this point of view, logic programs are declarative, focusing on what the problem is, rather than on how to solve it.\n\nHowever, the backward reasoning technique, implemented by SLD resolution, used to solve problems in logic programming languages such as Prolog, treats programs as goal-reduction procedures. Thus clauses of the form:\n\nH :- B1, …, Bn.\nhave a dual interpretation, both as procedures\n\nto show/solve H, show/solve B1 and … and Bn\nand as logical implications:\n\nB1 and … and Bn implies H.\nExperienced logic programmers use the procedural interpretation to write programs that are effective and efficient, and they use the declarative interpretation to help ensure that programs are correct.\n\nSee also[edit]\nComparison of programming paradigms\nDeclarative programming\nFunctional programming (contrast)\nImperative programming\nLogic programming\nObject-oriented programming\nProgramming paradigms\nProgramming language\nStructured programming\nSQL procedural extensions",
          "subparadigms": []
        },
        {
          "pdid": 34,
          "name": "Imperative",
          "details": "In computer science, imperative programming is a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\n\nThe term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.\n\nContents  [hide] \n1\tImperative and procedural programming\n2\tRationale and foundations of imperative programming\n3\tHistory of imperative and object-oriented languages\n4\tSee also\n5\tNotes\n6\tReferences\nImperative and procedural programming[edit]\nProcedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavily-procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. From the 1960s onwards, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.[1]\n\nProcedural programming could be considered a step towards declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.\n\nRationale and foundations of imperative programming[edit]\nThe hardware implementation of almost all computers is imperative.[note 1] Nearly all computer hardware is designed to execute machine code, which is native to the computer, written in the imperative style. From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.\n\nAssignment statements, in imperative paradigm, perform an operation on information located in memory and store the results in memory for later use. High-level imperative languages, in addition, permit the evaluation of complex expressions, which may consist of a combination of arithmetic operations and function evaluations, and the assignment of the resulting value to memory. Looping statements (as in while loops, do while loops, and for loops) allow a sequence of statements to be executed multiple times. Loops can either execute the statements they contain a predefined number of times, or they can execute them repeatedly until some condition changes. Conditional branching statements allow a sequence of statements to be executed only if some condition is met. Otherwise, the statements are skipped and the execution sequence continues from the statement following them. Unconditional branching statements allow an execution sequence to be transferred to another part of a program. These include the jump (called goto in many languages), switch, and the subprogram, subroutine, or procedure call (which usually returns to the next statement after the call).\n\nEarly in the development of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by hierarchical decomposition into simpler procedural structures.\n\nMany imperative programming languages (such as Fortran, BASIC, and C) are abstractions of assembly language.[2]\n\nHistory of imperative and object-oriented languages[edit]\nThe earliest imperative languages were the machine languages of the original computers. In these languages, instructions were very simple, which made hardware implementation easier, but hindered the creation of complex programs. FORTRAN, developed by John Backus at International Business Machines (IBM) starting in 1954, was the first major programming language to remove the obstacles presented by machine code in the creation of complex programs. FORTRAN was a compiled language that allowed named variables, complex expressions, subprograms, and many other features now common in imperative languages. The next two decades saw the development of many other major high-level imperative programming languages. In the late 1950s and 1960s, ALGOL was developed in order to allow mathematical algorithms to be more easily expressed, and even served as the operating system's target language for some computers. MUMPS (1966) carried the imperative paradigm to a logical extreme, by not having any statements at all, relying purely on commands, even to the extent of making the IF and ELSE commands independent of each other, connected only by an intrinsic variable named $TEST. COBOL (1960) and BASIC (1964) were both attempts to make programming syntax look more like English. In the 1970s, Pascal was developed by Niklaus Wirth, and C was created by Dennis Ritchie while he was working at Bell Laboratories. Wirth went on to design Modula-2 and Oberon. For the needs of the United States Department of Defense, Jean Ichbiah and a team at Honeywell began designing Ada in 1978, after a 4-year project to define the requirements for the language. The specification was first published in 1983, with revisions in 1995 and 2005–06.\n\nThe 1980s saw a rapid growth in interest in object-oriented programming. These languages were imperative in style, but added features to support objects. The last two decades of the 20th century saw the development of many such languages. Smalltalk-80, originally conceived by Alan Kay in 1969, was released in 1980, by the Xerox Palo Alto Research Center (PARC). Drawing from concepts in another object-oriented language—Simula (which is considered the world's first object-oriented programming language, developed in the 1960s)—Bjarne Stroustrup designed C++, an object-oriented language based on C. C++ was first implemented in 1985. In the late 1980s and 1990s, the notable imperative languages drawing on object-oriented concepts were Perl, released by Larry Wall in 1987; Python, released by Guido van Rossum in 1990; Visual Basic and Visual C++ (which included Microsoft Foundation Class Library (MFC) 2.0), released by Microsoft in 1991 and 1993 respectively; PHP, released by Rasmus Lerdorf in 1994; Java, released by Sun Microsystems in 1994, and Ruby, released by Yukihiro \"Matz\" Matsumoto in 1995. Microsoft's .NET Framework (2002) is imperative at its core, as are its main target languages, VB.NET and C# that run on it; however Microsoft's F#, a functional language, also runs on it.\n\nSee also[edit]\nicon\tComputer science portal\nFunctional programming\nComparison of programming paradigms\nDeclarative programming (contrast)\nProgramming paradigms\nObject-oriented programming\nReactive programming\nHistory of programming languages\nList of imperative programming languages\nCategory:Procedural programming languages lists additional imperative programming languages",
          "subparadigms": [
            32,
            33
          ]
        },
        {
          "pdid": 35,
          "name": "Inductive",
          "details": "Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\nContents  [hide] \n1\tDefinition\n2\tHistory\n3\tApplication areas\n4\tSee also\n5\tExternal links\n6\tFurther reading\n7\tReferences\nDefinition[edit]\nInductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.\n\nOutput of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.\n\nIn many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis,[1][2] usually opposed to 'deductive' program synthesis,[3][4][5] where the specification is usually complete.\n\nIn other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.\n\nThe diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.\n\nHistory[edit]\nResearch on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers[6] and work of Biermann.[7] These approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith.[8] Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.\n\nThe advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro[9] eventually spawning the new field of inductive logic programming (ILP).[10] The early works of Plotkin,[11][12] and his \"relative least general generalization (rlgg)\", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM.[13] But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs[14][15][16] with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.[17]\n\nIn parallel to work in ILP, Koza[18] proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE[19] and the systematic-search-based system MagicHaskeller.[20] Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.\n\nThe early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem.[21] The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold.[22] More recently, the language learning problem was addressed by the inductive programming community.[23][24]\n\nIn the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).\n\nOther ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures;[25][26][27] abstraction has also been explored as a more powerful approach to cumulative learning and function invention.[28][29]\n\nOne powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).[30][31][32][33]\n\nApplication areas[edit]\nThe first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where \"learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations\".\n\nSince then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming,[34] the related areas of programming by example[35] and programming by demonstration,[36] and intelligent tutoring systems.\n\nOther areas where inductive inference has been recently applied are knowledge acquisition,[37] artificial general intelligence,[38] reinforcement learning and theory evaluation,[39][40] and cognitive science in general.[41][42] There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.\n\nSee also[edit]\nAutomatic programming\nDeclarative programming\nEvolutionary programming\nFunctional programming\nGenetic programming\nGrammar induction\nInductive reasoning\nInductive logic programming\nInductive functional programming\nLogic programming\nMachine learning\nProbabilistic programming language\nProgram synthesis\nProgramming by example\nProgramming by demonstration\nStructure mining\nTest-driven development",
          "subparadigms": []
        },
        {
          "pdid": 36,
          "name": "Natural language",
          "details": "Natural Language Programming (NLP) is an ontology-assisted way of programming in terms of natural language sentences, e.g. English. A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural languages and natural language user interfaces include Inform7, a natural programming language for making interactive fiction; Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural language input.\n\nContents  [hide] \n1\tInterpretation\n2\tSoftware paradigm\n3\tPublication value of NLP programs and documents\n4\tContribution of NLP programs to machine knowledge\n5\tSee also\n6\tBibliography\n7\tReferences\n8\tExternal links\nInterpretation[edit]\nThe smallest unit of statement in NLP is a sentence. Each sentence is stated in terms of concepts from the underlying ontology, attributes in that ontology and named objects in capital letters. In an NLP text every sentence unambiguously compiles into a procedure call in the underlying high-level programming language such as MATLAB, Octave, SciLab, Python, etc.\n\nSymbolic languages such as Mathematica are capable of interpreted processing of queries by sentences. This can allow interactive requests such as that implemented in Wolfram Alpha.[1][2] The difference between these and NLP is that the latter builds up a single program or a library of routines that are programmed through natural language sentences using an ontology that defines the available data structures in a high level programming language.\n\nAn example text from an English language NLP program (in sEnglish) is as follows:\n\nIf U_ is 'smc01-control', then do the following. Define surface weights Alpha as \"[0.5, 0.5]\". Initialise matrix Phi as a 'unit matrix'. Define J as the 'inertia matrix' of Spc01. Compute matrix J2 as the inverse of J . Compute position velocity error Ve and angular velocity error Oe from dynamical state X, guidance reference Xnow . Define the joint sliding surface G2 from the position velocity error Ve and angular velocity error Oe using the surface weights Alpha. Compute the smoothed sign function SG2 from the joint sliding surface G2 with sign threshold 0.01. Compute special dynamical force F from dynamical state X and surface weights Alpha. Compute control torque T and control force U from matrix J2, surface weights Alpha, special dynamical force F, smoothed sign function SG2. Finish conditional actions.\n\nthat defines a feedback control scheme using a sliding mode control method.\n\nSoftware paradigm[edit]\nNatural language programming is a top down method of writing software. Its stages are as follows:\n\nDefinition of an ontology - taxonomy - of concepts needed to describe tasks in the topic addressed. Each concept and all their attributes are defined in natural language words. This ontology will define the data structures the NLP can use in sentences.\nDefinition of one or more top level sentences in terms of concepts from the ontology. These sentences are later used to invoke the most important activities in the topic.\nDefining of each of the top level sentences in terms of a sequence of sentences.\nDefining each of the lower level sentences in terms of other sentences or by a simple sentence of the form Execute code \"...\". where ... stands for a code in terms of the associated high level programming language.\nRepeating the previous step until you have no sentences left undefined. During this process each of sentences can be classified to belong to a section of the document to be produced in HTML or Latex format to form the final NLP program.\nTesting the meaning of each sentence by executing its code using testing objects.\nProviding a library of procedure calls (in the underlying high level language) which are needed in the code definitions of some low-level-sentence meanings.\nProviding a title, author data and compiling the sentences into an HTML or LaTex file.\nPublishing the NLP program as a webpage on the Internet or as a PDF file compiled from the LaTex document.\nPublication value of NLP programs and documents[edit]\nAn NLP program is a precise formal description of some procedure that its author created. It is human readable and it can also be read by a suitable software agent. For example, a web page in an NLP format can be read by a software personal assistant agent to a person and she or he can ask the agent to execute some sentences, i.e. carry out some task or answer a question. There is a reader agent available for English interpretation of HTML based NLP documents that a person can run on her personal computer .\n\nContribution of NLP programs to machine knowledge[edit]\nAn ontology class in a natural language program that is not a concept in the sense as humans use concepts. Concepts in an NLP are examples (samples) of generic human concepts. Each sentence in an NLP program is either (1) stating a relationship in a world model or (2) carries out an action in the environment or (3) carries out a computational procedure or (4) invokes an answering mechanism in response to a question.\n\nA set of NLP sentences, with associated ontology defined, can also be used as a pseudo code that does not provide the details in any underlying high level programming language. In such an application the sentences used become high level abstractions (conceptualisations) of computing procedures that are computer language and machine independent.\n\nSee also[edit]\nAttempto Controlled English\nNatural language processing\nKnowledge representation\nEnd-user programming\nProgramming languages with English-like syntax\nAppleScript\nCOBOL\nFLOW-MATIC\nInform 7\nJOSS\nTranscript\nSQL\nxTalk",
          "subparadigms": []
        },
        {
          "pdid": 37,
          "name": "Service-oriented modeling",
          "details": "Service-oriented modeling is the discipline of modeling business and software systems, for the purpose of designing and specifying service-oriented business systems within a variety of architectural styles and paradigms, such as application architecture, service-oriented architecture, microservices, and cloud computing.\n\nAny service-oriented modeling method typically includes a modeling language that can be employed by both the 'problem domain organization' (the Business), and 'solution domain organization' (the Information Technology Department), whose unique perspectives typically influence the service development life-cycle strategy and the projects implemented using that strategy.\n\nService-oriented modeling typically strives to create models that provide a comprehensive view of the analysis, design, and architecture of all 'Software Entities' in an organization, which can be understood by individuals with diverse levels of business and technical understanding. Service-oriented modeling typically encourages viewing software entities as 'assets' (service-oriented assets), and refers to these assets collectively as 'services'. A key service design concern is to find the right service granularity both on the business (domain) level and on a technical (interface contract) level.\n\nContents  [hide] \n1\tPopular approaches\n1.1\tService-oriented design and development methodology\n1.2\tService-oriented modeling and architecture\n1.3\tService-oriented modeling framework (SOMF)\n2\tSee also\n3\tReferences\n4\tFurther reading\n5\tExternal links\nPopular approaches[edit]\nSeveral approaches that have been proposed specifically for designing and modeling services, including SDDM, SOMA and SOMF.\n\nService-oriented design and development methodology[edit]\nService-Oriented Design and Development Methodology (SDDM) is a fusion method created and compiled by M. Papazoglou and W.J. van den Heuvel.[1] The paper argues that SOA designers and service developers cannot be expected to oversee a complex service-oriented development project without relying on a sound design and development methodology. It provides an overview of the methods and techniques used in service-oriented design, approaches the service development methodology from the point of view of both service producers and requesters, and reviews the range of SDDM elements that are available to these roles.\n\nAn update to SDDM was later published in.[2]\n\nService-oriented modeling and architecture[edit]\nIBM announced service-oriented modeling and architecture (SOMA) as its SOA-related methodology in 2004 and published parts of it subsequently.[3] SOMA refers to the more general domain of service modeling necessary to design and create SOA. SOMA covers a broader scope and implements service-oriented analysis and design (SOAD) through the identification, specification and realization of services, components that realize those services (a.k.a. \"service components\"), and flows that can be used to compose services.\n\nSOMA includes an analysis and design method that extends traditional object-oriented and component-based analysis and design methods to include concerns relevant to and supporting SOA. It consists of three major phases of identification, specification and realization of the three main elements of SOA, namely, services, components that realize those services (aka service components) and flows that can be used to compose services.\n\nSOMA is an end-to-end SOA method for the identification, specification, realization and implementation of services (including information services), components, flows (processes/composition). SOMA builds on current techniques in areas such as domain analysis, functional areas grouping, variability-oriented analysis (VOA) process modeling, component-based development, object-oriented analysis and design and use case modeling. SOMA introduces new techniques such as goal-service modeling, service model creation and a service litmus test to help determine the granularity of a service.\n\nSOMA identifies services, component boundaries, flows, compositions, and information through complementary techniques which include domain decomposition, goal-service modeling and existing asset analysis. The service lifecyclein SOMA consists of the phases of identification, specification, realization, implementation, deployment and management in which the fundamental building blocks of SOA are identified then refined and implemented in each phase. The fundamental building blocks of SOA consists of services, components, flows and related to them, information, policy and contracts.[4]\n\nService-oriented modeling framework (SOMF)[edit]\nSOMF has been devised by author Michael Bell as a holistic and anthropomorphic modeling language for software development that employs disciplines and a universal language to provide tactical and strategic solutions to enterprise problems.[5] The term \"holistic language\" pertains to a modeling language that can be employed to design any application, business and technological environment, either local or distributed. This universality may include design of application-level and enterprise-level solutions, including SOA landscapes, cloud computing, or big data environments. The term \"anthropomorphic\", on the other hand, affiliates the SOMF language with intuitiveness of implementation and simplicity of usage.\n\n\nSOMF Version 2.0\nSOMF is a service-oriented development life cycle methodology, a discipline-specific modeling process. It offers a number of modeling practices and disciplines that contribute to a successful service-oriented life cycle development and modeling during a project (see image on left).\n\nIt illustrates the major elements that identify the “what to do” aspects of a service development scheme. These are the modeling pillars that will enable practitioners to craft an effective project plan and to identify the milestones of a service-oriented initiative—either a small or large-scale business or a technological venture.\n\nThe provided image thumb (on the left hand side) depicts the four sections of the modeling framework that identify the general direction and the corresponding units of work that make up a service-oriented modeling strategy: practices, environments, disciplines, and artifacts. These elements uncover the context of a modeling occupation and do not necessarily describe the process or the sequence of activities needed to fulfill modeling goals. These should be ironed out during the project plan – the service-oriented development life cycle strategy – that typically sets initiative boundaries, time frame, responsibilities and accountabilities, and achievable project milestones.\n\n\n\nSee also[edit]\nObject-oriented analysis and design\nDomain-driven design\nService-oriented architecture\nService granularity principle\nUnified Modeling Language",
          "subparadigms": []
        },
        {
          "pdid": 38,
          "name": "Domain-specific",
          "details": "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There is a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as Emacs Lisp for GNU Emacs and XEmacs. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\n\nThe line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.\n\nContents  [hide] \n1\tUse\n2\tOverview\n2.1\tIn design and implementation\n2.2\tProgramming tools\n3\tDomain-specific language topics\n3.1\tUsage patterns\n3.2\tDesign goals\n3.3\tIdioms\n4\tExamples\n4.1\tGame Maker Language\n4.2\tUnix shell scripts\n4.3\tColdFusion Markup Language\n4.4\tErlang OTP\n4.5\tFilterMeister\n4.6\tMediaWiki templates\n4.7\tSoftware engineering uses\n4.8\tMetacompilers\n4.9\tUnreal Engine before version 4 and other games\n4.10\tRules Engines for Policy Automation\n4.11\tStatistical modelling languages\n4.12\tGenerate model and services to multiple programming Languages\n4.13\tOther examples\n5\tAdvantages and disadvantages\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nUse[edit]\nThe design and use of appropriate DSLs is a key part of domain engineering, by using a language suitable to the domain at hand – this may consist of using an existing DSL or GPL, or developing a new DSL. Language-Oriented Programming considers the creation of special-purpose languages for expressing problems a standard part of the problem solving process. Creating a domain-specific language (with software to support it), rather than reusing an existing language, can be worthwhile if the language allows a particular type of problem or solution to be expressed more clearly than an existing language would allow and the type of problem in question reappears sufficiently often. Pragmatically, a DSL may be specialized to a particular problem domain, a particular problem representation technique, a particular solution technique, or other aspect of a domain.\n\nOverview[edit]\nA domain-specific language is created specifically to solve problems in a particular domain and is not intended to be able to solve problems outside it (although that may be technically possible). In contrast, general-purpose languages are created to solve problems in many domains. The domain can also be a business area. Some examples of business areas include:\n\ndomain-specific language for life insurance policies developed internally in large insurance enterprise\ndomain-specific language for combat simulation\ndomain-specific language for salary calculation\ndomain-specific language for billing\nA domain-specific language is somewhere between a tiny programming language and a scripting language, and is often used in a way analogous to a programming library. The boundaries between these concepts are quite blurry, much like the boundary between scripting languages and general-purpose languages.\n\nIn design and implementation[edit]\nDomain-specific languages are languages (or often, declared syntaxes or grammars) with very specific goals in design and implementation. A domain-specific language can be one of a visual diagramming language, such as those created by the Generic Eclipse Modeling System, programmatic abstractions, such as the Eclipse Modeling Framework, or textual languages. For instance, the command line utility grep has a regular expression syntax which matches patterns in lines of text. The sed utility defines a syntax for matching and replacing regular expressions. Often, these tiny languages can be used together inside a shell to perform more complex programming tasks.\n\nThe line between domain-specific languages and scripting languages is somewhat blurred, but domain-specific languages often lack low-level functions for filesystem access, interprocess control, and other functions that characterize full-featured programming languages, scripting or otherwise. Many domain-specific languages do not compile to byte-code or executable code, but to various kinds of media objects: GraphViz exports to PostScript, GIF, JPEG, etc., where Csound compiles to audio files, and a ray-tracing domain- specific language like POV compiles to graphics files. A computer language like SQL presents an interesting case: it can be deemed a domain-specific language because it is specific to a specific domain (in SQL's case, accessing and managing relational databases), and is often called from another application, but SQL has more keywords and functions than many scripting languages, and is often thought of as a language in its own right, perhaps because of the prevalence of database manipulation in programming and the amount of mastery required to be an expert in the language.\n\nFurther blurring this line, many domain-specific languages have exposed APIs, and can be accessed from other programming languages without breaking the flow of execution or calling a separate process, and can thus operate as programming libraries.\n\nProgramming tools[edit]\nSome domain-specific languages expand over time to include full-featured programming tools, which further complicates the question of whether a language is domain-specific or not. A good example is the functional language XSLT, specifically designed for transforming one XML graph into another, which has been extended since its inception to allow (particularly in its 2.0 version) for various forms of filesystem interaction, string and date manipulation, and data typing.\n\nIn model-driven engineering many examples of domain-specific languages may be found like OCL, a language for decorating models with assertions or QVT, a domain-specific transformation language. However languages like UML are typically general purpose modeling languages.\n\nTo summarize, an analogy might be useful: a Very Little Language is like a knife, which can be used in thousands of different ways, from cutting food to cutting down trees. A domain-specific language is like an electric drill: it is a powerful tool with a wide variety of uses, but a specific context, namely, putting holes in things. A General Purpose Language is a complete workbench, with a variety of tools intended for performing a variety of tasks. Domain-specific languages should be used by programmers who, looking at their current workbench, realize they need a better drill, and find that a particular domain-specific language provides exactly that.\n\nDomain-specific language topics[edit]\nUsage patterns[edit]\nThere are several usage patterns for domain-specific languages:[1][2]\n\nprocessing with standalone tools, invoked via direct user operation, often on the command line or from a Makefile (e.g., grep for regular expression matching, sed, lex, yacc, the GraphViz tool set, etc.)\ndomain-specific languages which are implemented using programming language macro systems, and which are converted or expanded into a host general purpose language at compile-time or read-time\nembedded (or internal) domain-specific languages, implemented as libraries which exploit the syntax of their host general purpose language or a subset thereof, while adding domain-specific language elements (data types, routines, methods, macros etc.). (e.g. Embedded SQL, LINQ)\ndomain-specific languages which are called (at runtime) from programs written in general purpose languages like C or Perl, to perform a specific function, often returning the results of operation to the \"host\" programming language for further processing; generally, an interpreter or virtual machine for the domain-specific language is embedded into the host application (e.g. format strings, a regular expression engine)\ndomain-specific languages which are embedded into user applications (e.g., macro languages within spreadsheets) and which are (1) used to execute code that is written by users of the application, (2) dynamically generated by the application, or (3) both.\nMany domain-specific languages can be used in more than one way.[citation needed] DSL code embedded in a host language may have special syntax support, such as regexes in sed, AWK, Perl or JavaScript, or may be passed as strings.\n\nDesign goals[edit]\nAdopting a domain-specific language approach to software engineering involves both risks and opportunities. The well-designed domain-specific language manages to find the proper balance between these.\n\nDomain-specific languages have important design goals that contrast with those of general-purpose languages:\n\ndomain-specific languages are less comprehensive.\ndomain-specific languages are much more expressive in their domain.\ndomain-specific languages should exhibit minimal redundancy.\nIdioms[edit]\nIn programming, idioms are methods imposed by programmers to handle common development tasks, e.g.:\n\nEnsure data is saved before the window is closed.\nEdit code whenever command-line parameters change because they affect program behavior.\nGeneral purpose programming languages rarely support such idioms, but domain-specific languages can describe them, e.g.:\n\nA script can automatically save data.\nA domain-specific language can parameterize command line input.\nExamples[edit]\nExamples of domain-specific languages include HTML, Logo for pencil-like drawing, Verilog and VHDL hardware description languages, MATLAB and GNU Octave for matrix programming, Mathematica, Maple and Maxima for symbolic mathematics, Specification and Description Language for reactive and distributed systems, spreadsheet formulas and macros, SQL for relational database queries, YACC grammars for creating parsers, regular expressions for specifying lexers, the Generic Eclipse Modeling System for creating diagramming languages, Csound for sound and music synthesis, and the input languages of GraphViz and GrGen, software packages used for graph layout and graph rewriting.\n\nGame Maker Language[edit]\nThe GML scripting language used by GameMaker: Studio is a domain-specific language targeted at novice programmers to easily be able to learn programming. While the language serves as a blend of multiple languages including Delphi, C++, and BASIC, there is a lack of structures, data types, and other features of a full-fledged programming language. Many of the built-in functions are sandboxed for the purpose of easy portability. The language primarily serves to make it easy for anyone to pick up the language and develop a game.\n\nUnix shell scripts[edit]\nUnix shell scripts give a good example of a domain-specific language for data[3] organization. They can manipulate data in files or user input in many different ways. Domain abstractions and notations include streams (such as stdin and stdout) and operations on streams (such as redirection and pipe). These abstractions combine to make a robust language to talk about the flow and organization of data.\n\nThe language consists of a simple interface (a script) for running and controlling processes that perform small tasks. These tasks represent the idioms of organizing data into a desired format such as tables, graphs, charts, etc.\n\nThese tasks consist of simple control-flow and string manipulation mechanisms that cover a lot of common usages like searching and replacing string in files, or counting occurrences of strings (frequency counting).\n\nEven though Unix scripting languages are Turing complete, they differ from general purpose languages.[clarification needed]\n\nIn practice, scripting languages are used to weave together small Unix tools such as AWK (e.g., gawk), ls, sort or wc.\n\nColdFusion Markup Language[edit]\nColdFusion's associated scripting language is another example of a domain-specific language for data-driven websites. This scripting language is used to weave together languages and services such as Java, .NET, C++, SMS, email, email servers, http, ftp, exchange, directory services, and file systems for use in websites.\n\nThe ColdFusion Markup Language (CFML) includes a set of tags that can be used in ColdFusion pages to interact with data sources, manipulate data, and display output. CFML tag syntax is similar to HTML element syntax.\n\nErlang OTP[edit]\nThe Erlang Open Telecom Platform was originally designed for use inside Ericsson as a domain-specific language. The language itself offers a platform of libraries to create finite state machines, generic servers and event managers that quickly allow an engineer to deploy applications, or support libraries, that have been shown in industry benchmarks to outperform other languages intended for a mixed set of domains, such as C and C++. The language is now officially open source and can be downloaded from their website.\n\nFilterMeister[edit]\nFilterMeister is a programming environment, with a programming language that is based on C, for the specific purpose of creating Photoshop-compatible image processing filter plug-ins; FilterMeister runs as a Photoshop plug-in itself and it can load and execute scripts or compile and export them as independent plug-ins. Although the FilterMeister language reproduces a significant portion of the C language and function library, it contains only those features which can be used within the context of Photoshop plug-ins and adds a number of specific features only useful in this specific domain.\n\nMediaWiki templates[edit]\nThe Template feature of MediaWiki is an embedded domain-specific language whose fundamental purpose is to support the creation of page templates and the transclusion (inclusion by reference) of MediaWiki pages into other MediaWiki pages.\n\nA detailed description of that domain-specific language can be found at the corresponding article at the Wikimedia Foundation's Meta-Wiki.\n\nSoftware engineering uses[edit]\nThere has been much interest in domain-specific languages to improve the productivity and quality of software engineering. Domain-specific language could possibly provide a robust set of tools for efficient software engineering. Such tools are beginning to make their way into development of critical software systems.\n\nThe Software Cost Reduction Toolkit[4] is an example of this. The toolkit is a suite of utilities including a specification editor to create a requirements specification, a dependency graph browser to display variable dependencies, a consistency checker to catch missing cases in well-formed formulas in the specification, a model checker and a theorem prover to check program properties against the specification, and an invariant generator that automatically constructs invariants based on the requirements.\n\nA newer development is language-oriented programming, an integrated software engineering methodology based mainly on creating, optimizing, and using domain-specific languages.\n\nMetacompilers[edit]\nFor more details on this topic, see Metacompiler.\nComplementing language-oriented programming, as well as all other forms of domain-specific languages, are the class of compiler writing tools called metacompilers. A metacompiler is not only useful for generating parsers and code generators for domain-specific languages, but a metacompiler itself compiles a domain-specific metalanguage specifically designed for the domain of metaprogramming.\n\nBesides parsing domain-specific languages, metacompilers are useful for generating a wide range of software engineering and analysis tools. The meta-compiler methodology is often found in program transformation systems.\n\nMetacompilers that played a significant role in both computer science and the computer industry include Meta-II[5] and its descendent TreeMeta.[6]\n\nUnreal Engine before version 4 and other games[edit]\nUnreal and Unreal Tournament unveiled a language called UnrealScript. This allowed for rapid development of modifications compared to the competitor Quake (using the Id Tech 2 engine). The Id Tech engine used standard C code meaning C had to be learned and properly applied, while UnrealScript was optimized for ease of use and efficiency. Similarly, the development of more recent games introduced their own specific languages, one more common example is Lua for scripting.\n\nRules Engines for Policy Automation[edit]\nVarious Business Rules Engines have been developed for automating policy and business rules used in both government and private industry. ILOG, Oracle Policy Automation, DTRules, Drools and others provide support for DSLs aimed to support various problem domains. DTRules goes so far as to define an interface for the use of multiple DSLs within a Rule Set.\n\nThe purpose of Business Rules Engines is to define a representation of business logic in as human readable fashion as possible. This allows both subject matter experts and developers to work with and understand the same representation of the business logic. Most Rules Engines provide both an approach to simplifying the control structures for business logic (for example, using Declarative Rules or Decision Tables) coupled with alternatives to programming syntax in favor of DSLs.\n\nStatistical modelling languages[edit]\nStatistical modellers have developed domain-specific languages such as Bugs, Jags, and Stan. These languages provide a syntax for describing a Bayesian model, and generate a method for solving it using simulation.\n\nGenerate model and services to multiple programming Languages[edit]\nGenerate object handling and services based on a Interface Description Language for a domain specific language such as JavaScript for web applications, HTML for documentation, C++ for high performance code, etc. This is done by cross language frameworks such as Apache Thrift or Google Protocol Buffers.\n\nOther examples[edit]\nOther prominent examples of domain-specific languages include:\n\nEmacs Lisp\nGame Description Language\nOpenGL Shading Language\nAdvantages and disadvantages[edit]\nSome of the advantages:[1][2]\n\nDomain-specific languages allow solutions to be expressed in the idiom and at the level of abstraction of the problem domain. The idea is domain experts themselves may understand, validate, modify, and often even develop domain-specific language programs. However, this is seldom the case.[7]\nDomain-specific languages allow validation at the domain level. As long as the language constructs are safe any sentence written with them can be considered safe.[citation needed]\nDomain-specific languages can help to shift the development of business information systems from traditional software developers to the typically larger group of domain-experts who (despite having less technical expertise) have deeper knowledge of the domain.[8]\nSome of the disadvantages:\n\nCost of learning a new language vs. its limited applicability\nCost of designing, implementing, and maintaining a domain-specific language as well as the tools required to develop with it (IDE)\nFinding, setting, and maintaining proper scope.\nDifficulty of balancing trade-offs between domain-specificity and general-purpose programming language constructs.\nPotential loss of processor efficiency compared with hand-coded software.\nProliferation of similar non-standard domain-specific languages, for example, a DSL used within one insurance company versus a DSL used within another insurance company.[9]\nNon-technical domain experts can find it hard to write or modify DSL programs by themselves.[7]\nIncreased difficulty of integrating the DSL with other components of the IT system (as compared to integrating with a general-purpose language).\nLow supply of experts in a particular DSL tends to raise labor costs.\nHarder to find code examples.\nSee also[edit]\n\nThis \"see also\" section may contain an excessive number of suggestions. Please ensure that only the most relevant links are given, that they are not red links, and that any links are not already in this article. (December 2013) (Learn how and when to remove this template message)\nArchitecture description language\nCognitive dimensions of notations\nCombinator library\nDomain analysis\nDomain-specific entertainment language\nDomain-specific modeling\nDomain-specific multimodeling\nConfiguration file\nFluent interface\nMetacompiler\nMetalinguistic abstraction\nMetamodeling\nModel-driven engineering\nMulti-paradigm programming language\nProgramming domain\nProgramming paradigm\nSpecification and Description Language\nProbabilistic programming language (PPL)",
          "subparadigms": []
        },
        {
          "pdid": 39,
          "name": "Dialect",
          "details": "A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly, as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC programming language has many dialects.\n\nThe explosion of Forth dialects led to the saying \"If you've seen one Forth... you've seen one Forth.\"\n\nSee also[edit]\nDomain-specific language\nDomain-specific modelling\nExtensible programming\nLanguage oriented programming\nList of BASIC dialects\nModeling language\nScripting language\nReflection\nMetaprogramming\nCategory:Extensible syntax programming languages\nRebol § Dialects\nRuby (programming language) § Metaprogramming\n",
          "subparadigms": []
        },
        {
          "pdid": 40,
          "name": "Grammar-oriented",
          "details": "Grammar-oriented programming (GOP) and Grammar-oriented Object Design (GOOD) are good for designing and creating a domain-specific programming language (DSL) for a specific business domain.\n\nGOOD can be used to drive the execution of the application or it can be used to embed the declarative processing logic of a context-aware component (CAC) or context-aware service (CAS). GOOD is a method for creating and maintaining dynamically reconfigurable software architectures driven by business-process architectures. The business compiler was used to capture business processes within real-time workshops for various lines of business and create an executable simulation of the processes used.\n\nInstead of using one DSL for the entire programming activity, GOOD suggests the combination of defining domain-specific behavioral semantics in conjunction with the use of more traditional, general purpose programming languages.\n\nSee also[edit]\nAdaptive grammar\nExtensible programming\nLanguage-oriented programming\nDialecting\nTransformation language",
          "subparadigms": [
            39
          ]
        },
        {
          "pdid": 41,
          "name": "Intentional",
          "details": "In computer programming, Intentional Programming is a programming paradigm developed by Charles Simonyi that encodes in software source code the precise intention which programmers (or users) have in mind when conceiving their work. By using the appropriate level of abstraction at which the programmer is thinking, creating and maintaining computer programs become easier. By separating the concerns for intentions and how they are being operated upon, the software becomes more modular and allows for more reusable software code.\n\nIntentional Programming was developed by former Microsoft chief architect Charles Simonyi, who led a team in Microsoft Research, which developed the paradigm and built an integrated development environment (IDE) called IP (for Intentional Programming) that demonstrated the paradigm. Microsoft decided not to productize the Intentional Programming paradigm, as in the early 2000s Microsoft was rolling out C# and .NET to counter Java adoption.[1] Charles Simonyi decided, with approval of Microsoft, to take his idea out from Microsoft and commercialize it himself. He founded the company Intentional Software to pursue this. Microsoft licensed the Intentional Programming patents Simonyi had acquired while at Microsoft, but no source code, to Intentional Software.\n\nAn overview of Intentional Programming as it was developed at Microsoft Research is given in Chapter 11 of the book Generative Programming: Methods, Tools, and Applications.[2]\n\nContents  [hide] \n1\tDevelopment cycle\n2\tSeparating source code storage and presentation\n3\tProgramming Example\n3.1\tIdentity\n3.2\tLevels of detail\n4\tSimilar works\n5\tSee also\n6\tReferences\n7\tExternal links\nDevelopment cycle[edit]\nAs envisioned by Simonyi, developing a new application via the Intentional Programming paradigm proceeds as follows. A programmer builds a WYSIWYG-like environment supporting the schema and notation of business knowledge for a given problem domain (such as productivity applications or life insurance). Users then use this environment to capture their intentions, which are recorded at high level of abstraction. The environment can operate on these intentions and assist the user to create semantically richer documents that can be processed and executed, similar to a spreadsheet. The recorded knowledge is executed by an evaluator or is compiled to generate the final program. Successive changes are done at the WYSIWYG level only. As opposed to word processors, spreadsheets or presentation software, an Intentional environment has more support for structure and semantics of the intentions to be expressed, and can create interactive documents that capture more richly what the user is trying to accomplish. A special case is when the content is program code, and the environment becomes an intelligent IDE.[3]\n\nSeparating source code storage and presentation[edit]\nKey to the benefits of Intentional Programming is that domain code which capture the intentions are not stored in source code text files, but in a tree-based storage (could be binary or XML). Tight integration of the environment with the storage format brings some of the nicer features of database normalization to source code. Redundancy is eliminated by giving each definition a unique identity, and storing the name of variables and operators in exactly one place. This makes it easier to intrinsically distinguish declarations from references, and the environment can show them differently.\n\nWhitespace in a program is also not stored as part of the source code, and each programmer working on a project can choose an indentation display of the source. More radical visualizations include showing statement lists as nested boxes, editing conditional expressions as logic gates, or re-rendering names in Chinese.\n\nThe system uses a normalized language for popular languages like C++ and Java, while letting users of the environment mix and match these with ideas from Eiffel and other languages. Often mentioned in the same context as language-oriented programming via domain-specific languages, and aspect-oriented programming, IP purports to provide some breakthroughs in generative programming. These techniques allow developers to extend the language environment to capture domain-specific constructs without investing in writing a full compiler and editor for any new languages.\n\nProgramming Example[edit]\nA Java program that writes out the numbers from 1 to 10, using a curly bracket syntax, might look like this:\n\n for (int i = 1; i <= 10; i++) {\n    System.out.println(\"the number is \" + i);\n }\nThe code above contains a common construct of most programming languages, the bounded loop, in this case represented by the for construct. The code, when compiled, linked and run, will loop 10 times, incrementing the value of i each time after printing it out.\n\nBut this code does not capture the intentions of the programmer, namely to \"print the numbers 1 to 10\". In this simple case, a programmer asked to maintain the code could likely figure out what it is intended to do, but it is not always so easy. Loops that extend across many lines, or pages, can become very difficult to understand, notably if the original programmer uses unclear labels. Traditionally the only way to indicate the intention of the code was to add source code comments, but often comments are not added, or are unclear, or drift out of sync with the source code they originally described.\n\nIn intentional programming systems the above loop could be represented, at some level, as something as obvious as \"print the numbers 1 to 10\". The system would then use the intentions to generate source code, likely something very similar to the code above. The key difference is that the intentional programming systems maintain the semantic level, which the source code lacks, and which can dramatically ease readability in larger programs.\n\nAlthough most languages contain mechanisms for capturing certain kinds of abstraction, IP, like the Lisp family of languages, allows for the addition of entirely new mechanisms. Thus, if a developer started with a language like C, they would be able to extend the language with features such as those in C++ without waiting for the compiler developers to add them. By analogy, many more powerful expression mechanisms could be used by programmers than mere classes and procedures.\n\nIdentity[edit]\nIP focuses on the concept of identity. Since most programming languages represent the source code as plain text, objects are defined by names, and their uniqueness has to be inferred by the compiler. For example, the same symbolic name may be used to name different variables, procedures, or even types. In code that spans several pages – or, for globally visible names, multiple files – it can become very difficult to tell what symbol refers to what actual object. If a name is changed, the code where it is used must carefully be examined.\n\nBy contrast, in an IP system, all definitions not only assign symbolic names, but also unique private identifiers to objects. This means that in the IP development environment, every reference to a variable or procedure is not just a name – it is a link to the original entity.\n\nThe major advantage of this is that if an entity is renamed, all of the references to it in the program remain valid (known as referential integrity). This also means that if the same name is used for unique definitions in different namespaces (such as \".to_string()\"), references with the same name but different identity will not be renamed, as sometimes happens with search/replace in current editors. This feature also makes it easy to have multi-language versions of the program; it can have a set of English-language names for all the definitions as well as a set of Japanese-language names which can be swapped in at will.\n\nHaving a unique identity for every defined object in the program also makes it easy to perform automated refactoring tasks, as well as simplifying code check-ins in versioning systems. For example, in many current code collaboration systems (e.g. CVS), when two programmers commit changes that conflict (i.e. if one programmer renames a function while another changes one of the lines in that function), the versioning system will think that one programmer created a new function while another modified an old function. In an IP versioning system, it will know that one programmer merely changed a name while another changed the code.\n\nLevels of detail[edit]\nIP systems also offer several levels of detail, allowing the programmer to \"zoom in\" or out. In the example above, the programmer could zoom out to get a level that would say something like:\n\n<<print the numbers 1 to 10>>\nThus IP systems are self-documenting to a large degree, allowing the programmer to keep a good high-level picture of the program as a whole.\n\nSimilar works[edit]\nThere are projects that exploit similar ideas to create code with higher level of abstraction. Among them are:\n\nConcept programming\nLanguage-oriented programming (LOP)\nDomain-specific language (DSL)\nProgram transformation\nSemantic-oriented programming (SOP)\nLiterate programming\nModel-driven architecture (MDA)\nSoftware factory\nMetaprogramming\nLisp (programming language)\nSee also[edit]\nProgramming paradigm\nAutomatic programming\nObject database\nProgramming by demonstration\nArtefaktur\nAbstract syntax tree\nSemantic resolution tree\nStructure editor",
          "subparadigms": []
        },
        {
          "pdid": 42,
          "name": "Language-oriented",
          "details": "Language-oriented programming (LOP) is a style of computer programming in which, rather than solving problems in general-purpose programming languages, the programmer creates one or more domain-specific languages for the problem first, and solves the problem in those languages. This concept is described in detail in the paper by Martin Ward entitled \"Language Oriented Programming\",[1] published in Software - Concepts and Tools, Vol.15, No.4, pp 147-161, 1994,[2] and in the article by Sergey Dmitriev entitled \"Language Oriented Programming: The Next Programming Paradigm\".[3]\n\nContents  [hide] \n1\tConcept\n2\tSee also\n3\tReferences\n4\tExternal links\nConcept[edit]\nThe concept of language-oriented programming takes the approach to capture requirements in the user's terms, and then to try to create an implementation language as isomorphic as possible to the user's descriptions, so that the mapping between requirements and implementation is as direct as possible. A measure of the closeness of this isomorphism is the \"redundancy\" of the language, defined as the number of editing operations needed to implement a stand-alone change in requirements. It is not assumed a-priori what is the best language for implementing the new language. Rather, the developer can choose among options created by analysis of the information flows — what information is acquired, what its structure is, when it is acquired, from whom, and what is done with it.[4]\n\nSee also[edit]\nGrammar-oriented programming\nDialecting\nDomain-specific language\nExtensible programming\nHomoiconicity\nReferences[edit]\nJump up ^ M.P. Ward. \"Language Oriented Programing\" (PDF). Cse.dmu.ac.uk. Retrieved 24 February 2015.\nJump up ^ \"dblp: Software - Concepts and Tools, Volume 15\". Informatik.uni-trier.de. Retrieved 2015-02-24.\nJump up ^ \"JetBrains onBoard Online Magazine :: Fabrique - a code generator\". Onboard.jetbrains.com. Retrieved 2015-02-24.\nJump up ^ Dunlavey (1994). Building Better Applications: a Theory of Efficient Software Development. International Thomson Publishing. ISBN 0-442-01740-5.",
          "subparadigms": [
            36,
            38,
            40,
            41
          ]
        },
        {
          "pdid": 43,
          "name": "Automatic",
          "details": "In computer science, the term automatic programming[1] identifies a type of computer programming in which some mechanism generates a computer program to allow human programmers to write the code at a higher abstraction level.\n\nThere has been little agreement on the precise definition of automatic programming, mostly because its meaning has changed over time. David Parnas, tracing the history of \"automatic programming\" in published research, noted that in the 1940s it described automation of the manual process of punching paper tape. Later it referred to translation of high-level programming languages like Fortran and ALGOL. In fact, one of the earliest programs identifiable as a compiler was called Autocode. Parnas concluded that \"automatic programming has always been a euphemism for programming in a higher-level language than was then available to the programmer.\"[2]\n\nProgram synthesis is one type of automatic programming where a procedure is created from scratch, based on mathematical requirements.\n\nContents  [hide] \n1\tOrigin\n2\tGenerative programming\n3\tSource code generation\n3.1\tImplementations\n4\tSee also\n5\tReferences\n6\tExternal links\nOrigin[edit]\nMildred Koss, an early UNIVAC programmer, explains: \"Writing machine code involved several tedious steps—breaking down a process into discrete instructions, assigning specific memory locations to all the commands, and managing the I/O buffers. After following these steps to implement mathematical routines, a sub-routine library, and sorting programs, our task was to look at the larger programming process. We needed to understand how we might reuse tested code and have the machine help in programming. As we programmed, we examined the process and tried to think of ways to abstract these steps to incorporate them into higher-level language. This led to the development of interpreters, assemblers, compilers, and generators—programs designed to operate on or produce other programs, that is, automatic programming.\"[3]\n\nGenerative programming[edit]\nGenerative programming is a style of computer programming that uses automated source code creation through generic frames, classes, prototypes, templates, aspects, and code generators to improve programmer productivity.[4] It is often related to code-reuse topics such as component-based software engineering and product family engineering.\n\nSource code generation[edit]\nSource code generation is the process of generating source code based on an ontological model such as a template and is accomplished with a programming tool such as a template processor or an integrated development environment (IDE). These tools allow the generation of source code through any of various means. A macro processor, such as the C preprocessor, which replaces patterns in source code according to relatively simple rules, is a simple form of source code generator.\n\nImplementations[edit]\n\nThis section contains content that is written like an advertisement. Please help improve it by removing promotional content and inappropriate external links, and by adding encyclopedic content written from a neutral point of view. (October 2010) (Learn how and when to remove this template message)\nSome IDEs for Java and other languages have more advanced forms of source code generation, with which the programmer can interactively select and customize \"snippets\" of source code. Program \"wizards\", which allow the programmer to design graphical user interfaces interactively while the compiler invisibly generates the corresponding source code, are another common form of source code generation. This may be contrasted with, for example, user interface markup languages, which define user interfaces declaratively.\n\nBesides the generation of code from a wizard or template, IDEs can also generate and manipulate code to automate code refactorings that would require multiple (error prone) manual steps, thereby improving developer productivity.[5] Examples of such features in IDEs are the refactoring class browsers for Smalltalk and those found in Java IDEs like Eclipse.\n\nA specialized alternative involves the generation of optimized code for quantities defined mathematically within a Computer algebra system (CAS). Compiler optimization consisting of finding common intermediates of a vector of size {\\displaystyle n} n requires a complexity of {\\displaystyle O(n^{2})} O(n^{2}) or {\\displaystyle O(n^{3})} O(n^{3}) operations whereas the very design of a computer algebra system requires only {\\displaystyle O(n)} O(n) operations.[6][7][8] These facilities can be used as pre-optimizer before processing by the compiler. This option has been used for handling mathematically large expressions in e.g. computational (quantum) chemistry.\n\nExamples:\n\nAcceleo is an open source code generator for Eclipse used to generate any textual language (Java, PHP, Python, etc.) from EMF models defined from any metamodel (UML, SysML, etc.).\nActifsource is a plugin for Eclipse that allows graphical modelling and model-based code generation using custom templates.\nAltova MapForce is a graphical data mapping, conversion, and integration tool capable of generating application code in Java, C#, or C++ for executing recurrent transformations.\nCodeFluent Entities from SoftFluent is a graphical tool integrated into Microsoft Visual Studio that generates .NET source code, in C# or Visual Basic.\nDMS Software Reengineering Toolkit is a system for defining arbitrary domain specific languages and translating them to other languages.\nHPRCARCHITECT (from MNB Technologies, Inc) is an artificial intelligence-based software development tool with a Virtual Whiteboard human interface. Language and technology agnostic, the tool's development was funded by the US Air Force to solve the problem of code generation for systems targeting mixed processor technologies.\nSpring Roo is an open source active code generator for Spring Framework based Java applications. It uses AspectJ mixins to provide separation of concerns during round-trip maintenance.\nRISE is a free information modeling suite for system development using ERD or UML. Database code generation for MySQL, PostgreSQL and Microsoft SQL Server. Persistence code generation for C# (.NET) and PHP including both SOAP and JSON style web services and AJAX proxy code.\nThe Maple computer algebra system offers code generators optimizers with Fortran, MATLAB, C and Java. Wolfram Language (Mathematica), and MuPAD have comparable interfaces.\nScreen Sculptor,[9] SoftCode,[10] UI Programmer,[11] and Genifer[12] are examples of pioneering program generators that arose during the mid-1980s through the early 1990s. They developed and advanced the technology of extendable, template based source code generators on a mass market scale.\nGeneXus is a Cross-Platform, knowledge representation-based, development tool, mainly oriented to enterprise-class applications for Web applications, smart devices and the Microsoft Windows platform. A developer describes an application in a high-level, mostly declarative language, from which native code is generated for multiple environments.\nBidji is an Apache Ant project for code generation and data transformation.\nSee also[edit]\n\nThis \"see also\" section may contain an excessive number of suggestions. Please ensure that only the most relevant links are given, that they are not red links, and that any links are not already in this article. (June 2013) (Learn how and when to remove this template message)\nComparison of code generation tools\nSource-to-source compiler\nModel Driven Engineering (MDE)\nModel Driven Architecture (MDA)\nDomain-Specific Modeling (DSM)\nFeature Oriented Programming\nProgram synthesis\nProgram transformation\nInductive programming\nModeling language\nData transformation\nSemantic translation\nVocabulary-based transformation\nMetaprogramming\nLanguage-oriented programming",
          "subparadigms": []
        },
        {
          "pdid": 44,
          "name": "Attribute-oriented",
          "details": "Attribute-oriented programming (AOP) is a program-level marking technique. Programmers can mark program elements (e.g. classes and methods) to indicate that they maintain application-specific or domain-specific semantics. For example, some programmers may define a \"logging\" attribute and associate it with a method to indicate the method should implement a logging function, while other programmers may define a \"web service\" attribute and associate it with a class to indicate the class should be implemented as a web service. Attributes separate application's core logic (or business logic) from application-specific or domain-specific semantics (e.g. logging and web service functions). By hiding the implementation details of those semantics from program code, attributes increase the level of programming abstraction and reduce programming complexity, resulting in simpler and more readable programs. The program elements associated with attributes are transformed to more detailed programs by a supporting tool (e.g. preprocessor). For example, a preprocessor may insert a logging program into the methods associated with a \"logging\" attribute.\n\nContents  [hide] \n1\tAttribute-oriented programming in various languages\n1.1\tJava\n1.2\tC#\n1.3\tUML\n2\tReferences\n3\tTools\n4\tExternal links\nAttribute-oriented programming in various languages[edit]\nJava[edit]\nWith the inclusion of The Metadata Facility for the Java Programming Language (JSR-175) into the J2SE 5.0 release it is possible to utilize attribute-oriented programming right out of the box. XDoclet library makes it possible to use attribute-oriented programming approach in earlier versions of Java.\n\nC#[edit]\nThe C# language has supported attributes from its very first release. However these attributes are used to give run-time information and are not used by a pre-processor (there isn't one in C#'s reference implementation).\n\nUML[edit]\nThe Unified Modeling Language (UML) supports a kind of attribute named Stereotypes.\n\nReferences[edit]\n\nThis article includes a list of references, related reading or external links, but its sources remain unclear because it lacks inline citations. Please help to improve this article by introducing more precise citations. (August 2009) (Learn how and when to remove this template message)\n\"Attribute-Oriented Programming\". An Introduction to Attribute-Oriented Programming. Archived from the original on May 26, 2005. Retrieved July 22, 2005.\nWada, Hiroshi; Suzuki, Junichi (2005). \"Modeling Turnpike Frontend System: a Model-Driven Development Framework Leveraging UML Metamodeling and Attribute-Oriented Programming\" (PDF). In Proc. of the 8th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MoDELS/UML 2005).\nRouvoy, Romain; Merle, Philippe (2006). \"Leveraging Component-Oriented Programming with Attribute-Oriented Programming\" (PDF). In Proc. of the 11th ECOOP International Workshop on Component-Oriented Programming (WCOP 2006). Archived from the original (PDF) on 2006-12-23.\nTools[edit]\nAnnotation Processing Tool (apt)\nSpoon, an Annotation-Driven Java Program Transformer\nXDoclet, a Javadoc-Driven Program Generator",
          "subparadigms": []
        },
        {
          "pdid": 45,
          "name": "Reflection",
          "details": "In computer science, reflection is the ability of a computer program to examine, introspect, and modify its own structure and behavior at runtime.[1]\n\nContents  [hide] \n1\tHistorical background\n2\tUses\n3\tImplementation\n4\tExamples\n4.1\teC\n4.2\tECMAScript\n4.3\tGo\n4.4\tJava\n4.5\tObjective-C\n4.6\tDelphi\n4.7\tPerl\n4.8\tPHP\n4.9\tPython\n4.10\tR\n4.11\tRuby\n5\tSee also\n6\tReferences\n7\tFurther reading\n8\tExternal links\nHistorical background[edit]\nThe earliest computers were programmed in their native assembly language, which were inherently reflective as these original architectures could be programmed by defining instructions as data and using self-modifying code. As programming moved to compiled higher-level languages such as Fortran, Algol and Cobol (but also Pascal and C and many other languages) this reflective ability largely disappeared until programming languages with reflection built into their type systems appeared.[citation needed]\n\nBrian Cantwell Smith's 1982 doctoral dissertation[2][3] introduced the notion of computational reflection in programming languages, and the notion of the meta-circular interpreter as a component of 3-Lisp.\n\nUses[edit]\nReflection can be used for observing and modifying program execution at runtime. A reflection-oriented program component can monitor the execution of an enclosure of code and can modify itself according to a desired goal related to that enclosure. This is typically accomplished by dynamically assigning program code at runtime.\n\nIn object-oriented programming languages such as Java, reflection allows inspection of classes, interfaces, fields and methods at runtime without knowing the names of the interfaces, fields, methods at compile time. It also allows instantiation of new objects and invocation of methods.\n\nReflection can be used to adapt a given program to different situations dynamically. Reflection-oriented programming almost always requires additional knowledge, framework, relational mapping, and object relevance in order to take advantage of more generic code execution.\n\nReflection is often used as part of software testing, such as for the runtime creation/instantiation of mock objects.\n\nReflection is also a key strategy for metaprogramming.\n\nIn some object-oriented programming languages, such as C# and Java, reflection can be used to override member accessibility rules. For example, reflection makes it possible to change the value of a field marked \"private\" in a third-party library's class.\n\nImplementation[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2008) (Learn how and when to remove this template message)\nA language supporting reflection provides a number of features available at runtime that would otherwise be difficult to accomplish in a lower-level language. Some of these features are the abilities to:\n\nDiscover and modify source code constructions (such as code blocks, classes, methods, protocols, etc.) as a first-class object at runtime.\nConvert a string matching the symbolic name of a class or function into a reference to or invocation of that class or function.\nEvaluate a string as if it were a source code statement at runtime.\nCreate a new interpreter for the language's bytecode to give a new meaning or purpose for a programming construct.\nThese features can be implemented in different ways. In MOO, reflection forms a natural part of everyday programming idiom. When verbs (methods) are called, various variables such as verb (the name of the verb being called) and this (the object on which the verb is called) are populated to give the context of the call. Security is typically managed by accessing the caller stack programmatically: Since callers() is a list of the methods by which the current verb was eventually called, performing tests on callers()[1] (the command invoked by the original user) allows the verb to protect itself against unauthorised use.\n\nCompiled languages rely on their runtime system to provide information about the source code. A compiled Objective-C executable, for example, records the names of all methods in a block of the executable, providing a table to correspond these with the underlying methods (or selectors for these methods) compiled into the program. In a compiled language that supports runtime creation of functions, such as Common Lisp, the runtime environment must include a compiler or an interpreter.\n\nReflection can be implemented for languages not having built-in reflection facilities by using a program transformation system to define automated source code changes.\n\nExamples[edit]\nThe following code snippets create an instance foo of class Foo, and invoke its method hello. For each programming language, normal and reflection-based call sequences are shown.\n\n\nIt has been suggested that this article be split into a new article titled Comparison of programming languages (reflection). (Discuss.) (May 2016)\neC[edit]\nThe following is an example in eC:\n\n// without reflection\nFoo foo { };\nfoo.hello();\n\n// with reflection\nClass fooClass = eSystem_FindClass(__thisModule, \"Foo\");\nInstance foo = eInstance_New(fooClass);\nMethod m = eClass_FindMethod(fooClass, \"hello\", fooClass.module);\n((void (*)())(void *)m.function)(foo);\nECMAScript[edit]\nThe following is an example in ECMAScript, and therefore also applies to JavaScript and ActionScript:\n\n// Without reflection\nnew Foo().hello()\n\n// With reflection\n\n// assuming that Foo resides in this\nnew this['Foo']()['hello']()\n\n// or without assumption\nnew (eval('Foo'))()['hello']()\n\n// or simply\neval('new Foo().hello()')\n\n// Using ECMAScript 2015's new Reflect class:\nReflect.construct(Foo, [])['hello']()\nGo[edit]\nThe following is an example in Go:\n\nimport \"reflect\"\n\n// without reflection\nf := Foo{}\nf.Hello()\n\n// with reflection\nfT := reflect.TypeOf(Foo{})\nfV := reflect.New(fT)\n\nm := fV.MethodByName(\"Hello\")\nif m.IsValid() {\n    m.Call(nil)\n}\nJava[edit]\nThe following is an example in Java:\n\n// without reflection\nFoo foo = new Foo();\nfoo.hello();\n\n// with reflection\nObject foo = Class.forName(\"complete.classpath.and.Foo\").newInstance();\n// Alternatively: Object foo = Foo.class.newInstance();\nMethod m = foo.getClass().getDeclaredMethod(\"hello\", new Class<?>[0]);\nm.invoke(foo);\nObjective-C[edit]\nThe following is an example in Objective-C—implying either the OpenStep or Foundation Kit framework is used:\n\n// Foo class.\n@interface Foo : NSObject\n- (void)hello;\n@end\n\n// Sending \"hello\" to a Foo instance without reflection.\nFoo *obj = [[Foo alloc] init];\n[obj hello];\n\n// Sending \"hello\" to a Foo instance with reflection.\nid obj = [[NSClassFromString(@\"Foo\") alloc] init];\n[obj performSelector: @selector(hello)];\nDelphi[edit]\nThis Delphi example assumes a TFoo class has been declared in a unit called Unit1:\n\nuses RTTI, Unit1;\n\nprocedure WithoutReflection;\nvar\n  Foo: TFoo;\nbegin\n  Foo := TFoo.Create;\n  try\n    Foo.Hello;\n  finally\n    Foo.Free;\n  end;\nend;\n\nprocedure WithReflection;\nvar\n  RttiContext: TRttiContext;\n  RttiType: TRttiInstanceType;\n  Foo: TObject;\nbegin\n  RttiType := RttiContext.FindType('Unit1.TFoo') as TRttiInstanceType;\n  Foo := RttiType.GetMethod('Create').Invoke(RttiType.MetaclassType, []).AsObject;\n  try\n    RttiType.GetMethod('Hello').Invoke(Foo, []);\n  finally\n    Foo.Free;\n  end;\nend;\nThis is a notable example since Delphi is an unmanaged, fully natively compiled language, unlike most other languages that support reflection. Its language architecture inherits from strongly-typed Pascal, but with significant influence from SmallTalk. Compare with the other examples here, many of which are dynamic or script languages like Perl, Python or PHP, or languages with a runtime like Java or C#.\n\nPerl[edit]\nThe following is an example in Perl:\n\n# without reflection\nmy $foo = Foo->new;\n$foo->hello;\n\n# or\nFoo->new->hello;\n\n# with reflection\nmy $class = \"Foo\"\nmy $constructor = \"new\";\nmy $method = \"hello\";\n\nmy $f = $class->$constructor;\n$f->$method;\n\n# or\n$class->$constructor->$method;\n\n# with eval\neval \"new Foo->hello;\";\nPHP[edit]\nThe following is an example in PHP:\n\n// without reflection\n$foo = new Foo();\n$foo->hello();\n\n// with reflection\n$reflector = new ReflectionClass('Foo');\n$foo = $reflector->newInstance();\n$hello = $reflector->getMethod('hello');\n$hello->invoke($foo);\n\n// using callback\n$foo = new Foo();\ncall_user_func(array($foo, 'hello'));\n\n// using variable variables syntax\n$className = 'Foo';\n$foo = new $className();\n$method = 'hello';\n$foo->$method();\nPython[edit]\nThe following is an example in Python:\n\n# without reflection\nobj = Foo()\nobj.hello()\n\n# with reflection\nclass_name = \"Foo\"\nmethod = \"hello\"\nobj = globals()[class_name]()\ngetattr(obj, method)()\n\n# with eval\neval(\"Foo().hello()\")\nR[edit]\nThe following is an example in R:\n\n# Without reflection, assuming foo() returns an S3-type object that has method \"hello\"\nobj <- foo()\nhello(obj)\n\n# With reflection\nthe.class <- \"foo\"\nthe.method <- \"hello\"\nobj <- do.call(the.class, list())\ndo.call(the.method, alist(obj))\nRuby[edit]\nThe following is an example in Ruby:\n\n# without reflection\nobj = Foo.new\nobj.hello\n\n# with reflection\nclass_name = \"Foo\"\nmethod_name = :hello\nobj = Object.const_get(class_name).new\nobj.send method_name\n\n# with eval\neval \"Foo.new.hello\"\nSee also[edit]\nType introspection\nSelf-modifying code\nSelf-hosting\nProgramming paradigms\nList of reflective programming languages and platforms\nMirror (programming)",
          "subparadigms": [
            44
          ]
        },
        {
          "pdid": 46,
          "name": "Homoiconic",
          "details": "In computer programming, homoiconicity (from the Greek words homo meaning the same and icon meaning representation) is a property of some programming languages in which the program structure is similar to its syntax, and therefore the program's internal representation can be inferred by reading the text's layout.[1] If a language is homoiconic, it means that the language text has the same structure as its abstract syntax tree (i.e. the AST and the syntax are isomorphic). This allows all code in the language to be accessed and transformed as data, using the same representation.\n\nIn a homoiconic language the primary representation of programs is also a data structure in a primitive type of the language itself. This makes metaprogramming easier than in a language without this property, since code can be treated as data: reflection in the language (examining the program's entities at runtime) depends on a single, homogeneous structure, and it does not have to handle several different structures that would appear in a complex syntax. To put that another way, homoiconicity is where a program's source code is written as a basic data structure that the programming language knows how to access.\n\nA typical, commonly cited example is the programming language Lisp, which was created to be easy for lists manipulation and where the structure is given by S-expressions that take the form of nested lists. Lisp programs are written in the form of lists; the result is that the program can access its own functions and procedures while running, and programmatically reprogram itself on the fly. Homoiconic languages typically include full support of syntactic macros allowing the programmer to express program transformations in a concise way. Examples are the programming languages Clojure (a contemporary dialect of Lisp), Rebol and Refal.\n\nContents  [hide] \n1\tHistory\n2\tUses, advantages, and disadvantages\n3\tExamples\n3.1\tHomoiconicity in Lisp\n3.2\tHomoiconicity in Prolog\n3.3\tHomoiconicity in Rebol\n4\tSee also\n5\tReferences\n6\tExternal links\nHistory[edit]\nThe original source is the paper Macro Instruction Extensions of Compiler Languages,[2] according to the early and influential paper TRAC, A Text-Handling Language:[3]\n\nOne of the main design goals was that the input script of TRAC (what is typed in by the user) should be identical to the text which guides the internal action of the TRAC processor. In other words, TRAC procedures should be stored in memory as a string of characters exactly as the user typed them at the keyboard. If the TRAC procedures themselves evolve new procedures, these new procedures should also be stated in the same script. The TRAC processor in its action interprets this script as its program. In other words, the TRAC translator program (the processor) effectively converts the computer into a new computer with a new program language -- the TRAC language. At any time, it should be possible to display program or procedural information in the same form as the TRAC processor will act upon it during its execution. It is desirable that the internal character code representation be identical to, or very similar to, the external code representation. In the present TRAC implementation, the internal character representation is based upon ASCII. Because TRAC procedures and text have the same representation inside and outside the processor, the term homoiconic is applicable, from homo meaning the same, and icon meaning representation.\n\n[...]\n\nFollowing suggestion of McCullough, W. S., based upon terminology due to Peirce, C. S. s McIlroy. M. D., \"Macro Instruction Extensions of Compiler Languages,\" Comm. ACM, p. 214-220; April, 1960.\n\nAlan Kay used and possibly popularized the term \"homoiconic\" through his use of the term in his 1969 PhD thesis:[4]\n\nA notable group of exceptions to all the previous systems are Interactive LISP [...] and TRAC. Both are functionally oriented (one list, the other string), both talk to the user with one language, and both are \"homoiconic\" in that their internal and external representations are essentially the same. They both have the ability to dynamically create new functions which may then be elaborated at the users's pleasure.\n\nTheir only great drawback is that programs written in them look like King Burniburiach's letter to the Sumerians done in Babylonian cuniform! [...]\n\nUses, advantages, and disadvantages[edit]\nOne advantage of homoiconicity is that extending the language with new concepts typically becomes simpler, as data representing code can be passed between the meta and base layer of the program. The abstract syntax tree of a function may be composed and manipulated as a data structure in the meta layer, and then evaluated. It can be much easier to understand how to manipulate the code since it can be more easily understood as simple data (since the format of the language itself is as a data format).\n\nThe simplicity that allows this also presents a disadvantage: one blogger argues that, at least in the case of LISP-like list-oriented languages, it can do away with many of the visual cues that help humans visually parse the constructs of the language, and that this can lead to a steep learning curve for the language.[5] See also essay \"The Lisp Curse\"[6] for disadvantages.\n\nA typical demonstration of homoiconicity is the meta-circular evaluator.\n\nExamples[edit]\nLanguages which are considered to be homoiconic include\n\nCurl[7]\nIo[7]\nIoke\nJulia[1][2][7]\nLisp and its dialects,[7] such as Scheme,[5] Clojure[3], Racket[4]\nMathematica[5]\nProlog[7][8]\nRebol[7]\nRed\nSNOBOL[7]\nTcl[5][7]\nXSLT[9]\nREFAL[7]\nWolfram Language[10]\nIn Von Neumann architecture systems (including the vast majority of general purpose computers today), raw machine code also has this property, the data type being bytes in memory.\n\nHomoiconicity in Lisp[edit]\nLisp uses S-expressions as an external representation for data and code. S-expressions can be read with the primitive Lisp function READ. READ returns Lisp data: lists, symbols, numbers, strings. The primitive Lisp function EVAL uses Lisp code represented as Lisp data, computes side-effects and returns a result. The result will be printed by the primitive function PRINT, which creates an external S-expression from Lisp data.\n\nLisp data, a list using different data types: (sub)lists, symbols, strings and integer numbers.\n\n((:name \"john\" :age 20) (:name \"mary\" :age 18) (:name \"alice\" :age 22))\nLisp code. The example uses lists, symbols and numbers.\n\n(* (sin 1.1) (cos 2.03))      ; in infix: sin(1.1)*cos(2.03)\nCreate above expression with the primitive Lisp function LIST and set the variable EXPRESSION to the result\n\n(setf expression  (list '* (list 'sin 1.1) (list 'cos 2.03)) )  \n-> (* (SIN 1.1) (COS 2.03))    ; Lisp returns and prints the result\n\n(third expression)    ; the third element of the expression\n-> (COS 2.03)\nChange the COS term to SIN\n\n(setf (first (third expression)) 'SIN)\n; The expression is now (* (SIN 1.1) (SIN 2.03)).\nEvaluate the expression\n\n(eval expression)\n-> 0.7988834\nPrint the expression to a string\n\n(print-to-string expression)\n->  \"(* (SIN 1.1) (SIN 2.03))\"\nRead the expression from a string\n\n(read-from-string \"(* (SIN 1.1) (SIN 2.03))\")\n->  (* (SIN 1.1) (SIN 2.03))     ; returns a list of lists, numbers and symbols\nHomoiconicity in Prolog[edit]\n1 ?- X is 2*5.\nX = 10.\n\n2 ?- L = (X is 2*5), write_canonical(L).\nis(_, *(2, 5))\nL = (X is 2*5).\n\n3 ?- L = (ten(X):-(X is 2*5)), write_canonical(L).\n:-(ten(A), is(A, *(2, 5)))\nL = (ten(X):-X is 2*5).\n\n4 ?- L = (ten(X):-(X is 2*5)), assert(L).\nL = (ten(X):-X is 2*5).\n\n5 ?- ten(X).\nX = 10.\n\n6 ?-\nOn line 4 we create a new clause. The operator \":-\" separates the head and the body of a clause. With assert/1* we add it to the existing clauses(add it to the \"database\"), so we can call it later. In other languages we would call it \"creating a function during runtime\". We can also remove clauses from the database with abolish/1, or retract/1.\n\n* The number after the clause's name is the number of arguments it can take.(It is also called arity.)\n\nWe can also query the database to get the body of a clause:\n\n7 ?- clause(ten(X),Y).\nY = (X is 2*5).\n\n8 ?- clause(ten(X),Y), Y = (X is Z).\nY = (X is 2*5),\nZ = 2*5.\n\n9 ?- clause(ten(X),Y), call(Y).\nX = 10,\nY = (10 is 2*5).\n\"call\" is analogous to Lisp's \"eval\" function.\n\nHomoiconicity in Rebol[edit]\nThe concept of treating code as data and the manipulation and evaluation thereof can be demonstrated very neatly in Rebol. (Rebol, unlike Lisp, does not require parentheses to separate expressions).\n\nThe following is an example of code in Rebol (Note that '>>' represents the interpreter prompt; spaces between some elements have been added for readability):\n\n>> repeat i 3 [ print [ i \"hello\" ] ]\n\n1 hello\n2 hello\n3 hello\n(repeat is in fact a built-in function in Rebol and is not a language construct or keyword).\n\nBy enclosing the code in square brackets, the interpreter does not evaluate it, but merely treats it as a block containing words:\n\n[ repeat i 3 [ print [ i \"hello\" ] ] ]\nThis block has the type block! and can furthermore be assigned as the value of a word by using what appears to be a syntax for assignment, but is actually understood by the interpreter as a special type (set-word!) and takes the form of a word followed by a colon:\n\n>> block1: [ repeat i 3 [ print [ i \"hello\" ] ] ] ;; Assign the value of the block to the word `block1`\n== [repeat i 3 [print [i \"hello\"]]]\n>> type? block1 ;; Evaluate the type of the word `block1`\n== block!\nThe block can still be interpreted by using the do function provided in Rebol (similar to \"eval\" in Lisp).\n\nIt is possible to interrogate the elements of the block and change their values, thus altering the behavior of the code if it were to be evaluated:\n\n>> block1/3 ;; The third element of the block\n== 3\n>> block1/3: 5 ;; Set the value of the 3rd element to 5\n== 5\n>> probe block1 ;; Show the changed block\n[repeat i 5 [print [i \"hello\"]]]\n== [repeat i 5 [print [i \"hello\"]]]\n>> do block1 ;; Evaluate the block\n1 hello\n2 hello\n3 hello\n4 hello\n5 hello\nSee also[edit]\nConcatenative programming language\nCognitive dimensions of notations, design principles for programming languages' syntax\nLanguage-oriented programming",
          "subparadigms": []
        },
        {
          "pdid": 47,
          "name": "Template",
          "details": "Template metaprogramming (TMP) is a metaprogramming technique in which templates are used by a compiler to generate temporary source code, which is merged by the compiler with the rest of the source code and then compiled. The output of these templates include compile-time constants, data structures, and complete functions. The use of templates can be thought of as compile-time execution. The technique is used by a number of languages, the best-known being C++, but also Curl, D, and XL.\n\nTemplate metaprogramming was, in a sense, discovered accidentally.[1][better source needed]\n\nSome other languages support similar, if not more powerful compile-time facilities (such as Lisp macros), but those are outside the scope of this article.\n\nContents  [hide] \n1\tComponents of template metaprogramming\n1.1\tUsing template metaprogramming\n2\tCompile-time class generation\n3\tCompile-time code optimization\n4\tStatic polymorphism\n5\tBenefits and drawbacks of template metaprogramming\n6\tSee also\n7\tReferences\n8\tExternal links\nComponents of template metaprogramming[edit]\nThe use of templates as a metaprogramming technique requires two distinct operations: a template must be defined, and a defined template must be instantiated. The template definition describes the generic form of the generated source code, and the instantiation causes a specific set of source code to be generated from the generic form in the template.\n\nTemplate metaprogramming is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram.[2]\n\nTemplates are different from macros. A macro, which is also a compile-time language feature, generates code in-line using text manipulation and substitution. Macro systems often have limited compile-time process flow abilities and usually lack awareness of the semantics and type system of their companion language (an exception should be made with Lisp's macros, which are written in Lisp itself and involve manipulation and substitution of Lisp code represented as data structures as opposed to text).\n\nTemplate metaprograms have no mutable variables— that is, no variable can change value once it has been initialized, therefore template metaprogramming can be seen as a form of functional programming. In fact many template implementations implement flow control only through recursion, as seen in the example below.\n\nUsing template metaprogramming[edit]\nThough the syntax of template metaprogramming is usually very different from the programming language it is used with, it has practical uses. Some common reasons to use templates are to implement generic programming (avoiding sections of code which are similar except for some minor variations) or to perform automatic compile-time optimization such as doing something once at compile time rather than every time the program is run — for instance, by having the compiler unroll loops to eliminate jumps and loop count decrements whenever the program is executed.\n\nCompile-time class generation[edit]\nWhat exactly \"programming at compile-time\" means can be illustrated with an example of a factorial function, which in non-template C++ can be written using recursion as follows:\n\nunsigned int factorial(unsigned int n) {\n\treturn n == 0 ? 1 : n * factorial(n - 1); \n}\n\n// Usage examples:\n// factorial(0) would yield 1;\n// factorial(4) would yield 24.\nThe code above will execute at run time to determine the factorial value of the literals 4 and 0. By using template metaprogramming and template specialization to provide the ending condition for the recursion, the factorials used in the program—ignoring any factorial not used—can be calculated at compile time by this code:\n\ntemplate <unsigned int n>\nstruct factorial {\n\tenum { value = n * factorial<n - 1>::value };\n};\n\ntemplate <>\nstruct factorial<0> {\n\tenum { value = 1 };\n};\n\n// Usage examples:\n// factorial<0>::value would yield 1;\n// factorial<4>::value would yield 24.\nThe code above calculates the factorial value of the literals 4 and 0 at compile time and uses the result as if they were precalculated constants. To be able to use templates in this manner, the compiler must know the value of its parameters at compile time, which has the natural precondition that factorial<X>::value can only be used if X is known at compile time. In other words, X must be a constant literal or a constant expression.\n\nIn C++11, constexpr, a way to let the compiler execute simple constant expressions, was added. Using constexpr, one can use the usual recursive factorial definition.[3]\n\nCompile-time code optimization[edit]\nSee also: Compile time function execution\nThe factorial example above is one example of compile-time code optimization in that all factorials used by the program are pre-compiled and injected as numeric constants at compilation, saving both run-time overhead and memory footprint. It is, however, a relatively minor optimization.\n\nAs another, more significant, example of compile-time loop unrolling, template metaprogramming can be used to create length-n vector classes (where n is known at compile time). The benefit over a more traditional length-n vector is that the loops can be unrolled, resulting in very optimized code. As an example, consider the addition operator. A length-n vector addition might be written as\n\ntemplate <int length>\nVector<length>& Vector<length>::operator+=(const Vector<length>& rhs) \n{\n    for (int i = 0; i < length; ++i)\n        value[i] += rhs.value[i];\n    return *this;\n}\nWhen the compiler instantiates the function template defined above, the following code may be produced:[citation needed]\n\ntemplate <>\nVector<2>& Vector<2>::operator+=(const Vector<2>& rhs) \n{\n    value[0] += rhs.value[0];\n    value[1] += rhs.value[1];\n    return *this;\n}\nThe compiler's optimizer should be able to unroll the for loop because the template parameter length is a constant at compile time.\n\nHowever, take caution as this may cause code bloat as separate unrolled code will be generated for each 'N'(vector size) you instantiate with.\n\nStatic polymorphism[edit]\nPolymorphism is a common standard programming facility where derived objects can be used as instances of their base object but where the derived objects' methods will be invoked, as in this code\n\nclass Base\n{\npublic:\n    virtual void method() { std::cout << \"Base\"; }\n    virtual ~Base() {}\n};\n\nclass Derived : public Base\n{\npublic:\n    virtual void method() { std::cout << \"Derived\"; }\n};\n\nint main()\n{\n    Base *pBase = new Derived;\n    pBase->method(); //outputs \"Derived\"\n    delete pBase;\n    return 0;\n}\nwhere all invocations of virtual methods will be those of the most-derived class. This dynamically polymorphic behaviour is (typically) obtained by the creation of virtual look-up tables for classes with virtual methods, tables that are traversed at run time to identify the method to be invoked. Thus, run-time polymorphism necessarily entails execution overhead (though on modern architectures the overhead is small).\n\nHowever, in many cases the polymorphic behaviour needed is invariant and can be determined at compile time. Then the Curiously Recurring Template Pattern (CRTP) can be used to achieve static polymorphism, which is an imitation of polymorphism in programming code but which is resolved at compile time and thus does away with run-time virtual-table lookups. For example:\n\ntemplate <class Derived>\nstruct base\n{\n    void interface()\n    {\n         // ...\n         static_cast<Derived*>(this)->implementation();\n         // ...\n    }\n};\n\nstruct derived : base<derived>\n{\n     void implementation()\n     {\n         // ...\n     }\n};\nHere the base class template will take advantage of the fact that member function bodies are not instantiated until after their declarations, and it will use members of the derived class within its own member functions, via the use of a static_cast, thus at compilation generating an object composition with polymorphic characteristics. As an example of real-world usage, the CRTP is used in the Boost iterator library.[4]\n\nAnother similar use is the \"Barton–Nackman trick\", sometimes referred to as \"restricted template expansion\", where common functionality can be placed in a base class that is used not as a contract but as a necessary component to enforce conformant behaviour while minimising code redundancy.\n\nBenefits and drawbacks of template metaprogramming[edit]\nCompile-time versus execution-time tradeoff \nIf a great deal of template metaprogramming is used, compilation may become slow; section 14.7.1 [temp.inst] of the current standard defines the circumstances under which templates are implicitly instantiated. Defining a template does not imply that it will be instantiated, and instantiating a class template does not cause its member definitions to be instantiated. Depending on the style of use, templates may compile either faster or slower than hand-rolled code.\nGeneric programming \nTemplate metaprogramming allows the programmer to focus on architecture and delegate to the compiler the generation of any implementation required by client code. Thus, template metaprogramming can accomplish truly generic code, facilitating code minimization and better maintainability[citation needed].\nReadability \nWith respect to C++, the syntax and idioms of template metaprogramming are esoteric compared to conventional C++ programming, and template metaprograms can be very difficult to understand. [5][6]\nSee also[edit]\nSubstitution failure is not an error (SFINAE)\nMetaprogramming\nPreprocessor\nParametric polymorphism\nExpression templates\nVariadic Templates\nCompile time function execution",
          "subparadigms": []
        },
        {
          "pdid": 48,
          "name": "Policy-based",
          "details": "Policy-based design, also known as policy-based class design or policy-based programming, is a computer programming paradigm based on an idiom for C++ known as policies. It has been described as a compile-time variant of the strategy pattern, and has connections with C++ template metaprogramming. It was first popularized by Andrei Alexandrescu with his 2001 book Modern C++ Design and his column Generic<Programming> in the C/C++ Users Journal.\n\nAlthough the technique could theoretically be applied to other languages, it is currently closely associated with C++, and depends on the particular feature set of that language. Furthermore, even in C++ it requires a compiler with highly robust support for templates, which wasn't common before about 2003.\n\nContents  [hide] \n1\tOverview\n2\tSimple example\n3\tSee also\n4\tExternal links\nOverview[edit]\nThe central idiom in policy-based design is a class template (called the host class), taking several type parameters as input, which are instantiated with types selected by the user (called policy classes), each implementing a particular implicit interface (called a policy), and encapsulating some orthogonal (or mostly orthogonal) aspect of the behavior of the instantiated host class. By supplying a host class combined with a set of different, canned implementations for each policy, a library or module can support an exponential number of different behavior combinations, resolved at compile time, and selected by mixing and matching the different supplied policy classes in the instantiation of the host class template. Additionally, by writing a custom implementation of a given policy, a policy-based library can be used in situations requiring behaviors unforeseen by the library implementor. Even in cases where no more than one implementation of each policy will ever be used, decomposing a class into policies can aid the design process, by increasing modularity and highlighting exactly where orthogonal design decisions have been made.\n\nWhile assembling software components out of interchangeable modules is a far from new concept, policy-based design represents an innovation in the way it applies that concept at the (relatively low) level of defining the behavior of an individual class. Policy classes have some similarity to callbacks, but differ in that, rather than consisting of a single function, a policy class will typically contain several related functions (methods), often combined with state variables or other facilities such as nested types. A policy-based host class can be thought of as a type of metafunction, taking a set of behaviors represented by types as input, and returning as output a type representing the result of combining those behaviors into a functioning whole. (Unlike MPL metafunctions, however, the output is usually represented by the instantiated host class itself, rather than a nested output type.)\n\nA key feature of the policy idiom is that, usually (though it is not strictly necessary), the host class will derive from (make itself a child class of) each of its policy classes using (public) multiple inheritance. (Alternatives are for the host class to merely contain a member variable of each policy class type, or else to inherit the policy classes privately; however inheriting the policy classes publicly has the major advantage that a policy class can add new methods, inherited by the instantiated host class and accessible to its users, which the host class itself need not even know about.) A notable feature of this aspect of the policy idiom is that, relative to object-oriented programming, policies invert the relationship between base class and derived class - whereas in OOP interfaces are traditionally represented by (abstract) base classes and implementations of interfaces by derived classes, in policy-based design the derived (host) class represents the interfaces and the base (policy) classes implement them. It should also be noted that in the case of policies, the public inheritance does not represent an is-a relationship between the host and the policy classes. While this would traditionally be considered evidence of a design defect in OOP contexts, this doesn't apply in the context of the policy idiom.\n\nA disadvantage of policies in their current incarnation is that the policy interface doesn't have a direct, explicit representation in code, but rather is defined implicitly, via duck typing, and must be documented separately and manually, in comments. The main idea is to use commonality-variability analysis to divide the type into the fixed implementation and interface, the policy-based class, and the different policies. The trick is to know what goes into the main class, and what policies should one create. The article mentioned above gives the following answer: wherever we would need to make a possible limiting design decision, we should postpone that decision, we should delegate it to an appropriately named policy.\n\nPolicy classes can contain implementation, type definitions and so forth. Basically, the designer of the main template class will define what the policy classes should provide, what customization points they need to implement.\n\nIt may be a delicate task to create a good set of policies, just the right number (e.g., the minimum necessary). The different customization points, which belong together, should go into one policy argument, such as storage policy, validation policy and so forth. Graphic designers are able to give a name to their policies, which represent concepts, and not those which represent operations or minor implementation details.\n\nPolicy-based design may incorporate other techniques which will be useful, even if changed. One example is that the template method pattern can be reinterpreted for compile time; so that a main class has a skeleton algorithm, which — at customization points — calls the appropriate functions of some of the policies. Designers can also find themselves in using their policy classes as traits are used, asking type information, delegating type related tasks to it, a storage policy is one example where it can happen.[needs copy edit]\n\nSimple example[edit]\nPresented below is a simple (contrived) example of a C++ hello world program, where the text to be printed and the method of printing it are decomposed using policies. In this example, HelloWorld is a host class where it takes two policies, one for specifying how a message should be shown and the other for the actual message being printed. Note that the generic implementation is in run() and therefore the code is unable to be compiled unless both policies (print and message) are provided.\n\n#include <iostream>\n#include <string>\n \ntemplate <typename OutputPolicy, typename LanguagePolicy>\nclass HelloWorld : private OutputPolicy, private LanguagePolicy\n{\n    using OutputPolicy::print;\n    using LanguagePolicy::message;\n \npublic:\n    // Behaviour method\n    void run() const\n    {\n        // Two policy methods\n        print(message());\n    }\n};\n \nclass OutputPolicyWriteToCout\n{\nprotected:\n    template<typename MessageType>\n    void print(MessageType const &message) const\n    {\n        std::cout << message << std::endl;\n    }\n};\n \nclass LanguagePolicyEnglish\n{\nprotected:\n    std::string message() const\n    {\n        return \"Hello, World!\";\n    }\n};\n \nclass LanguagePolicyGerman\n{\nprotected:\n    std::string message() const\n    {\n        return \"Hallo Welt!\";\n    }\n};\n \nint main()\n{\n    /* Example 1 */\n    typedef HelloWorld<OutputPolicyWriteToCout, LanguagePolicyEnglish> HelloWorldEnglish;\n \n    HelloWorldEnglish hello_world;\n    hello_world.run(); // prints \"Hello, World!\"\n \n    /* Example 2 \n     * Does the same, but uses another language policy */\n    typedef HelloWorld<OutputPolicyWriteToCout, LanguagePolicyGerman> HelloWorldGerman;\n \n    HelloWorldGerman hello_world2;\n    hello_world2.run(); // prints \"Hallo Welt!\"\n}\nDesigners can easily write more OutputPolicies by adding new classes with the member function print() and take those as new OutputPolicies.",
          "subparadigms": [
            47
          ]
        },
        {
          "pdid": 49,
          "name": "Array",
          "details": "In computer science, array programming languages (also known as vector or multidimensional languages) generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.\n\nArray programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon to find array programming language one-liners that require more than a couple of pages of Java code.[1]\n\nModern programming languages that support array programming are commonly used in scientific and engineering settings; these include Fortran 90, Mata, MATLAB, Analytica, TK Solver (as lists), Octave, R, Cilk Plus, Julia, and the NumPy extension to Python. In these languages, an operation that operates on entire arrays can be called a vectorized operation,[2] regardless of whether it is executed on a vector processor or not.\n\nContents  [hide] \n1\tConcepts\n2\tUses\n3\tLanguages\n3.1\tScalar languages\n3.2\tArray languages\n3.2.1\tAda\n3.2.2\tAnalytica\n3.2.3\tBASIC\n3.2.4\tMata\n3.2.5\tMATLAB\n3.2.6\trasql\n3.2.7\tR\n4\tMathematical reasoning and language notation\n5\tThird-party libraries\n6\tSee also\n7\tReferences\n8\tExternal links\nConcepts[edit]\nThe fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\n\nIverson described the rationale behind array programming (actually referring to APL) as follows:[3]\n\nmost programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician. [...]\n\nThe thesis [...] is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. [...] it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\n[...] Users of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted, since a clear statement of an algorithm can usually be used as a basis from which one may easily derive more efficient algorithm.\n\nThe basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\n\nFunction rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.\n\nUses[edit]\nArray programming is very well suited to implicit parallelization; a topic of much research nowadays. Further, Intel and compatible CPUs developed and produced after 1997 contained various instruction set extensions, starting from MMX and continuing through SSSE3 and 3DNow!, which include rudimentary SIMD array capabilities. Array processing is distinct from parallel processing in that one physical processor performs operations on a group of items simultaneously while parallel processing aims to split a larger problem into smaller ones (MIMD) to be solved piecemeal by numerous processors. Processors with two or more cores are increasingly common today.\n\nLanguages[edit]\nThe canonical examples of array programming languages are APL, J, and Fortran. Others include: D, A+, Analytica, Chapel, IDL, Julia, K, Q, Mata, Mathematica, MATLAB, MOLSF, NumPy, GNU Octave, PDL, R, S-Lang, SAC, Nial and ZPL.\n\nScalar languages[edit]\nIn scalar languages such as C and Pascal, operations apply only to single values, so a+b expresses the addition of two numbers. In such languages, adding one array to another requires indexing and looping, the coding of which is tedious and error-prone[citation needed].\n\nfor (i = 0; i < n; i++)\n    for (j = 0; j < n; j++)\n        a[i][j] += b[i][j];\nArray languages[edit]\nIn array languages, operations are generalized to apply to both scalars and arrays. Thus, a+b expresses the sum of two scalars if a and b are scalars, or the sum of two arrays if they are arrays.\n\nAn array language simplifies programming but possibly at a cost known as the abstraction penalty.[4][5][6] Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).\n\nAda[edit]\nThe previous C code would become the following in the Ada language,[7] which supports array-programming syntax.\n\n A := A + B;\nAnalytica[edit]\nAnalytica provides the same economy of expression as Ada.\n\n A := A + B;\nThis operation works whether operands, A or B, are scalar or arrays with one more dimensions. Each dimension is identified by an index variable, which controls the nature of the operation. The result has the union of the dimensions of the operands. If A and B have the same dimensions (indexes), the result has those same dimensions. If A and B are vectors with different dimensions, the resulting A is 2-dimensional, containing both dimensions, with each element the sum of the corresponding values of A and B. Variable A must be a local variable; Analytica, as a declarative language, avoids side effects by disallowing assignment to global variables.\n\nBASIC[edit]\nDartmouth BASIC had MAT statements for matrix and array manipulation in its third edition (1966).\n\n DIM A(4),B(4),C(4)\n MAT A = 1\n MAT B = 2*A\n MAT C = A + B\n MAT PRINT A,B,C\nMata[edit]\nStata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.\n\n. mata:\n\n: A = (1,2,3) \\(4,5,6)\n\n: A\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n: B = (2..4) \\(1..3)\n\n: B\n       1   2   3\n    +-------------+\n  1 |  2   3   4  |\n  2 |  1   2   3  |\n    +-------------+\n\n: C = J(3,2,1)           // A 3 by 2 matrix of ones\n\n: C\n       1   2\n    +---------+\n  1 |  1   1  |\n  2 |  1   1  |\n  3 |  1   1  |\n    +---------+\n\n: D = A + B\n\n: D\n       1   2   3\n    +-------------+\n  1 |  3   5   7  |\n  2 |  5   7   9  |\n    +-------------+\n\n: E = A*C\n\n: E\n        1    2\n    +-----------+\n  1 |   6    6  |\n  2 |  15   15  |\n    +-----------+\n\n: F = A:*B\n\n: F\n        1    2    3\n    +----------------+\n  1 |   2    6   12  |\n  2 |   4   10   18  |\n    +----------------+\n\n: G = E :+ 3\n\n: G\n        1    2\n    +-----------+\n  1 |   9    9  |\n  2 |  18   18  |\n    +-----------+\n\n: H = F[(2\\1), (1, 2)]    // Subscripting to get a submatrix of F and\n\n:                         // switch row 1 and 2\n: H\n        1    2\n    +-----------+\n  1 |   4   10  |\n  2 |   2    6  |\n    +-----------+\n\n: I = invsym(F'*F)        // Generalized inverse (F*F^(-1)F=F) of a\n\n:                         // symmetric positive semi-definite matrix\n: I\n[symmetric]\n                 1             2             3\n    +-------------------------------------------+\n  1 |            0                              |\n  2 |            0          3.25                |\n  3 |            0         -1.75   .9444444444  |\n    +-------------------------------------------+\n\n: end\nMATLAB[edit]\nThe implementation in MATLAB allows the same economy allowed by using the Ada language.\n\nA = A + B;\nA variant of the MATLAB language is the GNU Octave language, which extends the original language with augmented assignments:\n\nA += B;\nBoth MATLAB and GNU Octave natively support linear algebra operations such as matrix multiplication, matrix inversion, and the numerical solution of system of linear equations, even using the Moore–Penrose pseudoinverse.[8][9]\n\nThe Nial example of the inner product of two arrays can be implemented using the native matrix multiplication operator. If a is a row vector of size [1 n] and b is a corresponding column vector of size [n 1].\n\na * b;\nThe inner product between two matrices having the same number of elements can be implemented with the auxiliary operator (:), which reshapes a given matrix into a column vector, and the transpose operator ':\n\nA(:)' * B(:);\nrasql[edit]\nThe rasdaman query language is a database-oriented array-programming language. For example, two arrays could be added with the following query:\n\nSELECT A + B\nFROM   A, B\nR[edit]\nThe R language supports array paradigm by default. The following example illustrates a process of multiplication of two matrices followed by an addition of a scalar (which is, in fact, a one-element vector) and a vector:\n\n> A <- matrix(1:6, nrow=2)                              !!this has nrow=2 ... and A has 2 rows\n> A\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n> B <- t( matrix(6:1, nrow=2) )  # t() is a transpose operator                           !!this has nrow=2 ... and B has 3 rows --- a clear contradiction to the definition of A\n> B\n     [,1] [,2]\n[1,]    6    5\n[2,]    4    3\n[3,]    2    1\n> C <- A %*% B\n> C\n     [,1] [,2]\n[1,]   28   19\n[2,]   40   28\n> D <- C + 1\n> D\n     [,1] [,2]\n[1,]   29   20\n[2,]   41   29\n> D + c(1, 1)  # c() creates a vector\n     [,1] [,2]\n[1,]   30   21\n[2,]   42   30\nMathematical reasoning and language notation[edit]\nThe matrix left-division operator concisely expresses some semantic properties of matrices. As in the scalar equivalent, if the (determinant of the) coefficient (matrix) A is not null then it is possible to solve the (vectorial) equation A * x = b by left-multiplying both sides by the inverse of A: A−1 (in both MATLAB and GNU Octave languages: A^-1). The following mathematical statements hold when A is a full rank square matrix:\n\nA^-1 *(A * x)==A^-1 * (b)\n(A^-1 * A)* x ==A^-1 * b       (matrix-multiplication associativity)\nx = A^-1 * b\nwhere == is the equivalence relational operator. The previous statements are also valid MATLAB expressions if the third one is executed before the others (numerical comparisons may be false because of round-off errors).\n\nIf the system is overdetermined - so that A has more rows than columns - the pseudoinverse A+ (in MATLAB and GNU Octave languages: pinv(A)) can replace the inverse A−1, as follows:\n\npinv(A) *(A * x)==pinv(A) * (b)\n(pinv(A) * A)* x ==pinv(A) * b       (matrix-multiplication associativity)\nx = pinv(A) * b\nHowever, these solutions are neither the most concise ones (e.g. still remains the need to notationally differentiate overdetermined systems) nor the most computationally efficient. The latter point is easy to understand when considering again the scalar equivalent a * x = b, for which the solution x = a^-1 * b would require two operations instead of the more efficient x = b / a. The problem is that generally matrix multiplications are not commutative as the extension of the scalar solution to the matrix case would require:\n\n(a * x)/ a ==b / a\n(x * a)/ a ==b / a       (commutativity does not hold for matrices!)\nx * (a / a)==b / a       (associativity also holds for matrices)\nx = b / a\nThe MATLAB language introduces the left-division operator \\ to maintain the essential part of the analogy with the scalar case, therefore simplifying the mathematical reasoning and preserving the conciseness:\n\nA \\ (A * x)==A \\ b\n(A \\ A)* x ==A \\ b       (associativity also holds for matrices, commutativity is no more required)\nx = A \\ b\nThis is not only an example of terse array programming from the coding point of view but also from the computational efficiency perspective, which in several array programming languages benefits from quite efficient linear algebra libraries such as ATLAS or LAPACK.[10][11]\n\nReturning to the previous quotation of Iverson, the rationale behind it should now be evident:\n\nit is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\n\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n\nThird-party libraries[edit]\nThe use of specialized and efficient libraries to provide more terse abstractions is also common in other programming languages. In C++ several linear algebra libraries exploit the language ability to overload operators. In some cases a very terse abstraction in those languages is explicitly influenced by the array programming paradigm, as the Armadillo and Blitz++ libraries do.[12][13]\n\nSee also[edit]\nArray slicing\nList of array programming languages",
          "subparadigms": []
        },
        {
          "pdid": 50,
          "name": "Non-structured",
          "details": "Non-structured programming is the historically earliest programming paradigm capable of creating Turing-complete algorithms. It is often contrasted with structured programming paradigms, including procedural, functional, and object-oriented programming.\n\nUnstructured programming has been heavily criticized for producing hardly-readable (\"spaghetti\") code and is sometimes considered a bad approach for creating major projects, but had been praised for the freedom it offers to programmers and has been compared to how Mozart wrote music.[1]\n\nThere are both high- and low-level programming languages that use non-structured programming. Some languages commonly cited as being non-structured include JOSS, FOCAL, TELCOMP, assembly languages, MS-DOS batch files, and early versions of BASIC, Fortran, COBOL, and MUMPS.\n\nContents  [hide] \n1\tFeatures and typical concepts\n1.1\tBasic concepts\n1.2\tData types\n2\tReferences\n3\tFurther reading\n4\tExternal links\nFeatures and typical concepts[edit]\nBasic concepts[edit]\nA program in a non-structured language usually consists of sequentially ordered commands, or statements, usually one in each line. The lines are usually numbered or may have labels: this allows the flow of execution to jump to any line in the program.\n\nNon-structured programming introduces basic control flow concepts such as loops, branches and jumps. Although there is no concept of procedures in the non-structured paradigm[citation needed], subroutines are allowed. Unlike a procedure, a subroutine may have several entry and exit points, and a direct jump into or out of subroutine is (theoretically) allowed. This flexibility allows realization of coroutines.\n\nThere is no concept of locally scoped variables in non-structured programming (although for assembly programs, general purpose registers may serve the same purpose after saving on entry), but labels and variables can have a limited area of effect (for example, a group of lines). This means there is no (automatic) context refresh when calling a subroutine, so all variables might retain their values from the previous call. This makes general recursion difficult, but some cases of recursion—where no subroutine state values are needed after the recursive call—are possible if variables dedicated to the recursive subroutine are explicitly cleared (or re-initialized to their original value) on entry to the subroutine. The depth of nesting also may be limited to one or two levels.\n\nData types[edit]\nNon-structured languages allow only basic data types, such as numbers, strings and arrays[citation needed] (numbered sets of variables of the same type). The introduction of arrays into non-structured languages was a notable step forward, making stream data processing possible despite the lack of structured data types[citation needed].",
          "subparadigms": [
            49
          ]
        },
        {
          "pdid": 51,
          "name": "Nondeterministic",
          "details": "A nondeterministic programming language is a language which can specify, at certain points in the program (called \"choice points\"), various alternatives for program flow. Unlike an if-then statement, the method of choice between these alternatives is not directly specified by the programmer; the program must decide at run time between the alternatives, via some general method applied to all choice points. A programmer specifies a limited number of alternatives, but the program must later choose between them. (\"Choose\" is, in fact, a typical name for the nondeterministic operator.) A hierarchy of choice points may be formed, with higher-level choices leading to branches that contain lower-level choices within them.\n\nOne method of choice is embodied in backtracking systems (such as AMB, or unification in Prolog), in which some alternatives may \"fail,\" causing the program to backtrack and try other alternatives. If all alternatives fail at a particular choice point, then an entire branch fails, and the program will backtrack further, to an older choice point. One complication is that, because any choice is tentative and may be remade, the system must be able to restore old program states by undoing side-effects caused by partially executing a branch that eventually failed.\n\nAnother method of choice is reinforcement learning, embodied in systems such as Alisp. In such systems, rather than backtracking, the system keeps track of some measure of success and learns which choices often lead to success, and in which situations (both internal program state and environmental input may affect the choice). These systems are suitable for applications to robotics and other domains in which backtracking would involve attempting to undo actions performed in a dynamic environment, which may be difficult or impractical.",
          "subparadigms": []
        },
        {
          "pdid": 52,
          "name": "Process-oriented",
          "details": "Process-oriented programming is a programming paradigm that separates the concerns of data structures and the concurrent processes that act upon them. The data structures in this case are typically persistent, complex, and large scale - the subject of general purpose applications, as opposed to specialized processing of specialized data sets seen in high productivity applications (HPC). The model allows the creation of large scale applications that partially share common data sets. Programs are functionally decomposed into parallel processes that create and act upon logically shared data.\n\nThe paradigm was originally invented for parallel computers in the 1980s, especially computers built with transputer microprocessors by INMOS, or similar architectures. Occam was an early process-oriented language developed for the Transputer.\n\nSome derivations have evolved from the message passing paradigm of Occam to enable uniform efficiency when porting applications between distributed memory and shared memory parallel computers[citation needed]. The first such derived example appears in the programming language Ease designed at Yale University[1][2] in 1990. Similar models have appeared since in the loose combination of SQL databases and objected oriented languages such as Java, often referred to as object-relational models and widely used in large scale distributed systems today. The paradigm is likely to appear on desktop computers as microprocessors increase the number of processors (multicore) per chip.\n\nThe Actor model might usefully be described as a specialised kind of process-oriented system in which the message-passing model is restricted to the simple fixed case of one infinite input queue per process (i.e. actor), to which any other process can send messages.\n\nSee also[edit]\nCommunicating process architectures\nMassively parallel processing\nParallel computing\nMulti-core\nActor model",
          "subparadigms": []
        },
        {
          "pdid": 53,
          "name": "Parallel",
          "details": "Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\n\nParallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU).[5][6] In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.\n\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.\n\nIn some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones,[7] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.\n\nA theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.\n\nContents  [hide] \n1\tBackground\n1.1\tAmdahl's law and Gustafson's law\n1.2\tDependencies\n1.3\tRace conditions, mutual exclusion, synchronization, and parallel slowdown\n1.4\tFine-grained, coarse-grained, and embarrassing parallelism\n1.5\tConsistency models\n1.6\tFlynn's taxonomy\n2\tTypes of parallelism\n2.1\tBit-level parallelism\n2.2\tInstruction-level parallelism\n2.3\tTask parallelism\n3\tHardware\n3.1\tMemory and communication\n3.2\tClasses of parallel computers\n3.2.1\tMulti-core computing\n3.2.2\tSymmetric multiprocessing\n3.2.3\tDistributed computing\n3.2.4\tSpecialized parallel computers\n4\tSoftware\n4.1\tParallel programming languages\n4.2\tAutomatic parallelization\n4.3\tApplication checkpointing\n5\tAlgorithmic methods\n6\tFault-tolerance\n7\tHistory\n8\tSee also\n9\tReferences\n10\tFurther reading\n11\tExternal links\nBackground[edit]\nTraditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.[8]\n\nParallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.[8]\n\nFrequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs.[9]\n\nHowever, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second).[10] Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[11]\n\nMoore's law is the empirical observation that the number of transistors in a microprocessor doubles every 18 to 24 months.[12] Despite power consumption issues, and repeated predictions of its end, Moore's law is still in effect. With the end of frequency scaling, these additional transistors (which are no longer used for frequency scaling) can be used to add extra hardware for parallel computing.\n\nAmdahl's law and Gustafson's law[edit]\n\nA graphical representation of Amdahl's law. The speedup of a program from parallelization is limited by how much of the program can be parallelized. For example, if 90% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 10 times no matter how many processors are used.\n\nAssume that a task has two independent parts, A and B. Part B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A be twice as fast. This will make the computation much faster than by optimizing part B, even though part B's speedup is greater by ratio, (5 times versus 2 times).\nOptimally, the speedup from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speedup. Most of them have a near-linear speedup for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements.\n\nThe potential speedup of an algorithm on a parallel computing platform is given by Amdahl's law[13]\n\n{\\displaystyle S_{\\text{latency}}(s)={\\frac {1}{1-p+{\\frac {p}{s}}}},} S_{\\text{latency}}(s)={\\frac {1}{1-p+{\\frac {p}{s}}}},\nwhere\n\nSlatency is the potential speedup in latency of the execution of the whole task;\ns is the speedup in latency of the execution of the parallelizable part of the task;\np is the percentage of the execution time of the whole task concerning the parallelizable part of the task before parallelization.\nSince Slatency < 1/(1 - p), it shows that a small part of the program which cannot be parallelized will limit the overall speedup available from parallelization. A program solving a large mathematical or engineering problem will typically consist of several parallelizable parts and several non-parallelizable (serial) parts. If the non-parallelizable part of a program accounts for 10% of the runtime (p = 0.9), we can get no more than a 10 times speedup, regardless of how many processors are added. This puts an upper limit on the usefulness of adding more parallel execution units. \"When a task cannot be partitioned because of sequential constraints, the application of more effort has no effect on the schedule. The bearing of a child takes nine months, no matter how many women are assigned.\"[14]\n\n\nA graphical representation of Gustafson's law.\nAmdahl's law only applies to cases where the problem size is fixed. In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently serial work.[15] In this case, Gustafson's law gives a less pessimistic and more realistic assessment of parallel performance:[16]\n\n{\\displaystyle S_{\\text{latency}}(s)=1-p+sp.} S_{\\text{latency}}(s)=1-p+sp.\nBoth Amdahl's law and Gustafson's law assume that the running time of the serial part of the program is independent of the number of processors. Amdahl's law assumes that the entire problem is of fixed size so that the total amount of work to be done in parallel is also independent of the number of processors, whereas Gustafson's law assumes that the total amount of work to be done in parallel varies linearly with the number of processors.\n\nDependencies[edit]\nUnderstanding data dependencies is fundamental in implementing parallel algorithms. No program can run more quickly than the longest chain of dependent calculations (known as the critical path), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel.\n\nLet Pi and Pj be two program segments. Bernstein's conditions[17] describe when the two are independent and can be executed in parallel. For Pi, let Ii be all of the input variables and Oi the output variables, and likewise for Pj. Pi and Pj are independent if they satisfy\n\n{\\displaystyle I_{j}\\cap O_{i}=\\varnothing ,} I_{j}\\cap O_{i}=\\varnothing ,\n{\\displaystyle I_{i}\\cap O_{j}=\\varnothing ,} I_{i}\\cap O_{j}=\\varnothing ,\n{\\displaystyle O_{i}\\cap O_{j}=\\varnothing .} O_{i}\\cap O_{j}=\\varnothing .\nViolation of the first condition introduces a flow dependency, corresponding to the first segment producing a result used by the second segment. The second condition represents an anti-dependency, when the second segment produces a variable needed by the first segment. The third and final condition represents an output dependency: when two segments write to the same location, the result comes from the logically last executed segment.[18]\n\nConsider the following functions, which demonstrate several kinds of dependencies:\n\n1: function Dep(a, b)\n2: c := a * b\n3: d := 3 * c\n4: end function\nIn this example, instruction 3 cannot be executed before (or even in parallel with) instruction 2, because instruction 3 uses a result from instruction 2. It violates condition 1, and thus introduces a flow dependency.\n\n1: function NoDep(a, b)\n2: c := a * b\n3: d := 3 * b\n4: e := a + b\n5: end function\nIn this example, there are no dependencies between the instructions, so they can all be run in parallel.\n\nBernstein's conditions do not allow memory to be shared between different processes. For that, some means of enforcing an ordering between accesses is necessary, such as semaphores, barriers or some other synchronization method.\n\nRace conditions, mutual exclusion, synchronization, and parallel slowdown[edit]\nSubtasks in a parallel program are often called threads. Some parallel computer architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, \"threads\" is generally accepted as a generic term for subtasks. Threads will often need to update some variable that is shared between them. The instructions between the two programs may be interleaved in any order. For example, consider the following program:\n\nThread A\tThread B\n1A: Read variable V\t1B: Read variable V\n2A: Add 1 to variable V\t2B: Add 1 to variable V\n3A: Write back to variable V\t3B: Write back to variable V\nIf instruction 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, the program will produce incorrect data. This is known as a race condition. The programmer must use a lock to provide mutual exclusion. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its critical section (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks:\n\nThread A\tThread B\n1A: Lock variable V\t1B: Lock variable V\n2A: Read variable V\t2B: Read variable V\n3A: Add 1 to variable V\t3B: Add 1 to variable V\n4A: Write back to variable V\t4B: Write back to variable V\n5A: Unlock variable V\t5B: Unlock variable V\nOne thread will successfully lock variable V, while the other thread will be locked out—unable to proceed until V is unlocked again. This guarantees correct execution of the program. Locks, while necessary to ensure correct program execution, can greatly slow a program.\n\nLocking multiple variables using non-atomic locks introduces the possibility of program deadlock. An atomic lock locks multiple variables all at once. If it cannot lock all of them, it does not lock any of them. If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. In such a case, neither thread can complete, and deadlock results.\n\nMany parallel programs require that their subtasks act in synchrony. This requires the use of a barrier. Barriers are typically implemented using a software lock. One class of algorithms, known as lock-free and wait-free algorithms, altogether avoids the use of locks and barriers. However, this approach is generally difficult to implement and requires correctly designed data structures.\n\nNot all parallelization results in speed-up. Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other. Eventually, the overhead from communication dominates the time spent solving the problem, and further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. This is known as parallel slowdown.\n\nFine-grained, coarse-grained, and embarrassing parallelism[edit]\nApplications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.\n\nConsistency models[edit]\nMain article: Consistency model\nParallel programming languages and parallel computers must have a consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced.\n\nOne of the first consistency models was Leslie Lamport's sequential consistency model. Sequential consistency is the property of a parallel program that its parallel execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if \"… the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program\".[19]\n\nSoftware transactional memory is a common type of consistency model. Software transactional memory borrows from database theory the concept of atomic transactions and applies them to memory accesses.\n\nMathematically, these models can be represented in several ways. Petri nets, which were introduced in Carl Adam Petri's 1962 doctoral thesis, were an early attempt to codify the rules of consistency models. Dataflow theory later built upon these, and Dataflow architectures were created to physically implement the ideas of dataflow theory. Beginning in the late 1970s, process calculi such as Calculus of Communicating Systems and Communicating Sequential Processes were developed to permit algebraic reasoning about systems composed of interacting components. More recent additions to the process calculus family, such as the π-calculus, have added the capability for reasoning about dynamic topologies. Logics such as Lamport's TLA+, and mathematical models such as traces and Actor event diagrams, have also been developed to describe the behavior of concurrent systems.\n\nSee also: Relaxed sequential\nFlynn's taxonomy[edit]\nMichael J. Flynn created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.\n\nFlynn's taxonomy\nSingle data stream\nSISD MISD\nMultiple data streams\nSIMD MIMD SPMD MPMD\nThe single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as systolic arrays), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.\n\nAccording to David A. Patterson and John L. Hennessy, \"Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme.\"[20]\n\nTypes of parallelism[edit]\nBit-level parallelism[edit]\nMain article: Bit-level parallelism\nFrom the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information the processor can manipulate per cycle.[21] Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.\n\nHistorically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until the early twothousands, with the advent of x86-64 architectures, did 64-bit processors become commonplace.\n\nInstruction-level parallelism[edit]\nMain article: Instruction-level parallelism\n\nA canonical processor without pipeline. It takes five clock cycles to complete one instruction and thus the processor can issue subscalar performance (IPC = 0.2 < 1).\n\nA canonical five-stage pipelined processor. In the best case scenario, it takes one clock cycle to complete one instruction and thus the processor can issue scalar performance (IPC = 1).\nA computer program, is in essence, a stream of instructions executed by a processor. Without instruction-level parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1). These processors are known as subscalar processors. These instructions can be re-ordered and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.[22]\n\nAll modern processors have multi-stage instruction pipelines. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC = 1). These processors are known as scalar processors. The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and register write back (WB). The Pentium 4 processor had a 35-stage pipeline.[23]\n\n\nA canonical five-stage pipelined superscalar processor. In the best case scenario, it takes one clock cycle to complete two instructions and thus the processor can issue superscalar performance (IPC = 2 > 1).\nMost modern processors also have multiple execution units. They usually combine this feature with pipelining and thus can issue more than one instruction per clock cycle (IPC > 1). These processors are known as superscalar processors. Instructions can be grouped together only if there is no data dependency between them. Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.\n\nTask parallelism[edit]\nMain article: Task parallelism\nTask parallelisms is the characteristic of a parallel program that \"entirely different calculations can be performed on either the same or different sets of data\".[24] This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks simultaneously and often cooperatively. Task parallelism does not usually scale with the size of a problem.[25]\n\nHardware[edit]\nMemory and communication[edit]\nMain memory in a parallel computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space).[26] Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory.\n\n\nA logical view of a non-uniform memory access (NUMA) architecture. Processors in one directory can access that directory's memory with less latency than they can access memory in the other directory's memory.\nComputer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, that can be achieved only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.\n\nComputer systems make use of caches—small and fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping is one of the most common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.[26]\n\nProcessor–processor and processor–memory communication can be implemented in hardware in several ways, including via shared (either multiported or multiplexed) memory, a crossbar switch, a shared bus or an interconnect network of a myriad of topologies including star, ring, tree, hypercube, fat hypercube (a hypercube with more than one processor at a node), or n-dimensional mesh.\n\nParallel computers based on interconnected networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.\n\nClasses of parallel computers[edit]\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.\n\nMulti-core computing[edit]\nMain article: Multi-core processor\nA multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip. This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one instruction stream (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. IBM's Cell microprocessor, designed for use in the Sony PlayStation 3, is a prominent multi-core processor. Each core in a multi-core processor can potentially be superscalar as well—that is, on every clock cycle, each core can issue multiple instructions from one thread.\n\nSimultaneous multithreading (of which Intel's Hyper-Threading is the best known) was an early form of pseudo-multi-coreism. A processor capable of simultaneous multithreading includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. Temporal multithreading on the other hand includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.\n\nSymmetric multiprocessing[edit]\nMain article: Symmetric multiprocessing\nA symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.[27] Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32 processors.[28] Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.[27]\n\nDistributed computing[edit]\nMain article: Distributed computing\nA distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable.\n\nCluster computing[edit]\nMain article: Computer cluster\n\nA Beowulf cluster.\nA cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer.[29] Clusters are composed of multiple standalone machines connected by a network. While machines in a cluster do not have to be symmetric, load balancing is more difficult if they are not. The most common type of cluster is the Beowulf cluster, which is a cluster implemented on multiple identical commercial off-the-shelf computers connected with a TCP/IP Ethernet local area network.[30] Beowulf technology was originally developed by Thomas Sterling and Donald Becker. The vast majority of the TOP500 supercomputers are clusters.[31]\n\nBecause grid computing systems (described below) can easily handle embarrassingly parallel problems, modern clusters are typically designed to handle more difficult problems—problems that require nodes to share intermediate results with each other more often. This requires a high bandwidth and, more importantly, a low-latency interconnection network. Many historic and current supercomputers use customized high-performance network hardware specifically designed for cluster computing, such as the Cray Gemini network.[32] As of 2014, most current supercomputers use some off-the-shelf standard network hardware, often Myrinet, InfiniBand, or Gigabit Ethernet.\n\nMassively parallel computing[edit]\nMain article: Massively parallel (computing)\n\nA cabinet from IBM's Blue Gene/L massively parallel supercomputer.\nA massively parallel processor (MPP) is a single computer with many networked processors. MPPs have many of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having \"far more\" than 100 processors.[33] In an MPP, \"each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect.\"[34]\n\nIBM's Blue Gene/L, the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP.\n\nGrid computing[edit]\nMain article: Grid computing\nGrid computing is the most distributed form of parallel computing. It makes use of computers communicating over the Internet to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, distributed computing typically deals only with embarrassingly parallel problems. Many distributed computing applications have been created, of which SETI@home and Folding@home are the best-known examples.[35]\n\nMost grid computing applications use middleware, software that sits between the operating system and the application to manage network resources and standardize the software interface. The most common distributed computing middleware is the Berkeley Open Infrastructure for Network Computing (BOINC). Often, distributed computing software makes use of \"spare cycles\", performing computations at times when a computer is idling.\n\nSpecialized parallel computers[edit]\nWithin parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific, they tend to be applicable to only a few classes of parallel problems.\n\nReconfigurable computing with field-programmable gate arrays[edit]\nReconfigurable computing is the use of a field-programmable gate array (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task.\n\nFPGAs can be programmed with hardware description languages such as VHDL or Verilog. However, programming in these languages can be tedious. Several vendors have created C to HDL languages that attempt to emulate the syntax and semantics of the C programming language, with which most programmers are familiar. The best known C to HDL languages are Mitrion-C, Impulse C, DIME-C, and Handel-C. Specific subsets of SystemC based on C++ can also be used for this purpose.\n\nAMD's decision to open its HyperTransport technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.[36] According to Michael R. D'Amour, Chief Operating Officer of DRC Computer Corporation, \"when we first walked into AMD, they called us 'the socket stealers.' Now they call us their partners.\"[36]\n\nGeneral-purpose computing on graphics processing units (GPGPU)[edit]\nMain article: GPGPU\n\nNvidia's Tesla GPGPU card\nGeneral-purpose computing on graphics processing units (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for computer graphics processing.[37] Computer graphics processing is a field dominated by data parallel operations—particularly linear algebra matrix operations.\n\nIn the early days, GPGPU programs used the normal graphics APIs for executing programs. However, several new programming languages and platforms have been built to do general purpose computation on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively. Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for computation in their Tesla series. The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs. AMD, Apple, Intel, Nvidia and others are supporting OpenCL.\n\nApplication-specific integrated circuits[edit]\nMain article: Application-specific integrated circuit\nSeveral application-specific integrated circuit (ASIC) approaches have been devised for dealing with parallel applications.[38][39][40]\n\nBecause an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by UV photolithography. This process requires a mask set, which can be extremely expensive. A mask set can cost over a million US dollars.[41] (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore's law) tend to wipe out these gains in only one or two chip generations.[36] High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the PFLOPS RIKEN MDGRAPE-3 machine which uses custom ASICs for molecular dynamics simulation.\n\nVector processors[edit]\nMain article: Vector processor\n\nThe Cray-1 is a vector processor.\nA vector processor is a CPU or computer system that can execute the same instruction on large sets of data. Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers.[42] They are closely related to Flynn's SIMD classification.[42]\n\nCray computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern processor instruction sets do include some vector processing instructions, such as with Freescale Semiconductor's AltiVec and Intel's Streaming SIMD Extensions (SSE).\n\nSoftware[edit]\nParallel programming languages[edit]\nMain article: List of concurrent and parallel programming languages\nConcurrent programming languages, libraries, APIs, and parallel programming models (such as algorithmic skeletons) have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP are two of the most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API.[43] One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.\n\nCAPS entreprise and Pathscale are also coordinating their effort to make hybrid multi-core parallel programming (HMPP) directives an open standard called OpenHMPP. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory. OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU main memory and the accelerator memory.\n\nThe rise of consumer GPUs has led to support for compute kernels, either in graphics APIs (referred to as compute shaders), in dedicated APIs (such as OpenCL), or in other language extensions.\n\nAutomatic parallelization[edit]\nMain article: Automatic parallelization\nAutomatic parallelization of a sequential program by a compiler is the holy grail of parallel computing. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.[44]\n\nMainstream parallel programming languages remain either explicitly parallel or (at best) partially implicit, in which a programmer gives the compiler directives for parallelization. A few fully implicit parallel programming languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.\n\nApplication checkpointing[edit]\nMain article: Application checkpointing\nAs a computer system grows in complexity, the mean time between failures usually decreases. Application checkpointing is a technique whereby the computer system takes a \"snapshot\" of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high performance computing.[45]\n\nAlgorithmic methods[edit]\nAs parallel computers become larger and faster, it becomes feasible to solve problems that previously took too long to run. Parallel computing is used in a wide range of fields, from bioinformatics (protein folding and sequence analysis) to economics (mathematical finance). Common types of problems found in parallel computing applications are:[46]\n\ndense linear algebra;\nsparse linear algebra;\nspectral methods (such as Cooley–Tukey fast Fourier transform)\nN-body problems (such as Barnes–Hut simulation);\nstructured grid problems (such as Lattice Boltzmann methods);\nunstructured grid problems (such as found in finite element analysis);\nMonte Carlo method;\ncombinational logic (such as brute-force cryptographic techniques);\ngraph traversal (such as sorting algorithms);\ndynamic programming;\nbranch and bound methods;\ngraphical models (such as detecting hidden Markov models and constructing Bayesian networks);\nfinite-state machine simulation.\nFault-tolerance[edit]\nFurther information: Fault-tolerant computer system\nParallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component should fail, and also allows automatic error detection and error correction if the results differ. These methods can be used to help prevent single event upsets caused by transient errors.[47] Although additional measures may be required in embedded or specialized systems, this method can provide a cost effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.\n\nHistory[edit]\nMain article: History of computing\n\nILLIAC IV, \"the most infamous of supercomputers\".[48]\nThe origins of true (MIMD) parallelism go back to Luigi Federico Menabrea and his Sketch of the Analytic Engine Invented by Charles Babbage.[49][50][51]\n\nIn April 1958, S. Gill (Ferranti) discussed parallel programming and the need for branching and waiting.[52] Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.[53] Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch.[54] In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference.[53] It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.\n\nIn 1969, company Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel.[53] C.mmp, a 1970s multi-processor project at Carnegie Mellon University, was among the first multiprocessors with more than a few processors.[50] The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984.\"[50]\n\nSIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions.[55] In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory.[53] His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV.[53] The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called \"the most infamous of supercomputers\", because the project was only one fourth completed, but took 11 years and cost almost four times the original estimate.[48] When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.\n\nSee also[edit]\nList of important publications in concurrent, parallel, and distributed computing\nList of distributed computing conferences\nConcurrency (computer science)\nSynchronous programming\nContent Addressable Parallel Processor\nManycore\nSerializability\nTransputer\nParallel programming model\nvector processing\nMulti tasking\nFeng's Classification",
          "subparadigms": [
            52
          ]
        },
        {
          "pdid": 54,
          "name": "Concatenative",
          "details": "A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition.[1] Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.\n\nContents  [hide] \n1\tExample\n2\tProperties\n3\tImplementations\n4\tSee also\n5\tReferences\n6\tExternal links\nExample[edit]\nFor example, a sequence of operations in an applicative language like the following:\n\ny = foo(x)\nz = bar(y)\nw = baz(z)\n...is written in a concatenative language as a sequence of functions, without parameters:[2]\n\nfoo bar baz\nFunctions and procedures written in concatenative style are not value level, i.e. they typically don't represent the data structures they operate on with explicit names or identifiers; instead they are function level - a function is defined as a pipeline, a sequence of operations that take parameters from an implicit data structure on which all functions operate, and return the function results to that shared structure so that it will be used by the next operator.[3]\n\nThe combination of a compositional semantics with a syntax that mirrors such a semantics makes concatenative languages highly amenable to algebraic manipulation of programs;[4] although it may be difficult to write mathematical expressions directly in them.[5] Concatenative languages can be implemented in an efficient way with a stack machine, and are a common strategy to program virtual machines.[5]\n\nMuch of the original work on concatenative language theory was carried out by Manfred von Thun.[citation needed]\n\nProperties[edit]\nThe properties of concatenative languages are the result of their compositional syntax and semantics:\n\nThe reduction of any expression is the simplification of one function to another function; it is never necessary to deal with the application of functions to objects.[6]\nAny subexpression can be replaced with a name that represents the same subexpression. This is referred to in the concatenative community as factoring and is used extensively to simplify programs into smaller parts.\nThe syntax and semantics of concatenative languages form the algebraic structure of a monoid.[7]\nConcatenative languages can be made well-suited to an implementation inspired by linear logic where no garbage is ever generated.[8]\nImplementations[edit]\nThe first concatenative programming language was Forth, although Joy was the first language to call itself concatenative. Other concatenative languages are Cat, Enchilada, Factor, Onyx, PostScript, RPL, Staapl, Trith, XY, Kitten, and Om.\n\nMost existing concatenative languages are stack-based; this is not a requirement and other models have been proposed.[9][10][11] Concatenative languages are currently used for embedded, desktop, and web programming, as target languages, and for research purposes.\n\nMost concatenative languages are dynamically typed. One exception is the statically typed Cat language.[12]\n\nSee also[edit]\nFunction-level programming\nStack-oriented programming language\nTacit programming\nHomoiconicity\nReferences[edit]\nJump up ^ \"Christopher Diggins: What is a concatenative language\". Drdobbs.com. 2008-12-31. Retrieved 2013-07-01.\nJump up ^ \"Name code not values\". Concatenative.org. Retrieved 13 September 2013.\nJump up ^ \"Concatenative language\". Concatenative.org. Retrieved 13 September 2013.\nJump up ^ \"Rationale for Joy, a functional language\". Archived from the original on 2011-01-15.\n^ Jump up to: a b \"Why Concatenative Programming Matters\". Retrieved 13 September 2013.\nJump up ^ \"von Thun, Manfred: Joy compared with other functional languages\". Archived from the original on 2011-10-06.\nJump up ^ \"von Thun, Manfred: Mathematical foundations of Joy\". Archived from the original on 2010-07-31.\nJump up ^ \"Henry Baker: Linear Logic and Permutation Stacks — The Forth Shall Be First\". Home.pipeline.com. Retrieved 2013-07-01.\nJump up ^ \"The Concatenative Language XY\". Nsl.com. Retrieved 2013-07-01.\nJump up ^ \"The Enchilada Programming Language\". Enchiladacode.nl. Retrieved 2013-07-01.\nJump up ^ \"The Om Programming Language\". Om-language.org. Retrieved 2013-07-01.\nJump up ^ \"Cat Specification\". Cat-language.com. Archived from the original on 2015-02-05. Retrieved 2013-07-01.",
          "subparadigms": []
        },
        {
          "pdid": 55,
          "name": "Tacit",
          "details": "Tacit programming, also called point-free style, is a programming paradigm in which function definitions do not identify the arguments (or \"points\") on which they operate. Instead the definitions merely compose other functions, among which are combinators that manipulate the arguments. Tacit programming is of theoretical interest, because the strict use of composition results in programs that are well adapted for equational reasoning.[1] It is also the natural style of certain programming languages, including APL and its derivatives,[2] and concatenative languages such as Forth. Despite this base, the lack of argument naming gives point-free style a reputation of being unnecessarily obscure, hence the epithet \"pointless style.\"[1]\n\nUNIX scripting uses the paradigm with pipes.\n\nFor example, a sequence of operations in an applicative language like the following:\n\ndef example(x):\n  y = foo(x)\n  z = bar(y)\n  w = baz(z)\n  return w\n...is written in point-free style as the composition of a sequence of functions, without parameters:[3]\n\ndef example: baz bar foo\nThe key idea in tacit programming is to assist in operating at the appropriate level of abstraction. That is, to translate the natural transformation given by currying\n\n{\\displaystyle \\hom(A\\times B,C)\\cong \\hom(B,C^{A})} \\hom(A\\times B,C)\\cong \\hom(B,C^{A})\ninto computer functions, where the left represents the uncurried form of a function and the right the curried. CA denotes the functionals from A to C, while A × B denotes the Cartesian product of A and B.\n\nContents  [hide] \n1\tExamples\n1.1\tFunctional programming\n1.2\tAPL family\n1.3\tStack-based\n1.4\tUNIX pipeline\n2\tSee also\n3\tReferences\n4\tExternal links\nExamples[edit]\nFunctional programming[edit]\nA simple example (in Haskell) is a program which takes a sum of a list. A programmer might define a sum recursively using a pointed (cf. value-level programming) method as:\n\nsum (x:xs) = x + sum xs\nsum [] = 0\nHowever, by noting this as a fold the programmer could replace this with:\n\nsum xs = foldr (+) 0 xs\nAnd then the argument is not needed, so this can be replaced with\n\nsum = foldr (+) 0\nwhich is point-free.\n\nAnother example uses the dot operator:\n\np x y z = f (g x y) z\nThe following Haskell-like pseudo-code exposes how to reduce a function definition to its point-free equivalent:\n\np = \\x -> \\y -> \\z -> f (g x y) z\n  = \\x -> \\y -> f (g x y)\n  = \\x -> \\y -> (f . (g x)) y\n  = \\x -> f . (g x)\n  = \\x -> ((.) f) (g x)\n  = ((.) f) . g\nso\n\np = ((.) f) . g\nFinally, to see a complex example imagine a map filter program which takes a list, applies a function to it, and then filters the elements based on a criterion\n\nmf criteria operator list = filter criteria (map operator list)\nIt can be expressed point-free[4] as\n\nmf = (. map) . (.) . filter\nNote that, as stated previously, the points in 'point-free' refer to the arguments, not to the use of dots; a common misconception.[5]\n\nAPL family[edit]\nIn J, the same sort of point-free code occurs in a function made to compute the average of a list (array) of numbers:\n\navg=: +/ % #\n+/ sums the items of the array by mapping (/) summation (+) to the array. % divides the sum by the number of elements (#) in the array.\n\nStack-based[edit]\nIn stack-oriented programming languages (and concatenative ones, most of which are stack based), point-free methods are commonly used. For example, a procedure to compute the Fibonacci numbers might look like:\n\n /fib\n {\n    dup dup 1 eq exch 0 eq or not\n    {\n       dup 1 sub fib\n       exch 2 sub fib\n       add\n    } if\n } def\nUNIX pipeline[edit]\nMain article: Pipeline (Unix)\nIn UNIX scripting the functions are computer programs which receive data from standard input and send the results to standard output. For example,\n\nsort | uniq -c | sort -rn\nis a tacit or point-free composition which returns the counts of its arguments and the arguments, in the order of decreasing counts. The 'sort' and 'uniq' are the functions, the '-c' and '-rn' control the functions, but the arguments are not mentioned. The '|' is the composition operator.\n\nSee also[edit]\nCombinatory logic\nConcatenative programming language\nFunction-level programming\nJoy (programming language), modern highly tacit language\nPointless topology",
          "subparadigms": [
            54
          ]
        },
        {
          "pdid": 56,
          "name": "Semantic",
          "details": "Semantic-oriented programming (SOP) is a programming paradigm in which the programmer formulizes the logic of a domain by means of semantic structures. Similar to Concept programming and Concept-oriented programming.\n\nContents  [hide] \n1\tCommon features\n2\tGoals\n3\tSOPlets\n4\tSymADE\n5\tSee also\n6\tExternal links\nCommon features[edit]\nThe way of how these semantic information are represented in the system vary according to the approach chosen (see below), common to these approaches are the following features:\n\nThe semantics represent static facts, that is: facts that describe the domain in question at a given moment, and which do not change during runtime (as opposed to Semantic Web for instance)\nThe system has native access to these semantic structures during compile time and runtime, and can interpret them in order to fulfill the requested features\nClear separation from logic and implementation (where possible)\nIn many cases, SOP supports the notion of Single Source of Truth (SSoT), such that every semantic concept is stored exactly once, Any possible linkages to this concept are by reference only\nA programmer can freely and quickly add new semantic meanings without breaking compatibility with the system environment\nGoals[edit]\nThe goals of SOP are:\n\nImproving the maintainability of a software\nImproving the transparency of a software\nFlexibility by allowing exchangeability of logic or implementation\nOptimal support for agile development processes (refactoring)\nSOPlets[edit]\nSoplets is a method of describing semantic concepts as blocks of code, using existing features of the (Java) language, namely annotations and enumerations. Each block of code (called Soplet) represents all properties and features of a given concept (as far as reasonable and feasible), including features outside of the traditional modelling scope, such as translations, documentation, requirement tracking and so on.\n\nSoplets can be referenced and used from anywhere inside the code. Given the strong-typed nature of the references they can be safely refactored as seen fit.\n\nA Soplet may be enhanced by one or more (stateless) functions, which are directly attached to the code block. That way also related features related to a given concept (such as calculations, validation, transformations etc.) beyond pure key-value pairs may be associated with a given Soplet.\n\nThe structure of a Soplet is formally defined by the stereotype it implements. This stereotype may be individually composed of several aspects (such as Translatable, Beanable, Bindable, Testable etc.), which in turn may be freely defined by the developer (or which are part of a framework which he uses).\n\nAn open-source plugin (based on the Project Lombok plugin) allows the creation of byte-code during compile-time, based on the information contained in the Soplets. For instance, a data bean may have all of its attributes, getters and setters generated.\n\nSymADE[edit]\nSymADE (Symbolic Adaptable Development Environment) is an open-source IDE and implementation of SOP (Semantic-oriented programming) paradigm.\n\nIn SymADE a program is edited and stored as a tree of semantic nodes (meanings). The tree is edited by structural editor, and programmers can edit either the semantic tree directly or a projection of the semantic tree onto syntax tree. There may be multiple projections of the same tree, and they can be rendered on the screen as reach text, as UML diagrams and so on.\n\nSemantic meanings are completely user-defined. This allows to use SymADE for creating and editing new domain-specific languages, modify existing languages, use in the same piece of code a mix of multiple languages.\n\nSymADE is common in spirit with IP (Intentional Programming)and JetBrains MPS. The main difference is that they define and edit syntax trees, but in SymADE you create and edit semantic trees. This gives an unbound possibility for automating code writing, i.e. the actual code can be written by computer based on dialog interaction with programmers. And of cause, the SymADE project is open-source, unlike proprietary IP and MPS development environments.\n\nThe higher automation of code writing will allow to create more complex programs without increasing the amount of abstraction layers - because the computer, not programmers, will take care of the code complexity. This will allow to write more complex programs without increasing resource requirements (CPU speed and memory size).\n\nSee also[edit]\nModel-driven engineering\nDomain-specific languages\nService-oriented programming\nLanguage-oriented programming\nAspect-oriented programming\nGenerative programming\nIntentional programming\nAutomatic programming\nResource-oriented architecture\nTransaction-level modeling\nConcept programming",
          "subparadigms": []
        },
        {
          "pdid": 57,
          "name": "Aspect-oriented",
          "details": "In computing, aspect-oriented programming (AOP) is a programming paradigm that aims to increase modularity by allowing the separation of cross-cutting concerns. It does so by adding additional behavior to existing code (an advice) without modifying the code itself, instead separately specifying which code is modified via a \"pointcut\" specification, such as \"log all function calls when the function's name begins with 'set'\". This allows behaviors that are not central to the business logic (such as logging) to be added to a program without cluttering the code core to the functionality. AOP forms a basis for aspect-oriented software development.\n\nAOP includes programming methods and tools that support the modularization of concerns at the level of the source code, while \"aspect-oriented software development\" refers to a whole engineering discipline.\n\nAspect-oriented programming entails breaking down program logic into distinct parts (so-called concerns, cohesive areas of functionality). Nearly all programming paradigms support some level of grouping and encapsulation of concerns into separate, independent entities by providing abstractions (e.g., functions, procedures, modules, classes, methods) that can be used for implementing, abstracting and composing these concerns. Some concerns \"cut across\" multiple abstractions in a program, and defy these forms of implementation. These concerns are called cross-cutting concerns or horizontal concerns.\n\nLogging exemplifies a crosscutting concern because a logging strategy necessarily affects every logged part of the system. Logging thereby crosscuts all logged classes and methods.\n\nAll AOP implementations have some crosscutting expressions that encapsulate each concern in one place. The difference between implementations lies in the power, safety, and usability of the constructs provided. For example, interceptors that specify the methods to intercept express a limited form of crosscutting, without much support for type-safety or debugging. AspectJ has a number of such expressions and encapsulates them in a special class, an aspect. For example, an aspect can alter the behavior of the base code (the non-aspect part of a program) by applying advice (additional behavior) at various join points (points in a program) specified in a quantification or query called a pointcut (that detects whether a given join point matches). An aspect can also make binary-compatible structural changes to other classes, like adding members or parents.\n\nContents  [hide] \n1\tHistory\n2\tMotivation and basic concepts\n3\tJoin point models\n3.1\tAspectJ's join-point model\n3.2\tOther potential join point models\n3.3\tInter-type declarations\n4\tImplementation\n4.1\tTerminology\n5\tComparison to other programming paradigms\n6\tAdoption issues\n7\tCriticism\n8\tImplementations\n9\tSee also\n10\tNotes and references\n11\tFurther reading\n12\tExternal links\nHistory[edit]\nAOP has several direct antecedents A1 and A2:[1] reflection and metaobject protocols, subject-oriented programming, Composition Filters and Adaptive Programming.[2]\n\nGregor Kiczales and colleagues at Xerox PARC developed the explicit concept of AOP, and followed this with the AspectJ AOP extension to Java. IBM's research team pursued a tool approach over a language design approach and in 2001 proposed Hyper/J and the Concern Manipulation Environment, which have not seen wide usage. The examples in this article use AspectJ as it is the most widely known AOP language.[citation needed]\n\nThe Microsoft Transaction Server is considered to be the first major application of AOP followed by Enterprise JavaBeans.[3][4]\n\nMotivation and basic concepts[edit]\nTypically, an aspect is scattered or tangled as code, making it harder to understand and maintain. It is scattered by virtue of the function (such as logging) being spread over a number of unrelated functions that might use its function, possibly in entirely unrelated systems, different source languages, etc. That means to change logging can require modifying all affected modules. Aspects become tangled not only with the mainline function of the systems in which they are expressed but also with each other. That means changing one concern entails understanding all the tangled concerns or having some means by which the effect of changes can be inferred.\n\nFor example, consider a banking application with a conceptually very simple method for transferring an amount from one account to another:[5]\n\nvoid transfer(Account fromAcc, Account toAcc, int amount) throws Exception {\n  if (fromAcc.getBalance() < amount)\n      throw new InsufficientFundsException();\n\n  fromAcc.withdraw(amount);\n  toAcc.deposit(amount);\n}\nHowever, this transfer method overlooks certain considerations that a deployed application would require: it lacks security checks to verify that the current user has the authorization to perform this operation; a database transaction should encapsulate the operation in order to prevent accidental data loss; for diagnostics, the operation should be logged to the system log, etc.\n\nA version with all those new concerns, for the sake of example, could look somewhat like this:\n\nvoid transfer(Account fromAcc, Account toAcc, int amount, User user,\n    Logger logger, Database database) throws Exception {\n  logger.info(\"Transferring money...\");\n  \n  if (!isUserAuthorised(user, fromAcc)) {\n    logger.info(\"User has no permission.\");\n    throw new UnauthorisedUserException();\n  }\n  \n  if (fromAcc.getBalance() < amount) {\n    logger.info(\"Insufficient funds.\");\n    throw new InsufficientFundsException();\n  }\n\n  fromAcc.withdraw(amount);\n  toAcc.deposit(amount);\n\n  database.commitChanges();  // Atomic operation.\n\n  logger.info(\"Transaction successful.\");\n}\nIn this example other interests have become tangled with the basic functionality (sometimes called the business logic concern). Transactions, security, and logging all exemplify cross-cutting concerns.\n\nNow consider what happens if we suddenly need to change (for example) the security considerations for the application. In the program's current version, security-related operations appear scattered across numerous methods, and such a change would require a major effort.\n\nAOP attempts to solve this problem by allowing the programmer to express cross-cutting concerns in stand-alone modules called aspects. Aspects can contain advice (code joined to specified points in the program) and inter-type declarations (structural members added to other classes). For example, a security module can include advice that performs a security check before accessing a bank account. The pointcut defines the times (join points) when one can access a bank account, and the code in the advice body defines how the security check is implemented. That way, both the check and the places can be maintained in one place. Further, a good pointcut can anticipate later program changes, so if another developer creates a new method to access the bank account, the advice will apply to the new method when it executes.\n\nSo for the above example implementing logging in an aspect:\n\naspect Logger {\n  void Bank.transfer(Account fromAcc, Account toAcc, int amount, User user, Logger logger)  {\n    logger.info(\"Transferring money...\");\n  }\n\n  void Bank.getMoneyBack(User user, int transactionId, Logger logger)  {\n    logger.info(\"User requested money back.\");\n  }\n\n  // Other crosscutting code.\n}\nOne can think of AOP as a debugging tool or as a user-level tool. Advice should be reserved for the cases where you cannot get the function changed (user level)[6] or do not want to change the function in production code (debugging).\n\nJoin point models[edit]\nThe advice-related component of an aspect-oriented language defines a join point model (JPM). A JPM defines three things:\n\nWhen the advice can run. These are called join points because they are points in a running program where additional behavior can be usefully joined. A join point needs to be addressable and understandable by an ordinary programmer to be useful. It should also be stable across inconsequential program changes in order for an aspect to be stable across such changes. Many AOP implementations support method executions and field references as join points.\nA way to specify (or quantify) join points, called pointcuts. Pointcuts determine whether a given join point matches. Most useful pointcut languages use a syntax like the base language (for example, AspectJ uses Java signatures) and allow reuse through naming and combination.\nA means of specifying code to run at a join point. AspectJ calls this advice, and can run it before, after, and around join points. Some implementations also support things like defining a method in an aspect on another class.\nJoin-point models can be compared based on the join points exposed, how join points are specified, the operations permitted at the join points, and the structural enhancements that can be expressed.\n\nAspectJ's join-point model[edit]\nMain article: AspectJ\nThe join points in AspectJ include method or constructor call or execution, the initialization of a class or object, field read and write access, exception handlers, etc. They do not include loops, super calls, throws clauses, multiple statements, etc.\nPointcuts are specified by combinations of primitive pointcut designators (PCDs).\n\"Kinded\" PCDs match a particular kind of join point (e.g., method execution) and tend to take as input a Java-like signature. One such pointcut looks like this:\n execution(* set*(*))\nThis pointcut matches a method-execution join point, if the method name starts with \"set\" and there is exactly one argument of any type.\n\"Dynamic\" PCDs check runtime types and bind variables. For example,\n\n  this(Point)\nThis pointcut matches when the currently executing object is an instance of class Point. Note that the unqualified name of a class can be used via Java's normal type lookup.\n\"Scope\" PCDs limit the lexical scope of the join point. For example:\n\n within(com.company.*)\nThis pointcut matches any join point in any type in the com.company package. The * is one form of the wildcards that can be used to match many things with one signature.\nPointcuts can be composed and named for reuse. For example:\n\n pointcut set() : execution(* set*(*) ) && this(Point) && within(com.company.*);\nThis pointcut matches a method-execution join point, if the method name starts with \"set\" and this is an instance of type Point in the com.company package. It can be referred to using the name \"set()\".\nAdvice specifies to run at (before, after, or around) a join point (specified with a pointcut) certain code (specified like code in a method). The AOP runtime invokes Advice automatically when the pointcut matches the join point. For example:\nafter() : set() {\n   Display.update();\n}\nThis effectively specifies: \"if the set() pointcut matches the join point, run the code Display.update() after the join point completes.\"\nOther potential join point models[edit]\nThere are other kinds of JPMs. All advice languages can be defined in terms of their JPM. For example, a hypothetical aspect language for UML may have the following JPM:\n\nJoin points are all model elements.\nPointcuts are some boolean expression combining the model elements.\nThe means of affect at these points are a visualization of all the matched join points.\nInter-type declarations[edit]\nInter-type declarations provide a way to express crosscutting concerns affecting the structure of modules. Also known as open classes and extension methods, this enables programmers to declare in one place members or parents of another class, typically in order to combine all the code related to a concern in one aspect. For example, if a programmer implemented the crosscutting display-update concern using visitors instead, an inter-type declaration using the visitor pattern might look like this in AspectJ:\n\n  aspect DisplayUpdate {\n    void Point.acceptVisitor(Visitor v) {\n      v.visit(this);\n    }\n    // other crosscutting code...\n  }\nThis code snippet adds the acceptVisitor method to the Point class.\n\nIt is a requirement that any structural additions be compatible with the original class, so that clients of the existing class continue to operate, unless the AOP implementation can expect to control all clients at all times.\n\nImplementation[edit]\nAOP programs can affect other programs in two different ways, depending on the underlying languages and environments:\n\na combined program is produced, valid in the original language and indistinguishable from an ordinary program to the ultimate interpreter\nthe ultimate interpreter or environment is updated to understand and implement AOP features.\nThe difficulty of changing environments means most implementations produce compatible combination programs through a process known as weaving - a special case of program transformation. An aspect weaver reads the aspect-oriented code and generates appropriate object-oriented code with the aspects integrated. The same AOP language can be implemented through a variety of weaving methods, so the semantics of a language should never be understood in terms of the weaving implementation. Only the speed of an implementation and its ease of deployment are affected by which method of combination is used.\n\nSystems can implement source-level weaving using preprocessors (as C++ was implemented originally in CFront) that require access to program source files. However, Java's well-defined binary form enables bytecode weavers to work with any Java program in .class-file form. Bytecode weavers can be deployed during the build process or, if the weave model is per-class, during class loading. AspectJ started with source-level weaving in 2001, delivered a per-class bytecode weaver in 2002, and offered advanced load-time support after the integration of AspectWerkz in 2005.\n\nAny solution that combines programs at runtime has to provide views that segregate them properly to maintain the programmer's segregated model. Java's bytecode support for multiple source files enables any debugger to step through a properly woven .class file in a source editor. However, some third-party decompilers cannot process woven code because they expect code produced by Javac rather than all supported bytecode forms (see also \"Criticism\", below).\n\nDeploy-time weaving offers another approach.[7] This basically implies post-processing, but rather than patching the generated code, this weaving approach subclasses existing classes so that the modifications are introduced by method-overriding. The existing classes remain untouched, even at runtime, and all existing tools (debuggers, profilers, etc.) can be used during development. A similar approach has already proven itself in the implementation of many Java EE application servers, such as IBM's WebSphere.\n\nTerminology[edit]\nStandard terminology used in Aspect-oriented programming may include:\n\nCross-cutting concerns\nMain article: Cross-cutting concern\nEven though most classes in an OO model will perform a single, specific function, they often share common, secondary requirements with other classes. For example, we may want to add logging to classes within the data-access layer and also to classes in the UI layer whenever a thread enters or exits a method. Further concerns can be related to security such as access control [8] or information flow control.[9] Even though each class has a very different primary functionality, the code needed to perform the secondary functionality is often identical.\nAdvice\nMain article: Advice (programming)\nThis is the additional code that you want to apply to your existing model. In our example, this is the logging code that we want to apply whenever the thread enters or exits a method.\nPointcut\nMain article: Pointcut\nThis is the term given to the point of execution in the application at which cross-cutting concern needs to be applied. In our example, a pointcut is reached when the thread enters a method, and another pointcut is reached when the thread exits the method.\nAspect\nMain article: Aspect (computer science)\nThe combination of the pointcut and the advice is termed an aspect. In the example above, we add a logging aspect to our application by defining a pointcut and giving the correct advice.\nComparison to other programming paradigms[edit]\nAspects emerged from object-oriented programming and computational reflection. AOP languages have functionality similar to, but more restricted than metaobject protocols. Aspects relate closely to programming concepts like subjects, mixins, and delegation. Other ways to use aspect-oriented programming paradigms include Composition Filters and the hyperslices approach. Since at least the 1970s, developers have been using forms of interception and dispatch-patching that resemble some of the implementation methods for AOP, but these never had the semantics that the crosscutting specifications provide written in one place.\n\nDesigners have considered alternative ways to achieve separation of code, such as C#'s partial types, but such approaches lack a quantification mechanism that allows reaching several join points of the code with one declarative statement.\n\nThough it may seem unrelated, in testing, the use of mocks or stubs requires the use of AOP techniques, like around advice, and so forth. Here the collaborating objects are for the purpose of the test, a cross cutting concern. Thus the various Mock Object frameworks provide these features. For example, a process invokes a service to get a balance amount. In the test of the process, where the amount comes from is unimportant, only that the process uses the balance according to the requirements.\n\nAdoption issues[edit]\nProgrammers need to be able to read code and understand what is happening in order to prevent errors.[10] Even with proper education, understanding crosscutting concerns can be difficult without proper support for visualizing both static structure and the dynamic flow of a program.[11] Beginning in 2002, AspectJ began to provide IDE plug-ins to support the visualizing of crosscutting concerns. Those features, as well as aspect code assist and refactoring are now common.\n\nGiven the power of AOP, if a programmer makes a logical mistake in expressing crosscutting, it can lead to widespread program failure. Conversely, another programmer may change the join points in a program – e.g., by renaming or moving methods – in ways that the aspect writer did not anticipate, with unforeseen consequences. One advantage of modularizing crosscutting concerns is enabling one programmer to affect the entire system easily; as a result, such problems present as a conflict over responsibility between two or more developers for a given failure. However, the solution for these problems can be much easier in the presence of AOP, since only the aspect needs to be changed, whereas the corresponding problems without AOP can be much more spread out.\n\nCriticism[edit]\nThe most basic criticism of the effect of AOP is that control flow is obscured, and is not only worse than the much-maligned GOTO, but is in fact closely analogous to the joke COME FROM statement. The obliviousness of application, which is fundamental to many definitions of AOP (the code in question has no indication that an advice will be applied, which is specified instead in the pointcut), means that the advice is not visible, in contrast to an explicit method call.[12][13] For example, compare the COME FROM program:[12]\n\n 5 input x\n10 print 'result is :'\n15 print x\n\n20 come from 10\n25      x = x * x\n30 return\nwith an AOP fragment with analogous semantics:\n\nmain() {\n    input x\n    print(result(x))\n}\ninput result(int x) { return x }\naround(int x): call(result(int)) && args(x) {\n    int temp = proceed(x)\n//    return temp * temp\n}\nIndeed, the pointcut may depend on runtime condition and thus not be statically deterministic. This can be mitigated but not solved by static analysis and IDE support showing which advices potentially match.\n\nGeneral criticisms are that AOP purports to improve \"both modularity and the structure of code\", but some counter that it instead undermines these goals and impedes \"independent development and understandability of programs\".[14] Specifically, quantification by pointcuts breaks modularity: \"one must, in general, have whole-program knowledge to reason about the dynamic execution of an aspect-oriented program.\"[15] Further, while its goals (modularizing cross-cutting concerns) are well-understood, its actual definition is unclear and not clearly distinguished from other well-established techniques.[14] Cross-cutting concerns potentially cross-cut each other, requiring some resolution mechanism, such as ordering.[14] Indeed, aspects can apply to themselves, leading to problems such as the liar paradox.[16]\n\nTechnical criticisms include that the quantification of pointcuts (defining where advices are executed) is \"extremely sensitive to changes in the program\", which is known as the fragile pointcut problem.[14] The problems with pointcuts are deemed intractable: if one replaces the quantification of pointcuts with explicit annotations, one obtains attribute-oriented programming instead, which is simply an explicit subroutine call and suffers the identical problem of scattering that AOP was designed to solve.[14]\n\nImplementations[edit]\nThe following programming languages have implemented AOP, within the language, or as an external library:\n\n.NET Framework languages (C# / VB.NET)[17]\nUnity, It provides an API to facilitate proven practices in core areas of programming including data access, security, logging, exception handling and others.\nActionScript[18]\nAda[19]\nAutoHotkey[20]\nC / C++[21]\nCOBOL[22]\nThe Cocoa Objective-C frameworks[23]\nColdFusion[24]\nCommon Lisp[25]\nDelphi[26][27][28]\nDelphi Prism[29]\ne (IEEE 1647)\nEmacs Lisp[30]\nGroovy\nHaskell[31]\nJava[32]\nAspectJ\nJavaScript[33]\nLogtalk[34]\nLua[35]\nmake[36]\nMatlab[37]\nML[38]\nPerl[39]\nPHP[40]\nProlog[41]\nPython[42]\nRacket[43]\nRuby[44][45][46]\nSqueak Smalltalk[47][48]\nUML 2.0[49]\nXML[50]\nSee also[edit]\nAspect-oriented software development\nDistributed AOP\nAttribute grammar, a formalism that can be used for aspect-oriented programming on top of functional programming languages\nProgramming paradigms\nSubject-oriented programming, an alternative to Aspect-oriented programming\nRole-oriented programming, an alternative to Aspect-oriented programming\nPredicate dispatch, an older alternative to Aspect-oriented programming\nExecutable UML\nDecorator pattern\nDomain-driven design",
          "subparadigms": []
        },
        {
          "pdid": 58,
          "name": "Role-oriented",
          "details": "Role-oriented programming is a form of computer programming aimed at expressing things in terms that are analogous to human conceptual understanding of the World. This should make programs easier to understand and maintain.[citation needed]\n\nThe main idea of role-oriented programming is that humans think in terms of roles. This claim is often backed up by examples of social relations. For example, a student attending a class and the same student at a party are the same person, yet he plays two different roles. In particular, the interactions of this person with the outside world depend on his current role. The roles typically share features, e.g., the intrinsic properties of being a person. This sharing of properties is often handled by the delegation mechanism.\n\nIn the older literature and in the field of databases, it seems that there has been little consideration for the context in which roles interplay with each other. Such a context is being established in newer role- and aspect-oriented programming languages such as Object Teams.\n\nMany researchers have argued the advantages of roles in modeling and implementation. Roles allow objects to evolve over time, they enable independent and concurrently existing views (interfaces) of the object, explicating the different contexts of the object, and separating concerns. Generally roles are a natural element of our daily concept forming. Roles in programming languages enable objects to have changing interfaces, as we see it in real life - things change over time, are used differently in different contexts, etc.\n\nContents  [hide] \n1\tAuthors of role literature\n2\tProgramming languages with explicit support for roles\n3\tSee also\n4\tReferences\n5\tExternal links\nAuthors of role literature[edit]\nBarbara Pernici\nBent Bruun Kristensen[1]\nBruce Wallace\nCharles Bachman[2]\nFriedrich Steimann\nGeorg Gottlob\nKasper B. Graversen\nKasper Østerbye\nStephan Herrmann\nTrygve Reenskaug[3]\nThomas Kühn\nProgramming languages with explicit support for roles[edit]\nChameleon\nEpsilonJ\nJavaScript Delegation - Functions as Roles (Traits and Mixins)\nObject Teams\nPerl 5 (Moose)\nPerl 6\npowerJava\nSCala ROLes Language\nSee also[edit]\nAspect-oriented programming\nData, context and interaction\nObject Oriented Role Analysis Method\nObject-role modeling\nSubject (programming)\nSubject-oriented programming\nTraits (computer science)",
          "subparadigms": []
        },
        {
          "pdid": 59,
          "name": "Subject-oriented",
          "details": "In computing, Subject-Oriented Programming is an object-oriented software paradigm in which the state (fields) and behavior (methods) of objects are not seen as intrinsic to the objects themselves, but are provided by various subjective perceptions (“subjects”) of the objects. The term and concepts were first published in September 1993 in a conference paper[1] which was later recognized as being one of the three most influential papers to be presented at the conference between 1986 and 1996.[2] As illustrated in that paper, an analogy is made with the contrast between the philosophical views of Plato and Kant with respect to the characteristics of “real” objects, but applied to software ones. For example, while we may all perceive a tree as having a measurable height, weight, leaf-mass, etc., from the point of view of a bird, a tree may also have measures of relative value for food or nesting purposes, or from the point of view of a tax-assessor, it may have a certain taxable value in a given year. Neither the bird’s nor the tax-assessor’s additional state information need be seen as intrinsic to the tree, but are added by the perceptions of the bird and tax-assessor, and from Kant’s analysis, the same may be true even of characteristics we think of as intrinsic.\n\nSubject-oriented programming advocates the organization of the classes that describe objects into “subjects”, which may be composed to form larger subjects. At points of access to fields or methods, several subjects’ contributions may be composed. These points were characterized as the join-points[3] of the subjects. For example, if a tree is cut-down, the methods involved may need to join behavior in the bird and tax-assessor’s subjects with that of the tree’s own. It is therefore fundamentally a view of the compositional nature of software development, as opposed to the algorithmic (procedural) or representation-hiding (object) nature.\n\nContents  [hide] \n1\tExamples\n2\tRelationship to aspect-oriented programming\n3\tRelationship to aspect-oriented software development\n4\tMulti-dimensional separation of concerns, Hyper/J, and the Concern Manipulation Environment\n5\tSubject-oriented programming as a \"third dimension\"\n6\tSee also\n7\tReferences\n8\tExternal links\nExamples[edit]\n[icon]\tThis section is empty. You can help by adding to it. (March 2012)\nRelationship to aspect-oriented programming[edit]\nThe introduction of aspect-oriented programming in 1997[4] raised questions about its relationship to subject-oriented programming, and about the difference between subjects and aspects. These questions were unanswered for some time, but were addressed in the patent on Aspect-oriented programming filed in 1999[5] in which two points emerge as characteristic differences from earlier art:\n\nthe aspect program comprises both a) a cross-cut that comprises a point in the execution where cross-cutting behavior is to be included; and b) a cross-cut action comprising a piece of implementation associated with the cross-cut, the piece of implementation comprising computer readable program code that implements the cross-cutting behavior.\nthe aspect transparently forces the cross-cutting behavior on object classes and other software entities\nIn the subject-oriented view, the cross-cut may be placed separately from the aspect (subject) and the behavior is not forced by the aspect, but governed by rules of composition. Hindsight[6] makes it also possible to distinguish aspect-oriented programming by its introduction and exploitation of the concept of a query-like pointcut to externally impose the join-points used by aspects in general ways.\n\nIn the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension. The use of externally imposed pointcuts is an important linguistic capability, but remains one of the most controversial features of aspect-oriented programming.[7]\n\nRelationship to aspect-oriented software development[edit]\nBy the turn of the millennium, it was clear that a number of research groups were pursuing different technologies that employed the composition or attachment of separately packaged state and function to form objects.[8] To distinguish the common field of interest from Aspect-Oriented Programming with its particular patent definitions and to emphasize that the compositional technology deals with more than just the coding phase of software development, these technologies were organized together under the term Aspect-Oriented Software Development,[9] and an organization and series on international conferences begun on the subject. Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches.\n\nMulti-dimensional separation of concerns, Hyper/J, and the Concern Manipulation Environment[edit]\nThe original formulation of subject-oriented programming deliberately envisioned it as a packaging technology – allowing the space of functions and data types to be extended in either dimension. The first implementations had been for C++,[10] and Smalltalk.[11] These implementations exploited the concepts of software labels and composition rules to describe the joining of subjects.\n\nTo address the concern that a better foundation should be provided for the analysis and composition of software not just in terms of its packaging but in terms of the various concerns these packages addressed, an explicit organization of the material was developed in terms of a multi-dimensional “matrix” in which concerns are related to the software units that implement them. This organization is called Multi-Dimensional Separation of Concerns, and the paper describing it[12] has been recognized as the most influential paper of the ICSE 1999 Conference[13]\n\nThis new concept was implemented for composing Java software, using the name Hyper/J for the tool.[14]\n\nComposition and the concept of subject can be applied to software artifacts that have no executable semantics, like requirement specifications or documentation. A research vehicle for Eclipse, called the Concern Manipulation Environment (CME), has been described[15] in which tools for query, analysis, modelling,[16] and composition are applied to artifacts in any language or representation, through the use of appropriate plug-in adapters to manipulate the representation.\n\nA successor to the Hyper/J composition engine[17] was developed as part of CME which uses a general approach for the several elements of a composition engine:\n\na query language with unification to identify join points,\na flexible structural-attachment model,\na nested-graph specification for ordering identified elements,\nand a priority ordering specification to resolve conflicts among conflicting rules.\nBoth Hyper/J and CME are available, from alphaWorks[18] or sourceforge,[19] respectively, but neither is actively supported.\n\nSubject-oriented programming as a \"third dimension\"[edit]\nMethod dispatch in object oriented programming can be thought of as \"two dimensional\" in the sense that the code executed depends on both the method name and the object in question. This can be contrasted[20] with procedural programming, where a procedure name resolves directly, or one dimensionally, onto a subroutine, and also to subject oriented programming, where the sender or subject is also relevant to dispatch, constituting a third dimension.\n\nSee also[edit]\nSeparation of concerns\nData, context and interaction",
          "subparadigms": []
        },
        {
          "pdid": 60,
          "name": "Prototype-based",
          "details": "Prototype-based programming is a style of object-oriented programming in which behaviour reuse (known as inheritance) is performed via a process of reusing existing objects via delegation that serve as prototypes. This model can also be known as prototypal, prototype-oriented, classless, or instance-based programming. Delegation is the language feature that supports prototype-based programming.\n\nA fruit bowl serves as one example. A \"fruit\" object would represent the properties and functionality of fruit in general. A \"banana\" object would be cloned from the \"fruit\" object, and would also be extended to include general properties specific to bananas. Each individual \"banana\" object would be cloned from the generic \"banana\" object.\n\nThe first prototype-oriented programming language was Self, developed by David Ungar and Randall Smith in the mid-1980s to research topics in object-oriented language design. Since the late 1990s, the classless paradigm has grown increasingly popular.[citation needed] Some current prototype-oriented languages are JavaScript (and other ECMAScript implementations, JScript and Flash's ActionScript 1.0), Lua, Cecil, NewtonScript, Io, Ioke, MOO, REBOL, Lisaac and AHk.\n\nContents  [hide] \n1\tDesign and implementation\n2\tObject construction\n3\tDelegation\n4\tConcatenation\n5\tCriticism\n6\tLanguages supporting prototype-based programming\n7\tSee also\n8\tReferences\n9\tFurther reading\nDesign and implementation[edit]\nPrototypal inheritance in JavaScript is described by Douglas Crockford as: you make prototype objects, and then … make new instances. Objects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects. We don't need classes to make lots of similar objects… Objects inherit from objects. What could be more object oriented than that?.[1]\n\nAdvocates of prototype-based programming argue that it encourages the programmer to focus on the behavior of some set of examples and only later worry about classifying these objects into archetypal objects that are later used in a fashion similar to classes.[2] Many prototype-based systems encourage the alteration of prototypes during run-time, whereas only very few class-based object-oriented systems (such as the dynamic object-oriented system, Common Lisp, Dylan, Objective-C, Perl, Python, Ruby, or Smalltalk) allow classes to be altered during the execution of a program.\n\nAlmost all prototype-based systems are based on interpreted and dynamically typed languages. Systems based on statically typed languages are technically feasible, however. The Omega language discussed in Prototype-Based Programming[3] is an example of such a system, though according to Omega's website even Omega is not exclusively static, but rather its \"compiler may choose to use static binding where this is possible and may improve the efficiency of a program.\"\n\nObject construction[edit]\nIn prototype-based languages there are no explicit classes. Objects inherit directly from other objects through a prototype property. The prototype property is called prototype in Self, proto in Io and __proto__ in JavaScript. There are two methods of constructing new objects: ex nihilo (\"from nothing\") object creation or through cloning an existing object. The former is supported through some form of object literal, declarations where objects can be defined at runtime through special syntax such as {...} and passed directly to a variable. While most systems support a variety of cloning, ex nihilo object creation is not as prominent.[4]\n\nIn class-based languages, a new instance is constructed through a class's constructor function, a special function that reserves a block of memory for the object's members (properties and methods) and returns a reference to that block. An optional set of constructor arguments can be passed to the function and are usually held in properties. The resulting instance will inherit all the methods and properties that were defined in the class, which acts as a kind of template from which similar typed objects can be constructed.\n\nSystems that support ex nihilo object creation allow new objects to be created from scratch without cloning from an existing prototype. Such systems provide a special syntax for specifying the properties and behaviors of new objects without referencing existing objects. In many prototype languages there exists a root object, often called Object, which is set as the default prototype for all other objects created in run-time and which carries commonly needed methods such as a toString() function to return a description of the object as a string. One useful aspect of ex nihilo object creation is to ensure that a new object's slot (properties and methods) names do not have namespace conflicts with the top-level Object object. (In the JavaScript language, one can do this by using a null prototype, i.e. Object.create(null).)\n\nCloning refers to a process whereby a new object is constructed by copying the behavior of an existing object (its prototype). The new object then carries all the qualities of the original. From this point on, the new object can be modified. In some systems the resulting child object maintains an explicit link (via delegation or resemblance) to its prototype, and changes in the prototype cause corresponding changes to be apparent in its clone. Other systems, such as the Forth-like programming language Kevo, do not propagate change from the prototype in this fashion, and instead follow a more concatenative model where changes in cloned objects do not automatically propagate across descendants.[2]\n\n// Example of true prototypal inheritance style \n// in JavaScript.\n\n// \"ex nihilo\" object creation using the literal \n// object notation {}.\nvar foo = {name: \"foo\", one: 1, two: 2};\n\n// Another \"ex nihilo\" object.\nvar bar = {two: \"two\", three: 3};\n\n// Object.setPrototypeOf() is a method introduced in ECMAScript 2015.\n// For the sake of simplicity, let us pretend \n// that the following line works regardless of the \n// engine used:\nObject.setPrototypeOf(bar, foo); // foo is now the prototype of bar.\n\n// If we try to access foo's properties from bar \n// from now on, we'll succeed. \nbar.one // Resolves to 1.\n\n// The child object's properties are also accessible.\nbar.three // Resolves to 3.\n\n// Own properties shadow prototype properties\nbar.two; // Resolves to \"two\"\nbar.name; // unaffected, resolves to \"foo\"\nfoo.name; // Resolves to \"foo\"\nThis example in JS 1.8.5 + ( see http://kangax.github.com/es5-compat-table/ )\n\nvar foo = {one: 1, two: 2};\n\n// bar.[[prototype]] = foo\nvar bar = Object.create( foo );\n\nbar.three = 3;\n\nbar.one; // 1\nbar.two; // 2\nbar.three; // 3\nDelegation[edit]\nIn prototype-based languages that use delegation, the language runtime is capable of dispatching the correct method or finding the right piece of data simply by following a series of delegation pointers (from object to its prototype) until a match is found. All that is required to establish this behavior-sharing between objects is the delegation pointer. Unlike the relationship between class and instance in class-based object-oriented languages, the relationship between the prototype and its offshoots does not require that the child object have a memory or structural similarity to the prototype beyond this link. As such, the child object can continue to be modified and amended over time without rearranging the structure of its associated prototype as in class-based systems. It is also important to note that not only data, but also methods can be added or changed. For this reason, some prototype-based languages refer to both data and methods as \"slots\" or \"members\".[citation needed]\n\nConcatenation[edit]\nUnder pure prototyping, which is also referred to as concatenative prototyping, and is exemplified in the Kevo language, there are no visible pointers or links to the original prototype from which an object is cloned. The prototype (parent) object is copied rather than linked to. As a result, changes to the prototype will not be reflected in cloned objects.[5]\n\nThe main conceptual difference under this arrangement is that changes made to a prototype object are not automatically propagated to clones. This may be seen as an advantage or disadvantage. (However, Kevo does provide additional primitives for publishing changes across sets of objects based on their similarity — so-called family resemblances or clone family mechanism[5] — rather than through taxonomic origin, as is typical in the delegation model.) It is also sometimes claimed that delegation-based prototyping has an additional disadvantage in that changes to a child object may affect the later operation of the parent. However, this problem is not inherent to the delegation-based model and does not exist in delegation-based languages such as JavaScript, which ensure that changes to a child object are always recorded in the child object itself and never in parents (i.e. the child's value shadows the parent's value rather than changing the parent's value).\n\nIn simplistic implementations, concatenative prototyping will have faster member lookup than delegation-based prototyping (because there is no need to follow the chain of parent objects), but will conversely use more memory (because all slots are copied, rather than there being a single slot pointing to the parent object). More sophisticated implementations can avoid these problems, however, although trade-offs between speed and memory are required. For example, systems with concatenative prototyping can use a copy-on-write implementation to allow for behind-the-scenes data sharing — and such an approach is indeed followed by Kevo.[6] Conversely, systems with delegation-based prototyping can use caching to speed up data lookup..\n\nCriticism[edit]\nAdvocates of class-based object models who criticize prototype-based systems often have concerns similar to the concerns that proponents of static type systems for programming languages have of dynamic type systems (see datatype). Usually, such concerns involve: correctness, safety, predictability, efficiency and programmer unfamiliarity.\n\nOn the first three points, classes are often seen as analogous to types (in most statically typed object-oriented languages they serve that role) and are proposed to provide contractual guarantees to their instances, and to users of their instances, that they will behave in some given fashion.\n\nRegarding efficiency, declaring classes simplifies many compiler optimizations that allow developing efficient method and instance-variable lookup. For the Self language, much development time was spent on developing, compiling, and interpreting techniques to improve the performance of prototype-based systems versus class-based systems.\n\nA common criticism made against prototype-based languages is that the community of software developers is unfamiliar with them, despite the popularity and market permeation of JavaScript. This knowledge level of prototype-based systems seems to be increasing with the proliferation of JavaScript frameworks and the complex use of JavaScript as the Web matures.[citation needed] ECMAScript 6 introduced classes as syntactic sugar over JavaScript's existing prototype-based inheritance, providing an alternative way to create objects and deal with inheritance.[7]\n\nLanguages supporting prototype-based programming[edit]\nActor-Based Concurrent Language (ABCL): ABCL/1, ABCL/R, ABCL/R2, ABCL/c+\nAgora\nAutoHotkey\nCecil and Diesel of Craig Chambers\nColdC\nCOLA\nCommon Lisp\nECMAScript\nActionScript 1.0, used by Adobe Flash and Adobe Flex\nE4X\nJavaScript\nJScript\nFalcon\nIo\nIoke\nLisaac\nLogtalk\nLPC\nLua\nMaple\nMOO\nNeko\nNewtonScript\nObject Lisp\nObliq\nOmega\nOpenLaszlo\nPerl, with the Class::Prototyped module\nPython with prototype.py.\nR, with the proto package\nREBOL\nSelf\nSeph\nSmartFrog\nTADS\nTcl with snit extension\nUmajin[8]",
          "subparadigms": []
        },
        {
          "pdid": 61,
          "name": "Concurrent object-oriented",
          "details": "Concurrent object-oriented programming is a programming paradigm which combines object-oriented programming (OOP) together with concurrency. While numerous programming languages, such as Java, combine OOP with concurrency mechanisms like threads, the phrase \"concurrent object-oriented programming\" primarily refers to systems where objects themselves are a concurrency primitive, such as when objects are combined with the actor model.",
          "subparadigms": []
        },
        {
          "pdid": 62,
          "name": "Class-based",
          "details": "Class-based programming, or more commonly class-orientation, is a style of object-oriented programming (OOP) in which inheritance is achieved by defining classes of objects, as opposed to the objects themselves (compare prototype-based programming).\n\nThe most popular and developed model of OOP is a class-based model, as opposed to an object-based model. In this model, objects are entities that combine state (i.e. data), behavior (i.e. procedures, or methods) and identity (unique existence among all other objects). The structure and behavior of an object are defined by a class, which is a definition, or blueprint, of all objects of a specific type. An object must be explicitly created based on a class and an object thus created is considered to be an instance of that class. An object is similar to a structure, with the addition of method pointers, member access control, and an implicit data member which locates instances of the class (i.e. actual objects of that class) in the class hierarchy (essential for runtime inheritance features).\n\nContents  [hide] \n1\tEncapsulation\n2\tInheritance\n3\tCritique of class-based models\n4\tExample languages\n5\tSee also\n6\tReferences\nEncapsulation[edit]\nEncapsulation prevents users from breaking the invariants of the class, which is useful because it allows the implementation of a class of objects to be changed for aspects not exposed in the interface without impact to user code. The definitions of encapsulation focus on the grouping and packaging of related information (cohesion) rather than security issues. OOP languages do not normally offer formal security restrictions to the internal object state. Using a method of access is a matter of convention for the interface design.\n\nInheritance[edit]\nMain article: Inheritance\nIn class-based programming, inheritance is done by defining new classes as extensions of existing classes: the existing class is the parent class and the new class is the child class. If a child class has only one parent class, this is known as single inheritance, while if a child class can have more than one parent class, this is known as multiple inheritance. This organizes classes into a hierarchy, either a tree (if single inheritance) or lattice (if multiple inheritance).\n\nThe defining feature of inheritance is that both interface and implementation are inherited; if only interface is inherited, this is known as interface inheritance or subtyping. Inheritance can also be done without classes, as in prototype-based programming.\n\nCritique of class-based models[edit]\nClass-based languages, or, to be more precise, typed languages, where subclassing is the only way of subtyping, have been criticized for mixing up implementations and interfaces—the essential principle in object-oriented programming. The critics say one might create a bag class that stores a collection of objects, then extend it to make a new class called a set class where the duplication of objects is eliminated.[1][2] Now, a function that takes an object of the bag class may expect that adding two objects increases the size of a bag by two, yet if one passes an object of a set class, then adding two objects may or may not increase the size of a bag by two. The problem arises precisely because subclassing implies subtyping even in the instances where the principle of subtyping, known as the Liskov substitution principle, does not hold. Barbara Liskov and Jeannette Wing formulated the principle succinctly in a 1994 paper as follows:\n\nSubtype Requirement: Let {\\displaystyle \\phi (x)} \\phi (x) be a property provable about objects x of type T. Then {\\displaystyle \\phi (y)} {\\displaystyle \\phi (y)} should be true for objects y of type S where S is a subtype of T.\n\nTherefore normally one must distinguish subtyping and subclassing. Most current object-oriented languages distinguish subtyping and subclassing, however some approaches to design do not.\n\nAlso, another common example is that a person object created from a child class cannot become an object of parent class because a child class and a parent class inherit a person class but class-based languages mostly do not allow to change the kind of class of the object at runtime. For class-based languages, this restriction is essential in order to preserve unified view of the class to its users. The users should not need to care whether one of the implementations of a method happens to cause changes that break the invariants of the class. Such changes can be made by destroying the object and constructing another in its place. Polymorphism can be used to preserve the relevant interfaces even when such changes are done, because the objects are viewed as black box abstractions and accessed via object identity. However, usually the value of object references referring to the object is changed, which causes effects to client code.\n\nExample languages[edit]\nSee also: Category:Class-based programming languages\nAlthough Simula introduced the class abstraction, the canonical example of a class-based language is Smalltalk. Others include PHP, C++, Java, C#, and Objective-C.\n\nSee also[edit]\nPrototype-based programming (contrast)\nProgramming paradigms\nClass (computer programming)",
          "subparadigms": []
        },
        {
          "pdid": 63,
          "name": "Actor-based",
          "details": "The actor model in computer science is a mathematical model of concurrent computation that treats \"actors\" as the universal primitives of concurrent computation. In response to a message that it receives, an actor can: make local decisions, create more actors, send more messages, and determine how to respond to the next message received. Actors may modify private state, but can only affect each other through messages (avoiding the need for any locks).\n\nThe actor model originated in 1973.[1] It has been used both as a framework for a theoretical understanding of computation and as the theoretical basis for several practical implementations of concurrent systems. The relationship of the model to other work is discussed in Indeterminacy in concurrent computation and Actor model and process calculi.\n\nContents  [hide] \n1\tHistory\n2\tFundamental concepts\n3\tFormal systems\n4\tApplications\n5\tMessage-passing semantics\n5.1\tUnbounded nondeterminism controversy\n5.2\tDirect communication and asynchrony\n5.3\tActor creation plus addresses in messages means variable topology\n5.4\tInherently concurrent\n5.5\tNo requirement on order of message arrival\n5.6\tLocality\n5.7\tComposing Actor Systems\n5.8\tBehaviors\n5.9\tModeling other concurrency systems\n5.10\tComputational Representation Theorem\n5.11\tRelationship to logic programming\n5.12\tMigration\n5.13\tSecurity\n5.14\tSynthesizing addresses of actors\n5.15\tContrast with other models of message-passing concurrency\n6\tInfluence\n6.1\tTheory\n6.2\tPractice\n7\tCurrent issues\n8\tEarly Actor researchers\n9\tProgramming with Actors\n9.1\tEarly Actor programming languages\n9.2\tLater Actor programming languages\n9.3\tActor libraries and frameworks\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\n13.1\tVideos\n13.2\tArticles\n13.3\tProcedural Libraries\nHistory[edit]\nMain article: History of the Actor model\nAccording to Carl Hewitt, unlike previous models of computation, the Actor model was inspired by physics, including general relativity and quantum mechanics. It was also influenced by the programming languages Lisp, Simula and early versions of Smalltalk, as well as capability-based systems and packet switching. Its development was \"motivated by the prospect of highly parallel computing machines consisting of dozens, hundreds, or even thousands of independent microprocessors, each with its own local memory and communications processor, communicating via a high-performance communications network.\"[2] Since that time, the advent of massive concurrency through multi-core and manycore computer architectures has revived interest in the Actor model.\n\nFollowing Hewitt, Bishop, and Steiger's 1973 publication, Irene Greif developed an operational semantics for the Actor model as part of her doctoral research.[3] Two years later, Henry Baker and Hewitt published a set of axiomatic laws for Actor systems.[4][5] Other major milestones include William Clinger's 1981 dissertation introducing a denotational semantics based on power domains[2] and Gul Agha's 1985 dissertation which further developed a transition-based semantic model complementary to Clinger's.[6] This resulted in the full development of actor model theory.\n\nMajor software implementation work was done by Russ Atkinson, Giuseppe Attardi, Henry Baker, Gerry Barber, Peter Bishop, Peter de Jong, Ken Kahn, Henry Lieberman, Carl Manning, Tom Reinhardt, Richard Steiger and Dan Theriault in the Message Passing Semantics Group at Massachusetts Institute of Technology (MIT). Research groups led by Chuck Seitz at California Institute of Technology (Caltech) and Bill Dally at MIT constructed computer architectures that further developed the message passing in the model. See Actor model implementation.\n\nResearch on the Actor model has been carried out at California Institute of Technology, Kyoto University Tokoro Laboratory, MCC, MIT Artificial Intelligence Laboratory, SRI, Stanford University, University of Illinois at Urbana-Champaign,[7] Pierre and Marie Curie University (University of Paris 6), University of Pisa, University of Tokyo Yonezawa Laboratory, Centrum Wiskunde & Informatica (CWI) and elsewhere.\n\nFundamental concepts[edit]\nThe Actor model adopts the philosophy that everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages.\n\nAn actor is a computational entity that, in response to a message it receives, can concurrently:\n\nsend a finite number of messages to other actors;\ncreate a finite number of new actors;\ndesignate the behavior to be used for the next message it receives.\nThere is no assumed sequence to the above actions and they could be carried out in parallel.\n\nDecoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages.[8]\n\nRecipients of messages are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created.\n\nThe Actor model is characterized by inherent concurrency of computation within and among actors, dynamic creation of actors, inclusion of actor addresses in messages, and interaction only through direct asynchronous message passing with no restriction on message arrival order.\n\nFormal systems[edit]\nOver the years, several different formal systems have been developed which permit reasoning about systems in the Actor model. These include:\n\nOperational semantics[3][9]\nLaws for Actor systems[4]\nDenotational semantics[2][10]\nTransition semantics[6]\nThere are also formalisms that are not fully faithful to the Actor model in that they do not formalize the guaranteed delivery of messages including the following (See Attempts to relate Actor semantics to algebra and linear logic):\n\nSeveral different Actor algebras[11][12][13]\nLinear logic[14]\nApplications[edit]\n\nThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)\nThe Actor model can be used as a framework for modeling, understanding, and reasoning about a wide range of concurrent systems. For example:\n\nElectronic mail (e-mail) can be modeled as an Actor system. Accounts are modeled as Actors and email addresses as Actor addresses.\nWeb Services can be modeled with SOAP endpoints modeled as Actor addresses.\nObjects with locks (e.g., as in Java and C#) can be modeled as a Serializer, provided that their implementations are such that messages can continually arrive (perhaps by being stored in an internal queue). A serializer is an important kind of Actor defined by the property that it is continually available to the arrival of new messages; every message sent to a serializer is guaranteed to arrive.\nTesting and Test Control Notation (TTCN), both TTCN-2 and TTCN-3, follows Actor model rather closely. In TTCN, Actor is a test component: either parallel test component (PTC) or main test component (MTC). Test components can send and receive messages to and from remote partners (peer test components or test system interface), the latter being identified by its address. Each test component has a behaviour tree bound to it; test components run in parallel and can be dynamically created by parent test components. Built-in language constructs allow the definition of actions to be taken when an expected message is received from the internal message queue, like sending a message to another peer entity or creating new test components.\nMessage-passing semantics[edit]\nThe Actor model is about the semantics of message passing.\n\nUnbounded nondeterminism controversy[edit]\nArguably, the first concurrent programs were interrupt handlers. During the course of its normal operation a computer needed to be able to receive information from outside (characters from a keyboard, packets from a network, etc). So when the information arrived the execution of the computer was \"interrupted\" and special code called an interrupt handler was called to put the information in a buffer where it could be subsequently retrieved.\n\nIn the early 1960s, interrupts began to be used to simulate the concurrent execution of several programs on a single processor.[15] Having concurrency with shared memory gave rise to the problem of concurrency control. Originally, this problem was conceived as being one of mutual exclusion on a single computer. Edsger Dijkstra developed semaphores and later, between 1971 and 1973,[16] Tony Hoare[17] and Per Brinch Hansen[18] developed monitors to solve the mutual exclusion problem. However, neither of these solutions provided a programming-language construct that encapsulated access to shared resources. This encapsulation was later accomplished by the serializer construct ([Hewitt and Atkinson 1977, 1979] and [Atkinson 1980]).\n\nThe first models of computation (e.g., Turing machines, Post productions, the lambda calculus, etc.) were based on mathematics and made use of a global state to represent a computational step (later generalized in [McCarthy and Hayes 1969] and [Dijkstra 1976] see Event orderings versus global state). Each computational step was from one global state of the computation to the next global state. The global state approach was continued in automata theory for finite state machines and push down stack machines, including their nondeterministic versions. Such nondeterministic automata have the property of bounded nondeterminism; that is, if a machine always halts when started in its initial state, then there is a bound on the number of states in which it halts.\n\nEdsger Dijkstra further developed the nondeterministic global state approach. Dijkstra's model gave rise to a controversy concerning unbounded nondeterminism (also called unbounded indeterminacy), a property of concurrency by which the amount of delay in servicing a request can become unbounded as a result of arbitration of contention for shared resources while still guaranteeing that the request will eventually be serviced. Hewitt argued that the Actor model should provide the guarantee of service. In Dijkstra's model, although there could be an unbounded amount of time between the execution of sequential instructions on a computer, a (parallel) program that started out in a well defined state could terminate in only a bounded number of states [Dijkstra 1976]. Consequently, his model could not provide the guarantee of service. Dijkstra argued that it was impossible to implement unbounded nondeterminism.\n\nHewitt argued otherwise: there is no bound that can be placed on how long it takes a computational circuit called an arbiter to settle (see metastability in electronics).[19] Arbiters are used in computers to deal with the circumstance that computer clocks operate asynchronously with respect to input from outside, e.g., keyboard input, disk access, network input, etc. So it could take an unbounded time for a message sent to a computer to be received and in the meantime the computer could traverse an unbounded number of states.\n\nThe Actor Model features unbounded nondeterminism which was captured in a mathematical model by Will Clinger using domain theory.[2] There is no global state in the Actor model.[dubious – discuss]\n\nDirect communication and asynchrony[edit]\nMessages in the Actor model are not necessarily buffered. This was a sharp break with previous approaches to models of concurrent computation. The lack of buffering caused a great deal of misunderstanding at the time of the development of the Actor model and is still a controversial issue. Some researchers argued that the messages are buffered in the \"ether\" or the \"environment\". Also, messages in the Actor model are simply sent (like packets in IP); there is no requirement for a synchronous handshake with the recipient.\n\nActor creation plus addresses in messages means variable topology[edit]\nA natural development of the Actor model was to allow addresses in messages. Influenced by packet switched networks [1961 and 1964], Hewitt proposed the development of a new model of concurrent computation in which communications would not have any required fields at all: they could be empty. Of course, if the sender of a communication desired a recipient to have access to addresses which the recipient did not already have, the address would have to be sent in the communication.\n\nFor example, an Actor might need to send a message to a recipient Actor from which it later expects to receive a response, but the response will actually be handled by a third Actor component that has been configured to receive and handle the response (for example, a different Actor implementing the Observer pattern). The original Actor could accomplish this by sending a communication that includes the message it wishes to send, along with the address of the third Actor that will handle the response. This third Actor that will handle the response is called the resumption (sometimes also called a continuation or stack frame). When the recipient Actor is ready to send a response, it sends the response message to the resumption Actor address that was included in the original communication.\n\nSo, the ability of Actors to create new Actors with which they can exchange communications, along with the ability to include the addresses of other Actors in messages, gives Actors the ability to create and participate in arbitrarily variable topological relationships with one another, much as the objects in Simula and other object-oriented languages may also be relationally composed into variable topologies of message-exchanging objects.\n\nInherently concurrent[edit]\nAs opposed to the previous approach based on composing sequential processes, the Actor model was developed as an inherently concurrent model. In the Actor model sequentiality was a special case that derived from concurrent computation as explained in Actor model theory.\n\nNo requirement on order of message arrival[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nHewitt argued against adding the requirement that messages must arrive in the order in which they are sent to the Actor. If output message ordering is desired, then it can be modeled by a queue Actor that provides this functionality. Such a queue Actor would queue the messages that arrived so that they could be retrieved in FIFO order. So if an Actor X sent a message M1 to an Actor Y, and later X sent another message M2 to Y, there is no requirement that M1 arrives at Y before M2.\n\nIn this respect the Actor model mirrors packet switching systems which do not guarantee that packets must be received in the order sent. Not providing the order of delivery guarantee allows packet switching to buffer packets, use multiple paths to send packets, resend damaged packets, and to provide other optimizations.\n\nFor example, Actors are allowed to pipeline the processing of messages. What this means is that in the course of processing a message M1, an Actor can designate the behavior to be used to process the next message, and then in fact begin processing another message M2 before it has finished processing M1. Just because an Actor is allowed to pipeline the processing of messages does not mean that it must pipeline the processing. Whether a message is pipelined is an engineering tradeoff. How would an external observer know whether the processing of a message by an Actor has been pipelined? There is no ambiguity in the definition of an Actor created by the possibility of pipelining. Of course, it is possible to perform the pipeline optimization incorrectly in some implementations, in which case unexpected behavior may occur.\n\nLocality[edit]\nAnother important characteristic of the Actor model is locality.\n\nLocality means that in processing a message, an Actor can send messages only to addresses that it receives in the message, addresses that it already had before it received the message, and addresses for Actors that it creates while processing the message. (But see Synthesizing Addresses of Actors.)\n\nAlso locality means that there is no simultaneous change in multiple locations. In this way it differs from some other models of concurrency, e.g., the Petri net model in which tokens are simultaneously removed from multiple locations and placed in other locations.\n\nComposing Actor Systems[edit]\nThe idea of composing Actor systems into larger ones is an important aspect of modularity that was developed in Gul Agha's doctoral dissertation,[6] developed later by Gul Agha, Ian Mason, Scott Smith, and Carolyn Talcott.[9]\n\nBehaviors[edit]\nA key innovation was the introduction of behavior specified as a mathematical function to express what an Actor does when it processes a message, including specifying a new behavior to process the next message that arrives. Behaviors provided a mechanism to mathematically model the sharing in concurrency.\n\nBehaviors also freed the Actor model from implementation details, e.g., the Smalltalk-72 token stream interpreter. However, it is critical to understand that the efficient implementation of systems described by the Actor model require extensive optimization. See Actor model implementation for details.\n\nModeling other concurrency systems[edit]\nOther concurrency systems (e.g., process calculi) can be modeled in the Actor model using a two-phase commit protocol.[20]\n\nComputational Representation Theorem[edit]\nSee also: Denotational semantics of the Actor model\nThere is a Computational Representation Theorem in the Actor model for systems which are closed in the sense that they do not receive communications from outside. The mathematical denotation denoted by a closed system S is constructed from an initial behavior ⊥S and a behavior-approximating function progressionS. These obtain increasingly better approximations and construct a denotation (meaning) for S as follows [Hewitt 2008; Clinger 1981]:\n\n{\\displaystyle \\mathbf {Denote} _{\\mathtt {S}}\\equiv \\lim _{i\\to \\infty }\\mathbf {progression} _{{\\mathtt {S}}^{i}}(\\bot _{\\mathtt {S}})} {\\displaystyle \\mathbf {Denote} _{\\mathtt {S}}\\equiv \\lim _{i\\to \\infty }\\mathbf {progression} _{{\\mathtt {S}}^{i}}(\\bot _{\\mathtt {S}})}\nIn this way, S can be mathematically characterized in terms of all its possible behaviors (including those involving unbounded nondeterminism). Although DenoteS is not an implementation of S, it can be used to prove a generalization of the Church-Turing-Rosser-Kleene thesis [Kleene 1943]:\n\nA consequence of the above theorem is that a finite Actor can nondeterministically respond with an uncountable[clarify] number of different outputs.\n\nRelationship to logic programming[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nOne of the key motivations for the development of the actor model was to understand and deal with the control structure issues that arose in development of the Planner programming language.[citation needed] Once the actor model was initially defined, an important challenge was to understand the power of the model relative to Robert Kowalski's thesis that \"computation can be subsumed by deduction\". Hewitt argued that Kowalski's thesis turned out to be false for the concurrent computation in the actor model (see Indeterminacy in concurrent computation).\n\nNevertheless, attempts were made to extend logic programming to concurrent computation. However, Hewitt and Agha [1991] claimed that the resulting systems were not deductive in the following sense: computational steps of the concurrent logic programming systems do not follow deductively from previous steps (see Indeterminacy in concurrent computation). Recently, logic programming has been integrated into the actor model in a way that maintains logical semantics.[19]\n\nMigration[edit]\nMigration in the Actor model is the ability of Actors to change locations. E.g., in his dissertation, Aki Yonezawa modeled a post office that customer Actors could enter, change locations within while operating, and exit. An Actor that can migrate can be modeled by having a location Actor that changes when the Actor migrates. However the faithfulness of this modeling is controversial and the subject of research.[citation needed]\n\nSecurity[edit]\nThe security of Actors can be protected in the following ways:\n\nhardwiring in which Actors are physically connected\ncomputer hardware as in Burroughs B5000, Lisp machine, etc.\nvirtual machines as in Java virtual machine, Common Language Runtime, etc.\noperating systems as in capability-based systems\nsigning and/or encryption of Actors and their addresses\nSynthesizing addresses of actors[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nA delicate point in the Actor model is the ability to synthesize the address of an Actor. In some cases security can be used to prevent the synthesis of addresses (see Security). However, if an Actor address is simply a bit string then clearly it can be synthesized although it may be difficult or even infeasible to guess the address of an Actor if the bit strings are long enough. SOAP uses a URL for the address of an endpoint where an Actor can be reached. Since a URL is a character string, it can clearly be synthesized although encryption can make it virtually impossible to guess.\n\nSynthesizing the addresses of Actors is usually modeled using mapping. The idea is to use an Actor system to perform the mapping to the actual Actor addresses. For example, on a computer the memory structure of the computer can be modeled as an Actor system that does the mapping. In the case of SOAP addresses, it's modeling the DNS and the rest of the URL mapping.\n\nContrast with other models of message-passing concurrency[edit]\nRobin Milner's initial published work on concurrency[21] was also notable in that it was not based on composing sequential processes. His work differed from the Actor model because it was based on a fixed number of processes of fixed topology communicating numbers and strings using synchronous communication. The original Communicating Sequential Processes model[22] published by Tony Hoare differed from the Actor model because it was based on the parallel composition of a fixed number of sequential processes connected in a fixed topology, and communicating using synchronous message-passing based on process names (see Actor model and process calculi history). Later versions of CSP abandoned communication based on process names in favor of anonymous communication via channels, an approach also used in Milner's work on the Calculus of Communicating Systems and the π-calculus.\n\nThese early models by Milner and Hoare both had the property of bounded nondeterminism. Modern, theoretical CSP ([Hoare 1985] and [Roscoe 2005]) explicitly provides unbounded nondeterminism.\n\nPetri nets and their extensions (e.g., coloured Petri nets) are like actors in that they are based on asynchronous message passing and unbounded nondeterminism, while they are like early CSP in that they define fixed topologies of elementary processing steps (transitions) and message repositories (places).\n\nInfluence[edit]\nThe Actor Model has been influential on both theory development and practical software development.\n\nTheory[edit]\nThe Actor Model has influenced the development of the Pi-calculus and subsequent Process calculi. In his Turing lecture, Robin Milner wrote:[23]\n\nNow, the pure lambda-calculus is built with just two kinds of thing: terms and variables. Can we achieve the same economy for a process calculus? Carl Hewitt, with his Actors model, responded to this challenge long ago; he declared that a value, an operator on values, and a process should all be the same kind of thing: an Actor.\nThis goal impressed me, because it implies the homogeneity and completeness of expression ... But it was long before I could see how to attain the goal in terms of an algebraic calculus...\nSo, in the spirit of Hewitt, our first step is to demand that all things denoted by terms or accessed by names--values, registers, operators, processes, objects--are all of the same kind of thing; they should all be processes.\nPractice[edit]\nThe Actor Model has had extensive influence on commercial practice. For example, Twitter has used actors for scalability.[24] Also, Microsoft has used the Actor Model in the development of its Asynchronous Agents Library.[25] There are numerous other Actor libraries listed in the Actor Libraries and Frameworks section below.\n\nCurrent issues[edit]\nAccording to Hewitt [2006], the Actor model addresses issues in computer and communications architecture, concurrent programming languages, and Web Services including the following:\n\nscalability: the challenge of scaling up concurrency both locally and nonlocally.\ntransparency: bridging the chasm between local and nonlocal concurrency. Transparency is currently a controversial issue. Some researchers[who?] have advocated a strict separation between local concurrency using concurrent programming languages (e.g., Java and C#) from nonlocal concurrency using SOAP for Web services. Strict separation produces a lack of transparency that causes problems when it is desirable/necessary to change between local and nonlocal access to Web Services (see distributed computing).\ninconsistency: Inconsistency is the norm because all very large knowledge systems about human information system interactions are inconsistent. This inconsistency extends to the documentation and specifications of very large systems (e.g., Microsoft Windows software, etc.), which are internally inconsistent.\nMany of the ideas introduced in the Actor model are now also finding application in multi-agent systems for these same reasons [Hewitt 2006b 2007b]. The key difference is that agent systems (in most definitions) impose extra constraints upon the Actors, typically requiring that they make use of commitments and goals.\n\nThe Actor model is also being applied to client cloud computing.[26]\n\nEarly Actor researchers[edit]\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2012) (Learn how and when to remove this template message)\nGnome-searchtool.svg\nThis article's factual accuracy is disputed. Please help to ensure that disputed statements are reliably sourced. See the relevant discussion on the talk page. (March 2012) (Learn how and when to remove this template message)\nThere is a growing community of researchers working on the Actor Model as it is becoming commercially more important. Early Actor researchers included:\n\nImportant contributions to the semantics of Actors have been made by: Gul Agha, Beppe Attardi, Henry Baker, Will Clinger, Irene Greif, Carl Hewitt, Carl Manning, Ian Mason, Ugo Montanari, Maria Simi, Scott Smith, Carolyn Talcott, Prasanna Thati, and Akinori Yonezawa.\nImportant contributions to the implementation of Actors have been made by: Bill Athas, Russ Atkinson, Beppe Attardi, Henry Baker, Gerry Barber, Peter Bishop, Nanette Boden, Jean-Pierre Briot, Bill Dally, Peter de Jong, Jessie Dedecker, Travis Desell, Ken Kahn, Carl Hewitt, Henry Lieberman, Carl Manning, Tom Reinhardt, Chuck Seitz, Richard Steiger, Dan Theriault, Mario Tokoro, Carlos Varela, Darrell Woelk.\nProgramming with Actors[edit]\nA number of different programming languages employ the Actor model or some variation of it. These languages include:\n\nEarly Actor programming languages[edit]\nAct 1, 2 and 3[27][28]\nActtalk[29]\nAni[30]\nCantor[31]\nRosette[32]\nLater Actor programming languages[edit]\nABCL\nAmbientTalk[33]\nAxum[34]\nCAL Actor Language\nD\nE\nElixir\nErlang\nFantom\nHumus[35]\nIo\nLFE\nEncore[36]\nPony[37]\nPtolemy Project\nRebeca Modeling Language\nReia\nSALSA[38]\nScala[39][40]\nScratch\nTNSDL\nActor libraries and frameworks[edit]\nActor libraries or frameworks have also been implemented to permit actor-style programming in languages that don't have actors built-in. Among these frameworks are:\n\nName\tStatus\tLatest release\tLicense\tLanguages\nAojet\tActive\t2016-10-17\tMIT\tSwift\nActor\tActive\t2013-05-31\tMIT\tJava\nVert.x\tActive\t2016-09-12\tApache 2.0\tJava, Groovy, Javascript, Ruby, Scala\nActor Framework\tActive\t2013-11-13\tApache 2.0\t.NET\nAkka (toolkit)\tActive\t2016-08-19\tApache 2.0\tJava and Scala\nAkka.NET\tActive\t2016-01-18\tApache 2.0\t.NET\nRemact.Net\tActive\t2016-06-26\tMIT\t.NET, Javascript\nAteji PX\tActive\t ?\t ?\tJava\nF# MailboxProcessor\tActive\tsame as F# (built-in core library)\tApache License\tF#\nKorus\tActive\t2010-02-04\tGPL 3\tJava\nKilim[41]\tActive\t2011-10-13[42]\tMIT\tJava\nActorFoundry (based on Kilim)\tActive\t2008-12-28\t ?\tJava\nActorKit\tActive\t2011-09-13[43]\tBSD\tObjective-C\nCloud Haskell\tActive\t2015-06-17[44]\tBSD\tHaskell\nCloudI\tActive\t2015-12-24[45]\tBSD\tC/C++, Elixir/Erlang/LFE, Java, Javascript, Perl, PHP, Python, Ruby\nNAct\tActive\t2012-02-28\tLGPL 3.0\t.NET\nRetlang\tActive\t2011-05-18[46]\tNew BSD\t.NET\nJActor\tActive\t2013-01-22\tLGPL\tJava\nJetlang\tActive\t2013-05-30[47]\tNew BSD\tJava\nHaskell-Actor\tActive?\t2008\tNew BSD\tHaskell\nGPars\tActive\t2014-05-09[48]\tApache 2.0\tGroovy\nOOSMOS\tActive\t2016-02-17[49]\tGPL 2.0 and commercial (dual licensing)\tC. C++ friendly\nPanini\tActive\t2014-05-22\tMPL 1.1\tProgramming Language by itself\nPARLEY\tActive?\t2007-22-07\tGPL 2.1\tPython\nPeernetic\tActive\t2007-06-29\tLGPL 3.0\tJava\nPostSharp\tActive\t2014-09-24\tCommercial / Freemium\t.NET\nPulsar\tActive\t2016-07-09[50]\tNew BSD\tPython\nPulsar\tActive\t2016-02-18[51]\tLGPL/Eclipse\tClojure\nPykka\tActive\t2015-07-20[52]\tApache 2.0\tPython\nTermite Scheme\tActive?\t2009-05-21\tLGPL\tScheme (Gambit implementation)\nTheron\tActive\t2014-01-18[53]\tMIT[54]\tC++\nThespian\tActive\t2016-02-11\tGoDaddy Public Release[55]\tPython\nQuasar\tActive\t2016-01-18[56]\tLGPL/Eclipse\tJava\nLibactor\tActive?\t2009\tGPL 2.0\tC\nActor-CPP\tActive\t2012-03-10[57]\tGPL 2.0\tC++\nS4\tActive\t2012-07-31[58]\tApache 2.0\tJava\nC++ Actor Framework (CAF)\tActive\t2016-03-14[59]\tBoost Software License 1.0 and BSD 3-Clause\tC++11\nCelluloid\tActive\t2016-01-19[60]\tMIT\tRuby\nLabVIEW Actor Framework\tActive\t2012-03-01[61]\tNational Instruments SLA\tLabVIEW\nLabVIEW Messenger Library\tActive\t2016-06-01\tBSD\tLabVIEW\nOrbit\tActive\t2016-04-22[62]\tNew BSD\tJava\nQP frameworks for real-time embedded systems\tActive\t2015-09-29[63]\tGPL 2.0 and commercial (dual licensing)\tC and C++\nlibprocess\tActive\t2013-06-19\tApache 2.0\tC++\nSObjectizer\tActive\t2016-09-29\tNew BSD\tC++11\nOrleans\tActive\t2016-05-18[64]\tMIT License\tC#/.NET\nSkynet\tActive\t2016-07-11\tMIT License\tC/Lua\nReactors.IO\tActive\t2016-06-14\tBSD License\tJava/Scala\nPlease note that not all frameworks and libraries are listed here.\n\nSee also[edit]\nActor model theory\nActor model early history\nActor model and process calculi\nActor model implementation\nData flow\nMulti-agent system\nGordon Pask\nScientific Community Metaphor\nCommunicating sequential processes\nInput/output automaton",
          "subparadigms": []
        },
        {
          "pdid": 64,
          "name": "Object-oriented",
          "details": "Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated (objects have a notion of \"this\" or \"self\"). In OOP, computer programs are designed by making them out of objects that interact with one another.[1][2] There is significant diversity of OOP languages, but the most popular ones are class-based, meaning that objects are instances of classes, which typically also determine their type.\n\nMany of the most widely used programming languages are multi-paradigm programming languages that support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include Java, C++, C#, Python, PHP, Ruby, Perl, Delphi, Objective-C, Swift, Common Lisp, and Smalltalk.\n\nContents  [hide] \n1\tFeatures\n1.1\tShared with non-OOP predecessor languages\n1.2\tObjects and classes\n1.3\tDynamic dispatch/message passing\n1.4\tEncapsulation\n1.5\tComposition, inheritance, and delegation\n1.6\tPolymorphism\n1.7\tOpen recursion\n2\tHistory\n3\tOOP languages\n3.1\tOOP in dynamic languages\n3.2\tOOP in a network protocol\n4\tDesign patterns\n4.1\tInheritance and behavioral subtyping\n4.2\tGang of Four design patterns\n4.3\tObject-orientation and databases\n4.4\tReal-world modeling and relationships\n4.5\tOOP and control flow\n4.6\tResponsibility- vs. data-driven design\n4.7\tSOLID and GRASP guidelines\n5\tCriticism\n6\tFormal semantics\n7\tSee also\n7.1\tSystems\n7.2\tModeling languages\n8\tReferences\n9\tFurther reading\n10\tExternal links\nFeatures[edit]\nObject-oriented Programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP. The features listed below are, however, common among languages considered strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.[3][4][5][6]\n\nSee also: Comparison of programming languages (object-oriented programming) and List of object-oriented programming terms\nShared with non-OOP predecessor languages[edit]\nObject-oriented programming languages typically share low-level features with high-level procedural programming languages (which were invented first). The fundamental tools that can be used to construct a program include:\n\nVariables that can store information formatted in a small number of built-in data types like integers and alphanumeric characters. This may include data structures like strings, lists, and hash tables that are either built-in or result from combining variables using memory pointers\nProcedures – also known as functions, methods, routines, or subroutines – that take input, generate output, and manipulate data. Modern languages include structured programming constructs like loops and conditionals.\nModular programming support provides the ability to group procedures into files and modules for organizational purposes. Modules are namespaced so code in one module will not be accidentally confused with the same procedure or variable name in another file or module.\n\nObjects and classes[edit]\nLanguages that support object-oriented programming typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. Those that use classes support two main concepts:\n\nClasses – the definitions for the data format and available procedures for a given type or class of object; may also contain data and procedures (known as class methods) themselves, i.e. classes contains the data members and member functions\nObjects – instances of classes\nObjects sometimes correspond to things found in the real world. For example, a graphics program may have objects such as \"circle\", \"square\", \"menu\". An online shopping system might have objects such as \"shopping cart\", \"customer\", and \"product\".[7] Sometimes objects represent more abstract entities, like an object that represents an open file, or an object that provides the service of translating measurements from U.S. customary to metric.\n\nEach object is said to be an instance of a particular class (for example, an object with its name field set to \"Mary\" might be an instance of class Employee). Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties. This leads to the following terms:\n\nClass variables – belong to the class as a whole; there is only one copy of each one\nInstance variables or attributes – data that belongs to individual objects; every object has its own copy of each one\nMember variables – refers to both the class and instance variables that are defined by a particular class\nClass methods – belong to the class as a whole and have access only to class variables and inputs from the procedure call\nInstance methods – belong to individual objects, and have access to instance variables for the specific object they are called on, inputs, and class variables\nObjects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack. They provide a layer of abstraction which can be used to separate internal from external code. External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. Objects are created by calling a special type of method in the class known as a constructor. A program may create many instances of the same class as it runs, which operate independently. This is an easy way for the same procedures to be used on different sets of data.\n\nObject-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. As a result, a significantly different yet analogous terminology is used to define the concepts of object and instance.\n\nIn some languages classes and objects can be composed using other concepts like traits and mixins.\n\nDynamic dispatch/message passing[edit]\nIt is the responsibility of the object, not any external code, to select the procedural code to execute in response to a method call, typically by looking up the method at run time in a table associated with the object. This feature is known as dynamic dispatch, and distinguishes an object from an abstract data type (or module), which has a fixed (static) implementation of the operations for all instances. If there are multiple methods that might be run for a given name, it is known as multiple dispatch.\n\nA method call is also known as message passing. It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.\n\nEncapsulation[edit]\nEncapsulation is an Object Oriented Programming concept that binds together the data and functions that manipulate the data, and that keeps both safe from outside interference and misuse. Data encapsulation led to the important OOP concept of data hiding.\n\nIf a class does not allow calling code to access internal object data and permits access through methods only, this is a strong form of abstraction or information hiding known as encapsulation. Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the private keyword and designating methods intended for use by code outside the class with the public keyword. Methods may also be designed public, private, or intermediate levels such as protected (which allows access from the same class and its subclasses, but not objects of a different class). In other languages (like Python) this is enforced only by convention (for example, private methods may have names that start with an underscore). Encapsulation prevents external code from being concerned with the internal workings of an object. This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as \"public\" method calls work the same way). It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers. Encapsulation is a technique that encourages decoupling.\n\nComposition, inheritance, and delegation[edit]\nObjects can contain other objects in their instance variables; this is known as object composition. For example, an object in the Employee class might contain (point to) an object in the Address class, in addition to its own instance variables like \"first_name\" and \"position\". Object composition is used to represent \"has-a\" relationships: every employee has an address, so every Employee object has a place to store an Address object.\n\nLanguages that support classes almost always support inheritance. This allows classes to be arranged in a hierarchy that represents \"is-a-type-of\" relationships. For example, class Employee might inherit from class Person. All the data and methods available to the parent class also appear in the child class with the same names. For example, class Person might define variables \"first_name\" and \"last_name\" with method \"make_full_name()\". These will also be available in class Employee, which might add the variables \"position\" and \"salary\". This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.[8]\n\nSubclasses can override the methods defined by superclasses. Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated. Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes. For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don't share a common parent.\n\nAbstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other \"concrete\" classes which can be instantiated. In Java, the final keyword can be used to prevent a class from being subclassed.\n\nThe doctrine of composition over inheritance advocates implementing has-a relationships using composition instead of inheritance. For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods. Some languages, like Go do not support inheritance at all.\n\nThe \"open/closed principle\" advocates that classes and functions \"should be open for extension, but closed for modification\".\n\nDelegation is another language feature that can be used as an alternative to inheritance.\n\nPolymorphism[edit]\nSubtyping, a form of polymorphism, is when calling code can be agnostic as to whether an object belongs to a parent class or one of its descendants. For example, a function might call \"make_full_name()\" on an object, which will work whether the object is of class Person or class Employee. This is another type of abstraction which simplifies code external to the class hierarchy and enables strong separation of concerns.\n\nOpen recursion[edit]\nIn languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called this or self. This variable is late-bound; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.\n\nHistory[edit]\nTerminology invoking \"objects\" and \"oriented\" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. In the environment of the artificial intelligence group, as early as 1960, \"object\" could refer to identified items (LISP atoms) with properties (attributes);[9][10] Alan Kay was later to cite a detailed understanding of LISP internals as a strong influence on his thinking in 1966.[11] Another early MIT example was Sketchpad created by Ivan Sutherland in 1960–61; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.[12] Also, an MIT ALGOL version, AED-0, established a direct link between data structures (\"plexes\", in that dialect) and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".[13][14]\n\nThe formal programming concept of objects was introduced in the mid-1960s with Simula 67, a major revision of Simula I, a programming language designed for discrete event simulation, created by Ole-Johan Dahl and Kristen Nygaard of the Norwegian Computing Center in Oslo.[15][not in citation given][citation needed] [16][not in citation given][citation needed] [17] [18] [19]\n\nSimula 67 was influenced by SIMSCRIPT and C.A.R. \"Tony\" Hoare's proposed \"record classes\".[13][20] Simula introduced the notion of classes and instances or objects (as well as subclasses, virtual procedures, coroutines, and discrete event simulation) as part of an explicit programming paradigm. The language also used automatic garbage collection that had been invented earlier for the functional programming language Lisp. Simula was used for physical modeling, such as models to study and improve the movement of ships and their content through cargo ports. The ideas of Simula 67 influenced many later languages, including Smalltalk, derivatives of LISP (CLOS), Object Pascal, and C++.\n\nThe Smalltalk language, which was developed at Xerox PARC (by Alan Kay and others) in the 1970s, introduced the term object-oriented programming to represent the pervasive use of objects and messages as the basis for computation. Smalltalk creators were influenced by the ideas introduced in Simula 67, but Smalltalk was designed to be a fully dynamic system in which classes could be created and modified dynamically rather than statically as in Simula 67.[21] Smalltalk and with it OOP were introduced to a wider audience by the August 1981 issue of Byte Magazine.\n\nIn the 1970s, Kay's Smalltalk work had influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine. Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.\n\nIn 1985, Bertrand Meyer produced the first design of the Eiffel language. Focused on software quality, Eiffel is among the purely object-oriented languages, but differs in the sense that the language itself is not only a programming language, but a notation supporting the entire software lifecycle. Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. Essential to the quality focus of Eiffel is Meyer's reliability mechanism, Design by Contract, which is an integral part of both the method and language.\n\nObject-oriented programming developed as the dominant programming methodology in the early and mid 1990s when programming languages supporting the techniques became widely available. These included Visual FoxPro 3.0,[22][23][24] C++,[25] and Delphi[citation needed]. Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).\n\nAt ETH Zürich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.\n\nObject-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.\n\nMore recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. Two such languages are Python and Ruby. Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft's .NET platform. Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.\n\nOOP languages[edit]\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2009) (Learn how and when to remove this template message)\nSee also: List of object-oriented programming languages\nSimula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. It was created for making simulation programs, in which what came to be called objects were the most important information representation. Smalltalk (1972 to 1980) is another early example, and the one with which much of the theory of OOP was developed. Concerning the degree of object orientation, the following distinctions can be made:\n\nLanguages called \"pure\" OO languages, because everything in them is treated consistently as an object, from primitives such as characters and punctuation, all the way up to whole classes, prototypes, blocks, modules, etc. They were designed specifically to facilitate, even enforce, OO methods. Examples: Python, Ruby, Scala, Smalltalk, Eiffel, Emerald,[26] JADE, Self.\nLanguages designed mainly for OO programming, but with some procedural elements. Examples: Java, C++, C#, Delphi/Object Pascal, VB.NET.\nLanguages that are historically procedural languages, but have been extended with some OO features. Examples: PHP, Perl, Visual Basic (derived from BASIC), MATLAB, COBOL 2002, Fortran 2003, ABAP, Ada 95, Pascal.\nLanguages with most of the features of objects (classes, methods, inheritance), but in a distinctly original form. Examples: Oberon (Oberon-1 or Oberon-2).\nLanguages with abstract data type support which may be used to resemble OO programming, but without all features of object-orientation. This includes object-based and prototype-based languages. Examples: JavaScript, Lua, Modula-2, CLU.\nChameleon languages that support multiple paradigms, including OO. Tcl stands out among these for TclOO, a hybrid object system that supports both prototype-based programming and class-based OO.\nOOP in dynamic languages[edit]\nIn recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\n\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.\n\nOOP in a network protocol[edit]\nThe messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server. For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value. A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command's parameters. Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:\n\nFields defining the data values that form messages, such as their length, codepoint and data values.\nObjects and collections of objects similar to what would be found in a Smalltalk program for messages and parameters.\nManagers similar to AS/400 objects, such as a directory to files and files consisting of metadata and records. Managers conceptually provide memory and processing resources for their contained objects.\nA client or server consisting of all the managers necessary to implement a full processing environment, supporting such aspects as directory services, security and concurrency control.\nThe initial version of DDM defined distributed file services. It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).\n\nDesign patterns[edit]\nChallenges of object-oriented design are addressed by several methodologies. Most common is known as the design patterns codified by Gamma et al.. More broadly, the term \"design patterns\" can be used to refer to any general, repeatable solution to a commonly occurring problem in software design. Some of these commonly occurring problems have implications and solutions particular to object-oriented development.\n\nInheritance and behavioral subtyping[edit]\nSee also: Object-oriented design\nIt is intuitive to assume that inheritance creates a semantic \"is a\" relationship, and thus to infer that objects instantiated from subclasses can always be safely used instead of those instantiated from the superclass. This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. This issue is known as the Liskov substitution principle.\n\nGang of Four design patterns[edit]\nMain article: Design pattern (computer science)\nDesign Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1995 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them. As of April 2007, the book was in its 36th printing.\n\nThe book describes the following patterns:\n\nCreational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern\nStructural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern\nBehavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern\nObject-orientation and databases[edit]\nMain articles: Object-relational impedance mismatch, Object-relational mapping, and Object database\nBoth object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. Since relational databases don't store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides.[27] One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails' ActiveRecord.\n\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.\n\nReal-world modeling and relationships[edit]\nOOP can be used to associate real-world objects and processes with digital counterparts. However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in Object-Oriented Software Construction[28] that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\". At the same time, some principal limitations of OOP had been noted.[29] For example, the circle-ellipse problem is difficult to handle using OOP's concept of inheritance.\n\nHowever, Niklaus Wirth (who popularized the adage now known as Wirth's law: \"Software is getting slower more rapidly than hardware becomes faster\") said of OOP in his paper, \"Good Ideas through the Looking Glass\", \"This paradigm closely reflects the structure of systems 'in the real world', and it is therefore well suited to model complex systems with complex behaviours\"[30] (contrast KISS principle).\n\nSteve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs).[31] This problem may cause OOP to suffer more convoluted solutions than procedural programming.[32]\n\nOOP and control flow[edit]\nOOP was developed to increase the reusability and maintainability of source code.[33] Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.[34][35][36][37]\n\nResponsibility- vs. data-driven design[edit]\nResponsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. The authors hold that responsibility-driven design is preferable.\n\nSOLID and GRASP guidelines[edit]\nSOLID is a mnemonic invented by Michael Feathers that stands for and advocates five programming practices:\n\nSingle responsibility principle\nOpen/closed principle\nLiskov substitution principle\nInterface segregation principle\nDependency inversion principle\nGRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.\n\nCriticism[edit]\nThe OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity,[38][39] and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).[40][41]\n\nLuca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex.[38] The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:[39]\n\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\n\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.[42]\n\nChristopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP;[43] however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.[44]\n\nIn an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.[45]\n\nAlexander Stepanov compares object orientation unfavourably to generic programming:[40]\n\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.\n\nPaul Graham has suggested that OOP's popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".[46]\n\nSteve Yegge noted that, as opposed to functional programming:[47]\n\nObject Oriented Programming puts the Nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It's not as if OOP has suddenly made verbs less important in the way we actually think. It's a strangely skewed perspective.\n\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.[41]\n\nEric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency.[48] Raymond compares this unfavourably to the approach taken with Unix and the C programming language.[48]\n\nRob Pike, a programmer involved in the creation of UTF-8 and Go, has called the design of object-oriented programming \"the Roman numerals of computing\"[49] and has said that OOP languages frequently shift the focus from data structures and algorithms to types.[50] Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.[51]\n\nFormal semantics[edit]\nSee also: Formal semantics of programming languages\nObjects are the run-time entities in an object-oriented system. They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.\n\nThere have been several attempts at formalizing the concepts used in object-oriented programming. The following concepts and constructs have been used as interpretations of OOP concepts:\n\nco algebraic data types[52]\nabstract data types (which have existential types) allow the definition of modules but these do not support dynamic dispatch\nrecursive types\nencapsulated state\ninheritance\nrecords are basis for understanding objects if function literals can be stored in fields (like in functional programming languages), but the actual calculi need be considerably more complex to incorporate essential features of OOP. Several extensions of System F<: that deal with mutable objects have been studied;[53] these allow both subtype polymorphism and parametric polymorphism (generics)\nAttempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, A Theory of Objects[53] for formal definitions of many OOP concepts and constructs), and often diverge widely. For example, some definitions focus on mental activities, and some on program structuring. One of the simpler definitions is that OOP is the act of using \"map\" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. Inheritance can be performed by cloning the maps (sometimes called \"prototyping\").\n\nSee also[edit]\nicon\tComputer programming portal\nComparison of programming languages (object-oriented programming)\nComparison of programming paradigms\nComponent-based software engineering\nDesign by contract\nObject association\nObject database\nObject modeling language\nObject-oriented analysis and design\nObject-relational impedance mismatch (and The Third Manifesto)\nObject-relational mapping\nSystems[edit]\nCADES\nCommon Object Request Broker Architecture (CORBA)\nDistributed Component Object Model\nDistributed Data Management Architecture\nJeroo\nModeling languages[edit]\nIDEF4\nInterface description language\nLepus3\nUML",
          "subparadigms": [
            63,
            62,
            60,
            57,
            59
          ]
        },
        {
          "pdid": 65,
          "name": "Block",
          "details": "In computer programming, a block or code block is a section of code which is grouped together. Blocks consist of one or more declarations and statements. A programming language that permits the creation of blocks, including blocks nested within other blocks, is called a block-structured programming language. Blocks are fundamental to structured programming, where control structures are formed from blocks.\n\nThe function of blocks in programming is to enable groups of statements to be treated as if they were one statement, and to narrow the lexical scope of variables, procedures and functions declared in a block so that they do not conflict with variables having the same name used elsewhere in a program for different purposes. In a block-structured programming language, the names of variables and other objects such as procedures which are declared in outer blocks are visible inside other inner blocks, unless they are shadowed by an object of the same name.\n\nContents  [hide] \n1\tHistory\n2\tSyntax\n3\tLimitations\n4\tBasic semantics\n5\tHoisting\n6\tSee also\n7\tReferences\nHistory[edit]\nIdeas of block structure were developed in the 1950s during the development of the first autocodes, and were formalized in the Algol 58 and Algol 60 reports. Algol 58 introduced the notion of the \"compound statement\", which was related solely to control flow.[1] The subsequent Revised Report which described the syntax and semantics of Algol 60 introduced the notion of a block and block scope, with a block consisting of \" A sequence of declarations followed by a sequence of statements and enclosed between begin and end...\" in which \"[e]very declaration appears in a block in this way and is valid only for that block.\"[2]\n\nSyntax[edit]\nBlocks use different syntax in different languages. Two broad families are:\n\nthe ALGOL family in which blocks are delimited by the keywords \"begin\" and \"end\"\nthe C family in which blocks are delimited by curly braces - \"{\" and \"}\"\nSome other techniques used are as follows :\n\nparentheses - \"(\" and \")\", as in batch language and ALGOL 68.\nindentation, as in Python\ns-expressions with a syntactic keyword such as lambda or let (as in the Lisp family)\nIn 1968 (with ALGOL 68), then in Edsger W. Dijkstra's 1974 Guarded Command Language the conditional and iterative code block are alternatively terminated with the block reserved word reversed: e.g. if ~ then ~ elif ~ else ~ fi, case ~ in ~ out ~ esac and for ~ while ~ do ~ od\nLimitations[edit]\nSome languages which support blocks with variable declarations do not fully support all declarations; for instance many C-derived languages do not permit a function definition within a block (nested functions). And unlike its ancestor Algol, Pascal does not support the use of blocks with their own declarations inside the begin and end of an existing block, only compound statements enabling sequences of statements to be grouped together in if, while, repeat and other control statements.\n\nBasic semantics[edit]\nThe semantic meaning of a block is twofold. Firstly, it provides the programmer with a way for creating arbitrarily large and complex structures that can be treated as units. Secondly, it enables the programmer to limit the scope of variables and sometimes other objects that have been declared.\n\nIn primitive languages such as early Fortran and BASIC, there were a few built-in statement types, and little or no means of extending them in a structured manner. For instance, until 1978 standard Fortran had no \"block if\" statement, so to write a standard-complying code to implement simple decisions the programmer had to resort to gotos:\n\nC     LANGUAGE: ANSI STANDARD FORTRAN 66\nC     INITIALIZE VALUES TO BE CALCULATED\n      PAYSTX = .FALSE.\n      PAYSST = .FALSE.\n      TAX = 0.0\n      SUPTAX = 0.0\nC     SKIP TAX DEDUCTION IF EMPLOYEE EARNS LESS THAN TAX THRESHOLD\n      IF (WAGES .LE. TAXTHR) GOTO 100\n      PAYSTX = .TRUE.\n      TAX = (WAGES - TAXTHR) * BASCRT\nC     SKIP SUPERTAX DEDUCTION IF EMPLOYEE EARNS LESS THAN SUPERTAX THRESHOLD\n      IF (WAGES .LE. SUPTHR) GOTO 100\n      PAYSST = .TRUE.\n      SUPTAX = (WAGES - SUPTHR) * SUPRAT\n  100 TAXED = WAGES - TAX - SUPTAX\nEven in this very brief Fortran fragment, written to the Fortran 66 standard, it is not easy to see the structure of the program, because that structure is not reflected in the language. Without careful study it is not easy to see the circumstances in which a given statement is executed.\n\nBlocks allow the programmer to treat a group of statements as a unit, and the default values which had to appear in initialization in this style of programming can, with a block structure, be placed closer to the decision:\n\n    { Language: Jensen and Wirth Pascal }\n    if wages > tax_threshold then\n        begin\n        paystax := true;\n        tax := (wages - tax_threshold) * tax_rate\n        { The block structure makes it easier to see how the code could\n          be refactored for clarity, and also makes it easier to do,\n          because the structure of the inner conditional can easily be moved\n          out of the outer conditional altogether and the effects of doing\n          so are easily predicted. }\n        if wages > supertax_threshold then\n            begin\n            pays_supertax := true;\n            supertax := (wages - supertax_threshold) * supertax_rate\n            end\n        else begin\n            pays_supertax := false;\n            supertax := 0\n            end\n        end\n    else begin\n        paystax := false; pays_supertax := false;\n        tax := 0; supertax := 0\n        end;\n    taxed := wages - tax - supertax;\nUse of blocks in the above fragment of Pascal enables the programmer to be clearer about what he or she intends, and to combine the resulting blocks into a nested hierarchy of conditional statements. The structure of the code reflects the programmer's thinking more closely, making it easier to understand and modify.\n\nFrom looking at the above code the programmer can easily see that he or she can make the source code even clearer by taking the inner if statement out of the outer one altogether, placing the two blocks one after the other to be executed consecutively. Semantically there is little difference in this case, and the use of block structure, supported by indenting for readability, makes it easy for the programmer to refactor the code.\n\nIn primitive languages, variables had broad scope. For instance, an integer variable called IEMPNO might be used in one part of a Fortran subroutine to denote an employee social security number (ssn), but during maintenance work on the same subroutine, a programmer might accidentally use the same variable, IEMPNO, for a different purpose, and this could result in a bug that was difficult to trace. Block structure makes it easier for programmers to control scope to a minute level.\n\n;; Language: R5RS Standard Scheme\n(let ((empno (ssn-of employee-name)))\n  (while (is-manager empno)\n    (let ((employees (length (underlings-of empno))))\n      (printf \"~a has ~a employees working under him:~%\" employee-name employees)\n      (for-each\n        (lambda(empno)\n          ;; Within this lambda expression the variable empno refers to the ssn\n          ;; of an underling. The variable empno in the outer expression,\n          ;; referring to the manager's ssn, is shadowed.\n          (printf \"Name: ~a, role: ~a~%\"\n                  (name-of empno)\n                  (role-of empno)))\n        (underlings-of empno)))))\nIn the above Scheme fragment, empno is used to identify both the manager and his or her underlings each by their respective ssn, but because the underling ssn is declared within an inner block it does not interact with the variable of the same name that contains the manager's ssn. In practice, considerations of clarity would probably lead the programmer to choose distinct variable names, but he or she has the choice and it is more difficult to introduce a bug inadvertently.\n\nHoisting[edit]\nIn a few circumstances, code in a block is evaluated as if the code were actually at the top of the block or outside the block. This is often colloquially known as hoisting, and includes:\n\nLoop-invariant code motion, a compiler optimization where code in the loop that is invariant is evaluated before the loop;\nVariable hoisting, scope rule in JavaScript, where variables have function scope, and behave as if they were declared (but not defined) at the top of a function.\nSee also[edit]\nicon\tComputer programming portal\nBasic block\nBlock scope\nClosure (computer programming)\nControl flow",
          "subparadigms": []
        },
        {
          "pdid": 66,
          "name": "Modular",
          "details": "Modular programming is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules, such that each contains everything necessary to execute only one aspect of the desired functionality.\n\nA module interface expresses the elements that are provided and required by the module. The elements defined in the interface are detectable by other modules. The implementation contains the working code that corresponds to the elements declared in the interface. Modular programming is closely related to structured programming and object-oriented programming, all having the same goal of facilitating construction of large software programs and systems by decomposition into smaller pieces, and all originating around the 1960s. While historically usage of these terms has been inconsistent, today \"modular programming\" refers to high-level decomposition of the code of an entire program into pieces, structured programming to the low-level code use of structured control flow, and object-oriented programming to the data use of objects, a kind of data structure.\n\nIn object-oriented programming, the use of interfaces as an architectural pattern to construct modules is known as interface-based programming[citation needed].\n\nContents  [hide] \n1\tTerminology\n2\tLanguage support\n3\tKey aspects\n4\tHistory\n5\tSee also\n6\tNotes\n7\tReferences\nTerminology[edit]\nThe term assembly (as in .NET languages like C#, F# or Visual Basic .NET) or package (as in Dart, Go or Java) is sometimes used instead of module . In other implementations, this is a distinct concept; in Python a package is a collection of modules, while in the upcoming Java 9 the introduction of the new module concept (a collection of packages with enhanced access control) is planned.\n\nFurthermore, the term \"package\" has other uses in software (for example .NET NuGet packages). A component is a similar concept, but typically refers to a higher level; a component is a piece of a whole system, while a module is a piece of an individual program. The scale of the term \"module\" varies significantly between languages; in Python it is very small-scale and each file is a module, while in Java 9 it is planned to be large-scale, where a module is a collection of packages, which are in turn collections of files.\n\nOther terms for modules include unit, used in Pascal dialects.\n\nLanguage support[edit]\nLanguages that formally support the module concept include Ada, Algol, BlitzMax, C#, Clojure, COBOL, D, Dart, eC, Erlang, Elixir, F, F#, Fortran, Go, Haskell, IBM/360 Assembler, IBM i Control Language (CL), IBM RPG, Java,[a] MATLAB, ML, Modula, Modula-2, Modula-3, Morpho, NEWP, Oberon, Oberon-2, Objective-C, OCaml, several derivatives of Pascal (Component Pascal, Object Pascal, Turbo Pascal, UCSD Pascal), Perl, PL/I, PureBasic, Python, Ruby,[2] Rust, JavaScript,[3] Visual Basic .NET and WebDNA.\n\nConspicuous examples of languages that lack support for modules are C, C++,[4] and Pascal (in its original form). As of 2014, modules have been proposed for C++;[5] modules were added to Objective-C in iOS 7 (2013); and Pascal was superseded by Modula and Oberon, which included modules from the start, and various derivatives that included modules. JavaScript has got native modules since ECMAScript 2015.\n\nModular programming can be performed even where the programming language lacks explicit syntactic features to support named modules. For example, the IBM System i also uses modules when programming in the Integrated Language Environment (ILE).\n\nKey aspects[edit]\nWith modular programming, concerns are separated such that modules perform logically discrete functions, interacting through well-defined interfaces. Often modules form a directed acyclic graph (DAG); in this case a cyclic dependency between modules is seen as indicating that these should be a single module. In the case where modules do form a DAG they can be arranged as a hierarchy, where the lowest-level modules are independent, depending on no other modules, and higher-level modules depend on lower-level ones. A particular program or library is a top-level module of its own hierarchy, but can in turn be seen as a lower-level module of a higher-level program, library, or system.\n\nWhen creating a modular system, instead of creating a monolithic application (where the smallest component is the whole), several smaller modules are written separately so that, when composed together, they construct the executable application program. Typically these are also compiled separately, via separate compilation, and then linked by a linker. A just-in-time compiler may perform some of this construction \"on-the-fly\" at run time.\n\nThis makes modular designed systems, if built correctly, far more reusable than a traditional monolithic design, since all (or many) of these modules may then be reused (without change) in other projects. This also facilitates the \"breaking down\" of projects into several smaller projects. Theoretically, a modularized software project will be more easily assembled by large teams, since no team members are creating the whole system, or even need to know about the system as a whole. They can focus just on the assigned smaller task (this, it is claimed, counters the key assumption of The Mythical Man Month—making it actually possible to add more developers to a late software project—without making it later still).\n\nHistory[edit]\nModular programming, in the form of subsystems (particularly for I/O) and software libraries, dates to early software systems, where it was used for code reuse. Modular programming per se, with a goal of modularity, developed in the late 1960s and 1970s, as a programming in the large analog of the programming in the small concept of structured programming (1960s). The term \"modular programming\" dates at least to the National Symposium on Modular Programming, organized at the Information and Systems Institute in July 1968 by Larry Constantine; other key concepts were information hiding (1972) and separation of concerns (SoC, 1974).\n\nModules were not included in the original specification for ALGOL 68 (1968), but were included as extensions in early implementations, ALGOL 68-R (1970) and ALGOL 68C (1970), and later formalized.[6] One of the first languages designed from the start for modular programming was the short-lived Modula (1975), by Niklaus Wirth. Another early modular language was Mesa (1970s), by Xerox PARC, and Wirth drew on Mesa as well as the original Modula in its successor, Modula-2 (1978), which influenced later languages, particularly through its successor, Modula-3 (1980s). Modula's use of dot-qualified names, like M.a to refer to object a from module M, coincides with notation to access a field of a record (and similarly for attributes or methods of objects), and is now widespread, seen in C#, Dart, Go, Java, and Python, among others. Modular programming became widespread from the 1980s: the original Pascal language (1970) did not include modules, but later versions, notably UCSD Pascal (1978) and Turbo Pascal (1983) included them in the form of \"units\", as did the Pascal-influenced Ada (1980). The Extended Pascal ISO 10206:1990 standard kept closer to Modula2 in its modular support. Standard ML (1984)[7] has one of the most complete module systems, including functors (parameterized modules) to map between modules.\n\nIn the 1980s and 1990s modular programming was overshadowed by and often conflated with object-oriented programming, particularly due to the popularity of C++ and Java; this was also seen in the failure of Modula-3, which included modules but not objects. For example, the C family of languages had support for objects and classes in C++ (originally C with Classes, 1980) and Objective-C (1983), only supporting modules 30 years or more later. Java (1995) supports modules in the form of packages, though the primary unit of code organization is a class. However, Python (1991) prominently used both modules and objects from the start, using modules as the primary unit of code organization and \"packages\" as a larger-scale unit; and Perl 5 (1994) includes support for both modules and objects, with a vast array of modules being available from CPAN (1993).\n\nModular programming is now widespread, and found in virtually all major languages developed since the 1990s. The relative importance of modules varies between languages, and in class-based object-oriented languages there is still overlap and confusion with classes as a unit of organization and encapsulation, but these are both well-established as distinct concepts.\n\nSee also[edit]\nArchitecture description language\nCohesion (computer science)\nComponent-based software engineering\nConstructionist design methodology, a methodology for creating modular, broad Artificial Intelligence systems\nConway's law\nCoupling (computer science)\nDavid Parnas\nInformation hiding (encapsulation)\nLibrary (computing)\nList of system quality attributes\nPlug-in (computing)\nSnippet (programming)\nStructured Design\nStructured programming",
          "subparadigms": []
        },
        {
          "pdid": 67,
          "name": "Structured",
          "details": "Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of subroutines, block structures, for and while loops—in contrast to using simple tests and jumps such as the go to statement which could lead to \"spaghetti code\" causing difficulty to both follow and maintain.\n\nIt emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages,[1] with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966,[2] and the publication of the influential \"Go To Statement Considered Harmful\" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term \"structured programming\".[3]\n\nStructured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.\n\nContents  [hide] \n1\tElements\n1.1\tControl structures\n1.2\tSubroutines\n1.3\tBlocks\n2\tStructured programming languages\n3\tHistory\n3.1\tTheoretical foundation\n3.2\tDebate\n3.3\tOutcome\n4\tCommon deviations\n4.1\tEarly exit\n4.2\tException handling\n4.3\tMultiple entry\n4.4\tState machines\n5\tSee also\n6\tReferences\n7\tExternal links\nElements[edit]\nControl structures[edit]\nFollowing the structured program theorem, all programs are seen as composed of control structures:\n\n\"Sequence\"; ordered statements or subroutines executed in sequence.\n\"Selection\"; one or a number of statements is executed depending on the state of the program. This is usually expressed with keywords such as if..then..else..endif.\n\"Iteration\"; a statement or block is executed until the program reaches a certain state, or operations have been applied to every element of a collection. This is usually expressed with keywords such as while, repeat, for or do..until. Often it is recommended that each loop should only have one entry point (and in the original structural programming, also only one exit point, and a few languages enforce this).\n\"Recursion\"; a statement is executed by repeatedly calling itself until termination conditions are met. While similar in practice to iterative loops, recursive loops may be more computationally efficient, and are implemented differently as a cascading stack.\n\nGraphical representations of the three basic patterns using NS diagrams (blue) and flow charts (green).\nSubroutines[edit]\nSubroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.\n\nBlocks[edit]\nBlocks are used to enable groups of statements to be treated as if they were one statement. Block-structured languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by if..fi as in ALGOL 68, or a code section bracketed by BEGIN..END, as in PL/I, whitespace indentation as in Python - or the curly braces {...} of C and many later languages.\n\nStructured programming languages[edit]\nIt is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada – but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult. Structured programming (sometimes known as modular programming) is a subset of imperative programming that enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.\n\nHistory[edit]\nTheoretical foundation[edit]\nThe structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a \"structured program\" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself.[4] The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.\n\nDebate[edit]\nP. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:\n\nUs converts waved this interesting bit of news under the noses of the unreconstructed assembly-language programmers who kept trotting forth twisty bits of logic and saying, 'I betcha can't structure this.' Neither the proof by Böhm and Jacopini nor our repeated successes at writing structured code brought them around one day sooner than they were ready to convince themselves.[5]\nDonald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees[citation needed]) with abolishing the GOTO statement. In his 1974 paper, \"Structured Programming with Goto Statements\",[6] he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs[when defined as?].[who?]\n\nStructured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for the New York Times research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.[citation needed]\n\nAs late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled \"\"GOTO considered harmful\" considered harmful\".[7] Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.\n\nOutcome[edit]\nBy the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.\n\nCommon deviations[edit]\nWhile goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.\n\nEarly exit[edit]\nThe most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a return statement. At the level of loops, this is a break statement (terminate the loop) or continue statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have \"labeled breaks\", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.\n\nMultiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered \"exceptional\" circumstances that prevent it from continuing, hence needing exception handling.\n\nThe most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not deallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal don't have this problem.\n\nMost modern languages provide language-level support to prevent such leaks;[8] see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a goto. This is most often known as try...finally, and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.\n\nKent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that \"one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t\". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors).[9] Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.[10]\n\nIn his 2004 textbook, David Watt writes that \"single-entry multi-exit control flows are often desirable\". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as escape sequencers, defined as a \"sequencer that terminates execution of a textually enclosing command or procedure\", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce \"spaghetti code\". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.[11]\n\nIn contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like break and continue \"are just the old goto in sheep's clothing\" and strongly advised against their use.[12]\n\nException handling[edit]\nBased on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and propose that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as throw()[13] Bonang proposes that all single-exit conforming C++ should be written along the lines of:\n\nbool myCheck1() throw()\n{\n  bool success = false;\n  try {\n    // do something that may throw exceptions\n    if(myCheck2() == false) {\n      throw SomeInternalException();\n    }\n    // other code similar to the above\n    success = true;\n  }\n  catch(...) { // all exceptions caught and logged\n  }\n  return success;\n}\nPeter Ritchie also notes that, in principle, even a single throw right before the return in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.[14]\n\nDavid Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic overflows or input/output failures like file not found) is a kind of error that \"is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit\". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where \"the application code tends to get cluttered by tests of status flags\" and that \"the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!\" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.[15]\n\nThe textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like while loops because the transfer of control \"is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred.\"[16] Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like for, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if init() throws an exception in for (init(); check(); increm()), then the usual exit point after check() is not reached.[17] Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they \"create hidden control-flow paths that are difficult for programmers to reason about\".[18]:8:27\n\nThe necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like parallel do, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from break to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.[19]\n\nMultiple entry[edit]\nFurther information: Coroutine\nMore rarely, subprograms allow multiple entry. This is most commonly only re-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.\n\nIt is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.\n\nState machines[edit]\nSome programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers (including Knuth[citation needed]) implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.\n\nHowever, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.\n\nSee also[edit]\nDRAKON\nMinimal evaluation\nNassi–Shneiderman diagram\nStructure chart\nSwitch statement",
          "subparadigms": [
            65,
            66,
            64,
            68
          ]
        },
        {
          "pdid": 68,
          "name": "Recursive",
          "details": "Recursion in computer science is a method where the solution to a problem depends on solutions to smaller instances of the same problem (as opposed to iteration).[1] The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[2]\n\n\"The power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions.\"[3]\n\nMost computer programming languages support recursion by allowing a function to call itself within the program text. Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. Computability theory proves that these recursive-only languages are Turing complete; they are as computationally powerful as Turing complete imperative languages, meaning they can solve the same kinds of problems as imperative languages even without iterative control structures such as “while” and “for”.\n\nContents  [hide] \n1\tRecursive functions and algorithms\n2\tRecursive data types\n2.1\tInductively defined data\n2.2\tCoinductively defined data and corecursion\n3\tTypes of recursion\n3.1\tSingle recursion and multiple recursion\n3.2\tIndirect recursion\n3.3\tAnonymous recursion\n3.4\tStructural versus generative recursion\n4\tRecursive programs\n4.1\tRecursive procedures\n4.2\tRecursive data structures (structural recursion)\n5\tImplementation issues\n5.1\tWrapper function\n5.2\tShort-circuiting the base case\n5.3\tHybrid algorithm\n6\tRecursion versus iteration\n6.1\tExpressive power\n6.2\tPerformance issues\n6.3\tStack space\n6.4\tMultiply recursive problems\n7\tTail-recursive functions\n8\tOrder of execution\n8.1\tFunction 1\n8.2\tFunction 2 with swapped lines\n9\tTime-efficiency of recursive algorithms\n9.1\tShortcut rule (master theorem)\n10\tSee also\n10.1\tRecursive functions\n10.2\tBooks\n11\tNotes and references\n12\tFurther reading\n13\tExternal links\nRecursive functions and algorithms[edit]\nA common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.\n\nA recursive function definition has one or more base cases, meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases, meaning input(s) for which the program recurs (calls itself). For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n! = n(n − 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\".\n\nThe job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances—for example, some system and server processes—are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop.\n\nFor some functions (such as one that computes the series for e = 1/0! + 1/1! + 1/2! + 1/3! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case. Such an example is more naturally treated by co-recursion, where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the nth term (nth partial sum)\".\n\nRecursive data types[edit]\nMany computer programs must process or generate an arbitrarily large quantity of data. Recursion is one technique for representing data whose exact size the programmer does not know: the programmer can specify this data with a self-referential definition. There are two types of self-referential definitions: inductive and coinductive definitions.\n\nFurther information: Algebraic data type\nInductively defined data[edit]\nMain article: Recursive data type\nAn inductively defined recursive data definition is one that specifies how to construct instances of the data. For example, linked lists can be defined inductively (here, using Haskell syntax):\n\ndata ListOfStrings = EmptyList | Cons String ListOfStrings\nThe code above specifies a list of strings to be either empty, or a structure that contains a string and a list of strings. The self-reference in the definition permits the construction of lists of any (finite) number of strings.\n\nAnother example of inductive definition is the natural numbers (or positive integers):\n\nA natural number is either 1 or n+1, where n is a natural number.\nSimilarly recursive definitions are often used to model the structure of expressions and statements in programming languages. Language designers often express grammars in a syntax such as Backus-Naur form; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition:\n\n <expr> ::= <number>\n          | (<expr> * <expr>)\n          | (<expr> + <expr>)\nThis says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complex arithmetic expressions such as (5 * ((3 * 6) + 8)), with more than one product or sum operation in a single expression.\n\nCoinductively defined data and corecursion[edit]\nMain articles: Coinduction and Corecursion\nA coinductive data definition is one that specifies the operations that may be performed on a piece of data; typically, self-referential coinductive definitions are used for data structures of infinite size.\n\nA coinductive definition of infinite streams of strings, given informally, might look like this:\n\nA stream of strings is an object s such that:\n head(s) is a string, and\n tail(s) is a stream of strings.\nThis is very similar to an inductive definition of lists of strings; the difference is that this definition specifies how to access the contents of the data structure—namely, via the accessor functions head and tail—and what those contents may be, whereas the inductive definition specifies how to create the structure and what it may be created from.\n\nCorecursion is related to coinduction, and can be used to compute particular instances of (possibly) infinite objects. As a programming technique, it is used most often in the context of lazy programming languages, and can be preferable to recursion when the desired size or precision of a program's output is unknown. In such cases the program requires both a definition for an infinitely large (or infinitely precise) result, and a mechanism for taking a finite portion of that result. The problem of computing the first n prime numbers is one that can be solved with a corecursive program (e.g. here).\n\nTypes of recursion[edit]\nSingle recursion and multiple recursion[edit]\nRecursion that only contains a single self-reference is known as single recursion, while recursion that contains multiple self-references is known as multiple recursion. Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal, such as in a depth-first search.\n\nSingle recursion is often much more efficient than multiple recursion, and can generally be replaced by an iterative computation, running in linear time and requiring constant space. Multiple recursion, by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack.\n\nMultiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively is multiple iteration, as each value requires two previous values, it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion, building up from the initial values, tracking at each step two successive values – see corecursion: examples. A more sophisticated example is using a threaded binary tree, which allows iterative tree traversal, rather than multiple recursion.\n\nIndirect recursion[edit]\nMain article: Mutual recursion\nMost basic examples of recursion, and most of the examples presented here, demonstrate direct recursion, in which a function calls itself. Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again.\n\nIndirect recursion is also called mutual recursion, which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.\n\nAnonymous recursion[edit]\nMain article: Anonymous recursion\nRecursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions, and is known as anonymous recursion.\n\nStructural versus generative recursion[edit]\nSee also: Structural recursion\nSome authors classify recursion as either \"structural\" or \"generative\". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data:\n\n[Functions that consume structured data] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS.[4]\n\nThus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals, including XML processing, binary tree creation and search, etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion.\n\nGenerative recursion is the alternative:\n\nMany well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it. HtDP (How To Design Programs) refers to this kind as generative recursion. Examples of generative recursion include: gcd, quicksort, binary search, mergesort, Newton's method, fractals, and adaptive integration.[5]\n\nThis distinction is important in proving termination of a function.\n\nAll structurally recursive functions on finite (inductively defined) data structures can easily be shown to terminate, via structural induction: intuitively, each recursive call receives a smaller piece of input data, until a base case is reached.\nGeneratively recursive functions, in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions – each step generates the new data, such as successive approximation in Newton's method – and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed.\nIn terms of loop variants, structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step.\nBy contrast, generative recursion is when there is not such an obvious loop variant, and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis.\nRecursive programs[edit]\nRecursive procedures[edit]\nFactorial[edit]\nA classic example of a recursive procedure is the function used to calculate the factorial of a natural number:\n\n{\\displaystyle \\operatorname {fact} (n)={\\begin{cases}1&{\\mbox{if }}n=0\\\\n\\cdot \\operatorname {fact} (n-1)&{\\mbox{if }}n>0\\\\\\end{cases}}} \\operatorname {fact} (n)={\\begin{cases}1&{\\mbox{if }}n=0\\\\n\\cdot \\operatorname {fact} (n-1)&{\\mbox{if }}n>0\\\\\\end{cases}}\nPseudocode (recursive):\nfunction factorial is:\ninput: integer n such that n >= 0\noutput: [n × (n-1) × (n-2) × … × 1]\n\n    1. if n is 0, return 1\n    2. otherwise, return [ n × factorial(n-1) ]\n\nend factorial\nThe function can also be written as a recurrence relation:\n\n{\\displaystyle b_{n}=nb_{n-1}} b_{n}=nb_{n-1}\n{\\displaystyle b_{0}=1} b_{0}=1\nThis evaluation of the recurrence relation demonstrates the computation that would be performed in evaluating the pseudocode above:\n\nComputing the recurrence relation for n = 4:\nb4           = 4 * b3\n             = 4 * (3 * b2)\n             = 4 * (3 * (2 * b1))\n             = 4 * (3 * (2 * (1 * b0)))\n             = 4 * (3 * (2 * (1 * 1)))\n             = 4 * (3 * (2 * 1))\n             = 4 * (3 * 2)\n             = 4 * 6\n             = 24\nThis factorial function can also be described without using recursion by making use of the typical looping constructs found in imperative programming languages:\n\nPseudocode (iterative):\nfunction factorial is:\ninput: integer n such that n >= 0\noutput: [n × (n-1) × (n-2) × … × 1]\n\n    1. create new variable called running_total with a value of 1\n\n    2. begin loop\n          1. if n is 0, exit loop\n          2. set running_total to (running_total × n)\n          3. decrement n\n          4. repeat loop\n\n    3. return running_total\n\nend factorial\nThe imperative code above is equivalent to this mathematical definition using an accumulator variable t:\n\n{\\displaystyle {\\begin{array}{rcl}\\operatorname {fact} (n)&=&\\operatorname {fact_{acc}} (n,1)\\\\\\operatorname {fact_{acc}} (n,t)&=&{\\begin{cases}t&{\\mbox{if }}n=0\\\\\\operatorname {fact_{acc}} (n-1,nt)&{\\mbox{if }}n>0\\\\\\end{cases}}\\end{array}}} {\\begin{array}{rcl}\\operatorname {fact} (n)&=&\\operatorname {fact_{acc}} (n,1)\\\\\\operatorname {fact_{acc}} (n,t)&=&{\\begin{cases}t&{\\mbox{if }}n=0\\\\\\operatorname {fact_{acc}} (n-1,nt)&{\\mbox{if }}n>0\\\\\\end{cases}}\\end{array}}\nThe definition above translates straightforwardly to functional programming languages such as Scheme; this is an example of iteration implemented recursively.\n\nGreatest common divisor[edit]\nThe Euclidean algorithm, which computes the greatest common divisor of two integers, can be written recursively.\n\nFunction definition:\n\n{\\displaystyle \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\\\end{cases}}} \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\\\end{cases}}\nPseudocode (recursive):\nfunction gcd is:\ninput: integer x, integer y such that x > 0 and y >= 0\n\n    1. if y is 0, return x\n    2. otherwise, return [ gcd( y, (remainder of x/y) ) ]\n\nend gcd\nRecurrence relation for greatest common divisor, where {\\displaystyle x\\%y} x\\%y expresses the remainder of {\\displaystyle x/y} x/y:\n\n{\\displaystyle \\gcd(x,y)=\\gcd(y,x\\%y)} \\gcd(x,y)=\\gcd(y,x\\%y) if {\\displaystyle y\\neq 0} y\\neq 0\n{\\displaystyle \\gcd(x,0)=x} \\gcd(x,0)=x\nComputing the recurrence relation for x = 27 and y = 9:\ngcd(27, 9)   = gcd(9, 27% 9)\n             = gcd(9, 0)\n             = 9\nComputing the recurrence relation for x = 111 and y = 259:\ngcd(111, 259)   = gcd(259, 111% 259)\n                = gcd(259, 111)\n                = gcd(111, 259% 111)\n                = gcd(111, 37)\n                = gcd(37, 111% 37)\n                = gcd(37, 0)\n                = 37\nThe recursive program above is tail-recursive; it is equivalent to an iterative algorithm, and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls. Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack.\n\nPseudocode (iterative):\nfunction gcd is:\ninput: integer x, integer y such that x >= y and y >= 0\n\n    1. create new variable called remainder\n\n    2. begin loop\n          1. if y is zero, exit loop\n          2. set remainder to the remainder of x/y\n          3. set x to y\n          4. set y to remainder\n          5. repeat loop\n\n    3. return x\n\nend gcd\nThe iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.\n\nTowers of Hanoi[edit]\n\nTowers of Hanoi\nMain article: Towers of Hanoi\nThe Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.[6][7] There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack?\n\nFunction definition:\n\n{\\displaystyle \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\\\end{cases}}} \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\\\end{cases}}\nRecurrence relation for hanoi:\n\n{\\displaystyle h_{n}=2h_{n-1}+1} h_{n}=2h_{n-1}+1\n{\\displaystyle h_{1}=1} h_{1}=1\nComputing the recurrence relation for n = 4:\nhanoi(4)     = 2*hanoi(3) + 1\n             = 2*(2*hanoi(2) + 1) + 1\n             = 2*(2*(2*hanoi(1) + 1) + 1) + 1\n             = 2*(2*(2*1 + 1) + 1) + 1\n             = 2*(2*(3) + 1) + 1\n             = 2*(7) + 1\n             = 15\n\n\nExample implementations:\n\nPseudocode (recursive):\nfunction hanoi is:\ninput: integer n, such that n >= 1\n\n    1. if n is 1 then return 1\n\n    2. return [2 * [call hanoi(n-1)] + 1]\n\nend hanoi\nAlthough not all recursive functions have an explicit solution, the Tower of Hanoi sequence can be reduced to an explicit formula.[8]\n\nAn explicit formula for Towers of Hanoi:\nh1 = 1   = 21 - 1\nh2 = 3   = 22 - 1\nh3 = 7   = 23 - 1\nh4 = 15  = 24 - 1\nh5 = 31  = 25 - 1\nh6 = 63  = 26 - 1\nh7 = 127 = 27 - 1\nIn general:\nhn = 2n - 1, for all n >= 1\nBinary search[edit]\nThe binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for.\n\nRecursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass.\n\nExample implementation of binary search in C:\n\n /*\n  Call binary_search with proper initial conditions.\n\n  INPUT:\n    data is an array of integers SORTED in ASCENDING order,\n    toFind is the integer to search for,\n    count is the total number of elements in the array\n\n  OUTPUT:\n    result of binary_search\n\n */\n int search(int *data, int toFind, int count)\n {\n    //  Start = 0 (beginning index)\n    //  End = count - 1 (top index)\n    return binary_search(data, toFind, 0, count-1);\n }\n\n /*\n   Binary Search Algorithm.\n\n   INPUT:\n        data is a array of integers SORTED in ASCENDING order,\n        toFind is the integer to search for,\n        start is the minimum array index,\n        end is the maximum array index\n   OUTPUT:\n        position of the integer toFind within array data,\n        -1 if not found\n */\n int binary_search(int *data, int toFind, int start, int end)\n {\n    //Get the midpoint.\n    int mid = start + (end - start)/2;   //Integer division\n\n    //Stop condition.\n    if (start > end)\n       return -1;\n    else if (data[mid] == toFind)        //Found?\n       return mid;\n    else if (data[mid] > toFind)         //Data is greater than toFind, search lower half\n       return binary_search(data, toFind, start, mid-1);\n    else                                 //Data is less than toFind, search upper half\n       return binary_search(data, toFind, mid+1, end);\n }\nRecursive data structures (structural recursion)[edit]\nMain article: Recursive data type\nAn important application of recursion in computer science is in defining dynamic data structures such as lists and trees. Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.\n\n\"Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"[9]\n\nThe examples in this section illustrate what is known as \"structural recursion\". This term refers to the fact that the recursive procedures are acting on data that is defined recursively.\n\nAs long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.[5]\n\nLinked lists[edit]\nMain article: Linked list\nBelow is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of struct node is a pointer to another struct node, effectively creating a list type.\n\nstruct node\n{\n  int data;           // some integer data\n  struct node *next;  // pointer to another struct node\n};\nBecause the struct node data structure is defined recursively, procedures that operate on it can be implemented naturally as recursive procedures. The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the list_print procedure.\n\nvoid list_print(struct node *list)\n{\n    if (list != NULL)               // base case\n    {\n       printf (\"%d \", list->data);  // print integer data followed by a space\n       list_print (list->next);     // recursive call on the next node\n    }\n}\nBinary trees[edit]\nMain article: Binary tree\nBelow is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree).\n\nstruct node\n{\n  int data;            // some integer data\n  struct node *left;   // pointer to the left subtree\n  struct node *right;  // point to the right subtree\n};\nOperations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls:\n\n// Test if tree_node contains i; return 1 if so, 0 if not.\nint tree_contains(struct node *tree_node, int i) {\n    if (tree_node == NULL)\n        return 0;  // base case\n    else if (tree_node->data == i)\n        return 1;\n    else\n        return tree_contains(tree_node->left, i) || tree_contains(tree_node->right, i);\n}\nAt most two recursive calls will be made for any given call to tree_contains as defined above.\n\n// Inorder traversal:\nvoid tree_print(struct node *tree_node) {\n        if (tree_node != NULL) {                  // base case\n                tree_print(tree_node->left);      // go left\n                printf(\"%d \", tree_node->data);   // print the integer followed by a space\n                tree_print(tree_node->right);     // go right\n        }\n}\nThe above example illustrates an in-order traversal of the binary tree. A Binary search tree is a special case of the binary tree where the data elements of each node are in order.\n\nFilesystem traversal[edit]\nSince the number of files in a filesystem may vary, recursion is the only practical way to traverse and thus enumerate its contents. Traversing a filesystem is very similar to that of tree traversal, therefore the concepts behind tree traversal are applicable to traversing a filesystem. More specifically, the code below would be an example of a preorder traversal of a filesystem.\n\nimport java.io.*;\n\npublic class FileSystem {\n\n\tpublic static void main (String [] args) {\n\t\ttraverse ();\n\t}\n\n\t/**\n\t * Obtains the filesystem roots\n\t * Proceeds with the recursive filesystem traversal\n\t */\n\tprivate static void traverse () {\n\t\tFile [] fs = File.listRoots ();\n\t\tfor (int i = 0; i < fs.length; i++) {\n\t\t\tif (fs[i].isDirectory () && fs[i].canRead ()) {\n\t\t\t\trtraverse (fs[i]);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Recursively traverse a given directory\n\t *\n\t * @param fd indicates the starting point of traversal\n\t */\n\tprivate static void rtraverse (File fd) {\n\t\tFile [] fss = fd.listFiles ();\n\n\t\tfor (int i = 0; i < fss.length; i++) {\n\t\t\tSystem.out.println (fss[i]);\n\t\t\tif (fss[i].isDirectory () && fss[i].canRead ()) {\n\t\t\t\trtraverse (fss[i]);\n\t\t\t}\n\t\t}\n\t}\n\n}\nThis code blends the lines, at least somewhat, between recursion and iteration. It is, essentially, a recursive implementation, which is the best way to traverse a filesystem. It is also an example of direct and indirect recursion. The method \"rtraverse\" is purely a direct example; the method \"traverse\" is the indirect, which calls \"rtraverse.\" This example needs no \"base case\" scenario due to the fact that there will always be some fixed number of files or directories in a given filesystem.\n\nImplementation issues[edit]\nIn actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include:\n\nWrapper function (at top)\nShort-circuiting the base case, aka \"Arm's-length recursion\" (at bottom)\nHybrid algorithm (at bottom) – switching to a different algorithm once data is small enough\nOn the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.\n\nWrapper function[edit]\nA wrapper function is a function that is directly called but does not recurse itself, instead calling a separate auxiliary function which actually does the recursion.\n\nWrapper functions can be used to validate parameters (so the recursive function can skip these), perform initialization (allocate memory, initialize variables), particularly for auxiliary variables such as \"level of recursion\" or partial computations for memoization, and handle exceptions and errors. In languages that support nested functions, the auxiliary function can be nested inside the wrapper function and use a shared scope. In the absence of nested functions, auxiliary functions are instead a separate function, if possible private (as they are not called directly), and information is shared with the wrapper function by using pass-by-reference.\n\nShort-circuiting the base case[edit]\nShort-circuiting the base case, also known as arm's-length recursion, consists of checking the base case before making a recursive call – i.e., checking if the next call will be the base case, instead of calling and then checking for the base case. Short-circuiting is particularly done for efficiency reasons, to avoid the overhead of a function call that immediately returns. Note that since the base case has already been checked for (immediately before the recursive step), it does not need to be checked for separately, but one does need to use a wrapper function for the case when the overall recursion starts with the base case itself. For example, in the factorial function, properly the base case is 0! = 1, while immediately returning 1 for 1! is a short-circuit, and may miss 0; this can be mitigated by a wrapper function.\n\nShort-circuiting is primarily a concern when many base cases are encountered, such as Null pointers in a tree, which can be linear in the number of function calls, hence significant savings for O(n) algorithms; this is illustrated below for a depth-first search. Short-circuiting on a tree corresponds to considering a leaf (non-empty node with no children) as the base case, rather than considering an empty node as the base case. If there is only a single base case, such as in computing the factorial, short-circuiting provides only O(1) savings.\n\nConceptually, short-circuiting can be considered to either have the same base case and recursive step, only checking the base case before the recursion, or it can be considered to have a different base case (one step removed from standard base case) and a more complex recursive step, namely \"check valid then recurse\", as in considering leaf nodes rather than Null nodes as base cases in a tree. Because short-circuiting has a more complicated flow, compared with the clear separation of base case and recursive step in standard recursion, it is often considered poor style, particularly in academia.\n\nDepth-first search[edit]\nA basic example of short-circuiting is given in depth-first search (DFS) of a binary tree; see binary trees section for standard recursive discussion.\n\nThe standard recursive algorithm for a DFS is:\n\nbase case: If current node is Null, return false\nrecursive step: otherwise, check value of current node, return true if match, otherwise recurse on children\nIn short-circuiting, this is instead:\n\ncheck value of current node, return true if match,\notherwise, on children, if not Null, then recurse.\nIn terms of the standard steps, this moves the base case check before the recursive step. Alternatively, these can be considered a different form of base case and recursive step, respectively. Note that this requires a wrapper function to handle the case when the tree itself is empty (root node is Null).\n\nIn the case of a perfect binary tree of height h, there are 2h+1−1 nodes and 2h+1 Null pointers as children (2 for each of the 2h leaves), so short-circuiting cuts the number of function calls in half in the worst case.\n\nIn C, the standard recursive algorithm may be implemented as:\n\nbool tree_contains(struct node *tree_node, int i) {\n    if (tree_node == NULL)\n        return false;  // base case\n    else if (tree_node->data == i)\n        return true;\n    else\n        return tree_contains(tree_node->left, i) ||\n               tree_contains(tree_node->right, i);\n}\nThe short-circuited algorithm may be implemented as:\n\n// Wrapper function to handle empty tree\nbool tree_contains(struct node *tree_node, int i) {\n    if (tree_node == NULL)\n        return false;  // empty tree\n    else\n        return tree_contains_do(tree_node, i);  // call auxiliary function\n}\n\n// Assumes tree_node != NULL\nbool tree_contains_do(struct node *tree_node, int i) {\n    if (tree_node->data == i)\n        return true;  // found\n    else  // recurse\n        return (tree_node->left  && tree_contains_do(tree_node->left,  i)) ||\n               (tree_node->right && tree_contains_do(tree_node->right, i));\n}\nNote the use of short-circuit evaluation of the Boolean && (AND) operators, so that the recursive call is only made if the node is valid (non-Null). Note that while the first term in the AND is a pointer to a node, the second term is a bool, so the overall expression evaluates to a bool. This is a common idiom in recursive short-circuiting. This is in addition to the short-circuit evaluation of the Boolean || (OR) operator, to only check the right child if the left child fails. In fact, the entire control flow of these functions can be replaced with a single Boolean expression in a return statement, but legibility suffers at no benefit to efficiency.\n\nHybrid algorithm[edit]\nRecursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. An important example is merge sort, which is often implemented by switching to the non-recursive insertion sort when the data is sufficiently small, as in the tiled merge sort. Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort.\n\nRecursion versus iteration[edit]\nRecursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit stack, while iteration can be replaced with tail recursion. Which approach is preferable depends on the problem under consideration and the language used. In imperative programming, iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion. By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead, and sometimes explicit iteration is not available.\n\nCompare the templates to compute xn defined by xn = f(n, xn-1) from xbase:\n\nfunction recursive(n)\n    if n==base\n        return xbase\n    else\n        return f(n, recursive(n-1)) \nfunction iterative(n)\n    x = xbase\n    for i = n downto base\n        x = f(i, x)\n    return x\nFor imperative language the overhead is to define the function, for functional language the overhead is to define the accumulator variable x.\n\nFor example, the factorial function may be implemented iteratively in C by assigning to an loop index variable and accumulator variable, rather than passing arguments and returning values by recursion:\n\nunsigned int factorial(unsigned int n) {\n  unsigned int product = 1; // empty product is 1\n  while (n) {\n    product *= n;\n    --n;\n  }\n  return product;\n}\nExpressive power[edit]\nMost programming languages in use today allow the direct specification of recursive functions and procedures. When such a function is called, the program's runtime environment keeps track of the various instances of the function (often using a call stack, although other methods may be used). Every recursive function can be transformed into an iterative function by replacing recursive calls with iterative control constructs and simulating the call stack with a stack explicitly managed by the program.[10][11]\n\nConversely, all iterative functions and procedures that can be evaluated by a computer (see Turing completeness) can be expressed in terms of recursive functions; iterative control constructs such as while loops and do loops are routinely rewritten in recursive form in functional languages.[12][13] However, in practice this rewriting depends on tail call elimination, which is not a feature of all languages. C, Java, and Python are notable mainstream languages in which all function calls, including tail calls, may cause stack allocation that would not occur with the use of looping constructs; in these languages, a working iterative program rewritten in recursive form may overflow the call stack, although tail call elimination may be a feature that is not covered by a language's specification, and different implementations of the same language may differ in tail call elimination capabilities.\n\nPerformance issues[edit]\nIn languages (such as C and Java) that favor iterative looping constructs, there is usually significant time and space cost associated with recursive programs, due to the overhead required to manage the stack and the relative slowness of function calls; in functional languages, a function call (particularly a tail call) is typically a very fast operation, and the difference is usually less noticeable.\n\nAs a concrete example, the difference in performance between recursive and iterative implementations of the \"factorial\" example above depends highly on the compiler used. In languages where looping constructs are preferred, the iterative version may be as much as several orders of magnitude faster than the recursive one. In functional languages, the overall time difference of the two implementations may be negligible; in fact, the cost of multiplying the larger numbers first rather than the smaller numbers (which the iterative version given here happens to do) may overwhelm any time saved by choosing iteration.\n\nStack space[edit]\nIn some programming languages, the stack space available to a thread is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. Consequently, these languages sometimes place a limit on the depth of recursion to avoid stack overflows; Python is one such language.[14] Note the caveat below regarding the special case of tail recursion.\n\nMultiply recursive problems[edit]\nMultiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search; contrast with list traversal and linear search in a list, which is singly recursive and thus naturally iterative. Other examples include divide-and-conquer algorithms such as Quicksort, and functions such as the Ackermann function. All of these algorithms can be implemented iteratively with the help of an explicit stack, but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.\n\nTail-recursive functions[edit]\nTail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls, a tail-recursive function such as gcd will execute using constant space. Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops.\n\nTail recursion:\tAugmenting recursion:\n//INPUT: Integers x, y such that x >= y and y > 0\nint gcd(int x, int y)\n{\n  if (y == 0)\n     return x;\n  else\n     return gcd(y, x % y);\n}\n//INPUT: n is an Integer such that n >= 0\nint fact(int n)\n{\n   if (n == 0)\n      return 1;\n   else\n      return n * fact(n - 1);\n}\nThe significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.\n\nOrder of execution[edit]\nIn the simple case of a function calling itself only once, instructions placed before the recursive call are executed once per recursion before any of the instructions placed after the recursive call. The latter are executed repeatedly after the maximum recursion has been reached. Consider this example:\n\nFunction 1[edit]\nvoid recursiveFunction(int num) {\n   printf(\"%d\\n\", num);\n   if (num < 4)\n      recursiveFunction(num + 1);\n}\nRecursive1.svg\n\nFunction 2 with swapped lines[edit]\nvoid recursiveFunction(int num) {\n   if (num < 4)\n      recursiveFunction(num + 1);\n   printf(\"%d\\n\", num);\n}\nRecursive2.svg\n\nTime-efficiency of recursive algorithms[edit]\nThe time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation. They can (usually) then be simplified into a single Big-Oh term.\n\nShortcut rule (master theorem)[edit]\nMain article: Master theorem\nIf the time-complexity of the function is in the form\n\n{\\displaystyle T(n)=a\\cdot T(n/b)+f(n)} {\\displaystyle T(n)=a\\cdot T(n/b)+f(n)}\n\nThen the Big-Oh of the time-complexity is thus:\n\nIf {\\displaystyle f(n)=O(n^{\\log _{b}a-\\epsilon })} {\\displaystyle f(n)=O(n^{\\log _{b}a-\\epsilon })} for some constant {\\displaystyle \\epsilon >0} \\epsilon >0, then {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a})} {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a})}\nIf {\\displaystyle f(n)=\\Theta (n^{\\log _{b}a})} {\\displaystyle f(n)=\\Theta (n^{\\log _{b}a})}, then {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a}\\log n)} {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a}\\log n)}\nIf {\\displaystyle f(n)=\\Omega (n^{\\log _{b}a+\\epsilon })} {\\displaystyle f(n)=\\Omega (n^{\\log _{b}a+\\epsilon })} for some constant {\\displaystyle \\epsilon >0} \\epsilon >0, and if {\\displaystyle a\\cdot f(n/b)\\leq c\\cdot f(n)} {\\displaystyle a\\cdot f(n/b)\\leq c\\cdot f(n)} for some constant c < 1 and all sufficiently large n, then {\\displaystyle T(n)=\\Theta (f(n))} {\\displaystyle T(n)=\\Theta (f(n))}\nwhere a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f (n) represents the work the function does independent of any recursion (e.g. partitioning, recombining) at each level of recursion.\n\nSee also[edit]\nFunctional programming\nHierarchical and recursive queries in SQL\nKleene–Rosser paradox\nOpen recursion\nRecursion\nSierpiński curve\nRecursive functions[edit]\nMcCarthy 91 function\nμ-recursive functions\nPrimitive recursive functions\nTak (function)\nBooks[edit]\nStructure and Interpretation of Computer Programs\nWalls and Mirrors",
          "subparadigms": []
        },
        {
          "pdid": 69,
          "name": "Value-level",
          "details": "Value-level programming refers to one of the two contrasting programming paradigms identified by John Backus in his work on programs as mathematical objects, the other being function-level programming. Backus originally used the term object-level programming but that term is now prone to confusion with object-oriented programming.\n\nValue-level programs are those that describe how to combine various values (i.e., numbers, symbols, strings, etc.) to form other values until the final result values are obtained. New values are constructed from existing ones by the application of various value-to-value functions, such as addition, concatenation, matrix inversion, and so on.\n\nConventional, von Neumann programs are value-level: expressions on the right side of assignment statements are exclusively concerned with building a value that is then to be stored.\n\nConnection with Data Types[edit]\nThe value-level approach to programming invites the study of the space of values under the value-forming operations, and of the algebraic properties of those operations. This is what is called the study of data types, and it has advanced from focusing on the values themselves and their structure, to a primary concern with the value-forming operations and their structure, as given by certain axioms and algebraic laws, that is, to the algebraic study of data types.\n\nConnection with Lambda Calculus languages[edit]\nLambda calculus-based languages (such as Lisp, ISWIM, and Scheme) are in actual practice value-level languages, although they are not thus restricted by design.\n\nTo see why typical lambda style programs are primarily value-level, consider the usual definition of a value-to-value function, say\n\nf = λx.E\nhere, x must be a value variable (since the argument of f is a value by definition) and E must denote a value too (since f's result is a value by definition). Typically, E is an expression involving the application of value-forming functions to value variables and constants; nevertheless, a few value-forming functions having both function and value arguments do exist and are used for limited purposes.\n\nIf the term values is defined to include the value variables themselves, then the value-level view of programming is one of building values by the application of existing programs (value-forming operations/functions) to other values. Lambda-style programming builds a new program from the result-value by lambda-abstracting the value variables.",
          "subparadigms": []
        },
        {
          "pdid": 70,
          "name": "Probabilistic",
          "details": "A probabilistic programming language (PPL) is a programming language designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks, but are more expressive and flexible.[1] Probabilistic programming represents an attempt to \"[unify] general purpose programming with probabilistic modeling.\"[2]\n\nProbabilistic reasoning is a foundational technology of machine learning. It is used by companies such as Google, Amazon.com and Microsoft. Probabilistic reasoning has been used for predicting stock prices, recommending movies, diagnosing computers, detecting cyber intrusions and image detection.[3]\n\nPPLs often extend from a basic language. The choice of underlying basic language depends on the similarity of the model to the basic language's ontology, as well as commercial considerations and personal preference. For instance, Dimple[4] and Chimple[5] are based on Java, Infer.NET is based on .NET framework,[6] while PRISM extends from Prolog.[7] However, some PPLs such as WinBUGS and Stan offer a self-contained language, with no obvious origin in another language.[8][9]\n\nSeveral PPLs are in active development, including some in beta test.\n\nContents  [hide] \n1\tRelational\n2\tProbabilistic programming\n3\tApplications\n4\tList of probabilistic programming languages\n5\tSee also\n6\tNotes\n7\tExternal links\nRelational[edit]\nA probabilistic relational programming language (PRPL) is a PPL specially designed to describe and infer with probabilistic relational models (PRMs).\n\nA PRM is usually developed with a set of algorithms for reducing, inference about and discovery of concerned distributions, which are embedded into the corresponding PRPL.\n\nProbabilistic programming[edit]\nProbabilistic programming creates systems that help make decisions in the face of uncertainty. Probabilistic reasoning combines knowledge of a situation with the laws of probability. Until recently, probabilistic reasoning systems have been limited in scope, and have not successfully addressed real world situations. Probabilistic programming is a new approach that makes probabilistic reasoning systems easier to build and more widely applicable.[10] Reasoning about variables as probability distributions causes difficulties for novice programmers, but these difficulties can be addressed through use of Bayesian network visualisations and graphs of variable distributions embedded within the source code editor.[11]\n\nApplications[edit]\nIn 2015, a 50-line PPL computer vision program was used to generate 3D models of human faces based on 2D images of those faces. The program used inverse graphics as the basis of its inferencing,[3] done by a PPL language they call Picture, (and the Julia host language) that made possible \"in 50 lines of code what used to take thousands [whereas their experiments used their] probabilistic programming language they call Picture, which is an extension of Julia language, another language developed at MIT\".[12][13] A paper on the Picture language, shown at the 2015 Computer Vision and Pattern Recognition conference, was awarded \"Best Paper Honorable Mention\".[14]\n\nList of probabilistic programming languages[edit]\nThis list is incomplete; you can help by expanding it.\nName\tExtends from\tHost language\nVenture[15]\tScheme\tC++\nProbabilistic-C[16]\tC\tC\nAnglican[17]\tScheme\tClojure\nIBAL[18]\tOCaml\t\nPRISM[7]\tB-Prolog\t\nInfer.NET[6]\t.NET Framework\t.NET Framework\ndimple[4]\tMATLAB, Java\t\nchimple[5]\tMATLAB, Java\t\nBLOG[19]\tJava\t\nPSQL[20]\tSQL\t\nBUGS[8]\t\t\nFACTORIE[21]\tScala\t\nPMTK[22]\tMATLAB\tMATLAB\nAlchemy[23]\tC++\t\nDyna[24]\tProlog\t\nFigaro[25]\tScala\t\nChurch[26]\tScheme\tVarious: JavaScript, Scheme\nProbLog[27]\tProlog\tPython, Jython\nProBT[28]\tC++, Python\t\nStan (software)[9]\t\tC++\nHakaru[29]\tHaskell\tHaskell\nBAli-Phy (software)[30]\tHaskell\tC++\nProbCog[31]\t\tJava, Python\nGamble[32]\t\tRacket\nTuffy[33]\t\tJava\nPyMC[34]\tPython\tPython\nLea[35]\tPython\tPython\nWebPPL[36]\tJavaScript\tJavaScript\nPicture[3]\tJulia\tJulia\nTuring.jl[37]\tJulia\tJulia\nSee also[edit]\nStatistical relational learning\nInductive programming\nBayesian programming",
          "subparadigms": []
        },
        {
          "pdid": 71,
          "name": "Concept",
          "details": "Concept programming is a programming paradigm focusing on how concepts, that live in the programmer's mind, translate into representations that are found in the code space. This approach was introduced in 2001 by Christophe de Dinechin with the XL Programming Language.\n\nContents  [hide] \n1\tPseudo-metrics\n2\tRule of equivalence, equivalence breakdown\n3\tMethodology\n4\tLanguages\n5\tSimilar works\n6\tSee also\n7\tExternal links\nPseudo-metrics[edit]\nConcept programming uses pseudo-metrics to evaluate the quality of code. They are called pseudo-metrics because they relate the concept space and the code space, with a clear understanding that the concept space cannot be formalized strictly enough for a real metric to be defined. Concept programming pseudo-metrics include:\n\nSyntactic noise measures discrepancies between the concept and the syntax used to represent it. For instance, the semi-colon at the end of statements in C can be considered as syntactic noise, because it has no equivalent in the concept space.\nSemantic noise measures discrepancies between the expected meaning or behavior of the concept and its actual meaning or behavior in the code. For instance, the fact that integer data types overflow (when mathematical integers do not) is a form of semantic noise.\nBandwidth measures how much of the concept space a given code construct can represent. For instance, the overloaded addition operator in C has higher bandwidth than the Add instruction in assembly language, because the C operator can represent addition on floating-point numbers and not just integer numbers.\nSignal/noise ratio measures what fraction of the code space is used for representing actual concepts, as opposed to implementation information.\nRule of equivalence, equivalence breakdown[edit]\nThe rule of equivalence is verified when the code behavior matches the original concept. This equivalence may break down in many cases. Integer overflow breaks the equivalence between the mathematical integer concept and the computerized approximation of the concept.\n\nMany ways to break the equivalence have been given specific names, because they are very common:\n\nA domain error is a condition where code executes outside of the domain of equivalence, which is the domain where the concept and the implementation match. An integer overflow is an example of domain error.\nA concept cast (also concept recast or concept recasting) is a rewrite of a concept as a different concept because the original concept cannot be represented by the tools. In C, using pointers for output arguments because C doesn't support output arguments explicitly is an example of concept cast.\nA priority inversion is a form of syntactic or semantic noise introduced by some language-enforced general rule. It is called a priority inversion because the language takes precedence over the concept. In Smalltalk, everything is an object, and that rule leads to the undesirable consequence that an expression like 2+3*5 doesn't obey the usual order of operations (Smalltalk interprets this as sending the message * to the number resulting from 2+3, which yields result 25 instead of 17).\nMethodology[edit]\nTo write code, concept programming recommends the following steps:\n\nIdentify and define the relevant concepts in the concept space.\nIdentify traditional notations for the concepts, or invent usable notations.\nIdentify a combination of programming constructs that allows the concepts to be represented comfortably in code - That includes finding a code notation that matches the notation identified in the previous step as closely as possible.\nWrite code that preserves, as much as possible, the expected behavior and semantics of the relevant aspects of the original concept.\nMany programming tools often lack in notational abilities, thus concept programming sometimes requires the use of preprocessors, domain-specific languages, or metaprogramming techniques.\n\nLanguages[edit]\nXL is the only programming language known to date to be explicitly created for concept programming, but concept programming can be done in nearly any language, with varying degrees of success. Lisp and Forth (and their derivatives) are examples of pre-existing languages which lend themselves well to concept programming.[citation needed]\n\nSimilar works[edit]\nThere are projects that exploit similar ideas to create code with higher level of abstraction. Among them are:\n\nIntentional Programming\nLanguage-oriented programming\nLiterate programming\nModel-driven architecture\nSee also[edit]\nProgramming paradigms\nAutomatic programming\nArtefaktur (AAL)\nAbstract syntax tree (AST)\nLanguage syntax tree (LST)\nSemantic resolution tree (RST)\nInterpretation syntax tree (IST)\nCode generation syntax tree (CST)\nDomain-specific programming language\nExternal links[edit]\nThe XL Programming Language on SourceForge\nA presentation of Concept Programming\nAn interview about Concept Programming on The Register",
          "subparadigms": []
        }
      ],
      "programminglanguages": [],
      "havings": []
    }
  ]
}